 If I understand your situation correctly, you've misapplied the idea of classification. A classifier is a procedure for taking an observation which belongs to one of several classes, each of which is assumed to exist, and predicting its class. For example, a classifier might take the length of a person's name (an integer) and predict the person's gender (male or female). This isn't your situation because you don't have observations each of which could have been produced by one of two processes, with both processes definitely existing; rather, either all your observations are produced by one process, or all of them are produced by the other process. The statistical tool you want for this is model selection, which is indeed a core topic in statistics you can read about in most statistics books. Consider then that at the end of the process I have a classifier that has an area under the ROC curve of, say, 0.7. If I understand it correctly, that means that from a set of events, 70% will be correctly classified (is this right?) No, those are different animals. Proportion classified correctly is applied to a single classifier and is computed by simply counting how many of the observations in the test data are correctly predicted. ROC curves can be created for a model which can produce a classifier for any setting of a classification threshold (such as a logistic-regression model), and the area under ROC curves is difficult to interpret, which is one reason that computing and reporting the area under an ROC curve is of dubious value. 
 First puzzle : I am taught that the lag order of VECM does not affect the cointegration rank because the lag order is for the differenced regressors. But, I see the contrary: I experimented with sample data of 4 variables, using lag orders between 1 and 12. At low lag orders the cointegration rank is low, but as I increase the lag length, cointegration rank goes up. Does anyone know of a paper or source discussing this issue? Any suggestions as to what to do in this situation? Second puzzle : If I go by what information criteria indicate as an optimal lag length, establish the coinegration rank and then set up a VECM model, the residuals of the model are heteroscedastic and serially correlated. The info criteria based lag order is 2. To circumvent the issue, I have two options: Go back, increase the lag order and set up a new VECM and do diagnostic tests Just keep the cointegration rank as it is and increase the lag order until the residuals are homoscedastic and free of autocorrelation What would be the optimal course of action? 
 I have a question regarding the code of function , from library , which implements linear classification using the stochastic gradient descent (SGD) algorithm. It concerns its application for the Support Vector Machine (SVM) model - i.e. SVM is parametrized by a hinge loss function and a L2 penalty function. The SVM decision function is: $$ d(x) = sign\left(f(x)\right) = sign\left(w^Tx\right) $$ The optimal solution $w^*$ is found by solving the following regularized problem ($[x]_+ \equiv \max[0,x]$): $$ \min_w \left(\sum_{i=1}^{n}{\left[1-y_i\,w^Tx_i\right]_+}+\lambda\|w\|^2\right) $$ From [Bottou, 2012] , page 3, I recall the SGD algorithm's main update equation for the SVM: $$ w \leftarrow w \ - \ \eta_t \left(\lambda w - y_tx_t\mathbf{1}_{\{y_tw^Tx_t&lt;1\}} \right) $$ However, I am having trouble finding this same equation in . The code of is based on the function written in Cython: for an optimal learning rate (), a hinge loss $-$ see definition of in the code from line 108 on; for a hinge loss function (SVM) is in line 160 $-$ and a L2 penalty (), and ignoring class and sample weights, I have the impression the value for the of $w$ is computed in line 639 and is equal to : $$ w \leftarrow w \ - \ \eta_ty_t\mathbf{1}_{\{y_tw^Tx_t&lt;1\}} $$ There is a last computation in line 656: But I do not understand what this does. Is there anything I have missed on _plain_sgd code ? Has anybody a clue on where these differences come from? Especially given that claims that they follow Bottou's approach: The implementation of SGD is influenced by the Stochastic Gradient SVM of Léon Bottou. 
 I want to test the correlation between variables considering several study areas. I aim to study habitat selection using telemetry data (locations) in five study areas, and I have variables regarding landcover (10; in % for each study area), topography (1) and distance to water/human settlements (2; mean for each study area). In total, I have 13 variables Before starting with habitat selection analyses, I want to check if the variables I am using are correlated, in order to perform analyses using only independent variables. I am not sure how to analyse this with several study areas. What is the best way to perform the analysis? 
 To get the 'intuition' you may reason like this: I am taking $\chi^2$ as an example, and referring to the formulas (5.1) and (5.2) in this link , which is the book by Hosmer-Lemeshow. The $\chi^2$ variable is defined as a sum of squared standard normal variables, so if you look at the formulas (5.1) and (5.2) it is assumed that $r(m_j, \hat{\pi}_j)=\frac{y_j - m_j \hat{\pi}_j }{\sqrt{m_j \hat{\pi}_j(1-\hat{\pi}_j)}}$ are standard normal variables. Note that $y_j$ is the number of successes among the cases with the $j$-th covariate pattern, and $m_j$ is the number of cases with the $j$-th covariate pattern. Now if you look at that $j$-th covariate pattern as a Binomial random variable with success probability $\pi_j$ and size $m_j$, then the mean of this random variable is (see properties of the Binomial variable) $\mu_j=m_j\pi_j$ and its standard deviation is $\sigma_j=\sqrt{m_j \pi_j (1-\pi_j)}$. If the size ($m_j$) of this Binomial random variable is ''large'' then the Binomial variable is approximately normal with the same mean and variance. So if all the $m_j$ are ''large'' then $\frac{y_j - m_j \pi_j}{\sqrt{m_j \pi_j (1-\pi_j)}}$ is approximately standard normal, and then, by definition of $\chi^2$, the sum $X^2=\sum_j \left( \frac{y_j - m_j \pi_j}{\sqrt{m_j \pi_j (1-\pi_j)}} \right) ^2$ has a $\chi^2$ distribution. If you compare this to the formulas (5.1) and (5.2) in the link supra, you will see that the $\chi^2$ distribution in Hosmer-Lemeshow's book uses the above property, which only holds if $m_j$ are all large ! If $J \approx n$ then all $m_j$ are more or less equal to one, which is not ''large'', so if $J \approx n$ the variable $X^2$ defined in (5.1) and (5.2) can not be shown to follow a $\chi^2$ distribution. The p-values are derived under the property that $X^2$ is $\chi^2$ , so if you can not show this $\chi^2$ property, then the p-values derived from it are also ' doubtfull '. 
 If you're satisfied that a symmetric bivariate normal is an appropriate model (as are most marksmen), then there is a closed-form solution for this using the Rayleigh distribution . When the distribution is parameterized by &sigma; then the Rayleigh CDF gives us $Pr(r \leq \alpha) = 1 - e^{-\alpha^2 / 2 \sigma^2}$. This probability is more commonly known as a Circular Error Probable (CEP). You're looking for the 90% CEP, which is thus $\sigma \sqrt{-2 \ln(90\%)}\approx 2.15\sigma$. The estimator for the Rayleigh parameter &sigma; is not terribly complex if you can measure your sample impact coordinates and have basic spreadsheet functions. The unbiased estimator for n sample shots is computed as follows: Calculate the sample center $(\bar{x}, \bar{y})$. For each sample shot calculate the sample radius $r_i = \sqrt{(x_i - \bar{x})^2 + (y_i - \bar{y})^2}$. The unbiased Rayleigh estimator is $\widehat{\sigma_R^2} = \frac{n}{n-1} \frac{\sum r_i^2}{2n}$. Use the same Gaussian correction factor to remove the bias when taking the square root of $\widehat{\sigma_R^2}$ as when taking the square root of variance. I.e., ${}^1/c_{G}(n) = \sqrt{\frac{2}{n-1}}\,\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}$. Or, in code or spreadsheet formula, $c_{G}(n)=$ . Finally, the unbiased parameter estimate is $\hat{\sigma} = c_G(2n-1) \sqrt{\widehat{\sigma_R^2}}$. 
 I know that log-loss penalises models that are confident with the wrong predicted classes. Can this be translated to percentage accuracy? If not, then how do I report the error or compare it to other percentage error metrics? For example, on training a neural network with 128 output layers with sigmoid activation, I get a loss reduction from to over 20 epochs. How do I evaluate classifier accuracy based on this? It is a multi-label classification problem. 
 Are diagnostics developed for MCMC (e.g. Gelman-Rubin, Geweke) suitable for output from Evolutionary Monte Carlo (EMC) or Sequential Monte Carlo (SMC)? 
 I'm trying to plot a generalized pareto distribution with fixed initial values "scale" and "shape" to a random sequence of numbers. When I produce the plot with scale=1 and shape=1, I get the density on the y-axis with limits (0,1). If I produce the plot with different parameters (like the ones below), on the y-axis I get density values between (0,2), which obviously don't make sense. (the third line was my way to rename the x-axis, it shouldn't matter for my question.) Could someone replicate this and tell me whether they also get the weird limits? You know how I can fix this? Thanks! PS: Sorry for the inconvenience, I only thought that there was a problem with MY specifications since for some parameters I had the interval [0,1] and for others [0,2]. Erroneously enough, I was asking what was wrong with my parameters rather than with the density plot and values. But I get it, it's a question that has been asked before, so it's been flagged as duplicate. Moreover, this being my first experience with stackexchange, I can only learn from it. Good day to you all, lads! 
 Just to add a small explanation: as already pointed out in the comments to your question, the density itself can be above 1. The basic requirement is that it integrates to 1, i.e. that if the support of the density is small enough (or the corresponding part of the distribution where values above 1 occur is small enough), then this is not a problem. As an illustration, consider the following: Here, I have generated five different uniformly distributed random variables. What I have varied is the support. goes from 0 to 2. goes from 0 to 1, up to which goes from 0 to 0.05. Now, the quiz : think of rectangles where one side has length 2, 1, 0.5, 0.25 or 0.05, respectively. How long is the other side of the rectangle if the area of the rectangle must equal to 1? Well, the area of a rectangle is given by , so if , we can calculate the value of the other side. For instance, if one side is 0.05, the other side needs to be 1/0.05 = 20. And this is why sometimes densities go beyond 1, as you can see in this image: 
 The tricks are: 1.You have to use (1-p)^(1/p) is converged to e when p is small. 2.Gamma function(k) = (k+1)! 3.(r+x choose r ) converages to (r+x)(r+x-1)(r+x-2)...r / r! You will easily derive the formula with all the three above plugged in 
 I'm trying to make a predictive model using logistic regression but before that I'm wondering what kind of explore I can do on my data to get a better understanding or even help me with the model selection and analysis that follows. I'm thinking of things like tables, plots. What are some good general practice? 
 I'm dealing with similar problems, and haven't found a straightforward function. So I wrote a function myself. Although it's not very concise, it does the work. Hope it also helps you. With this, if you have, let's say, 4 gene lists. and the background number is 14 genes (number of all balls in the urn) in the world, the result would be: where the upper triangle on the right is the lengths of overlap of each pair, and the bottom triangle on the left is the significance of the overlap by hypergeometric test. Here in this toy example, the overlap between and is significant in a world a 14 genes if you choose 0.05 as your p-value cutoff. Any other pair is not significantly overlapping. 
 I have difficulty interpreting some results. I am doing an hierarhical related regression with . If I enter the code I receive output with oddsratio's, confidence ratio's and a 2x maximized log likelihood. However, I do not fully understand how to interpreted the 2x maximized log likelihood. As far as I know log likelihood is used as a convenient way to calculate a likelihood and it calculates the value of the parameters based on the outcomes. But I do not understand if a higher or lower value is better. I looked at several online sources e.g. What is log-likelihood? , but I am still stuck. Below the outcome I receive: So, how should I interpreted a value of 237.4882 compared to an outcome of 206 or 1083? Help is much appreciated! 
 A higher value of likelihood or log-likelihood (since log is an increasing function) is better, since it says that the parameters are more likely given the observation/data. I suggest the wiki article on maximum likelihood estimation for further reference. ( https://en.wikipedia.org/wiki/Maximum_likelihood ) 
 I am trying to fit accuracy data (taking values 0 or 1) using glmer and I am puzzled to observe that the residuals of the model don't have a null mean. Wasn't this the whole point of the optimization..? Does it mean that the model fails to converge although there was no warning message? I observe the same behavior with simple glm models. My data has very high mean accuracy (.88), could it be part of the problem? An example model: and here the binned plot for fitted vs residuals Thank you very much for your answers. (A reproducible example can be found here ) 
 I think what we have here is just the effect of Monte Carlo sampling noise. If you look at the % error, you can see that each Bayes factor is estimated with about 1% error. These Bayes factors are not exact; they will change due to the sampling from computation to computation. Try re-running the anovaBF function; you'll see the value change. The BF10 column in the JASP output gives you the same Bayes factor as the anovaBF output, just with different sampling noise. I suspect that the 1% sampling error accounts for this difference. 
 I'm a beginner in data science/machine learning and am attempting to work through some problems on my own I am running a K-means clustering on a dataset consisting of "mission statements". These can range from 1-3 paragraphs long. I would like to run a K-means algo that attempts to group mission statements rather than the specific words The current algo I am using can run k-means on words given N clusters. How would I go about retooling my algorithm to now sort larger paragraphs? I assume that I'd try and create a TD-IDF matrix for each document, and then run a k-means algo on the set of documents? Or is there another way I am not considering? 
 Suppose I have two classification algorithms A1 and A2, and a test set of size $n$. I evaluate A1 and A2 on this test set, and get corresponding Matthew's Correlation Coefficient scores $\textit{MCC}_1$ and $\textit{MCC}_2$. Suppose $\textit{MCC}_1 &gt; \textit{MCC}_2$. So I would like to conclude that A1 is better than A2. I'm not sure what the right definition of "better" is here, but I had this in mind: "Given an infinite sized sample from the same distribution as the test set, the expected MCC score of A1 will be higher than the expected MCC score of A2". How could I make this conclusion in a statistically significant way? The t-test is often used to compare classifiers, but I think it's not appropriate here since the null hypothesis is about the means (i.e. accuracies). This question's answer mentions ROC curves and the McNemar's test. ROC curves are not possible since we can assume that the algorithms are black boxes. As for the McNemar's test, I think it's again the problem of the null hypothesis being different from what I'm interested in. I don't know if I'm even "chasing" the right thing here. If not, what should I be looking at? 
 These are valid approaches. However it will work better if you normalize the word counts for each document, and the distance metric in k-means is sine distance $d(x_1,x_2)=1-\frac{\langle x_1,x_2 \rangle}{\|x_1\|\|x_2\|}$ Another approach which might work better for larger documents is to first convert each document into a feature vector using topic features, via topic modeling (e.g. by running LDA topic modeling using $N$ topics) Then you can run regular k-mean clustering using this length $N$ feature vector for each document. This approach will also be faster (using a fast algorithm for topic modeling ) since typically the number of topics $N$ will be much smaller than the total number of words in the dictionary. 
 In relation with my other question here where I observe a strange behavior of the residuals after fitting binary data using glm/glmer, I now wonder: Are there boundaries on the proportion of 0 (or 1) necessary in order to be able to regress binary data? If there are too many 1 compared to the 0 or conversely, does it prevent attempts of modeling? Cheers 
 Why is the number 1 applied to the matrix X. Is this so that X has 2 columns so that it can be multiplied by theta - y? Including an intercept or "bias term" is routine in linear regression to allow flexibility in the fitted line. Otherwise the line would always go through the origin. Hence, it is the presence of this column of $1$'s that yields the estimated parameter $\hat \beta_0$, or in your terminology, $\theta_0$. The column of $1$'s is not a trick to accommodate the $\theta_0$ in the matrix multiplication of the residuals or error term in your OP. What is the formula delta actually calculating and why is the Transpose of X being used in your code stands for the partial derivatives of the cost function with respect to the $\theta_i$ parameters that we will use in gradient descent . From the formula for the cost function in the OP, $\nabla J(\theta_0,\theta_1)=\nabla \frac{1}{2m}\displaystyle\sum_1^m(h_\theta(x^{(i)})−y^{(i)})^2$, the partial derivative with respect to $\theta_0$ is: $\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\displaystyle\sum_{i=1}^m\big(h_\theta(x^{(i)})−y^{(i)}\big)\color{red}{1}$, where the superscript $(i)$ denotes individual measurements or training "examples." And with respect to $\theta_1$it is $\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\displaystyle\sum_{i=1}^m\big(h_\theta(x^{(i)})−y^{(i)}\big)\,\color{red}{x_1^{(i)}}$. In your example linked to R-Bloggers there is only one feature, $x_1$. Now, in linear algebraic notation this can be expressed as: $X^T(X\theta-y)$, noticing that $X\theta$ is a matrix multiplication $[\text{m}\times\text{n}][\text{n}\times1]$ of $\text{m}$ examples and $\text{n}$ features, corresponding to calculating the predicted values or "hypothesis" function. $X\theta-y$ are the errors, corresponding to your line of code . Finally, $X^T$ would express the red part of the expressions of the partial derivatives above. In this regard, notice that since the first column of $X$ is the column of $1$'s, now it is the first row, ready to yield the first derivative when $X\theta-y$ is left multiplied by $X^T$. Notice that $X^T$ is $[\text{n}\times\text{m}]$ and $X\theta-y$ is $[\text{m}\times1]$, yielding a partial derivative for each feature $n$, or column in the initial $X$ matrix, including the intercept or bias feature. This is encapsulated in . 
 In several well known cases, yes, variable selection is not necessary. Deep learning has become a bit overhyped for precisely this reason. For example, when a convoluted neural network ( ) tries to predict if a centered image contains a human face, the corners of the image tend to have minimal predictive value. Traditional modeling and variable selection would have the modeler remove the corner pixels as predictors; however, the convoluted neural network is smart enough to essentially discard these predictors automatically. This is true for most deep learning models that try to predict the presence of some object in an image (e.g., self drivings cars "predicting" lane markings, obstacles or other cars in frames of onboard streaming video). Deep learning is probably overkill for a lot of traditional problems such as where datasets are small or where domain knowledge is abundant, so traditional variable selection will probably remain relevant for a long time, at least in some areas. Nonetheless, deep learning is great when you want to throw together a "pretty good" solution with minimal human intervention. It might take me many hours to handcraft and select predictors to recognize handwritten digits in images, but with a convoluted neural network and zero variable selection, I can have a state-of-the-art model in just under 20 minutes using Google's TensorFlow ( https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html ). 
 I was trying to solve ridge regression problem using gradient descent. Now without normalization I set some appropriate step size and ran the code. In order to make sure my coding was error-free, I coded the same objective in CVX too. Now CVX took only few iterations to converge to a certain optimal value but I ran my code for the best step size I could find by 10k iterations and I was close to the optimal value of CVX but still not exact. After normalizing the data-set and feeding it to my code and CVX, I was surprised to see that now convergence only took 100 iterations and the optimal value to which gradient descent converged was exactly equal to that of CVX. Also the amount of "explained variance" by model after normalization was more compared to the original one. So just from this naive experiment I realized that as far as regression problem is concerned I would go for normalization of data. BTW here normalization implies subtracting by mean and dividing by standard deviation. For backing me on regression please see this relevant question and discussion on it: When conducting multiple regression, when should you center your predictor variables &amp; when should you standardize them? 
 To compare a subset with a whole that includes the subset is the same as comparing it with the while excluding that subset -- if it differs from one it differs from the other, and vice-versa. However, it's easier to do the calculations when the sets being compared don't overlap; both algebraically (because if you include the subset you will have dependence) and practically (this is the way standard statistical models are all set up). 
 I am running a regression tree using rpart and I would like to understand how well it is performing. I know that rpart has cross validation built in, so I should not divide the dataset before of the training. Now, I build my tree and finally I ask to see the cp. Now I don't follow anymore. What are those number? If it was a classification I could follow those number thanks to this question But what about a regresion tree ? The test sample is here 
 There are no hard limits aside from the fact that your data can't be 100% one class. It is true, however, that the less balanced your classes are, the harder it is to learn to distinguish them. 
 Don't look at all at the association between the DV and any of the predictors. If you stay ignorant of these relationships before creating models, it's easier to avoid accidentally cheating by using that information to make your models. Do look at a univariate plot or table of every variable, including the DV. If a predictor has very little variability, leave it out; it can't possibly be much use for prediction. If a variable seems to have a funny distribution or outliers, consider how you might transform it. Avoid histograms, which can deceive you through unlucky bin placement. Use dot plots or density plots instead. 
 While i can't give you a complete analysis, i'm pretty sure, that these differences are due to the fact, that the implementation is targeting sparsity in the data. Have a look at the Bottou paper you linked, especially part 5.1 ! There is a special treatment mentioned for sparse input and the update rule is splitted into several lines. Further evidence: the code-line you refer to: x nnz means most of the time nonzeros (as in nonzeros of sparse-matrix) like for example here sklearns SGDClassifier is sparse-matrix ready (see here ) personal experience tells me that sparsity-information is used! 
 I'm puzzled about why a dependent variable with the weakest correlation to the outcome variable emerges as the most important factor when I run my random Forest on the same dataset. It beats out factors with much stronger correlations with the outcome variable. Now I'm not sure whether to trust my random Forest. All the predictor variables were numeric and the outcome variable was a factor variable with two levels. Do you have insight about 1) why this happens and 2) what I should do next to assess which variables are in fact most predictive? The RF code I used was: 
 I am not claiming that this is a case for your data (as this requires analysis of the data), but here is a general, relatively important, issue with depending on correlation: Correlation of the variable with the dependent one has the same assumption as Naive Bayes - independence of the variables. Thus, the only thing you are able to test this way is "in the absence of all other features, how well would I describe my dependent variable, if I fit a monotonic model to this one, selected feature". These are two, extremely strong, assumptions. Consequently you completely omit any, even a bit complex, relation. For example if you have a numeric variable, such that every time it is odd, your true label is 1, and every time it is even it is 0 - your correlation test will say "there is nothing interesting in this variable", while you have full knowledge about dependent variable, as it is . Furthermore, even if the relation is monotonic and simple, but involves multiple factors - you will miss it too! Random Forest, on the other hand, does not make this kind of assumptions, it builds a complex, nonlinear model, and assess variables importance according to degree to which it is used in its internal decision process. Consequently - it is incomparably better method of assessing the importance, than the correlation test. 
 I don't think so, no. You could examine the interaction term of a linear model as you did first, or you could test the effect of $X$ at different levels of $M$ individually as you did second, but it doesn't make sense to do both; that's double-dipping. 
 What is the uncertainty (68% confidence level) of $N/M$, where $N$ is the number of entries that pass a cut and $M$ is the total number of entries? ($N$ and $M$ are both integers, and I'm interested in the extreme where $N$ or $M - N$ is a small integer, maybe zero.) In the past, I've always assumed a Binomial model , where $N$ is the number of coin tosses that come up heads and $M$ is the total number of coin tosses. Following this logic, I've used the variance $Mp(1-p)$ to conclude that the uncertainty is $\sqrt{\frac{p(1-p)}{M}}$ (with $p=N/M$). However, I'm beginning to think this is flawed: this "uncertainty" is exactly zero if $p=0$ or $p=1$. Getting five heads in a row shouldn't lead one to conclude that the coin will always yield heads with perfect certainty. In general, I think the upper uncertainty will be different from the lower uncertainty; that it should come from some integration that has a hard cut-off at $p=0$ and $p=1$, introducing an asymmetry close to the border. Should this come from a Bayesian formalism because I'm making inferences about an unknown distribution from measurement? (I'm also surprised that I haven't found an answer online: I would have thought it to be a very common problem. Putting uncertainties on trigger efficiencies in physics, for instance.) 
 CP table is the most important part of the RPART, it gives the complexity of the tree model (cp column) training error (rel error) and cross validation error (xerror). I have a set of notes on how every numbers are calculated. But I am running a regression on the mtcar data set. Note directly to your question but I think it can answer your question well. Sorry the annotation might be little messy. I would suggest you to read RPART manual Page 20. And if possible the original cart book . 
 Since this doesn't have an answer yet, I'll expand my comment a little: $x$ is the argument you're evaluating (calculating) the function (the density estimate) at. $x_i$ the value of the $i$-th data point. To draw the density, you'd normally evaluate $x$ across some reasonably fine grid of values. But if, for example, my $x_i$ are: 2, 4, 6, 8... etc what is my corresponding x value? $x$ will be whatever value you want to know $\hat{f}(x)$ at. This is like any other function -- If I said "here's a parabola, $g(x) = 3x^2-8x+5$" you're basically asking "but what value is $x$?". The answer is whatever value(s) you want to know $g$ at. 
 I always think you can do most task by two approaches: knowledge driven and data driven include binning your continuous features. By knowledge driven, you can think about what binning will make sense from what the actual feature represents. For example, if you are binning a household income, you definitely can find some references on basic statistics of the US household income and use those statistical metrics to bin it (e.g., what is the typical value for middle class, rich etc.). By data driven, you are essential want to use this binning to improve your model performance. You can think about you are essentially doing feature engineering or basis expansion. Suppose you want to sacrifice your interpretability, you can even use Neural Network to "train the basis expansion", where you expand one continuous features to many "engineered features", those engineered features can be continuous or discrete. I am thinking you are using RPART to bin, is similar to this approach. Best research always come with combining both knowledge driven and data driven, where you use knowledge to specify a "rough shape of the model" and use data to fit it to get more details. In your case of binning continuous variables, you may also do this. I am not sure if my answer is too high level, but feel free to ask me to explain any part in details. Hopefully this is helpful. 
 Recall, random forests (RF) assigns split values for each feature, which is not the same as correlation. Fundamentally, RF, is a decision tree type classifier which identifies the best value of each feature to split on. Correlation and RF is very different -- and there should be little expectation for similar results. Within RF at run-time, each feature is randomly selected for splitting a parent node (in the tree) into two daughter nodes, and the value of the feature where the split occurs depends on the objects in the parent node which need to be partitioned. Within these objects, there can be any combination of classes (dependent variable values), because of the previous object split histories of parent nodes in the splits above. In other words, when a feature is randomly selected for making a split, only parts of the feature's values are used because only part of the objects are in the parent node. You'll rarely have all the objects that you ran correlation on within a parent node. Even when you start RF, you're doing CV training/testing, so you'll only have objects for all of the training folds in the first parent node. The training set is also a bootstrap, so there can be multiple copies of the same objects in any node. 
 Probably look at spherical k-means for text mining here 
 There are many ways to train a model with though Jurafsky suggests to choose those words that occur very few times in training and simply change them to . Then simply train the probabilities as you normally would. See this video starting at 3:40 – https://class.coursera.org/nlp/lecture/19 Another approach is to simply consider a word as the very first time it is seen in training, though from my experience this approach assigns too much of the probability mass to . 
 It's not a textbook but Bayesian Methods in the Search for the MH370 is a great introduction to particle filters. 
 I have a sparse feature matrix with 50K observations and 150K features. All features are binary. On this I have to run a linear regression. I want just a decent fit. Data: Let us consider training dataset as a matrix: observations are rows and features are columns. Each column has on average only 10~60 entries of 1s. Similarly, each row has on average 10~60 entries of 1s. Context: Each row represents a movie and columns represents an actor. I want to predict movie ratings just based on cast. I have removed actors who appeared in less than 5 movies. What will be the fastest way to run this? I would like to refrain from dimentionality reduction, because test users may not have any actor in common with the reduced feature vector. 
 " An Introduction to Statistical Learning with Applications in R " by two of the 3 authors of the well-known " The Elements of Statistical Learning " plus 2 other authors. An Introduction to Statistical Learning with Applications in R is written at a more introductory level with less mathematical background required than The Elements of Statistical Learning, makes use of R (unlike The Elements of Statistical Learning), and was first published in 2013, some years after this thread was started. 
 I ran an analysis using the 2005 healthy eating index (HEI) score as the dependent variable and am a bit confused by my results. The HEI score ranges form 0 to 100, and categorized as bad (&lt;51), needs improvement (52-80), and good (81-100). In my dataset, none of the participants scored in the "good" category, so I created a binary variable for my dependent variable. In the logistic regression models, the overall model is significant, and my primary independent variable of interest was also significant. I received a comment questioning my used of a binary variable, because dichotomizing variables typically results in losing valuable information. So, I ran my analysis again with the dependent variable as a continuous variable, and the overall model was not significant and the independent variable of interest was significant. Am I doing something wrong? Why would I see significance with a binary versus continuous variable? Also, is there any reason to justify dichotomizing a variable? I thought it would be best to treat the dependent variable using its most practical form, and I have seen other precedence for this as well. Any help would be much appreciated. Thanks! 
 When I was doing my undergraduate studies, I remembered that my lecturers usually uses raw residuals to obtain the plots mentioned above. Some books that I have also used raw residuals. However, recently I've read that some authors recommend using standardized residuals. What are the drawbacks of using raw residuals instead if standardized residuals to obtain Q-Q/P-P plot and residual vs fitted plot in order to check assumptions for multiple regression? 
 I want to perform an outlier-test for my data. The distribution of the data looks like a hyperbola (there are many values near 0, all values are &gt;0). Sample size of data is 5000. I want to find an appropriate outlier-test (not like the usual boxplot) for my data. 
 I was looking at this What does the level of a variable mean? I can only think of categorical variables with 2 values, like gender. Is there continuous variables with only 2 levels? 
 This is a usual problem with the two steps procedure, where one selects first the lag, then the cointegration rank depending on the lag chose in the first step. Puzzle 1: The claim that the lag order does not matter for the cointegration test depends on the type of test: VAR based test: if you use a Johansen test, the lag matters! Residual-based test (cf Engle-Granger, Phulips-Ouliaris), then yes, the lag order does not matter Puzzle 2: My favourite approach is the simultaneous-selection approach, where you choose the lag and rank based on a simlutaneous criterion selection (This is implemented in package tsDyn in R for example, see ). See references below. Now what should you do if the ideal criterion selects a model where there is still some auto-correlation in errors? Great question, and I am not aware of papers tackling this directly. So one option would be to increase lags till you get a model with good residuals. Another option would be to stick with the AIC/BIC choice, and use heteroskedasticity and autocorrelation robust estimators (HAC) for any test you do (t-test, Granger causality, etc). Cheng an Philips (2009) show that their AIC/BIC lag/rank procedure is robust even in presence of auto-correlation (hence the name of semi-parametric). The issue is probably that few softwares will allow you do to this for all tests, in particular, IRF, usually computed with bootstrap, might not include a bootstrap scheme mimicking auto-correlation. Aznar A and Salvador M (2002). Selecting The Rank Of The Cointegration Space And The Form Of The Intercept Using An Information Criterion. Econometric Theory, 18 (04), pp. 926-947. . Cheng X and Phillips PCB (2009). Semiparametric cointegrating rank selection. Econometrics Journal , 12 (s1), pp. S83-S104. 
 As the paper puts it, "Most of the directed techniques can be thought of as selecting an action to perform based on the expected value of the action plus some exploration bonus[11]." If you follow that citation, you'll find this paper , which gives these definitions: undirected techniques that do not use any "exploration-specific" knowledge about the learning process, directed techniques that remember knowledge about the learning process and use it to direct exploration. To get an idea what "knowledge about the learning process" might mean, see this snippet: The exploration bonus $\delta_k$ represents the maximum amount (of reward or utility) that one is willing to pay for one observation of the output of arm $k$ (fig. 1). It measures the importance of sampling this arm to obtain information rather than simply obtaining the expected reward $\bar\rho_k$. [...] This reflects the fact that, when an arm has been tried an infinite number of times, its characteristics are known with certainty and thus, there is no further information to be learned from sampling it. In other words, directed methods account for uncertainty, and formalize an agent's preference for exploration, in the form of a bonus function. This has the effect of directing an agent to explore actions of uncertain utility. 
 Suppose I have $M$ samples from unknown distributions $F(X)$ and $F(Z|X)$. Is there a way from these two vectors to get samples of $F(X|Z)$? I understand Bayes rule, but I only know how to apply it to probability density and distribution functions, not random samples from these distributions. 
 I am in doubt the best test to be chosen. I want to investigate the relationship between the type of mutation in a protein (binary categorical data) with the type of protein structure (three types, nominal). In addition to the type of mutation, there are the amount of each of related types, namely, a numerical data. I thought of the PCA but after review it only applies to numeric data. What is the best treatment for data in this way? 
 I'm using the nnet package in R. One of the parameters is "maxit" but there is no batch size parameter. As such, I am confused. Is an iteration one pass through an entire data set? Or is the batch size 1 so after every additional observation and back propogation occurs to tweak the network? Thanks! 
 I would like to test the relationship between an ordinal variable (stress level, a ranking from 1 'not stressful' to 5 'very stressful') and a continuous scale (academic performance, CGPA). What correlation should I use? 
 Apparently, there are many answers to this question: it has its own Wikipedia page and R package . The uncertainty range I described above is the "normal approximation interval": $\displaystyle p + z \sqrt{\frac{p(1 - p)}{M}}$ where $z$ is signed, $z=0$ is the central value ($p = N/M$), $z=1$ is "one sigma" (68% confidence level) above the central value and $z=-1$ is "one sigma" below the central value. It has the failures I discussed, and they stem from the fact that the distribution of true values around an observation are not themselves binomial. There are many ways to correct the problem; this blog shows an overlay plot of the different estimators (drawn from the R package). Each has different properties and a different justification. The simplest correction (if you're not running R) is the Wilson score interval: $\displaystyle \frac{1}{1+z^2/M} \left(p + \frac{z^2}{2M} + z \sqrt{\frac{p(1-p)}{M} + \frac{z^2}{4M^2}} \right)$ Note that $z \to 0$ in the above yields $p$, so the central value is unchanged, but the positive and negative errors ($z = 1$ and $z = -1$) are now asymmetric and not equal to each other as $p \to 0$ or $1$. I would take this as a simple formula to use for putting error bars on plots. 
 I want to draw a following table in latex please help me 
 You can run stochastic gradient descent / batch gradient descent to solve the regression objective. Since your feature matrix is sparse, this should run pretty fast. If the regression equation is $y=Ax$ each SGD step is of the form $$ x_{t+1} = x_t + \eta (y_t - a_t^T x_t) a_t $$ Now if each row $a_t^T$ has just $z$ entries on average, each update step will take approx $2z$ multiplication and $z$ additions. 
 I want to find outliers in power consumption in real-time, at hourly rate, i.e., at the end of the hour, I should say whether power consumption in current hour was outlier/anomalous or not. Approach: Till now, I am done with following steps Say I want to find whether power usage between 9 AM to 10 AM was anomalous? For this, I first find the usage of past n days during the same time interval, then I find the mean/median of all the previous usages Now, I have usage of the current day and the mean/median usage of previous n days. Which statistical measure should I use to declare whether current day usage was anomalous or not? Using above approach, for 24 hours of a specific (test) day and using past 10 days consumption, I have obtained results as: Figure interpretation: Black line represents usage of current hour of current day; red and blue lines represent mean and median of past 10 days for same time interval From the visual inspection, I can say that the usage between 07:10 - 08:00 and between 22:10 - 23:00 is anomalous as there is big difference between actual and previous mean/median usage. I don't know which statistical measure should I use to point out such anomalous instances automatically, using the discussed approach. 
 I have a single problem of predicting the expenses of customers? For that I have a linear model which is performing good for middle range of Income of family. This model has limitation at higher and lower values of Income. So is it recommended to have 3 categories (low, medium, high) of income and preparing model for each category or should I portray this as a limitation that can't be overcome? Kindly suggest. 
 I have values of attribute between 0 and 1 which i want to predict. The distribution of values is shown in fig. I want to predict this attribute. The problem is there are around 15 classes in this attribute . I want to label 15 classes to 6 like value of attribute greater than 0.9 describe as excellent. Between 0.7 and 0.9 very good and between 0.5 and 0.7 is good etc. But the problem is i am unable to find the standard way of doing this type of labeling. Like why i have given 0.9 a excellent label etc. Someone told me it has something to do with mean and stddev. But i am clueless about it. Also mean and stddev of column is shown in fig. 
 I have the following dataset: where Since the limit of the regression function for large X should be 1, I assumed a proper regression should be in a form of $Y = 1 - a^{-bx}$. However, I'm having difficulty to obtain it. Any idea would be appreciated. NOTE: The function, indeed, approaches the asymptote but the variability is insignificant for in my experiments. That's why I assumed 1 after some larger 's. 
 I need to develop a prognostic model, i have the survival data, and i need to split into validation set and training set. However, I want the Ratio of event to censoring in both sets to be equal. so that the validation set is representative of training set in terms of events and non events. Is there any way that this can be done in R ? 
 You need to specify more; either a distributional model for Y|X=x (or at the least how the spread would be expected to change - e.g. how it will change with x, or how it will relate to E(Y|x=x)) OR you need to specify a loss function. For example if you expect the spread to be constant as the function increases and then approaches the asymptote, you might consider using nonlinear least squares. However this seems unlikely from your data (once it reaches 1 there's literally no variability in the data). if the observations cannot exceed or exactly reach 1 and you expect Y to be less spread as the curve approaches the asymptote you might consider fitting a linear regression to $\log(1-Y)$. This doesn't seem to apply to your data since it appears to reach "1" (at least to the given accuracy, but since we only have 2 significant figures here, greater data accuracy might avoid the 'exact' '1' values). There are many other possibilities. 
 One way is to estimate $F(X)$ and $F(Z \mid X)$ from the samples, use Bayes rule rule to obtain $F(X \mid Z)$, then sample from it. Or, if you have many samples that are pairs of values $(x, z)$ drawn from the joint distribution $F(X, Z)$, you could 'slice' these to obtain subsamples of $F(X \mid Z)$. For example, if $Z$ is discrete and you're interested in the case $Z=z$, select out all your sampled $X$ values where the corresponding $Z=z$. If $Z$ is continuous, you could define a threshold $\epsilon$, then select out the sampled $X$ values with corresponding $Z$ values $\in [z-\epsilon, z+\epsilon]$. In this case, there's a tradeoff between the accuracy and the number of samples you get. 
 I'm pretty new to LDA and I came across other terminology called Gaussian discriminant analysis elsewhere. Since LDA assumes the normality or normal distribution of the data which is same as Gaussian distribution. Are they both referring to the same algorithm? 
 In general "level" could mean any of several different things. However, the context is sufficient to identify the intent here. In phrases such as "1 IV with 2 levels (independent groups)" the independent variable is a grouping variable which take two values, indicating two independent groups. For example: Here 1=female, 2=male, say, making a categorical variable taking two different possible values. 
 You have several times more features than data points, which means the problem is underdetermined. So, you can't use ordinary least squares. The common ways around this are to penalize the $l_1$ or $l_2$ norm of the weights (called lasso or ridge regression, respectively). Lasso will make your weights sparse. Unclear whether you'd want this, because it would mean that your predicted ratings depend only on a sparse subset of the actors. In either case, you'll have to perform a search for a good value of the regularization parameter. When there are many data points, you can often get faster convergence by training on minibatches than running through the entire dataset. The reason is that many points are redundant, and updating after each minibatch allows more frequent weight updates. Minibatch training can also be faster than stochastic gradient descent (i.e. training on single examples) because the underlying numerical computation libraries can play more tricks to speed things up. Your weight update rule will also affect convergence speed, and may also require hyperparameter tuning. For example, using gradient descent, you'll have to play with the learning rate, possibly adjusting it over time. More sophisticated update rules are also available. Note that your objective function is convex in the case of linear regression, so this makes life easier than a lot of the situations you'll find people trying to deal with in this literature. In terms of hardware tricks, you can try running your computation on a GPU. You can also parallelize the hyperparameter search across different machines. 
 What are some good ways to make a heatmap understandable when printed in black ink? (without having to butcher the color version too much, as some reader might have some color printer, or read on the computer) For example, the following heatmap isn't good as blue and red cannot be easily distinguishable when printed in black and white: With one color: 
 You can use a grayscale colormap that ranges from white to dark gray. If you want it to have color but still look ok when printed in grayscale, you have to make the 'brightness' of the color scale monotonically from the lowest to highest value. One way to do this would be to define the colormap in HSV colorspace, where V corresponds to 'brightness'. You could then convert to RGB if necessary. Or, working in RGB colorspace, you could use the mean value of R, G, and B as your working measure of 'brightness'. The perceived value will depend on the transfer function of the printer/display device (which can be handled automatically by gamma correction in your print/display system), and on the transfer function of the visual system (which is part of why more complicated colorspaces are defined the way they are). If you wanted to get fancy you could use one of these colorspaces. Ideally, there'd be a linear mapping between perceived value and the value represented in the plot. But it's not strictly necessary if the goal is just to look reasonable when printed in grayscale. If you're talking about truly black and white (not grayscale), one option would be to use something like dithering . 
 There are several techniques known in "parameter selection" section part of a work. After parameter selection comes modelling. In parameter selection you analyze relations between parameters and rank the parameters in order of importance. Here are some of the works: A Regression model and observe the p-values of the coefficients of each variable Pearson Correlation Spearman Correlation Kendall Correlation Mutual Information RReliefF algorithm Decision trees Principal Component Analysis, etc. Note that you need enough amount of data for your results to be accurate. After you have performed these variable importance analyses, you can rank the variables and then compare the ranks and thus the case studies. 
 Can a linear SVM support more than 2 classes for classification? 
 What is the reason used Cauchy distribution in error term for simulation of data. I see a lot of researcher used the distribution but does not stated the reason why used Cauchy Distribution. 
 I run a survey. The participant should rank 10 items with weight 1..10. Unfortunately I did not emphasis enough that every weight can be used once. Now I have a mixed dataset. Some items are ranked 1..10 others are ranked 1..3 or 1..6. Is there a recipe to on how to analyse such data? Any keywords? 
 A possible explanation is that the Cauchy distribution has symmetric heavy tails https://en.wikipedia.org/wiki/Heavy-tailed_distribution . Thus, it can be a good choice to generate data with possible outliers to assess robustness properties of the investigated methods. 
 I want to do some calculations on a binomial distribution Bin(100,0.55). In particular: P[μₓ - σₓ ≤ X ≤ μₓ + σₓ] I also want to calculate it using the normal approximation for the binomial: f(x)=1/√(2πσ²) exp[-(x-μ)²/2σ²] and attempt to do so using the Chebyshev inequality (say for k=2) P[|X-μ| ≥ kσ] ≤ 1/k² I would like to know if there is a relatively straightforward way to implement these calculations easily in R (that is, using in built functions rather than coding this manually). 
 SVMs (linear or otherwise) inherently do binary classification. However, there are various procedures for extending them to multiclass problems. The most common methods involve transforming the problem into a set of binary classification problems, by one of two strategies: One vs. the rest . For $k$ classes, $k$ binary classifiers are trained. Each determines whether an example belongs to its 'own' class versus any other class. The classifier with the largest output is taken to be the class of the example. One vs. one . A binary classifier is trained for each pair of classes. A voting procedure is used to combine the outputs. More sophisticated methods exist. Search for "multiclass SVM". 
 Yes, support vector machines were originally designed to only support two-class-problems. That is not only true for linear SVMs, but for support vector machines in general. There are ways to work around this, but they usually come as a kind of afterthought. There is an ongoing discussion about the merit of these approaches. If you would like to delve into this discussion, this paper may serve as a starting point. 
 Why would we use odds instead of probabilities when performing logistic regression? 
 I have run 2 Bayesian regression models and would like to compare the posterior samples of a parameter that is common to both models. For example, if model A is $y=\alpha + \beta_1x_1$ and model B is $y=\alpha + \beta_1x_1 + \beta_2x_2$ (This is just an example for a difference between models, other differences could be the priors used, hierarchical vs non-hierarchical regression, ...) what would be the best way to compare the two posterior samples for $\beta_1$ from models A and B, including situations where both posterior samples vary around 0? 
 It seems there's a minus sign missing in the "Filling this in the Lagrange" step . I got something like $$L = \frac{1}{2}w'w - \sum_i \alpha_i (y_i(w'x_i+b)-1)$$ $$=\frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j x_i' x_j-\sum_i \sum_j \alpha_i \alpha_j y_i y_j x_i' x_j- b\sum_i \alpha_i y_i + \sum_i{\alpha_i} $$ $$=-\frac{1}{2}\sum_i \sum_j \alpha_i \alpha_j y_i y_j x_i' x_j+\sum_i{\alpha_i} $$ which is the same as the other representation. 
 The average weight of 100 M population is 150 lbs. + or - 45 ÷ SQRT of 25 . The estimate of weight is in pounds. 
 The odds is the expected number of "successes" per "failure", so it can take values less than one, one or more than one, but negative values won't make sense; you can have 3 successes per failure, but -3 successes per failure does not make sense. The logarithm of an odds can take any positive or negative value. Logistic regression is a linear model for the log(odds). This works because the log(odds) can take any positive or negative number, so a linear model won't lead to impossible predictions. We can do a linear model for the probability, a linear probability model, but that can lead to impossible predictions as a probability must remain between 0 and 1. 
 The docs say it's using BFGS algorithm to optimize the network (which should limit it's usability for big networks; even L-BFGS then has problems). This is a batch-method (unlike Stochastic gradient descent), so it will work on complete batches (therefore no batch-size parameter). For a good overview of optimization functions used in NN-learning, see this paper . 
 The advantage is that the odds defined on $(0,\infty)$ map to log-odds on $(-\infty, \infty)$, while this is not the case of probabilities. As a result, you can use regression equations like $$\log \left(\frac{p_i}{1-p_i}\right) = \beta_0 + \sum_{j=1}^J \beta_j x_{ij}$$ for the log-odds without any problem (i.e. for any value of the regression coefficients and covariates a valid value for the odds are predicted). You would need extremely complicated multi-dimensional constraints on the regression coefficients $\beta_0,\beta_1,\ldots$, if you wanted to do the same for the log probability (and of course this would not work in a straightforward way for the untransformed probability or odds, either). As a consequence you get effects like being unable to have a constant risk ratio across all baseline probabilities (some risk ratios would result in probabilities &gt; 1), while this is not an issue with an odds-ratio. 
 There is a relevant paper: LA Gatus, AS Ecker, M Bethge, 2015, A Neural Algorithm of Artistic Style . Quoting from the abstract, Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Here is Figure 2 from this paper: There is also a very popular open-source implementation based on torch here which is quite easy to use. See the link for more examples. Keep in mind, that the computations are heavy and therefore the processing of single images is the scope of this work. Edit: after checking your mentioned DeepArt project, it seems it is using the same techniques. I'm not sure why this is not what you want, because the concept of style-transfer is as general as it gets. 
 2PL model allows the discrimination value to vary for each question, but what is the maximum range beyond which the accuracy of the models suffers significantly? Or it does not matter? I feel that the wider the range of item discrimination, the more error will be there in the model. Could not find any reference on the net. Any practical/empirical tip on this will also be helpful. 
 In backpropagation the usual way to calculate the amount of error of a layer is $$\delta_0 = y_{expected} - y\\ \delta_i = \sigma'(input)\sum_{j \in outputs(i)}{\delta_jw_{i,j}}$$ where $\sigma$ is the activation function (usually a sigmoid). Layer 0 is the output layer and therefore easy to compute. The inner ones use per each neuron a summation of the errors of the neurons linked to that one multiplied by the weights, times the derivative of the activation function according to the neuron input. Let's assume I have a sigmoid, as it is often the case.. the sigmoid has the following graph and derivative (images via wolfram alpha) I can understand the error for a neuron as being the summation of the errors generated in the next linked neurons times the associated weights, but I don't get the intuition behind the derivative of the activation function evaluated at the input value . If you look at the derivative graph, it seems to me that a neuron which only slightly activated, e.g. x = 0.5, will be penalized by a bigger amount of error rather than a neuron which fully activated (e.g. x = 50). I don't get this: why penalizing less the neurons that completely got the data wrong and penalizing more the neurons which were more uncertain of the result? 
 Currently I'm graduating and I'm finishing up my master's thesis as we speak. It's a study about the effect of hand positioning on response time. I'm submitting my data to a 2x6 repeated measures ANOVA and have found no significant interaction effect, no main effect for one parameter (hand positioning in 2 ways) and a significant main effect for the other parameter (overall response time at 6 different time points). I'm wondering how I should interpret this and if I need to further investigate these effects. My hypothesis is that one type of hand positioning would enhance response speed more than the other type of hand positioning on a certain point in space surrounding our bodies. Can I conclude that there is no significant effect of hand positioning on overall response time and at certain points in space? In my data analyses section I wrote that if there was indeed a significant interaction effect and post hoc main effect of hand positioning, that I needed to perform another one way ANOVA for each hand (to see which type of hand positioning is causing this effect) and a dependant samples T-test to see when/where/if multisensory stimuli (vision and touch) start resulting in significantly faster response times compared to just unisensory stimuli (touch; to see if and where there is a boundary in the space directly surrounding our bodies). 
 I want to learn machine learning. I found tons of material on the internet but couldn't decide which book to get started with. 
 The Esscher-transform is a well know tool in the financial section. Given a Levy-process $(X_t)_{t\geq 0}$ under $P$. Let be $u$ be real such that $\phi_{t}(u)=\log\left(E\left[\exp(uX_t)\right]\right)$ is finite. We have with levy-properties $\phi_{t}(u)=t\phi_{1}(u)$. We know that the Esscher-transform $$ Z_{t}=e^{uX_t-t\phi_{1}(u)} \tag 1 $$ is a positive Martingale wrt to $\mathcal{F}_t=\sigma(X_s:s\leq t)$. We can define a measure $Q$ under which $(X_t)_{t\geq 0}$ is again a Levy process and its density on $\mathcal{F}_t$ is given by $$ \frac{dQ}{dP}|_{\mathcal{F}_t}=Z_{t} $$ Now however the restriction of $dQ/dP$ on $\mathcal{F}_t$ first says that $Z_t$ is an $\mathcal{F}_t$ measurable function and can be expressed almost surely as an function of the sample path $(X_s)_{s\leq t}$. Like in sequential statistics, we observe on $[0,t]$ and the induced likelihood-ratio depends on the process observed on this interval. So i want an understanding how we get this expression of $Z_t$. It would be nice to show that one has first can construct $Z_t$ in a common sense $$ Z_{t}=e^{\int_{0}^{t}udX_s-\int_{0}^{t}\phi_{1}(u)ds} \tag 2 $$ which coincides with (1). I want to show (2) or construct it. We know that the induced measure of a path, is generated by its finite dimensional distributions. Basawa "Statistical inference for Stochastic Processes" p. 181 recommends to calculate the finite dimensional distributions of $X=(X_{t_0},X_{t_1},\ldots X_{t_n})$ for $0=t_0&lt;t_1&lt;\ldots&lt;t_n=t$ and let $\max_{1\leq i \leq n} t_i -t_{i-1}\rightarrow \infty$ as $n\rightarrow \infty$ For a sequece $K_{i}$ of independent random variables with finite mean we have with $S_{n}=\sum_{i=1}^{n}K_{i}$ $$ Z_n=\frac{e^{uS_n}}{E[e^{uS_n}]} $$ Which is the classical sampling scheme. Lets $(K_{1},\ldots,K_{n})=(X_{t_1}-X_0(=0),X_{t_2}-X_{t_1},\ldots,X_{t_n}-X_{t_{n-1}})$ the increments of the increments of the levy process. Then $$ Z_n=e^{(uS_{n})-t_1\phi_{1}(u)-(t_2-t_1)\phi_{1}(u)-\ldots (t_n-t_{n-1})\phi_{1}(u)} $$ Letting $\max_{1\leq i \leq n} t_i -t_{i-1}\rightarrow \infty$ as $n\rightarrow \infty$ we can conclude (2). But this is the law induced by the increments if i'm right. I need a connection of $(K_{1},\ldots,K_n)$ and $X$ defined as above. 
 I'll just give some R examples to demonstrate what @PeyM87 already mentioned, including feature filters and wrappers, and classic feature correlation (using data from the small dataset): Lets assume the target variable is . You could employ classic feature selection using feature filters and feature wrappers . Here's a feature filter example (using univariate filters) that states the "most important" features for the target variable: And here a feature wrapper example (using recursive feature elimination), which states the actually most important features for the target variable by including model training in the process: Note that those approaches give similar but not exact same results. Alternatively, after training a model on predicting the target variable, the variable importance for some models can just be stated: And in case your data would only have numeric features you could also employ classic correlation, PCA, and similar approaches that only work with numeric values. Here's a feature correlation example that shown correlation between features and the target variable: You can also reduce features by selecting them so that the maximum correlation is bounded (exclude the target variable from the process): 
 I've had it asserted to me that any consistent estimator must necessarily also grow less variable with increased sample size. I felt that this couldn't be correct, since there was nothing in the definition of a consistent estimator that forced this to be so, and the class of possible estimators is infinite. What's an example of a consistent estimator that doesn't grow less variable with increased sample size? I'd prefer the example be an estimator that's commonly used, rather than one contrived just to meet the demands of the question. 
 Here's an example of an estimator that's consistent but that doesn't grow less variable with increased sample size. My first attempt at an answer was $X_{1} + \frac{1}{n} $ However, as noted in the comment by @hejseb and also the answer by @EdM, this doesn't work. How about this, which I saw here: If $(X_1,...,X_n)$ is a sample from $Normal(μ,1), μ≠0$, then $1/\bar{X}$ is a (strong) consistent estimator of $1/μ$, but $var(1/\bar{X}) = ∞$, for all $n$. 
 In my opinion, one of the best: The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., 2009, by Trevor Hastie, Robert Tibshirani, Jerome Friedman. And you don't even have to buy it: 
 We need to score a combined test of English + Maths + Science. Would prefer to take IRT route to classical scoring as want to identify top 0.5% performers. Looked into multidimensional models as an possible solution. The literature on MIRT talks about loading on multiple skills but could not find any actual use case similar to what I need to do. Is there any theoretical/empirical guidance on how disparate the skills can be. Is MIRT only used for a subject and subskills within that or multiple subjects can be merged? 
 Does each imply the other? If not, does one imply the other? Why/why not? This issue came up in response to a comment on an answer I posted here . Although google searching the relevant terms didn't produce anything that seemed particularly useful, I did notice an answer on the math stackexchange. However, I thought that this question was appropriate for this site too. EDIT after reading the comments Relative to the math.stackexchange answer I was after something more in depth, covering some of the issues dealt with in the comment thread @whuber linked . Also, as I see it the math.stackexchange question shows that consistency doesn't imply asymptotically unbiasedness but doesn't explain much if anything about why. The OP there also takes for granted that asymptotic unbiasedness doesn't imply consistency, and thus the sole answerer so far doesn't address why this is. 
 This is a pretty difficult problem to solve. You can see some examples here on how a cartoon style, e.g. from the Simpson's has been applied to an image. A cartoon image generally doesn't have the structure that gives this artsy effect. The easiest way to try to apply this in some way would be to have a face-tracker, and then try to align two faces, e.g. a cartoon face and a human face, and then apply this. That might get you somewhere, but it might also look weird. You might then annotate landmarks in the images to help further and do a non-rigid registration before this. This is still somewhat a shitmix solution, but the closest I can think of that could work for faces. Edit: The comment by @TannerSwett adds something to this, it is potential to go onto some artists webpages and try to find their illustrations and try to learn "their" style. I still do not think that will satisfactory or yield enough data, but that would be an interesting thing to test. There is no generally available solution right now, but I think that are definitely some people working on this, and we will see better results soon. I think that maybe the way to go is not the artistic neural network approach. Maybe it is better to have a network that can classify objects in an image and then learn the correspondences between the objects and their cartoon counterparts, then blend the results in some meaningful way. 
 It's known that ROC is overly optimistic in case of imbalanced data sets. How big can this bias be? For example if I read paper where they report 0.75 ROC on a dataset with 5 percent of samples being form the minority class, how would the ROC change if the dataset is balanced? 
 I have data which consist of 0,1,-1 values somethink like this. I need to confirm that (x and y) match with formula.I mean those data generate a formula same as or almost same as the below formula. How can I achive that. Formula: 
 While you use the same notation $\beta_1$ in both models, the parameters $\beta_1^1$ for the first model and $\beta_1^2$ for the second model are different parameters and hence cannot be compared in a Bayesian manner. (Take for instance the extreme case when $X_2=X_1$.) The models can be compared by a Bayes factor, for instance, but this is another issue. 
 I am trying to use a convolutional neural network (implemented with keras ) to solve a modified version of the MNIST classification problem (I am trying the background variations as described here ). I started from this example and played around a bit with the parameters to get better accuracies, but I seem to get stuck at about 90% accuracy on my validation set. I've read papers who manage to get near-human accuracy on those datasets, but I seem not to be able to improve my network to get over 95% (something I would expect to be possible). Because I have only been guessing the parameters for the network thus far and I don't seem to find anything online, I was wondering whether there are any guidelines to find a good architecture and good parameters for convolutional neural networks. Would anybody be aware of how to tackle the problem of finding good parameters (including architecture) for a CNN apart from trying? 
 As we all know that the formula for crude death rate is $$CDR = \frac{D}{P} \times 1000$$ where $D$ is the totla number of deaths(during a calender year) among residents of a community and $P$ is the number of person living in that community during that year . I have read that if two population have different age distributions then it is not reasonable to compare the the $CDR$ of these two population. This comparison can be misleading. I don't see how can the comparison based on $CDR$ can be misleading in such a case. If a population has more number of old people then the $CDR$ must be higher than the population (let's say of same strength) which comprises of more younger people. And simply based on this comparison we can tell which population has more old people and which one has more younger people. Then what is the problem in such comparison? 
 What distribution could represent a "flipped" (skewed left) lognormal distribution? For ex: what name would you do to the distribution in the figure below? I fitted the histogram with a Beta distribution since the values of regularity are between 0 and 1. Is that correct? Is there a better/more flexible approach? I use Matlab.. 
 At least in psychology, it's common to use plain old Pearson correlation in this sort of situation, as part of the broader practice of treating ordinal rating scales as if they were interval-scaled. However, I'd recommend the Kendall correlation , which has a straightforward interpretation that only requires both variables to be on ordinal scales. As a bonus, Kendall correlation can handle nonlinearity, which poses a problem for Pearson correlation. 
 A simple way to quantify how well the formula describes the data is with root mean squared error (RMSE). For each $i$, compute $(-0.398 \ln x_i + 1.2694 - y_i)^2$, then take the square root of the mean of these all these squares. The smaller the RMSE, the better the formula describes the data. 
 I have data on an entire social network of individuals. I'd like to know whether a particular individual-level characteristic is more similar among individuals who are directly linked than among random pairs. (For now I'm not worried by the direction of causality: the correlation could reflect homophily or social influence.) The comparison isn't hard, but given that cases are non-independent by hypothesis - everybody is linked in a single network - what's the best way of constructing standard errors? 
 While is can be justified that the sales (or revenues) of firms in an industry follow a Pareto-distribution, I wonder how the value added (sales minus input costs minus taxes, excluding depreciation) can be summarized by a parametric distribution. Pareto seems not to work, since negative values are possible. Is there an econometric theory that gives ground for a specific class of distributions? My dataset suggest something like Gumbel. 
 The empirical (microeconomic) data I am working with currently is incomplete, that is why I try to fit a parametric distribution using extra knowledge on data generation. 
 I am trying out Logistic Model Trees (Weka implementation) with very imbalanced datasets. Is anyone aware of their robustness without the need of preprocessing steps (such as SMOTE)? My preliminary experiments suggest that SMOTE makes the LMTs perform worse than without any preprocessing step. What kind of specific approaches would you suggest to use in conjunction to Logistic Model Trees to address class imbalance? Thanks 
 If $_nm_x$ is the death rate at age group $x$ to $x+n$ and $_nP_x$ is the population in the same age group,the total deaths will be $\sum{_nm_x}\times{_nP_x}$,the sum is over all age groups. Then $CDR$ is $m=\frac{\sum{_nm_x}\times{_nP_x}}{\sum{_nP_x}}$ Clearly,if the values $_nP_x$ are increased for older ages at the expense of the younger ages then not withstanding the constancy of the age rates of mortality,the $CDR$ will rise.The weights used in such an average are therefore important. Consider the CDR's for two communities $A$ and $B$, $m^a=\frac{\sum{_nm_x^{a}}\times{_nP_x^{a}}}{\sum{_nP_x^{a}}}$ and $m^b=\frac{\sum{_nm_x^{b}}\times{_nP_x^{b}}}{\sum{_nP_x^{b}}}$ Even when two communities have same mortality situations at different age groups,then $m^a$ and $m^b$ may be unequal simply beacuse the proportions $\frac{_nm_x^{a}}{_nP_x^{a}}$ and $\frac{_nm_x^{b}}{_nP_x^{b}}$ may not be the same beacuse the age distributions of the two communities may not be identical.Hence,$CDR$ can not be used to compare mortality situations in different places unless the populations of the places have identical age/sex distributions,a condition which is seldom fulfilled. 
 The common meaning of "consistency" and its technical meaning are different. See this page for some discussion. Also, as noted by @hejseb in a comment on another answer here, lack of bias and consistency are not the same. This quote from the Wikipedia page may help remove some confusion: Bias is related to consistency as follows: a sequence of estimators is consistent if and only if it converges to a value and the bias converges to zero. Consistent estimators are convergent and asymptotically unbiased (hence converge to the correct value): individual estimators in the sequence may be biased, but the overall sequence still consistent, if the bias converges to zero. Conversely, if the sequence does not converge to a value, then it is not consistent, regardless of whether the estimators in the sequence are biased or not. The estimator for the mean of a sequence proposed in another answer here: $$X_1 + \frac{1}{n}$$ thus is not consistent because it does not converge to the true value of the mean as the number of observations increases. The requirement for convergence means that the estimator must get arbitrarily close to to the true value as sample size increases. That would seem to require that the estimator "grow less variable with increased sample size," for any reasonable definition of "less variable." 
 I have a multivariate dataset of linear measurements. Where I measured several characters (e.g. skull length, skull width, skull height, ...) for several different species. My questions is, if the variation in one species is higher than in the others. How can I claculate that in R? Since a MANOVA claculates the differences of the means, I think that this is not the right approach and with an ANOVA I can see, if cariation between the different groups for each character is significant, but not which is higher and also not for the whole dataset together. 
 Consider the following problem. We are interested in approximating from samples the expectation of $h$: $$ \int_t p(t) h(t) d(t) $$ We seek to obtain a lower-variance estimate by using importance sampling. $$ \int_t p(t) h(t) d(t) = \int_t f(t) \frac{p(t)}{f(t)} h(t) d(t) $$ The question is how to choose $f$. Assume $f$ comes from a family of functions parametrised by some vector $\phi$ i.e. $f = f_{\phi}$. The question is now how to find a value for $\phi$ so that the variance of the above quantity is small. In particular, we want to minimise the variance of the estimator $\frac1N \sum_i^N \frac{p(t_i)}{f(t_i)} h(t_i)$, where $t_i$s are sampled from $f$. This is equivalent to minimising the variance below. $$ \int_t f(t) \left(\frac{p(t)}{f(t)} h(t) - \int_t f(t) \frac{p(t)}{f(t)} h(t) d(t)\right)^2 d(t) $$ We could now take the gradient of this expression wrt. to $\phi$. This leads to a stochastic gradient descent algorithm where we alternately sample a bunch of points to compute the gradient and then follow the gradient. This is a non-convex problem, but this should at least lead us to a locally optimal estimate of $\phi$. My questions are: Is this a valid process, i.e. does it lead to a $\phi$ which reduces the variance of the estimator we started out with? Because this is such an `obvious thing', I would be grateful if you could point me to references in literature - I couldn't find many, but I might have searched using the wrong keywords. 
 My objective is to simulate daily temporal series of wet/dry sequence of 2 years (730 days). I'm using a logistic regression with one continuous covariate, $x_{i}$, which is the rain amount of the day $i$ from a numerical model. My categorical variable $y_{i}$ is the observed rain/no rain state, i.e. a vector of 1 and 0 respectively.My logistic regression is expressed as below: \begin{eqnarray} logit(Pr(Y=1|X=x_{i})) = logit(p_{i}) = \alpha_{0} + \alpha_{1}x_{i} \end{eqnarray} I estimated my coefficients ($\alpha_{0},\alpha_{1}$) with the glm package in R. Then I estimated my probability of rain for each day $p_{i}$. Finally, I simulated the daily sequences: PROBLEM AND QUESTION: I know that my temporal series of observed wet/dry displays significant autocorrelation coefficients for the lag 1. The way I simulated my series kill all the autocorrelation. Is there any way to "reproduce" the observed autocorrelation? I looked GEE, I'm not sure if it is the right direction 
 To avoid the heteroscedasticity problem (unequal variances), you could transform your data into ranks and then copy to a "rank" dataset, and rerun MANOVA. A problem, however, with ranks is that they are rectangularly-distributed, thus, you could transform your original values into van Der Waerden scores -- then rerun. van Der Waerden scores are used a lot in genetics. 
 I have data where a tobit model (based on normal distribution and censored at zero) would fit quite well, e.g.: I now want to check with a QQplot if the tobit distribution fits my data well, so I first estimate the parameters of a normal distribution: then I generate data from a tobit distribution with these parameters: and then make the QQplot: is this the correct procedure? 
 Is there any work to study the space of features learned by convolutional neural networks like if they perhaps lie on a manifold? What about other representations like SIFT? 
 Independent of scale (min,max of x-values), a beta distribution can have a left tail, and so can a power function distribution. However, if your data did result in a left tail on a histogram, you could probably fit it very well using the stable distribution, which has four parameters. The stable is used in QF (quantitative finance) to fit log-returns of asset prices, and can take on mixtures of distributions like Cauchy, Laplace, non-central Student's t, beta, logistic, normal, etc. Don't look for it in software packages, since it's not that popular of a distribution in statistics. 
 Probably difficult to answer without the data. Sensitivity and specificity do a very good job when there are small class sizes. Think of this example. Assume you have 100 patients, and 5 are diseased and 95 are normal. If the classifier is a "shoebox" and does nothing and assigns normal to all 100 patients, the predictive accuracy of the classifier is still 95%, since only 5/100 are misclassified. But if you apply sens, spec, they will be terribly low. Thus, sens, spec can catch cases like this. 
 You might decorrelate your data first using PCA, and then clamp the objects to your input nodes (i.e., input the PCs from PCA into the CNN). Did you select any features, or use everything? (don't know if the features were pre-selected and users are expected to use everything?). 
 It depends what you have. Do you know the date of the entry of each individual in the data set? In that case you would want to include the calendar year as a (time-dependent) covariate, with 2005 as baseline. Preferably, you would include it as a factor to allow for a non-linear effect in time, but also including it as numeric should tell you something. You can do that with in R. I think they do something similar in the paper you refer to: The models were adjusted for several time varying covariates, including categorical age per year, previous number of treatment switches, and previous number of suicide attempts. However, I would not expect it to bias the hazard ratio, because you use a different time-scale ("time since change of treatment status"), which means that you assume that the clock resets every time the treatment changes. Therefore, when an individual dies, all that it matters is, by your assumption, the time since the individual is in that state (treatment 1 or 0). I do not have a proof for this, it is more of a hunch at this point. Perhaps I'll come back later with a more expanded answer. 
 Principles of Data Mining by Hand, Mannila &amp; Smyth is a good entry level text. It has chapters on data, visualizing, analyses and uncertainty, models/patterns, score functions, search and optimization, descriptive modeling, predictive modeling for classification, predictive modeling for regression, data organization, finding patterns and rules, retrieval by content, optimization, etc. It has been used for computer science students, as a background reading text. 
 Casella-Berger provides the following two definitions for an interval estimator [L (X), U(X)] of a parameter $\theta $ :- Coverage Probability: probability that the random interval [L (X), U(X)] covers the parameter $\theta $ Confidence Coefficient: infimum of the coverage probability. While i understan the meaning of the two statements, I don't really understand the difference between the two conceptually. Can anyone help me? 
 I am attempting Attrition Analysis in R using the Survival &amp; KMsurv Package. My question is more related to how to use the R package / functionality for my situation. Let us say the analysis is for Department B. I have the following dataset: All employees who were associated with Department B for a period (say 1 Jan 2013-31 Dec 2015) (So this has some employees who joined way before 2013 ) Each employee has a start_date and an end_date While setting up the Survival object, I have done the following: spell is : end_date - start_date -&gt; for employess who have left: 31-dec-2015 - start_date -&gt; for employees who had not left by 31-dec-2015 event is: 1 - if employee has quit by 31-Dec-2015 0 - if employee has not quit by 31-Dec-2015 I then build the survival object using: Surv(spell, event). I have some doubts about this: I do not have data on all employees who joined before 1-Jan-2013 (I only have data for employees who remained till after 1-Jan-2013). Will this corrupt the analysis ? Should I consider the employees that joined before 1-Jan-2013 as "left truncated". So that means in the definition of spell for them, start_date is not their respective start_date but 1-Jan-2013. Thanks. 
 Imagine that for the purpose of a study a sample size is computed with the following formula for a given power $1-\beta$, difference in means $\epsilon$, standard deviation $\sigma$ and significance level $\alpha$ \begin{equation} n = \frac{2(z_{\alpha/2} + z _{\beta})\sigma^2}{\epsilon^2} \end{equation} When all $n$ subjects completed the study, we have the data required to perform the analysis, e.g. t-test. When I apply this t-test, does the significance level $\alpha$ has to be the same as the one used in the sample size calculation? Are the choices of $\alpha$ "before" and "after" the conduct of the study, if may say so, have to be identical? 
 It's a known fact that we can generate sample numbers at random (Uniform distribution) from any probability distribution, given its cumulative distribution function. But if we want to generate sample numbers at a spesific distribution from a uniform distribution what can be done? For example, i want to generate sample numbers at Beta distribution from Uniform distribution. Any ideas? I want to sample from Beta distribution to Uniform distribution!! I think I was clear, if you read again the question, you 'll get it. 
 For reference: $$ p(\sigma^{2}|y) \propto \tau_n N(\mu_n | \mu_0, \tau_0^{2}) \text{Inv}-\chi^{2}(\nu_0, \sigma^{2}_0) \prod_{i=1}^{n} N(y_i|\mu_n,\sigma^{2}) \tag{3.14} $$ The book states: As with the conjugate prior distribution, the easiest way to draw ($\mu,\sigma^{2})$ from their joint posterior distribution, is to draw $\sigma^{2}$ from its marginal posterior density and then $\mu$ from its conditional posterior density, given the drawn value $\sigma^{2}$. The first step -- drawing $\sigma^{2}$-- must be done numerically, for example using the inverse cdf method based on a computation of the posterior density (3.14) on a discrete grid of values $\sigma^{2}$. The second step is immediate: draw $\mu \sim N(\mu_n, \tau_n^{2})$, with $\mu_n$ and $\tau_n^{2}$ from (3.11) The author then uses the above method in an example with real data. I am trying to replicate the example, but am struggling with the sequencing of how to draw $\sigma^{2}$. I understand that $\sigma^{2}$ comes from the density listed in 3.14, I know what the inverse cdf method is, but not how I can use it in this situation. If you have the book, this is from p.82 of the 3rd edition. My python code below illustrates my attempt at the football data example, but I have an error thrown because of a dimensionality mismatch between the subtraction of my data values and $\mu_n$ in the normal density product that is at the end of (3.14). Any/all help appreciated. 
 If you want a good probability estimate that punishes extreme values when these are wrong, then logloss can do that. Read more on Making Sense of Logarithmic Loss . Log Loss heavily penalises classifiers that are confident about an incorrect classification. For example, if for a particular observation, the classifier assigns a very small probability to the correct class then the corresponding contribution to the Log Loss will be very large indeed. Naturally this is going to have a significant impact on the overall Log Loss for the classifier. The bottom line is that it’s better to be somewhat wrong than emphatically wrong. Of course it’s always better to be completely right, but that is seldom achievable in practice! 
 I have completed the principal component analysis (PCA), exploratory factor analysis (EFA), and confirmatory factor analysis (CFA), treating data with likert scale (5-level responses: none, a little, some,..) as a continuous variable. Then, using Lavaan, I repeated the CFA defining the variables as categorical. I would like to know what types of analyses would be appropriate for and would be equivalent of PCA and EFA when data are ordinal in nature. And when binary . I would also appreciate suggestions for specific packages or softwares that can be easily implemented for such analyses. Thank you. 
 One of the features of ROC curves is that they are insensitive to changes in class distribution (see here ). To answer your question, the ROC curve would not change much, or not change at all, if the dataset is balanced. The ingredients of a ROC curve are true positive rate = TP/P (# positives correctly classified / total positives in dataset) and false positive rate FP/N (# negatives incorrectly classified / total negatives). Imagine if you balance the dataset, for example, by cutting N in half. Certainly TP/P doesn't change at all. FP/N might not change at all, and probably won't change much. When you eliminate negative cases, you'll probably eliminate properly classified and improperly classified cases without prejudice. I'm guessing the "ROC overly optimistic for imbalanced dataset" comment has its roots in this paper . There are some good points and findings in this paper about the relationship between ROC and PR curves. However I don't believe the authors have properly justified their claim of over-optimism. They show that a PR curve has more space in the optimal area than the ROC for a specific example. So what? "Over optimistic" to me is a "bad" word, meaning the performance is worse than the diagnostic states. I'm pretty sure the FPRs and TPRs are what the ROC curves says they are. In any case, they are not going to change much by balancing the dataset. 
 A reversed Lognormal ... I will use the notation here that is common in defining the Johnson family, since the latter commonly provides a 3 or 4 parameter version of the Lognormal that captures that which you seek. If $Z \sim N(0,1)$, and $Y=\exp\big({\frac{Z-\gamma}{\delta }}\big)$, then $Y$ has a Lognormal distribution with pdf say $f(y)$: $$f(y) = \frac{\delta}{y \sqrt{2 \pi }} {\exp\big[{-\frac{1}{2} \big(\gamma +\delta \log (y)\big)^2}\big]} \quad \quad \text{ for } y &gt; 0$$ Applying a second transform $X=\xi -Y$ yields the reversed Lognormal that you seek, with pdf say $g(x)$: $$g(x) = \frac{\delta}{(\xi -x) \sqrt{2 \pi }} {\exp\big[{-\frac{1}{2} \big(\gamma +\delta \log (\xi -x)\big)^2}\big]} \quad \quad \text{ for } x&lt; \xi$$ Example The following diagram represents grouped data from Table 1 in Pretorius (1930, p.148). Here, $X$ denotes barometric height (grouped data), while the vertical axis denotes observed frequency. The blue square curve represents the grouped data The red curve is a fitted reverse Lognormal using the automated function from the mathStatica package for Mathematica . References Pretorius, S. J. (1930), Skew bivariate frequency surfaces, examined in the light of numerical illustrations, Biometrika , 22, 109-223. 
 No they don't "have" to be the same. The $\alpha$ you used in your power analysis before conducting the study, and the $\alpha$ you used in your test don't technically have to be the same. But, if you're only "relaxing" $\alpha$ in your test because you didn't get the result you wanted/expected -- and you're only changing this between your design and your analysis because you want to report a "statistically significant" result -- I would discourage this. That's not how hypothesis testing works. Instead, report your design; report your results (even if $p \ge 0.05$); and discuss the implications. Don't let your p-value be your only metric of success or failure in your research. Null findings can also have a great deal of scientific merit. 
 I am attempting Attrition Analysis in R using the Survival &amp; KMsurv Package. My question is more related to how to use the R package / functionality for my situation. Let us say the analysis is for Department B. I have the following dataset: All employees who were associated with Department B for a period (say 1 Jan 2013-31 Dec 2015) (So this has some employees who joined way before 2013 ) Each employee has a start_date and an end_date While setting up the Survival object, I have done the following: spell is : end_date - start_date -&gt; for employess who have left: 31-dec-2015 - start_date -&gt; for employees who had not left by 31-dec-2015 event is: 1 - if employee has quit by 31-Dec-2015 0 - if employee has not quit by 31-Dec-2015 I then build the survival object using: Surv(spell, event). I have some doubts about this: I do not have data on all employees who joined before 1-Jan-2013 (I only have data for employees who remained till after 1-Jan-2013). Will this corrupt the analysis ? Should I consider the employees that joined before 1-Jan-2013 as "left truncated". So that means in the definition of spell for them, start_date is not their respective start_date but 1-Jan-2013. 
 Can anyone suggest me some good reference books on Asymptotic Theory of Statistics and Probability for students pursuing a post-graduate degree in Statistics ? It would be very much helpful if the stated reference book(s) contained enough solved problems on the afore-mentioned topic, so as to get a good hold on the topic. Thanks in advance. 
 As for probability, I would recommend: Borovkov, Probability theory It seems to be the only textbook that contains Stone's local limit theorems. 
 For a gentle introduction, see the Georgia Tech &amp; Udacity course on reinforcement learning. You'll find the early videos in section 8, "Generalization" cover a simple example of how one might formalize a simple problem. For an example, start with the classic mountain car problem . The full details are nicely spelled out in the technical details section of the wikipedia article, but here's a brief, informal summary: A driver in a car, with a weak motor, wishes to get to a hill on the right side of a valley. The car lacks the horsepower to drive straight up, but the driver can reverse up the left hill, then follow rightward with more momentum. States are formalized as real numbers describing position and velocity, within a bounded range. The developers of the BURLAP (Brown-UMBC Reinforcement Learning and Planning) library have a tutorial on how to solve this problem using least-squares policy iteration, which includes this helpful description of how LSPI relies on function approximation. LSPI starts by initializing with a random policy and then uses the collected SARS samples to approximate the Q-value function of that policy for the continuous state space. Afterwards, the policy is updated by choosing actions that have the highest Q-values (this change in policy is known as policy improvement). This process repeats until the approximate Q-value function (and consequentially the policy) stops changing much. LSPI approximates the Q-value function for its current policy by fitting a linear function of state basis features to the SARS data it collected, similar to a typical regression problem, which is what enables the value function to generalize to unseen states. BURLAP is in Java, but the prose of the tutorial can be followed without much reference to the code. 
 The $\alpha$ is usually 0.05 unless multiple tests correction is applied. (The problem is usually in the effect size (in other words, in $\epsilon$). If the effect size is not exactly known, the sample size calculations are only approximate. Even after the experiment is done you can't know the power.) 
 I would like to understand what are the differences/advantages in using TF-IDF or the Log Entropy model for represeting documents and queries in an information retrieval system using diferent weights. I've tested both of them and computed the recall and precision metrics and I've obtained very similar metrics in both of them. The Log Entropy model has shown a little improvement in precision. The developed information retrieval system is available in the following repository: https://github.com/yolanda93/information_retrieval_system For the development of this IR I've used the TF-IDF and Log Entropy models of the Gensim library and I've implemented the TF model based on the TF-IDF. Thanks! 
 I have survey responses and I want to check if (continuous data), for example, correlates with a with ordinal responses ("Strongly Agree", "Agree", "Neither", "Disagree", "Strongly Disagree"). What type of correlation should I use? What about correlating between two Likert-scale questions? Is it valid to convert them to numbers (eg, "Strongly Agree" = 1, "Agree" = 2, etc) and then correlate using a Pearson and Spearman? For instance, consider this example data What is the proper way to obtain a p-value for the correlation between column (Age) and (Likert data)? What is the proper way to obtain a p-value for the correlation between column (Likert data) and (Likert data)? 
 I would like to match a probability distribution to the observed data by Hubé and Francastel (2015) The mean is 5889 bp and the median is 1520 bp. Can you help to match a distribution to these data? The log scale makes things harder to me. I tried mixture of one gamma distributions and a reversed gamma distribution both adding or subtracting from the observed median (1520 bp) but fail to find something that match nicely these data. 
 This answer develops a simple procedure to generate values from this distribution. It illustrates the procedure, analyzes its scope of application (that is, for which $p$ it might be considered a practical method), and provides executable code. The Idea Because $$x^2 = 2\binom{x}{2} + \binom{x}{1},$$ consider the distributions $f_{p;m}$ given by $$f_{p;m}(x) \propto \binom{x}{m-1}p^x$$ for $m=3$ and $m=2$. A recent thread on inverse sampling demonstrates that these distributions count the number of observations of independent Bernoulli$(1-p)$ variables needed before first seeing $m$ successes, with $x+1$ equal to that number. It also shows that the normalizing constant is $$C(p;m)=\sum_{x=m-1}^\infty \binom{x}{m-1}p^x = \frac{p^{m-1}}{(1-p)^m}.$$ Consider the probabilities in the question, $$x^2 p^x = \left( 2\binom{x}{2} + \binom{x}{1} \right)p^x = 2 \binom{x}{2}p^x + \binom{x}{1} p^x =2 C(p;3) f_{p;3}(x) + C(p;2) f_{p;2}(x).$$ Consequently, the given distribution is a mixture of $f_{p;3}$ and $f_{p;2}$. The proportions are as $$2C(p;3):C(p;2) = 2p:(1-p).$$ It is simple to sample from a mixture : generate an independent uniform variate $u$ and draw $x$ from $f_{p;2}$ when $u \lt (1-p)/(2p+1-p)$; that is, when $u(1+p) \lt 1-p$, and otherwise draw $x$ from $f_{p;3}$. (It is evident that this method generalizes: many probability distributions where the chance of $x$ is of the form $P(x)p^x$ for a polynomial $P$, such as $P(x)=x^2$ here, can be expressed as a mixture of these inverse-sampling distributions.) The Algorithm These considerations lead to the following simple algorithm to generate one realization of the desired distribution: These histograms show simulations (based on 100,000 iterations) and the true distribution for a range of values of $p$. Analysis How efficient is this? The expectation of $x+1$ under the distribution $f_{p;m}$ is readily computed; it equals $m/(1-p)$. Therefore the expected number of trials (that is, values of to generate in the algorithm) is $$\left((1-p) \frac{2}{1-p} + (2p) \frac{3}{1-p}\right) / (1-p+2p) = 2 \frac{1+2p}{1-p^2}.$$ Add one more for generating . The total is close to $3$ for small values of $p$. As $p$ approaches $1$, this count asymptotically is $$1 + 2\frac{1 + 2p}{(1-p)(1+p)} \approx \frac{3}{1-p}.$$ This shows us that the algorithm will, on the average, be reasonably quick for $p \lt 2/3$ (taking up to ten easy steps) and not too bad for $p \lt 0.97$ (taking under a hundred steps). Code Here is the code used to implement the algorithm and produce the figures. A $\chi^2$ test will show that the simulated results do not differ significantly from the expected frequencies. 
 I do not know of any standard approximate formulas for this. So I tried the following standard technique. Let $X_1,...,X_N$ be $N$ iid log-normal random variables. Let $Y=\max_i X_i$ be their maximum. Now we need to find $E[Y].$ Let $F_Y(y)$ be the cdf of $Y$ and $F_X(x)$ be the cdf of any $X_i$'s. Then, \begin{eqnarray} F_Y(y) &amp;=&amp; P(Y \leq y) = P(\max_i X_i \leq y) \\ &amp;=&amp; P(\cap_i \{X_i \leq y\}) \\ &amp;=&amp; \prod_i P(X_i \leq y) = F_X(y)^N \end{eqnarray} Since $X_i$'s are iid. Now note that $X_i$'s are non-negative random variables and so is $Y.$ For any non-negative RV we can express its expectation in terms if its cdf as follows. You can find its derivation here . $$ E[Y] = \int_0^\infty (1-F_Y(y))dy = \int_0^\infty (1-F_X(y)^N)dy $$ Now for any log-normal random variable $F_X(x) = \Phi\left(\frac{\ln x - \mu}{\sigma} \right),$ where $\Phi$ is the cdf of the standard normal distribution. Therefore $$ E[Y] = \int_0^\infty \left(1-\Phi\left(\frac{\ln y - \mu}{\sigma} \right)^N\right)dy $$ Till now we have an exact formula for $E[Y].$ All I can think of now is a numerical integration to approximately compute $E[Y].$ The function $\Phi$ is well approximated and there are library functions for it. For a given $\mu,\sigma,N$ we can approximately compute $E[Y]$ using standard numerical integration techniques. 
 Let $X_1$ and $X_2$ be a random sample from the geometric distribution with $Pr(X_i=j)=p(1-p)^{j-1}$ for $i = 1, 2$, $j = 1, 2, \ldots$ and $0&lt;p&lt;1$. Which statistics $T(X)$ could be an unbiased estimator for $\frac{p}{(1+p)}$? My attempt: Let $T(X)$ be the required unbiased estimator. Then, $E(T(X))=\frac{p}{(1+p)}$ and $$ \sum T(j)p(1-p)^{j-1}=p(1+p)^{-1}=p(1-p+p^{2}-p^{3}+...) \\ \sum T(j)(1-p)^{j-1}=(1-p+p^{2}-p^{3}+...) $$ Can we find the estimator by comparing the two sides of above equation? If not, how can we find the required unbiased estimator? As it is said that $X$ is a random sample of size $2$, should we consider the joint pmf while finding the unbiased estimator? 
 I am looking for a variant of Fleiss' Kappa to deal with interval data, rather than strictly nominal/ordinal data. The context that I intend to use it in is as follows: There are several (5-8) graders grading a total of 16 exams The exams are identical, and contain 7 questions. Each question is graded out of 3-8, depending on the question Every exam is graded by every grader (though there are spots of missing data) Please help me find where to look! I have seen an occasional internet utterance of a weighted Fleiss' Kappa, but never a reference for it. I have hope that it exists because a weighted Cohen's Kappa is used frequently. References to relevant R packages would also be appreciated (I have used the irr package before). 
 I need the function (sin(x)/x)^3 to be evaluated in R a huge number of times. What is the fastest way: (sin(x)/x)^3, (sin(x)/x)^3L, or { y=sin(x)/x; y*y*y } ? 
 Question: is minimizing test set mean validation error more important than the gap between train and test errors? Let's say I can tweak parameters in my model to give me mean validation error of 4500 RMSE on k-fold cross validation. When I use these parameters I found to compute RMSE on train and test, I get 1500 and 4500, respectively - quite overfit. Is that still more preferable than a model that would get me, say, 5000 train error, and 5500 test error? Something that performs worse on the test set, but is less overfit to the training set? 
 To estimate RRs for binary outcomes, sometimes the Poisson regression can be used. Specially in epidemiology, when the incidence rate of the binary outcome variable is above 10%, then it's necessary to use an alternative to the logistic regression because the ORs are no more interpretable as RRs. My question is about implementing the modified Poisson regression for binary outcomes: Why do we add "repeated subject = id" in the proc statement and how does it work? Considering that i'm working with a database from a cohort study which has no repeated subjects, does it make sense to apply "repeated subject = id"? Many thanks if someone can help. 
 The potential issue I see here is that new vs. long-term employees may have different hazards (e.g., new employees may be more likely to quit than employees who have been around for years). An easy way to deal with this would be to include a covariate in your model for the number of years an individual has been an employee as of 1/1/2013. Actually, if your t = 0 corresponds to the start of observation time (it seems like that is what you are going for), then you do not have left truncation because you aren't considering individuals "at risk" until you start observing. On the other hand, if you want t = 0 to correspond to the start of employment , then you do have left truncation because individuals were "at risk" during the period between start_date and 1/1/2013, but you were not observing them. 
 Following your lines, for just one observation X, you can get $$ \sum_{x=1}^{\infty} T(x) p (1-p)^{x-1} = p (1+p)^{-1} \\ \sum_{x=1}^{\infty} T(x) (1-p)^{x-1} = (1+p)^{-1} \\ \sum_{x=1}^{\infty} T(x) (1-p)^{x-1} (1+p) = 1. $$ So, any $T(x)$ that satisfy that last equation will be enough, like, for instance, $t(x) := 2^{-x}(1-p)^{-x+1} (1+p)^{-1}$, since $$ \sum_{x=1}^{\infty} t(x) (1-p)^{x-1} (1+p) = \sum_{x=1}^{\infty} 2^{-x}(1-p)^{-x+1} (1+p)^{-1} (1-p)^{x-1} (1+p) = \sum_{x=1}^{\infty} 2^{-x} = 1 $$ and, once you have $t$, you can use it with both $X_1$ and $X_2$ and take their average $$ T(X_1,X_2) = \frac{t(X_1)+t(X_2)}{2}, $$ for $$ E(T(X_1,X_2)) = E\left(\frac{t(X_1)+t(X_2)}{2}\right) = \frac{E(t(X_1))+E(t(X_2))}{2} = \frac{\frac{p}{1-p}+\frac{p}{1-p}}{2} = \frac{p}{1-p}. $$ That way, you will not need to deal with joint pmf's. 
 It doesn't surprise me that Bayesian data analysis by Gelman et al. does not cover ICC since it is a general handbook about Bayesian methods, so it cannot cover everything. For more focused book you can check handbook by Lee and Wagenmakers (2014) that describes similar kind of models (but as far as I remember does not describe ICC per se ). Recently ICC is defined in terms of random effects model (Gelman &amp; Hill, 2007): $$ y_{ij} = \mu + \alpha_j + \varepsilon_{ij} $$ where $y_{ij}$ is $i$-th observation for $j$-th group, $\mu$ is overall mean, $\alpha_i$ is a random effect, and $\varepsilon_{ij}$ is error term. So we have random variables $$ \alpha_i \sim \mathcal{N}(0, \sigma_\alpha^2) \\ \varepsilon_{ij} \sim \mathcal{N}(0, \sigma_\varepsilon^2) $$ and ICC is defined as $$ \frac{\sigma_\alpha^2}{\sigma_\alpha^2+\sigma_\varepsilon^2} $$ If $\sigma_\alpha^2$ is variance explained by $\alpha_i$ grouping factor and $\sigma_\varepsilon^2$ is variance unexplained by it, then ICC is a fraction of total variance explained by grouping. This means that you are interested in Bayesian estimation of hierarchical/random effects models that are nicely described by Gelman and Hill (2007). Since you will be mostly focused on variance components, you can check paper by Gelman (2006) who discusses choosing priors for them. Finally, you can easily find multiple papers focusing directly on Bayesian estimation of ICC, e.g. Burch and Harris (1999), Ahmed and Shoukri (2010), Jelenkowska (1998), or Chung and Dey (1998), to name some. But honestly, I never studied this topic in detail since ICC can be misleading and interpreting variances directly seems to be more straightforward approach. Gelman, A. &amp; Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. Lee, M. D., &amp; Wagenmakers, E. J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press. Burch, B. D., &amp; Harris, I. R. (1999). Bayesian estimators of the intraclass correlation coefficient in the one-way random effects model. Communications in Statistics-Theory and Methods, 28 (6), 1247-1272. Ahmed, M., &amp; Shoukri, M. (2010). A Bayesian estimator of the intracluster correlation coefficient from correlated binary responses. Journal of Data Science, 8 (1), 127-37. Jelenkowska, T. H. (1998). Bayesian estimation of the intraclass correlation coefficients in the mixed linear model. Applications of Mathematics, 43 (2), 103-110. Chung, Y., &amp; Dey, D. K. (1998). Bayesian approach to estimation of intraclass correlation using reference prior. Communications in Statistics-Theory and Methods, 27 (9), 2241-2255. Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian analysis, 1 (3), 515-534. 
 I have read about and implemented a MCMC sampling based optimization for one of the optimization problems that I'm facing. It seems the "magic" of MCMC, and the "inefficient" behavior comes in when, the jump to the next set of parameter based on the ratio of the current and the last sample's probabilties compared against a random function. Unlike hill climbing, this seems to sample the distribution randomly regardless of whether it improves the optimization or not. I feel this is very similar to a random walk without looking at any criteria that improves the optimization. What makes the jump to the next set of parameters better in MCMC than a random walk? 
 ReLus are one of the reasons why modern Deep Learning works so great. They seem to be very fast regarding evaluation and learning. And yes, you can use them with cross-entropy! The thing is, that you don't want to use ReLus ever as your last activation before the output! There you usually go for softmax or sigmoid (or even not using any activation at all) depending on your problem. 
 If I have run a Confirmatory Factor Analysis and have all of the standardized loadings of each item onto its respective variable, how would I calculate the R-squared for each item? Is it simply the standardized coefficient squared, or something else? 
 In chapter 8 section 8.7.1 it tries to explain batch normalization. In the second paragraph of that section it tells us to consider the simple example: $$ \hat{y} = x w_1 ... w_i ... w_l $$ and then claims: The output $\hat y$ is a linear function of the input x, but a nonlinear function of the weights $w_i$. which I believe is incorrect, however, I wanted to make sure I was not wrong myself. I will argue here why I think its wrong. Recall the definition of a linear function to be $ f(x+y) = f(x) + f(y) $. Now lets consider the function they wrote and see if it obeys that property. First it clearly obeys it with respect to x: $$ \hat{y}(x) + \hat{y}(y) = x w_1 ... w_i ... w_l + y w_1 ... w_i ... w_l $$ then by factoring out $w_1 ... w_i ... w_l$ we get: $$ \hat{y}(x) + \hat{y}(y) = ( x + y ) w_1 ... w_i ... w_l = \hat{y}(x + y) $$ One can do nearly an identical proof but with respect to $w_i$: $$\hat{y}(w_i) + \hat{y}(w'_i) = x w_1 ... w_i ... w_l + x w_1 ... w'_i ... w_l $$ but instead by factoring everything first from the left and then from the right. Similarly to why $abc+ab'c = a(bc + b'c) = a(b + b')c$ is true. Do that and we get: $$\hat{y}(w_i) + \hat{y}(w'_i) = (x w_1 ... )(w_i ... w_l + w'_i ... w_l) = (x w_1 ... )(w_i + w'_i)( ... w_l) = \hat{y}(w_i + w'_i)$$ which yields the desired result (that $\hat y$ linear wrt to $w_i$). From the above argument I can't see why they'd say its non-linear. Maybe I have a misunderstanding what they are trying to say? Or is there a small mistake on the draft of the book? If its not to much to ask, can a potential answer try to address why is my proof is wrong ? 
 I have following funnel plot for a meta-analysis of 6 studies with 2 sub-studies each. I would really appreciate if some one can tell me what this funnel plot tells me. 
 It is not incorrect. It is nonlinear in $\mathbf{w}$. Can you take a matrix, premultiply the weight vector by it, and get that? What I mean is that there is no $\mathbf{A}$ such that $\mathbf{A}' \mathbf{w} = x w_1 \cdots w_l$. The key is to see that it is a function in $\mathbf{w}$. $$f(\mathbf{w}) = x w_1 \cdots w_l$$ Pick two of these vectors, $\mathbf{w^1}$ and $\mathbf{w^2}$. Clearly $$f(\mathbf{w^1} + \mathbf{w^2}) \neq f(\mathbf{w^1}) + f(\mathbf{w^2})$$. I think the trouble is that you are thinking about it as a function in $x$. Your proof is correct if you were trying to demonstrate it's linear in this guy. Also, if you fix all the weight elements except one, $w_i$, and think about the function as a function in this one $w_i$, then it would be linear in that. But it is not linear in weight vectors $\mathbf{w}$. 
 I'm currently using from . My response is simply a prediction based on my independent variables. Is there a way to get the probability for each possible prediction as a response with naiveBayes? So if I was trying to predict the outcome of a race, the predicted results would look like this : As opposed to just If this is not possible with Naive Bayes, are there any other standard models that would support this? Like a Random Forest for instance? I would appreciate any guidance/literature on the matter. 
 I trained and SVM classifier and I noticed that I'm getting equal F1 and accuracy values (using a cross-validation), which means that the number of True-Positives and True-Negatives is the same. The training set has 91 negatives and 91 positives. I thought that the results are strange, especially that I keep getting F1=Accuracy for different hyper-paramters (kernels in this case). How can one interpret that? Is that usually the case? 
 I am new to R and analytics. I am trying to create weekly forecasting model. Additionally , I have been asked to see if following components impacts product movement : Weather data ( Mean temperature,rain,snowfall,humidity and precipitation) Holidays Promotions Data can be downloaded from below link : https://www.dropbox.com/s/nudm3vs7eo837ml/Store%20Sales%20Data.xlsx?dl=0 The objective is to create and validate weekly forecast for each store and each individual product category. Can anyone please validate my approach ? Also, do we have alternate efficient approach ? Code : for(pProduct in unique(df_sales$product_desc)) { } ## End of Product Loop 
 There are related questions being asked already but my problem is i can't find a good method of measuring similarity between two datasets that are represented by various lengths of matrices. For instance, first dataset is a sensor data with x,y,z,gyro,acc features of 1000 records. The second dataset's features are the same but with 1500 records. So how do I compute the similarity between these two. I've used dynamic time warping (DTW) but not sure about it because it is mostly used for time-based operations but my dataset doesn't contain any temporal info. Also, it doesn't output a score between [0-1], so not sure how to scale it. I checked Kolmogorov-Smirnov Test , as well, but it can give me the difference between only a particular feature (column) of different size. I thought of measuring the distance for each column separately and summing it up, but haven't tried yet. 
 The difference is that in MCMC algorithm, e.g. Metropolis-Hastings (or many others), there is a target distribution that the algorithm converges to. You can use it to sample e.g. from normal , or gamma distribution, circular one, joint distribution from a regression model , or even much more complicated ones. In case of M-H algorithm you have proposal distribution that is used for drawing random values that are accepted or rejected based on some target density. If you wanted to compare them, with using random walk instead of algorithm such as M-H, then you'd be drawing values from proposal distribution and accepting everything. With random walk you draw values that are "random" , you cannot use it to sample from some prespecified distribution. 
 Generally, yes. In a context where you're trying to estimate test error, what you probably care about is how well the model will predict unseen values. Test error is what tells you this, not the gap between test error and training error. A model that heavily overfits but has better test error than another model is still more likely to be more accurate for future observations. 
 I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable. However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions. For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level: I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R. Many thanks in advance. 
 I have within-subject design and want to compare the means of two conditions. I first run the test for normal distribution and used Shapiro and p values are How should I report these results in my paper in APA style? Is it enough to just say distribution was not normal and give the P value or is there any specific way of reporting this? I decided to use a Wilcoxon test to compare these two conditions mean scores (mainly because distribution was not normal). How I should report this result and is it right to use this test for such an analysis? 
 My question is from the book Introduction to Probability Models, 10th edition, by Sheldon Ross. Page 463, $\S$7.8. There is a paragraph in the book talking about the computation of renewal function: Let $Y$ be an exponential random variable having rate $\lambda$, and suppose that $Y$ is independent of the renewal process $\{N(t), t \geq 0\}$. We start by determining $E[N(Y)]$, the expected number of renewals by the random time $Y$. To do so, we first condition on $X_1$, the time of the first renewal. This yields $$E[N(Y)] = \int_0^\infty E[N(Y)|X_1=x]f(x)dx$$ where $f$ is the interarrival density. To determine $E[N(Y)|X_1 =x]$, we now condition on whether or not $Y$ exceeds $x$. Now, if $Y &lt; x$, then as the first renewal occurs at time $x$, it follows that the number of renewals by time $Y$ is equal to $0$. On the other hand, if we are given that x &lt; Y, then the number of renewals by time $Y$ will equal $1$ (the one at $x$) plus the number of additional renewals between $x$ and $Y$. But by the memoryless property of exponential random variables, it follows that, given that $Y &gt; x$, the amount by which it exceeds $x$ is also exponentialwith rate $\lambda$, and so given that $Y &gt; x$ the number of renewals between $x$ and $Y$ will have the same distribution as $N(Y)$. Hence, $$E[N(Y)|X_1 = x, Y &lt; x] = 0$$ $$E[N(Y)|X_1 = x, Y &gt; x] = 1 + \color{red}{E[N(Y)]}$$ My question is about the last item. While I think the last item should be $E[N(Y-x)]$ instead of $\color{red}{E[N(Y)]}$ in this case. Even though the author provides an explanation before the equations, I find it confusing. Can anybody help me explain why the final item would be $\color{red}{E[N(Y)]}$? 
 I'm trying to learn how to do ANCOVA for a 2x2 crossover study with baseline measurements. I have followed the analysis performed at Mehrotra 2014 "A recommended analysis for 2 x 2 crossover trials with baseline measurements" (The article is paywalled, the link is to the full text @ researchgate). I've recreated the analysis performed at table V with method IV with the following python code: This provides the same p-value presented in the paper, of 0.013. However, I find it very weird. AFAIK, there should not be an interaction between the two variates. But here there is one, as for one treatment the slope of inter~base is positive and for the other negative. (You can visualize with:) This interaction is even statistically significant, as can be seen by running: This is because the model is (Y2-Y1)|(X2-X1). There would not be a violation if they would do (Y_t1-Y_t2)|(X_t1-X_t2) (when t1 and t2 are the two types of treatment), but then I don't see the point in supplying to the model. So to summarize my question - can you please explain why the analysis advocated in this paper, of ANCOVA using (Y2-Y1)|(X2-X1), makes sense and does not violate the assumptions for ANCOVA? 
 You could try an interaction term between the levels of the independent variables whose combination you're interested in. I assume that the levels in X1 and X2 are stored as categorical variables. Then, in your regression, you could add a term X1* X2, which represents the product of the levels in X1 times those of X2. 
 Well? Does all variables in a VAR/VEC need to be normally distributed, or only the target variable? It is very hard to get all of them to meet criteria of normality without deleting too many outliers. 
 To your first question, the LDA implementations in both R's and python's set a default value inversely proportional to the number of topics, $\frac{50}{k}$ and $\frac{1}{k}$, respectively. Not a comprehensive survey, by any means, but it suggests it's common to choose the parameters based on the number of topics, and not an arbitrary constant. To the question of how high or low these values can go, both hyperparameters correspond to Dirichlet distributions: You can choose any value permissible as a Dirichlet distribution parameter. Any positive real will do, and it can go as high as you like. To the question of expected change from these hyperparameters, a bit on their role in LDA. Both control sparsity at different levels of the model. Recall that in LDA, for every document, the topic proportion $\theta$ is chosen from $Dir(\boldsymbol{\alpha})$. (With $\boldsymbol{\alpha}$ a $k$-length vector with every $\boldsymbol{\alpha_i} = \alpha$, $k$ the number of topics.) Do most documents have a few topics, or many? This is what your choice of $\alpha$ should capture: your sense of how topically sparse each document is. In smoothed LDA, each topic $\beta_k$ is drawn from a $Dir(\boldsymbol{\eta})$, and words from this topic are drawn from a categorical distribution with parameter $\beta_k$. Meaning, this expresses how word-sparse we expect topics to be. For intuition, it's helpful to look at how values drawn from a Dirichlet with different parameter choices look. In the image below, each subplot is a draw from a different choice of parameter. The parameter grows as you look right and down, starting $\frac{1}{20}$ and doubling with each subplot. (The second and third have $\alpha = 0.1, 0.2$ respectively, to correspond to the values in your question.) You can see from this that smaller parameter values will produce sparser draws. ("Sparse" in that more values of the topic proportion will be close to zero.) As the choice of value for the parameter grows, draws from the distribution will become more uniform, in that each entry of the vector drawn from $Dir(\boldsymbol\alpha)$ will vary less and less around $\frac{1}{k}$. (You can also see this from the variance expression for a Dirichlet—see wiki link above—that the variance of each $X_i, X \sim Dir(\boldsymbol\alpha)$ decreases as $\alpha$ grows.) To your last question, I don't see a way to recommend values from that information alone. (Especially without knowing more about the application.) Code to reproduce plot 
 About convolution: prof. Brad Osgood said during the course EE-261 said that we can not fully "visualize" convolution. E.g. https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf , p.105: "Now, tell the truth, do you really think you could just flip and drag and figure out what the convolution looks like of that thing with some other thing?" p.99: "What is Convolution, Really? There’s not a single answer to that question." "In brief, we’ll get to know the convolution by seeing it in action: Convolution is what convolution does." Via this I want to emphasize that: even from computational point of view - convolution is understandable operation, but from more deep point of view it is not. It isn't obvious how to design kernel even for 1D signals. About composition: For a long time mathematicians and physicist use superposition approach to analyze difficult thing via summ of elementary terms, e.g.: Taylor series Fourier Series Total acceleration of the point is proportional to the sum of forces in point Structure of solution of system of linear algebraic equation and ordinary differential equation I mean tha "sum" is very fundamental thing. Instead of using sum computer scientists decide to use compoistion for model neural network. But beside chain rule for differentiation there are (at least known for me) no useful equation for analyze g(f(x)). I can describe it via similar sentence as for convolution even from computational point of view - convolution is understandable operation, but from more deep point of view it is not. Question: So my point is in that such structure lead to not understanable model for humans. I want to understand why this two not so obvious operations have been choosen as building blocks in this field. 
 Unless the assumptions of regression modeling have changed, there is no stipulation about the distributions of the variables in the model -- normal or otherwise. There are some technical assumptions about the behavior of the residuals from the model but even those are subject to interpretation in the "art and practice" of modeling. This CV thread ( What is a complete list of the usual assumptions for linear regression? ) contains an excellent discussion of the various ins and outs of these assumptions. In particular, the comments between @AndyW and WHuber are illuminating. AndyW states, "There is no cook book, nor should there be given the potential variety of situations that linear regression could encompass." Which Whuber challenges, noting that he's extending the discussion into the "art and science" realm. 
 I don't really understand your complaint. Just because something is "difficult to visualize" doesn't preclude it from being useful. Convolutions are absolutely fundamental and arise naturally when viewing Fourier transforms as homomorphisms from abelian Banach algebras to their bidual: https://en.wikipedia.org/wiki/Pontryagin_duality Moreover in the link you have, pages 99-100 explain its usefulness, whether you are looking at autocorrelations in signals, to solving heat equations or wanting to mollify functions. Or, try proving the central limit theorem without convolutions. The point is that fourier transforms, with convolutions provide an extremely useful framework. So instead of "trying to visualize" convolutions, you'll be better off studying their properties and where they come up because those properties will give you a much better picture of what they really are and can do. The canonical example of all this is using convolutions along with fourier transforms to study the sums of two independent random variables $X+Y$. Or how you can solve the heat equation by fourier transforming it, and then convolving with your boundary conditions. 
 I was reading the batch normalization paper [1] and it had one section where is goes through an example, trying to show why normalization has to be done carefully. I honestly, can't understand how the example works and I am genuinely very curious to understand they paper as much as I can. First let me quote it here: For example, consider a layer with the input u that adds the learned bias b, and normalizes the result by subtracting the mean of the activation computed over the training data: $\hat{x} = x − E[x]$ where $x=u+b, X =\{x_1...N \}$ is the set of values of $x$ over the training set, and $E[x] = \sum^N_{i=1} x_i$. If a gradient descent step ignores the dependence of $ E[x] $ on $b$, then it will update $b ← b + \Delta &gt; b$, where $\Delta b \propto -\frac{\partial l}{\partial \hat{x}} $. Then $u+(b+\Delta b)−E[u+(b+\Delta b)] = u+b−E[u+b]$. Thus, the combination of the update to $b$ and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. I think I understand the message, that if one does not do normalization properly, it can be bad. I just don't how the example they are using portrays this. I am aware that its difficult to help someone if they are not more specific on what is confusing them so I will provide on the next section, the things that are confusing me about their explanation. I think most of my confusions might be notational, so I will clarify. First, I think one of the things that is confusing me a lot is what it means for the authors to have a unit in the network and what an activation is. Usually, I think of an activation as: $$ x^{(l)} = a^{(l)} = \theta(z^{(l)}) = \theta( \langle w^{(l)}, x^{(l-1)} \rangle + b^{(l)})$$ where $x^{(0)} = a^{(0)} = x $ is the raw feature vectors from the first input layer. Also, I think one of first thing that confuses me (due to the previous reason) is what the scenario they are trying to explaining really is. It says: normalizes the result by subtracting the mean of the activation computed over the training data: $\hat{x} = x − E[x]$ where $x=u+b$ I think what they are trying to say is that instead of using the activations $x^{(l)} = a^{(l)}$ as computed by the forward pass, one performs some kind of "normalization" by subtracting the mean activation: $$\bar{x}^{l} = \bar{a}^{l} = \frac{1}{N} \sum^{N}_{i=1} \bar{a}^{l} = \frac{1}{N} \sum^{N}_{i=1} \bar{x}^{l} $$ and then passes that to the back-propagation algorithm. Or at least thats what would make sense to me. Relating to this, I guess what they call $u$ is maybe $x^{(l)}$? Thats what I would guess because they call it "input" and have the equation $x = u + b$ (I guess they are using the identity/linear activation unit for their neural network? maybe). To further confuse me, they define $\Delta b$ as something proportional to the partial derivative, but the partial derivative is computed with respect to $\hat{x}$, which seems really bizarre to me. Usually, the partial derivatives when using gradient descent are with respect to the parameters of the network. In the case of an offset, I would have thought: $$ \Delta b^{(l)} \propto -\frac{\partial l}{\partial b^{(l)} } $$ makes more sense rather than taking the derivative of with respect to the normalized activations. I was trying to understand why they'd take the derivative with respect to $\hat{x}$ and I thought maybe they were referring to the deltas when they wrote $\frac{ \partial l }{ \partial \hat{x} }$ since usually that is the only part of the back-prop algorithm that has a derivative with respect to pre-activations since the equation of delta is: $$ \delta^{(l)}_j = \frac{\partial L}{\partial z^{(l)}_j}$$ Another thing that confuses me is : Then $u + (b + \Delta b) - E[u + (b + \Delta b)] = u + b - E[u + b]$. they don't really say what they are trying to compute in the above equation but I would infer that they are trying to compute the updated normalized activation (for the first layer?) after $b$ is updated to $b + \Delta b$? Not sure if I buy their point because I think the correct equation should have been: $$\hat{x} = \theta( u + (b + \Delta b) ) - E[\theta( u + (b + \Delta b) )] $$ which doesn't cancel $\Delta b$ the change in the parameter $b$. However, I don't really know what they are doing so I am just guessing. What exactly is that equation that they have written? I am not sure if this is the right understanding but I've given some thought to their example. It seems that their example has no non-linear activation unit (uses the identity) and they are talking about the first input layer only? Since they left out a lot of the details and the notation isn't very clear I can't deduce exactly what they are talking about. Does someone know how to express this example with notation that expresses whats going on at each layer? Does someone understand what is actually going on with that example and want to share their wisdom with me? [1]: Ioffe S. and Szegedy C. (2015), "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", Proceedings of the 32nd International Conference on Machine Learning , Lille, France, 2015. Journal of Machine Learning Research: W&amp;CP volume 37 
 If I have a variable C that is cointegrated with both variables A and B separately, can I use it in first-differenced form as an exogenous variable in a VEC model involving A and B? 
 When developing an instrument involving ordinal data (likert scale with 5-6 response levels), how does one reduce the initial item pool before completing the exploratory factor analysis? I have seen that categorical PCA (in SPSS) has been recommended here but what are additional options that are available outside of SPSS? 
 When scientists are using mark-recapture models on an open population model to estimate the survival probability and the recapture probability (also known as "detection"), how can we be sure that the model is estimating the right thing between the two parameters? Let me put an example. Imagine that you sampled a population of individuals like this one (where ch is the capture history and al is the alive matrix): Since you never know the "al" dataset, how can you be confident that you are actually separating the two parameters? If you have a perfect detection, you should be able to assess perfectly your survival probability. The less you are certain about the detection probability, the less confident you are with your survival estimation. In other words, I guess that if the capture history was like this one, the parameter estimation would be less interesting. It seems that there is much less information in the capture history. But I know that usually, it's not interesting to do this on small datasets. But the point is still there. Since we don't know "al", how can we estimate our parameters properly? Here are my questions: How can you be confident about the values that you observe? Is there a threshold where, if you have a detection probability low or a survival really low, it's impossible to really assess a value for the parameters? For example, if survival goes under 0.11 (meaning that individuals have a probability 11% to serve for the next event or time x#), is it still possible to compute a detection parameter? Is a Bayesian hierarchical model better at estimating parameters for detectability and survival than a frequentist model? Is there a minimum number, approximately, of individuals needed to compute a mark-recapture model on an open population to find detectability and survival? Since a model needs some false absences in order to estimate detectability, what is the proportion of individuals that is only seen once (e.g. c(0,0,0,1) or c(0,1,0,0), etc.) compared to the individuals that have a false negative (e.g. c(0,1,0,1) or c(1,0,0,1), etc.) before a mark-recapture model is not able to 1. discriminate between detectability and survival, and 2. is giving a value that is usable or interpretable without a loss of confidence in the parameter estimation? 
 I have a response variable that is 4 categories of behaviors (ly, rs,al and fd). I am trying to use a multinomial model with 7 habitat-related predictors as fixed factors and individuals ("bird.ID") as a random factor. the data looks like this: then I converted my 'wide' format to a 'long' one, specifying 'bird.ID' as a grouping variable that I want to later use as (individual-specific) random effect And wrote the model from 'mlogi' that I thought should produce what i want i.e. only fixed and individual specific variables, with individual categories as random effect. I read the note from here [on a very similar question]: Unable to provide random parameter with mlogit that specified that mlogit can consider individual-specific random effects, if I only specify the random factors exactly as they appear in the model with no-random effects (with hopes I understood correctly). this is what a came up with: which produced this error : I relized this might have something to do with the number of factors I have in Habitat, though I trimmed it down as much as I can. so my first question would be - how do I deal with multiple-factor predictors that I need, such as Habitat in this case. but just to focus on the main question, when I excluded Habitat like this (exactly the same - Habitat): mod2 &lt;- mlogit(labels ~ 1 |NDVI + path power_lines hunting,reflevel="rs", data= na.omit(birds), R=100, halton=NA,rpar=list("rs:bird.ID"="n","ly:bird.ID"="n","fd:bird.ID"="n","al:bird.ID"="n"), panel=F) I got this error: Error in names(sup.coef) &lt;- names.sup.coef : 'names' attribute [1] must be the same length as the vector [0] And I thought I followed the instructions in Viton (2014) vignette, but I am currently stuck with these errors. In a last attempt, since some posted that mlogit cannot include individual-specific random effects, I tried the gmnl package with the following formula (and many variants that also didn't work out) f1&lt;-gmnl(labels ~ 0 |Habitat2+NDVI + path power_lines hunting|0|bird.ID-1,data=birds, model = "mixl",ranp = c(Habitat2= "u",NDVI= "ln",path= "n",power_lines= "n",hunting="n"), mvar = list(Habitat2= c("bird.ID"),NDVI= c("bird.ID"),path= c("bird.ID"),power_lines= c("bird.ID"),hunting=c("bird.ID")),R = 50,haltons = list("primes"= c(2, 17),"drop" = rep(19, 2))) which produced a new error In this stage I am afraid I might have to try MCMCglmm, but would prefer focusing on these methods because they are more straightforward to me and fulfill my needs for my thesis (if they work). Any advise will be greatly appreciated thank you! 
 To use RFE, it is a must to have a supervised learning estimator which attribute coef_ is available, this is the case of the linear kernel. The error you are getting is because coef_ is not for SVM using kernels different from Linear. It is in RFE documentation A walk-around solution is presented in Feature selection for support vector machines with RBF kernel by Quanzhong Liu et. al. 
 You just need to set in the predict object. 
 I was reading a post that used score fusion to compare two scores from two different classifiers (after normalisation). I read another that suggested feeding the results of these two classifiers into a stacked approach. In what situation is each appropriate? 
 So I have a dataset that contains both categorical and numerical data for each data point, and a class for each data point. My goal is to plan to build an SVM model from the data to predict the class of data points i put into the model. Because SVMs needs numerical data to operate, i created dummy variables that are 0 or 1 depending on the value of the category since the categorical variables would not make sense to be ordered numerically. For example, if i have a category that has values {french, american, british}, I would create three new categories Is_french = {0, 1}, Is_american = {0,1}, Is_british = {0,1} and do the same for every other categorical attributes. So now I have my original numerical data that is NOT 0's and 1's, and MUCH MORE dummy variables from the categorical data, which are ALL 0's and 1's. I have read that, regardless if you are using a linear or a nonlinear kernel, you should scale your data before performing SVMs. I. I have read up on scaling such as on wikipedia https://en.wikipedia.org/wiki/Feature_scaling and this site, but I'm not sure how this scaling can take into account: (1) the fact that there exist unequal numbers of dummy variables that correspond to one attribute (for example, in the above example the category created 3 dummy variables, and in addition I might have 1 truly numerical variable, and another categorical variable that creates, say, 25 additional dummy variables. In this case, wouldn't the categorical variable that corresponds to 25 dummies have an unequal advantage in weight even if I perform feature scaling?) (2) sorry, I don't remember the second problem I had with this... I will update it later II. Also, HOW would you actually scale such a mixed data set? If i did rescaling, the numerical values would obviously change, but the categorical values would remain 0 and 1 Thanks all 
 I am trying to find information regarding a technique which is commonly used in the insurance pricing industry. This relates to a GLM model where a categorical variable is used in the model. The categorical variable will have several levels. After examining the model output, the levels are simplified in two ways: Non-significant levels are grouped back into the intercept; and / or Source: our team's GLM model (in the first, the intercept is level G, while the second, the intercept is a new group of level G and B combined. Levels with similar coefficients are grouped together (not sure if only significant levels or either) Since this is in a business context, this technique is seen as useful from a practical standpoint, but I was wondering about the appropriateness of this for the types of inferences we are trying to make. I think the purpose of the model is prediction, as these model coefficients are used to help decide on pricing differences between the groups, and the actual price, with these differences along with the intercept. I am wondering about the following issues: 1. If this technique is used, does that cause invalidation of statistical inferences? 2. Assuming this technique is not used, what can be done in cases where a prediction is made on a non-significant level with a large coefficient, to avoid showing this prediction as different from the intercept? 3. I have found very little discussion on this technique. We are also wondering about any other pros/cons, if anyone knows of any research that deals with this type of modeling problem, or has some advice otherwise. Thank you 
 We haven't updated the package yet but there is an updated version of for C5 trees that has been checked in. You can install it with the package via You can use the function shown here to get the rules: 
 I was going over the derivation of Naive Bayes, and the following 3 lines were given: Suppose $X = \left &lt; X_1, X_2 \right&gt;$ \begin{align} P(X|Y) &amp;= P(X_1, X_2 | Y) \\[2pt] &amp;= P(X_1 | X_2, Y)P(X_2 | Y) \\[2pt] &amp;= P(X_1 | Y)P(X_2 | Y) \end{align} So the third line comes from the fact that we have made the assumption that each $X_i$ is conditionally independent given Y, but how was line 2 derived? 
 I'm doing some practice problems for a quiz and I'm struggling with the following problem. Can you please let me know if I'm doing it correctly? Suppose that X and Y have a joint density f(x,y) = c for 0 &lt;= x &lt;= y &lt;= 1, and the density is zero outside that interval. Calculate the value of c and find: a) The marginal density of X. So first I double integrated from 0 to 1 c dxdy which should be 1. And found that c is 1. (A little suspicious) Then I integrated c from x to 1 with respect to y and got 1-x, which seems like a reasonable answer. Is this correct? b) The marginal density of Y. I did the same thing as part a) except from 0 to y with respect to x. I got y. (A little suspicious) c) The variance of X. So I calculated E(X) by integrating the answer to a) from 0 to 1 dx. Then E(X^2) by the same way. I got 1/12. d) The variance of Y. I did the same for Y and got 0. e) The conditional density of Y given X. That should just be f(x,y) / the marginal density of X, so I got 1/(1-x). f) The conditional density of X given Y. Same as before and I got 1/Y. g) The expected value of X given Y. Here I was really not sure. I did a bunch of things and got absurd results. I finally did the integral from 0 to y of part e) with respect to x and got something that wasn't a math error -log(1-y). 
 I am building a model whose outputs are between 0-1 and the goal is to minimize a cost function over the predicted values and labels. So far everything seems to be easy but my labels are real-valued and therefore, I cannot use the ordinary cross entropy loss function. For instance, suppose the predicted value is and the label value is . Then simply applying the cross entropy will not give the output of (as the desired output) and still generates gradients unnecessarily. Another problem is when the prediction value is greater than the corresponding label the cost ordinary cross entropy does not output sensible outputs. So I wonder, is there an equivalent version of the cross entropy function which deals with continuous [0, 1] labels? I may note that I have used loss functions such as the L2, L1 and squared loss and so far the ordinary cross entropy loss is getting me the best results! So that's why I think a cross entropy loss which is suitable for continuous labels will even work better. 
 From your comments, it sounds like you have a plot of the data from a figure in a paper (but not the data itself), and your goal is to sample points from the distribution. Besides emailing the authors (which might be wiser than the alternative I'm going to describe), you could try something like this: Draw a series of dots along the curve. If you strung them together, they'd give a line of best fit. You'll need the coordinates of each dot you draw. The more dots, the more accurate the representation will be. Connect the dots to obtain a piecewise linear approximation of the the function in the plot (or you could use something fancier like splines). This approximates the unnormalized PDF. Integrate the unnormalized PDF, then shift/scale it to have minimum value 0 and maximum value 1. This gives an approximation of the CDF. You should now have a set of intron values (a vector $x$) and the corresponding values of the CDF at those points (a vector $c$) Use the inverse CDF method to draw samples. Draw $n$ random values ($u_i, ..., u_n$) from the uniform distribution on $(0, 1)$. For each $u_i$, find the two values of $c$ that it falls between. Interpolate linearly (using the corresponding values of $x$) to find the intron length corresponding to $u_i$. This treats the CDF as a piecewise linear function, but you could do something fancier. You now have $n$ intron lengths sampled from the distribution in the plot. 
 I have a fair bit of experience with these kinds of techniques in the same context. They are, in my opinion, poor, kludgy attempts at regularization. Let me first critique your two points, and then offer you a better alternative. Non-significant levels are grouped back into the intercept This essentially amounts to by-hand variable selection. I have also often seen the threshold of 'statistical significance' used for this selection, but it is important to realize that this is not a problem that p-values or confidence intervals are designed for or intended to solve, so the use of 'statistical significance' as a gatekeeper to remove variables from the model (or, as you say, group with the intercept) is only a rule of thumb. If this technique is used, does that cause invalidation of statistical inferences? Yes. One simple way to see this is that, by removing a variable from the model, you are saying with 100% confidence that it has no effect on the response. Statistically, this is a an unreasonable thing to say, you can never be so sure. Another way to look at is is as follows. Imagine you drew hundreds of random data sets generated by the same process as your glm training data, and hundreds of copies of yourself trained glms on these data sets. Then, over training these hundreds of models using the technique you outlined, many times you would remove some variables through pure chance. This removal of variables is true variance in your model training process. This variance should be accounted for in any estimate of parameter variability, but the glm will not report it. In the presence of variable selection or grouping, all of your confidence intervals are going to be too thin. Since you are using them only as a rule of thumb for selection, this may not be such a big deal for you, but you should be aware of it. Levels with similar coefficients are grouped together (not sure if only significant levels or either) How are you determining 'like' levels? When I look at your graphs, there are no confidence intervals around the parameter estimates. Any grouping you do (if you do any at all) should definitely be taking into account how accurately each coefficient is estimated. Just because two coefficient estimates come out to be similar, does not at all mean that the true underlying parameters are similar. For example, you may have two similar coefficients where one is associated with a very rare class, and the other a very common class. This would make the second coefficient very accurately estimated (low variance) but the first parameter possibly very inaccurately (high variance). Grouping based on the parameter estimates alone would be unwise, as you are essentially arbitrarily allocating a high degree of confidence to the coefficient of the rare class. The Alternative The correct thing to do here is to use a generalized linear model with a penalized likelihood. There are two main flavors Ridge regression penalizes the likelihood with the sum of squares of the coefficient estimates. It has the effect of shrinking parameter estimates of rare classes towards zero, resulting in a more conservative model. Lasso regression penalizes the likelihood with the sum of absolute values of the coefficient estimates. It has the effect of performing variable selection , resulting in some or many of the estimated parameters being zero. Studying and then using these techniques instead of by hand rule-of-thumb selection will result in a superior model. You will not need to group coefficients together, and you should not submit to the temptation to do so unless there is a compelling business or implementation reason. The library in is a state of the art implementation of these methods. I have found it to be a faithful friend. 
 How do you perform a sensitivity/specificity analysis for survival curves in SPSS or Stata? Is using a standard ROC analysis inappropriate given the presence of time data? 
 I am trying to understand the Matrix and Vector Notations on page 2 here : (the page is also pasted below, to make it easier to explain the problem). Problem : For equation (2), I think it should be $\textbf{h}^{T}$ instead of $\textbf{h}$ for matrix multiplication to make sense: Why I think it should be $\textbf{h}^{T}$ $\textbf{x}$ is a Vx1 dimensional vector. $\textbf{W}$ is a VxN dimensional Matrix. Below I re-write equation (1) with dimensions for each vector and matrix: $$ \textbf{h}_{1xN} = \textbf{x}^{T}_{1xV}\textbf{W}_{VxN} = {\textbf{v}_{w_{I}}}_{1xN}$$ $\textbf{v}^{'}_{w_{j}}$ is a Nx1 dimensional j-th column of matrix $\textbf{W}^{'}$ and $u_{j}$ is a scalar. Therefore rewriting equation (2) with dimensions (you can only multiple when you use $\textbf{h}^{T}$ and not $\textbf{h}$ as shown in figure): $${u_{j}}_{1x1} = {\textbf{v}^{'}_{w_{j}}}^{T}_{1xN}.\textbf{h}^{T}_{Nx1}$$ 
 Thanks to those who tried to answer it. However, I don't think either of these answers are that much helpful to me. In fact there is a phd thesis written on this available here . There are also some R packages e.g. CatPredi that can be used as well. 
 Traditional (linear) PCA and Factor analysis require scale-level (interval or ratio) data. Often likert-type rating data are assumed to be scale-level, because such data are easier to analyze. And the decision is sometimes warranted statistically, especially when the number of ordered categories is greater than 5 or 6. (Albeit purely logically the question of the data type and the number of scale levels are distinct.) What if you prefer to treat polytomous likert scale as ordinal, though? Or you have dichotomous data? Is it possible to do exploratory factor analysis or PCA for them? There are currently three main approaches to perform FA (including PCA as its special case) on categorical ordinal or binary variables (read also this account about binary data case). Optimal scaling approach (a family of applications ). Also called Categorical PCA (CatPCA) or nonlinear FA . In CatPCA, ordinal variables are monotonically transformed ("quantified") into their "underlying" interval versions under the objective to maximize the variance explained by the selected number of principal components extracted from those interval data. Which makes the method openly goal-driven (rather than theory-driven) and important to decide on the number of principal components in advance. If true FA is needed instead of PCA, usual linear FA can then naturally be performed on those transformed variables output from CatPCA. With binary variables, CatPCA (regrettably?) behaves in the manner of usual PCA, that is, as if they are continuous variables. CatPCA accepts also nominal variables and any mixture of variable types (nice). Inferred underlying variable approach. Also known as PCA/FA performed on tetrachoric (for binary data) or polychoric (for ordinal data) correlations. Normal distribution is assumed for the underlying (then binned) continuous variable for every manifest variable. Then classic FA is applied to analyze the aforesaid correlations. The approach easily allows for a mixture of interval, ordinal, binary data. One disadvantage of the approach is that - at inferring the correlations - it has no clues to the multivariate distribution of the underlyind variables, - can "conceive of" at most bivariate distributions, thus bases itself not on full information. Item response theory (IRT) approach. Sometimes also called logistic FA or latent trait analysis. A model very close to binary logit (for binary data) or proportional log odds (for ordinal data) model is applied. The algorithm is not tied with decomposing of a correlation matrix, so it is a bit away from traditional FA, still it is a bona fide categorical FA. "Discrimination parameters" closely correspond to loadings of FA, but "difficulties" replace the notion of "uniquenesses" of FA. IRT fitting certainty quickly decreases as the number of factors grows, which is a problematic side of this approach. IRT is extandible in its own way to incorporate mixed interval+binary+ordinal and possibly nominal variables. Factor scores in approaches (2) and (3) are more difficult to estimate than factor scores in classic FA or in approach (1). However, several methods do exist (expected or maximum aposteriori methods, maximum likelihood method, etc.). Factor analysis model assumptions is chiefly the same in the three approaches as in traditional FA. Approach (1) is available in R, SPSS, SAS (to my mind). Approaches (2) and (3) are implemented mostly in specialized latent-variable packages - Mplus, LISREL, EQS. Polynomial approach. That has not been developed in full yet. Principal components can be modeled as polynomial combinations of variables ( using polynomials is a popular way to model nonlinear effects of ordinal regressors.). Also, observed categories in turn can be modeled as discrete manifestations of polynomial combinations of latent factors. There exist a flourishing field of nonlinear techniques of dimensionality reduction; some of them can be applied or adopted to work with categorical data (especially binary or after binarizing into a high-dimensional sparse dataset). Performing classic (linear) FA/PCA on rank correlations or other associations suited for categorical data (Spearman/Kendall/Somer's etc.). In case of ordinal data, that is purely heuristic approach, lacking theoretical grounds and not recommended at all. With binary data, Spearman rho and Kendall tau-b correlations and Phi association all equal Pearson r correlation, therefore using them is nothing but doing usual linear FA/PCA on binary data (some perils of it here ). Look also in this , this , this , this , this , this , this , this . 
 Cross entropy is defined on probability distributions, not single values. The reason it works for classification is that classifier output is (often) a probability distribution over class labels. For example, the outputs of logistic/softmax functions are interpreted as probabilities. The observed class label is also treated as a probability distribution: the empirical distribution (where the probability is 1 for the observed class and 0 for the others). The concept of cross entropy applies equally well to continuous distributions. But, it can't be used for regression models that output a point estimate (e.g. the conditional mean) but not a full probability distribution. If you had a model that gave the full conditional distribution (probability of output given input), you could use cross entropy as a loss function. For continuous distributions $p$ and $q$, the cross entropy is defined as: $$H(p, q) = -\int_{Y} p(y) \log q(y) dy$$ Just considering a single observed input/output pair $(x, y)$, $p$ would be the empirical conditional distribution (a delta function over the observed output value), and $q$ would be the modeled conditional distribution (probability of output given input). In this case, the cross entropy reduces to $-\log q(y \mid x)$. Summing over data points, this is just the negative log likelihood! 
 The PDFs are $$f_U(u) = C(1)u^{-1/2} e^{-u/2}$$ and $$f_V(v) = C(n)v^{n/2-1}e^{-v/2}$$ where $$C(k) = \frac{1}{2^{k/2}\Gamma(\frac{k}{2})}$$ are the normalizing constants. Use polar-like coordinates $u=(r\cos(\theta))^2$ and $v=(r\sin(\theta))^2$ to evaluate the expectation, after first computing $$\eqalign{du\wedge dv &amp;= (2 r \cos(\theta)^2 dr - 2r^2 \sin(\theta)\cos(\theta)d\theta)\wedge (2 r \sin(\theta)^2 dr + 2 r^2 \sin(\theta)\cos(\theta)d\theta) \\ &amp;= 4r^3\sin(\theta)\cos(\theta) dr\wedge d\theta}$$ and $$u+v = r^2(\cos(\theta)^2 + \sin(\theta)^2) = r^2,$$ so that (provided $n+2p \gt 1$) it splits into a Beta integral involving $\theta$ and a Gamma integral involving $r^2$ and a great deal of cancellation occurs: $$\eqalign{\mathbb{E}\left(\frac{U^p}{U+V}\right) &amp;= C(1)C(n)\int_0^\infty\int_0^\infty \frac{u^p}{u+v} u^{-1/2} v^{n/2-1} e^{-(u+v)/2}\, du\, dv,\\ &amp;= 4C(1)C(n)\color{blue}{\int_0^{\pi/2}\sin(\theta)^{n-1}\cos(\theta)^{2p} d\theta}\color{red}{ \int_0^\infty r^{2p+n-2} e^{-r^2/2} dr} \\ &amp;= 2^2 \frac{1}{2^{1/2}\Gamma(1/2)} \frac{1}{2^{n/2}\Gamma(n/2)} \color{blue}{\frac{\Gamma(n/2)\Gamma(p+1/2)}{2\Gamma(p+n/2-1/2)}}\; \color{red}{2^{p+n/2-3/2} \Gamma(p+n/2-1/2)} \\ &amp;= \frac{2^p \Gamma(p+1/2)}{\sqrt{\pi}(n+2p-1)}. }$$ Otherwise, if $n + 2p \le 1$, the integral diverges as $r\to 0$. 
 ${v'_{w_j}}^T$ and $h$ are both row vectors, as you mention. I think they're using the center dot between them to denote the dot product, which seems ok. If it were matrix multiplication notation, then I think you're right that it would be ${v'_{w_j}}^T h^T$. 
 I realize this sounds like a dumb question, since the score test is also known as the LM test... However, as I'm reading about how the test statistics/variances are found, I'm very confused as to how the test actually incorporates Lagrange multipliers. Could someone please elaborate? 
 I have conducted a survey where all my questions are asked in a dichotomous manner (Yes/No). Eg IV:"Are you a smoker?", "Are you obese", "Is your gender male/Female" etc. DV: "Have you ever had a stroke?" Therefore both my dependent variable and independent variables are all dichotomous(Binary= measured in 0s and 1s). My question is, is it appropriate to run a regression to determine the independent variables that drives the dependent variable given the fact that every single one of my variables (both dependent and independent) are dichotomous in nature? If so, what kind of regression is the most appropriate? (Logistic regression?) and is there anything I should do to make the regression model more accurate? I have rudimentary understanding of statistics and regression modelling and would be so grateful if someone would point me in the right direction. 
 In this case, you are relating binary properties of a person (answers to questions) to binary outcome (stroke/no stroke). A good place to start is to formulate this as a logistic regression problem, since it will constrain your dependent variable to be between 0 and 1. The result can be interpreted as the probability that the person will have a stroke given their answers to the survey. (Assumes we code "Yes=1, No=0"). Of course, you will need to (a) ensure your sample was representative of the group you intend to use it on (or of the general population being studied) and (b) cross-validate your data to see how robust your findings are. 
 I have a bunch of samples, about 35, drawn from a fat-tailed distribution. I think it is reasonable to assume that the samples are all drawn from distributions from the same distributional family, though the parameters will vary from sample to sample. The top end of the sample is censored. The sample itself is random. (Well, really it's probability weighted. But let's ignore that for purposes of this question). I have a couple hundred observations for short distance below the censorship threshold, enough to estimate the average value and the slope at the threshold point. I do not think they are spread out enough to estimate, e.g. curvature. For the interval between the censorship threshold and a second, higher value equal to about five times the threshold value, I have the number of observations and the average value. I have the number of observations above the higher value just described. So, that is five pieces of information for each sample. I have several candidate three- and four-parameter distributional families. I would like to use the data I have to estimate the parameters and select between the distributions. The best thing I have been able to think of is to use a basic hill-climbing optimization algorithm on the parameters of each distribution to minimize, say, the product of the squared differences between the observed and estimated value of the five parameters (the product because the observations are in three different units, so they can not be added). But this procedure is admittedly very ad-hoc, and I would prefer something more principled. Also, I'm really not sure I should be treating the slope the same way as the other values. Finally, I'd just be comparing fit, like selecting the model with the highest R squared. That does not seem ideal Is there a better way to do this? Is there any principled way to do it? I have seen maximum likelihood approaches for binned data, but I do not know how to implement them with such diverse summary information, especially when I am comparing different functional forms. 
 The answer is yes, but you have to define it the right way. Cross entropy is defined on probability distributions, not on single values. For discrete distributions $p$ and $q$, it's: $$H(p, q) = -\sum_y p(y) \log q(y)$$ When the cross entropy loss is used with 'hard' class labels, what this really amounts to is treating $p$ as the conditional empirical distribution over class labels. This is a distribution where the probability is 1 for the observed class label and 0 for all others. $q$ is the conditional distribution (probability of class label, given input) learned by the classifier. For a single observed data point with input $x_0$ and class $y_0$, we can see that the expression above reduces to the standard log loss (which would be averaged over all data points): $$-\sum_y I\{y = y_0\} \log q(y \mid x_0) = -\log q(y_0 \mid x_0)$$ Here, $I\{\cdot\}$ is the indicator function, which is 1 when its argument is true or 0 otherwise (this is what the empirical distribution is doing). The sum is taken over the set of possible class labels. In the case of 'soft' labels like you mention, the labels are no longer class identities themselves, but probabilities over two possible classes. Because of this, you can't use the standard expression for the log loss. But, the concept of cross entropy still applies. In fact, it seems even more natural in this case. Let's call the class $y$, which can be 0 or 1. And, let's say that the soft label $s(x)$ gives the probability that the class is 1 (given the corresponding input $x$). So, the soft label defines a probability distribution: $$p(y \mid x) = \left \{ \begin{array}{cl} s(x) &amp; \text{If } y = 1 \\ 1-s(x) &amp; \text{If } y = 0 \end{array} \right .$$ The classifier also gives a distribution over classes, given the input: $$ q(y \mid x) = \left \{ \begin{array}{cl} c(x) &amp; \text{If } y = 1 \\ 1-c(x) &amp; \text{If } y = 0 \end{array} \right . $$ Here, $c(x)$ is the classifier's estimated probability that the class is 1, given input $x$. The task is now to determine how different these two distributions are, using the cross entropy. Plug these expressions for $p$ and $q$ into the definition of cross entropy, above. The sum is taken over the set of possible classes $\{0, 1\}$: $$ \begin{array}{ccl} H(p, q) &amp; = &amp; p(y=0 \mid x) \log q(y=0 \mid x) + p(y=1 \mid x) \log q(y=1 \mid x)\\ &amp; = &amp; (1-s(x)) \log (1-q) + s(x) \log c(x) \end{array} $$ That's the expression for a single, observed data point. The loss function would be the mean over all data points. Of course, this can be generalized to multiclass classification as well. 
 For example, in , the function is useful for generating data to demonstrate various things in statistics. It takes a mandatory argument which is a symmetric matrix specifying the covariance matrix of the variables. How would I create a symmetric $n\times n$ matrix with arbitrary entries? 
 The likelihood of your sample writes as $$\begin{align*} \prod_{i=1}^3 \{p\mathbb{I}_1(x_i)+(1-p) \mathbb{I}_{-1}(x_i)\}&amp;=\prod_{i=1}^3 p^\frac{1+x_i}{2}(1-p)^\frac{1-x_i}{2}\\&amp;=[p(1-p)]^{3/2} p^\frac{\sum_{i=1}^3 x_i}{2}(1-p)^\frac{-\sum_{i=1}^3x_i}{2}\\ &amp;=[p(1-p)]^{3/2} \sqrt{p/(1-p)}^{\sum_{i=1}^3 x_i} \end{align*}$$ From this representation, you can deduce whether or not $\sum_{i=1}^3 x_i$ taking four different values and appearing directly in the likelihood: sufficient $\sum_{i=1}^3 x_i^2$ a constant equal to 3: insufficient $\prod_{i=1}^3 x_i$ only taking two different values, $1$ and $-1$: insufficient $\sum_{i=1}^3 x_i^3$ actually equal to $\sum_{i=1}^3 x_i$: sufficient are sufficient by looking at the values of the likelihood function when those statistics vary. 
 Let's call the bounds $u_1$ and $u_2$, let's further assumed your distributions have pdf $f(x)$ and cdf $F(x)$. Except for one thing, the easiest thing would seem to be to use maximum likelihood with contribution $f(x_i)$ for observations with $x_i \leq u_1$, $F(u_2)-F(u_1)$ for observations between the two bounds and $1-F(u_2)$ for observations beyond the upper bound. The one thing that does not fit perfectly into this framework is that you know the mean of the values falling between $u_1$ and $u_2$. It sort of feels wrong to throw away the available information in the average, so one alternative possibility is to use approximate Bayesian computation. What is the process for having that situation (with knowing the average)? The only type of data where this tends to happen is, if the data is really available (someone must have calculated the average), but somehow you cannot get it all (e.g. anonymized tax data). Perhaps looking into the literature of how other people deal with this type of data would be helpful. 
 I would like to find out if there's enough variance in my dependent variable which is binary. Which techniques would be best for this? 
 The family of the package has two parameters: (the target quantile) and (quantile of response distribution to be used as offset). The default for both parameters is 0.5. Should I change the parameter if I estimate another quantile? For example, if I want to estimate the 0.95 quantile, should I set ? 
 Create an $n\times n$ matrix $A$ with arbitrary values and then use $\Sigma = A^T A$ as your covariance matrix. For example 
 I have experiment that looks somewhat like that. Example dataset: I have couple of subjects (variable , 3 in this example). Each of them have quantitative measurements before and after event ( variable). But at the same time in each time point, each subject have two separate measurements A and B like in variable . I'm trying to model differences between group, changes in time, and also taking in account multiple measurements from each subject (though actual difference between subjects don't interest me). I assume i have to construct a mixed model, I looked through couple of books but it only got me more confused (nested ? grouped random effect? in what combination ?). I would be grateful for helping me out with either helping me out with this model, or suggesting other approach to this problem. 
 I would just start with a simple table and look at the proportion of "successes". You could look at the variance, but in a binary variable that is just a function of that proportion. So, the variance contains no extra information on top of that proportion. After that the main question becomes: Enough for what? One thing you could look into is a power analysis, for more see: here or here . The advantage is that this will force you to be very precise in specifying your question. 
 You could look into stratified sampling , i.e. constraining your train/test splits so that they have (approximately) the same relative frequencies for your predictor levels. However, I think it worth considering whether the current behavior is actually wanted: So random splitting with non-negligible frequency results in sets that do not cover all predictor levels. Can you consider such a set representative for whatever the application is? I've been working with such small sample sizes and went for stratified splitting. But I insist that thinking hard about the data and the consequences of working with such small samples is at least as necessary as fixing the pure computational error. 
 If you store your P@5 metrics in a couple of R vectors, say and you can carry out the t-test comparing their mean values as: This will compute a two sides unpaired test assuming unequal variances . In your case, it seems that your alternative hypothesis is just one side so you can use the parameter in the function that will test for the alternative hypothesis that the mean value in is greater than the mean value in (check if this is the direction you need... it assumes that the greater the score the better the ranking) You may also want to assume that the variance in both scores is the same. The use: Finally, if your samples are paired (I guess this means that your two lists contain different scores but for the same documents), you can use the parameter: Notice that in the paired analysis and should be in the same "order" meaning that each position in both vectors should refer to the same document... If you have named vectors in your R session you can reorder as doing: I hope this helps 
 I've been trying to perform a binary classification using an SVM classifier (scikit-learn's SVC with RBF kernel). I have a sample size of about 100, with about 70 features each. The features are of approximately the same order of magnitude in their raw form, and the values tend to be already distributed around the 0 (not always though). The distribution of two such features is shown in the histograms below. I performed a Z-score transformation on all features, as I know this to be considered a good practice when working with multiple features in machine learning. The problem is that when I use the raw data, I always manage to get better accuracy than with the Z-scores (about 2-3%). Bear in mind that the parameters of the SVMs in each case are optimized using a grid-search, so I'm not using exactly identical classifiers. Does this make sense, getting worse results with Z-scores? I would expect to get the same or better results. What could be the mathematical logic behind this? 
 I have given a data and I have to check the data if it's normally distributed and if not I have to transform the data into normality. I had done shapiro-wilk normality test and p-value is clearly smaller than 0.05. I had tried transforming to log, sqrt. But it is still not normally distributed. Is there other ways to transfer to normality or is it impossible for some data? If that's impossible I have explain why. Edit: As reply to the comments: that's the part of assignment. It says I either of transform to normal distribution. If it's not possible I have to say why is it not possible. 
 I was reading the section on k-statistics on wolfram alpha. It was known to me that for the sample variance $k_2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \overline{x})^2$ it holds that its variance equals $var(k_2) = \frac{\kappa_4}{n} + \frac{2 \kappa_2}{n-1} = \frac{\mu_4}{n} - \frac{\sigma^4(n-3)}{n(n-1)},$ where $\kappa_i$ denotes the $i$-th cumulant, $\mu_4$ the 4-th central moment and $\sigma^2$ the variance. Now, apparently there exists an unbiased estimator for $var(k_2)$ given by $\hat{var}(k_2) = \frac{2n k_2^2 + (n-1)k_4}{n(n+1)},$ where $k_4 = \frac{n^2}{(n-1)(n-2)(n-3)}\left( (n+1) m_4 - 3(n-1) m_2^2 \right)$. Here $m_p = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^p$. There reference given is Kenney and Keeping 1951, p. 189. However, I cannot find a copy anywhere, or a derivation of this equation. Can anyone help me with this derivation or point me towards a reference? Also, I was wondering if a similar equation would provide an unbiased estimator for the variance of the sample covariance. 
 Given two variables, X and Y, there is a way of obtaining a Mutual Information value between 0 and 1 by: where H(X) and H(Y) are entropies of X and Y respectively. Just wondering if there is a similar operation to obtain a Transfer Entropy value between 0 and 1. If so, what is it? Any help much appreciated. 
 I'm trying to understand back propagation algorithm for multiclass classification using gradient descent. The output layer is a softmax layer, in which each unit in that layer has activation function: Here, a k is the sum of inputs to unit 'k'. Differentiating the above equation, the author has achieved this result. I'm confused by the delta kk' and i have never seen anything like it. Another question is do we consider the summation while taking the derivative, why or why not? is a bit relevant, but the result of differentiation is different. 
 Is there some good heuristics to choose: Number of filters in a Convolutional layer Size of the filters Number of Convolutional layers I have 250k small images ( 28x28 ), and I have 37 outputs . So I don't know if knowing this can help me to choose a raisonnable range for the parameters above. 
 I have a dataset from a cross-sectional study (n=121) where people where asked about production characteristics in 2015 and how they recall their production in 2010. One set of example questions could be: " How many units of input A did you use in 2010? " " How many units of input A did you use in 2015? " " How much of B did you produce in 2010? " " How much of B did you produce in 2015 ". Is it possible to draw conclusions on causal relationships between utilized production inputs and the production level? I know about the weakness of recalling questions in general regarding response quality (e.g. a bias in what is recalled) but I am not sure about the formal part and how a model with only two timesteps (t=2) would perform compared to a model with longitudinal data of lets say t=10. I guess it would impact negativly the significance-level of my test-statistics which might be a problem especially given the small samplesize n... but that is a rather intuitive guess than a formal argument. Could anybody help me by clarifying that a little bit more? 
 Why do people use Quadratic Programming techniques (such as SMO) when dealing with kernelized SVMs? What is wrong with Gradient Descent? Is it impossible to use with kernels or is it just too slow (and why?). Here is a little more context: trying to understand SVMs a bit better, I used Gradient Descent to train a linear SVM classifier using the following cost function: $J(\mathbf{w}, b) = C {\displaystyle \sum\limits_{i=1}^{m} max\left(0, 1 - y^{(i)} (\mathbf{w}^t \cdot \mathbf{x}^{(i)} + b)\right)} \quad + \quad \dfrac{1}{2} \mathbf{w}^t \cdot \mathbf{w}$ I am using the following notations: $\mathbf{w}$ is the model's feature weights and $b$ is its bias parameter. $\mathbf{x}^{(i)}$ is the $i^\text{th}$ training instance's feature vector. $y^{(i)}$ is the target class (-1 or 1) for the $i^\text{th}$ instance. $m$ is the number of training instances. $C$ is the regularization hyperparameter. I derived a (sub)gradient vector (with regards to $\mathbf{w}$ and $b$) from this equation, and Gradient Descent worked just fine. Now I would like to tackle non-linear problems. Can I just replace all dot products $\mathbf{u}^t \cdot \mathbf{v}$ with $K(\mathbf{u}, \mathbf{v})$ in the cost function, where $K$ is the kernel function (for example the Gaussian RBF, $K(\mathbf{u}, \mathbf{v}) = e^{-\gamma \|\mathbf{u} - \mathbf{v}\|^2}$), then use calculus to derive a (sub)gradient vector and go ahead with Gradient Descent? If it is too slow, why is that? Is the cost function not convex? Or is it because the gradient changes too fast (it is not Lipschitz continuous) so the algorithm keeps jumping across valleys during the descent, so it converges very slowly? But even then, how can it be worse than Quadratic Programming's time complexity, which is $O({n_\text{samples}}^2 \times n_\text{features})$? If it's a matter of local minima, can't Stochastic GD with simulated annealing overcome them? Thanks! 
 The formula for F-measure (F1, with beta=1) is the same as the formula giving the equivalent resistance composed of two resistances placed in parallel in physics (forgetting about the factor 2). This could give you a possible interpretation, and you can think about both electronic or thermal resistances. This analogy would define F-measure as the equivalent resistance formed by sensitivity and precision placed in parallel. For F-measure, the maximum possible is 1, and you loose resistance as soon as one among he two looses resistance as well (that is too say, get a value below 1). If you want to understand better this quantity and its dynamic, think about the physic phenomenon. For example, it appears that the F-measure &lt;= max(sensitivity, precision). 
 I have the following equation: = the return of the portfolio .. = free risk rate.. : the return of a strategy. has the following expression: (Estimated using Generalized Method of Moments (GMM) (1). ,, and are the risk factors related to . My objective is to find the optimal values of , , and that maximize the utility function of the investor. with is the investor utility is the vector of parameters to maximize presents the 4 risk factors. The code is: As you can see in the code, I don’t know how to write (1)... I have this code to estimate beta parameters using GMM But I don’t know how to implement it in the function I am would be veryy grateful if you could help me on this ! Thank you 
 I have come across this brilliant site where I finally understood BPTT for RNN's and want to implement it. The code is given in python but I want to implement it in torch using lua. I have understood the code and have translated most of it to lua with torch functions. However, I have often come across packages for torch on github like this . How can we use these packages? If we were to use them, would we not need to type even a single line of code? Isn't it better if I first completely code the RNN myself to understand and then go on to the packages? 
 I would like to compare the statistical significance of a key difference in my population: 25% of my population have attribute B (let's say B is red trousers) 75% of my population don't have attribute B (no red trousers) My data is telling me that the population with red trousers are far more likely to have red or orange socks than those who don't wear red trousers. People without red trousers are more likely to wear blue socks.I am unsure of the type of test since I effectively have two different populations of different sizes (those with red trousers being fewer than those without) I usually know rougly what to do with these tests but the dual sample nature has thrown me; can anybody advise what test is appropriate plus whether I need to conduct it using raw nubmers of percentages? Many thanks, Henry 
 I am currently doing a project in which the dataset is a lung cancer dataset. There is a training file which consists of 7 unnamed parameters (Attributes) and each of them have around 1000 values which are binary. for example the file is somewhat like this, This is just a sample representation. The original file has thousands of such values. There is a target file which consists of target values which can be either 1 or -1. 1 indicates that there is lung cancer and -1 indicates that lung cancer is not present. The target values are for each rows of the training set. I want to form a causal graph from this much data given which should use the Bayesian approach. Any other approach is also appreciated. Can someone please enlighten me on this problem. 
 I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s)) I have a dataset with these variables Now I have used the mlogit pkg in R My Code is Generates this error : Now if I break the variables into different variables like.. IT WORKS without any errors. But here I would like to know, how breaking into different models would change my Coefficients? What should I do to avoid the error and try to incorporate all variables in one model? How should I interpret my results if break into 3 different models as opposed to one? Any help is highly appreciated. 
 I am trying to solve a problem for finding similarity score between objects to create a similarity score matrix based on multiple nominal/ordinal/continuous variables for each object. Example of how the data looks like : One solution which I was able to find was rbf_kernel/polynomial kernel, but i am not sure how to implement that on this type of data using R/Python. Please let me know what other kind of algorithm (other than kernel) would be best in this scenario. 
 Transfer entropy is defined as: $$T_{X \to Y} = H(Y_t \mid Y_{t-1 : t-L}) - H(Y_t \mid Y_{t-1 : t-L}, X_{t-1 : t-L})$$ where $t$ is the current time and $L$ is the history length. If $X$ perfectly predicts $Y$ then $$H(Y_t \mid Y_{t-1 : t-L}, X_{t-1 : t-L}) = 0$$ and $T_{X \to Y}$ therefore has a maximum value of $H(Y_t \mid Y_{t-1 : t-L})$. If $X$ contains no information about $Y$ then $$H(Y_t \mid Y_{t-1 : t-L}, X_{t-1 : t-L}) = H(Y_t \mid Y_{t-1 : t-L})$$ and $T_{X \to Y}$ therefore has a minimum value of $0$. So, if the goal is to normalize transfer entropy to the range $[0, 1]$, it seems reasonable to divide it by its maximum possible value: $$\frac{T_{X \to Y}}{H(Y_t \mid Y_{t-1 : t-L})}$$ I don't know whether this is standard; just working off the definition. So please check that this makes sense. A similar definition is given in Gourevitch and Eggermont (2007) . Evaluating Information Transfer Between Auditory Cortical Neurons and other neurophysiology papers (search for 'normalized transfer entropy'). They often include an additional correction term based on shuffled data to correct for bias when estimating from finite, noisy signals. 
 I don't fully understand why single link techinque in hierachial clustering is not good at handling (discovering) clusters of round or elliptical shapes. Simple examples will help me a lot. Thanks. 
 Suppose that I have a categorical response variable that consists of group 1-3, and I hope to see if predictors can differentiate group 1 vs group 3 (group 2 not included). The response variable is ordered; for example groups are assigned based on blood pressure,heights, etc. One way is to fit a multinomial logistic regression and interpret the results, but what happens if I erase out group 2 variable and just fit a logistic regression using group 1 and group 3? I think the multinomial model makes sense, but I could not think of any drawbacks of using the binary model. Would the binary model be biased, or is it an incorrect study design? (One problem I faced when using the multinomial model is that I have tried to do feature selection using LASSO, but the model produces different sets of nonzero coefficients, and that's why I have leaning toward using logistic version. I don't want to compare group 1 vs group 2,3 or group 1,2 vs group 3) 
 $X_1,X_2,..,X_n$ is a random sample from the random variable whose pdf is, \begin{align*} f(x)=\lambda e^{-\lambda(x-\mu)},\mu&lt;x&lt;\infty \end{align*} How can we find $E(X_{(2)}-X_{(1)})$, if $n=2?$ We know that,$f_{X(r)}(x)=\frac{n!}{(n-r)!(r-1)!}F(x)^{r-1}(1-F(x))^{n-r}f(x)$ Here,$r=2,n=2$ $\rightarrow f(X_{(2)}(x)=2\lambda^{2}[e^{\lambda\mu-\lambda x}-e^{-2\lambda x+\lambda\mu}]$ It is difficult to compute, $E(X_{(2)})=\int_{\mu}^\infty 2x\lambda^{2}[e^{\lambda\mu-\lambda x}-e^{-2\lambda x+\lambda\mu}]dx$ I have also tried to find, $E(X_{(2)})=\int_{\mu}^{\infty}[1-F(x_{(r)})]dx$ Similar problem arises in case of $E(X_{(1)})$ too. 
 The difference between Mean Square Error (MSE) and Mean Square Predicted Error (MSPE) is not the mathematical expression, as @David Robinson writes here . MSE measures the quality of an estimator, while MSPE measures the quality of a predictor. But was is curious to me is that the mathematical expressions for the relationship between bias and variance for MSE and MSPE is mathematically different: The MSPE can be decomposed into two terms (just like mean squared error is decomposed into bias and variance); however for MSPE one term is the sum of squared biases of the fitted values and another the sum of variances of the fitted values We have: $MSE(\hat{\theta})=E\left[\left(\hat{\theta}-E(\hat{\theta})\right)^2\right]+\left(E(\hat{\theta})-\hat{\theta}\right)^2=Var(\hat{\theta}) + Bias(\hat{\theta},\theta)^2$ From Wikipedia we read that for the MSPE we have the following relation: \begin{equation} MSPE(L)=E\left[(\hat{g}(x_i)-g(x_i))^2\right]=\sum_i (E[\hat{g}(x_i)]-g(x_i))^2 + \sum_i Var(\hat{g}(x_i)) =\sum_i Bias(\hat{g}(x_i),g(x_i))^2 + \sum_i Var(\hat{g}(x_i)) \end{equation} I'm looking for an intuitive explanation of the expression of bias and variance of the MSPE. Is it correct to think of this as each observation/fitted value having its own variance and bias? If so, it seems to me that increasing the amount of observations should increase the MSPE (more bias and variance sums). Should there maybe be a $\frac{1}{n}$ in front of the sums of the sums of the different bias and variance? 
 I have a couple of empirical studies examining the determinants of credit ratings. Here, the dependent variable is a binary variable indicating whether a firm has a credit rating or not ($rating$). The studies use logit or probit models to estimate the impact of certain firm characteristics $D_1,...$ (as e.g., the firm's leverage ratio, market-to-book ratio, ...). The model can be formalized as: $ rating_{ij} =\beta_0 + \beta_1D_1 + \beta_2D_2 + ... + \epsilon_{ij}$ I have two questions: (1) There are two different definitions for the dependent variable : (a) $rating_{ij}=1,$ if the firm has a credit rating, and 0 otherwise, or (b) $rating_{ij}=0,$ if the firm has a credit rating, and 1 otherwise. To directly compare the values for the coefficients $\beta_1,...$ from studies with type (a) and (b) definition, do I just have to change the sign for $\beta$ and the corresponding $t$-statistic? And the standard errors and p-values should be the same for both definitions? (2) In some studies the definition of the independent variables are inverse. E.g., some studies use market-to-book ratio as a independent variable $D_1$ and others use the inverse definition, which is book-to-market. How can I convert the $\beta_1,...$ from a study using book-to-market, such that the regression coefficient shows the marginal effect of a change in market-to-book? And how can I convert the corresponding standard errors, t-statistics, and p-values? Thanks a lot for your help! 
 If the three groups are assigned, as you state, by categorising an inherently continuous variable then you are wasting information by doing that especially if you choose the cut-offs in the light of the data. If you remove the middle group then you are just left with the ones which are more extreme and so easier to predict which seems likely to give you an optimistic view of the performance of your predictors. 
 I have a 2x2 repeated measures ANOVA (2 factors each with 2 levels). In addition each subject completes each condition multiple times (5 repetitions). So for example I would have Factor A, level 1 completed 5 times; Factor A, level 2 completed 5 times and so on. My question is how I should deal with these repetitions when it comes to the ANOVA. Should I average over repetitions to get a mean for each subject? Or should I use each repetition as a unique measure of the DV? A design somewhat like this: 
 Imagine I have the following 7 glmm models where $b_1$ through $b_3$ are fixed effects. $M_1 = y \sim b_1 \times b_2 \times b_3$ $M_2 = y \sim b_2 \times b_3$ $M_3 = y \sim b_1 \times b_3$ $M_4 = y \sim b_1 \times b_2$ $M_5 = y \sim b_3$ $M_6 = y \sim b_2$ $M_7 = y \sim b_1$ DIC can be used to asses the relative fit of models. However, I would like to know what comparisons would be valid to make because I am unsure and struggling to know what route to take. Imagine the following scenario, where the DIC for each model is shown in red. I am not sure what would and would not be valid comparisons/correct interpretations of using DIC compare the models. Would it be valid to compare, for example, the following: A) $M_1$ with $M_2$ - I think this is a valid comparison, and shows whether dropping the parameter $b_1$ improves the fit of the model. If $\Delta$ DIC &gt; 2 then model fit is improved by dropping the term. B) $M_1$ with $M_5$ - I suspect that this is not a valid comparison. It tests whether dropping two parameters at the same time improves model fit. I think I should first try dropping both $b_1$ and $b_2$ individually, then proceeding to testing $M_2$ and $M_3$ against $M_5$, retaining the most complex model where dropping terms does not improve fit. C) $M_5$ with $M_6$ - I suspect that this is not a valid comparison. It shows the effect of substituting the parameters, which may mean different data is included. Essentially it boils down to this. Can one only use DIC to compare models with sequentially dropped effects, or can I drop multiple effects simultaneously and/or compare models with different fixed effects? Then how would I draw conclusions from this? Would it be best to present the model $M_1$ as the best fitting model, because it is the most complex model of all of my models where $\Delta$ DIC is &lt;2 when dropping parameter $b_3$... or, that models $M_5$ and $M_6$ (and $M_1$) are the best fitting models when using parameters $b_3$, $b_2$, (and $b_1$) respectively. 
 Situation: I have fifteen years worth of monthly observations of price of a chemical, roughly 190 data-points. I want to develop a simple linear model of this price based on the price of other chemicals. (eg. price of milkshakes = f(price of milk, icecream, and syrup)) Over fifteen years the price-setting economics of chemicals change (eg. during the early days, milkshakes were runnier, so milk price was relatively more important. Now milkshakes with syrup are the rage, so syrup price has increased relative importance to milkshake price) Because the underlying price-setting mechanism is dynamic, I believe this makes developing a good model a bit trickier than, say, modelling the K constant of an unknown spring based on pairs of (weight,stretch) observations. In a spring experiment, I would include as many observations as possible, and I would also not care which subset of measurements I'd use to build my model because the K constant is same during every test) In the milkshake example, if I build my model from observations mostly in the 'runny milkshake' era, then if the future regime changes to preference for thicker milkshakes, my model will lose some predictive accuracy. Additionally, I've only got 190 precious observations to work with. So I think I don't dare do things like, "well, just build the model based on the last 24 points of monthly data, when the price setting regime would have been similar to what it will be next year". How I suspect I should build the model Training Set: train different models that use unique input variables model1: chemical_predict = f(chemA, chemB, chemC) model2: chemical_predict = f(chemX, chemY, factor1, factor2) model3: chemical_predict = f(chemJ, chemG, factor3) Validation Set: assess which of the three models works best Test Set: Get a fair assessment of the performance of my 'winning' model Confusion over scikit learn's cross_validation system Example code: My code gives me the test results for linear models built on five different 'folds' of data. I understand this as it builds a unique linear_model for each training set and then runs a test set on that same model. But I'm confused -- how would I retrieve the linear_model parameters that yielded, for example, the 3rd result in the scores array (ostensibly the best parameter set to use)). The cross_val_score just returns an nd.array; there is no way to see what parameter set created any one of those results. That this is not obvious scares me that maybe I'm using the cross_validation process incorrectly in the first place. So my questions are (in order of increasing broadness) How do I use the cross_validation to return the "best" fitting model based on the scores of the test set? The alternative is for me to explicitly do something where I run the folds and test-fits myself: kf = sk.cross_validation.KFold(len(X), n_folds=5, shuffle=True, random_state=123) for train, test in kf: X_train, X_test, y_train, y_test = X.ix[train,:], X.ix[test,:], y.ix[train], y.ix[test] lm.fit(X_train, y_train) Is there a scikit learn model that will give me a Degree 1 polynomial result, but as part of its optimization, penalize excessive parameterization, so that I avoid overfitting Any thoughts on my overall approach to this problem? 
 I'm trying to compute some mann-whitney tests on my datas, and I use the really handy "Realstats". As you can see on this link , there are several functions available. As a noob in stats, I used the MANN_EXACT function to gte my p-values, but after reading some articles I wonder if I shouldn't have used the MTEST function. I didn't find anything about the Mann-Whitney exact test on google. What is the difference between these two ? When should I use each ? 
 In reinforcement learning, where the state space is discrete and relatively small, a form of learning algorithm commonly used is the Q learning. This involves a look up table $Q(s,a)$ (Or you think of this as a matrix with the states on the row, and actions on the column). The entries are updated as the agent continues to learn. In the case of continuous state space, or a overly large state space, a look up table is not feasible anymore. And we turn to Q values with linear function approximation. In this case, Q values are now approximated by a linear combination of features. I was watching a lecture by David Silver on Reinforcement Learning, he mentioned that the look up table is just a special case of the linear function approximation. (The slide I am referring to begins at 28:43.) This never occurred to me, and he showed a little 'proof' that was not so clear to me. Anyone who could give some insights into the matter? Originally, I just accepted (without proof) that look up table and linear function approximation are just two independent things. It never occurred to me that the two are related. 
 It follows that $Y=\sum_{i=1}^{10} X_i \sim \text{Pois}(10\lambda)$. We want to estimate $\theta=e^\lambda$. As you say, a possible estimator would be \begin{equation} \hat\theta = e^{\bar X} = e^{Y/10}. \end{equation} Using the moment generating function of $Y$, \begin{equation} M_Y(t)=e^{10\lambda(e^t - 1)}, \end{equation} we find that \begin{equation} E(\hat\theta) = E(e^{\frac1{10}Y}) = M_Y(\frac1{10}) = e^{10\lambda(e^{1/10} - 1)} = \theta^{10(e^{1/10}-1)}, \end{equation} so $\hat\theta$ is biased. Some guesswork suggest that \begin{equation} \theta^* = e^{aY}, \end{equation} may be unbiased for suitable choice of the correction factor $a$. Again, using the mgf of $Y$ we find that \begin{equation} E(\theta^*) = e^{10\lambda(e^a - 1)} = \theta^{10(e^a-1)}, \end{equation} so this is unbiased if $10(e^a - 1) = 1$ which leads to $a=\ln\frac{11}{10}$ and $\theta^* = (\frac{11}{10})^Y$ as an unbiased estimator of $\theta=e^\lambda$. By the same reasoning, $\tilde\theta_i=(\frac21)^{X_i}$ is unbiased for $\theta$. Hence, $\bar\theta = \frac1{10}\sum_{i=1}^{10} 2^{X_i}$ is also unbiased but is it more efficient than $\theta^*$? It's up to you to find out! 
 There is this interpretation of the entropy $-\sum_i p_i \log_2 p_i$ as the average length (in bits) per character when using an optimal encoding of a message. Now, if we use the simple 3-letter case of A (80%), B (10%) and C(10%), the entropy is smaller than 1 bit, which seems strange (from a naive standpoint). How can this be reconciled with the interpretation above (if it can at all)? 
 You can use DIC to compare any of the models to each other without considerations to nesting of effects as long as at least one of the below holds The random effects structure is the same for all models If the random effects structure is different then all models are fit using maximum likelihood As far as the delta_DIC &lt; 2 policy, I know burnham and Anderson suggested a deltaAIC cut-off &lt; 2... but I am unsure how this will translate to DIC. But otherwise, IMHO you can just choose the best fitting model from all models 
 what are the assumptions of the chisquare outlier test? ( in R: chisq.out.test()) Is it only applicable if the data follow a certain Distribution? What is the idea of ist Definition of outliers? Any advantages or disadvantages of the test? 
 Code: Each triplet takes on average 0.512*1+0.384*4+0.096*7+0.008*8=2.784 bits, or 0.928 bits per character. With coding 4-character, 5-character etc. groups, you can further decrease the number of bits per character. With long groups of characters and optimal coding, you can make the number of bits per character as close to the entropy as you want, but not less than the entropy. 
 The example I use below to frame my questions is a bit contrived, but I am hoping the answer will help to clarify my understanding of more complex, related issues. Suppose we know in advance that the proportion of men (vs women) in a large metropolitan area is 50% (e.g.- from a just-completed census or recently published demographic data). A researcher is seeking to evaluate whether the proportion of men that smoke is different than the proportion of women that smoke in this population. A random sample of 40 adult smokers is performed, and their gender is recorded. Another independent random sample of 40 adult non-smokers is performed, and their gender is similarly recorded. The results are summarized below: Now to my thinking the test of interest would be a test of homogeneity (as opposed to a test of association or independence). I calculated the following p-values from this data: Z-Test of difference in proportions with continuity correction (cc): 0.0442 Lognormal Z-Test of difference in proportions with cc: 0.0433 Fisher Exact Test: 0.0435 Product Binomial Exact Test: 0.0930 Not surprisingly, the first 3 p-values or all very nearly the same. But the last p-value that I have labelled "Product Binomial Exact Test" is more than twice as high. I arrive at this p-value by summing the probabilities of all possible outcomes that have an equal or lower probability of occurrence compared to the observed outcome. The probability of any specific outcome is the product of 2 independent binomials: Probability that x of the 40 non-smokers are women, given that 50% of the population are women ; multiplied by Probability that x of the 40 smokers are women, given that 50% of the population are women So for the specific outcomes observed we have: Probability that 25 of the 40 non-smokers are women, given that 50% of the population are women (=.0366); multiplied by Probability that 15 of the 40 smokers are women, given that 50% of the population are women (=.0366) = 0.001338 My questions are as follows: 1) Is the Product Binomial Exact Test (PBET) calculation I have described above the correct way to calculate the probability of the observed outcome, given that we know the true proportion of women is 50% in this population? In effect, I think that I am calculating the probability of the observed outcome under a null hypothesis that the proportion of female smokers = proportion of male smokers = 50% 2) Why would the other p-values all be so much lower than PBET value? The column totals are fixed by design, and I purposely contrived the "observed" counts so the sample marginal proportion of women came out to equal the known population marginal proportion of women (50%). So in computing the other p-values we are still using the same proportion (50%) to calculate the test statistics. These tests are - as I understand it - effectively calculating the probability of the observed outcome under a null hypothesis that the proportion of female smokers = the proportion of male smokers, but NOT that the proportion is equal to 50% or any other specific proportion. My thinking is that the difference is due to the fact that all these other tests are NOT conditioned on the true proportion being equal to 50%...and the next thing I was about to type feels like circular reasoning (can u guess? ...something Bayesian...?). I hope the community does not feel I am beating a dead horse on this issue - I have posted a number of similar and related questions recently as I try to develop my intuition in this area of inferential statistics. Thanks for your continued patience and guidance with my (very slowly) developing knowledge! It is much denser than I imagined...comments and answers from Scortchi, Jmhuber and Bjorn in particular have been extremely helpful and greatly appreciated to date :) 
 I would like to perform General Linear Model with one response variable and two predictor variables (1 numeric, 1 categorical). The response variable is positively skewed and transformations don't seem to bring it closer to normality. I tried sqrt, logarithmic, inverse and Box Cox transformations (performed by SPSS). Is there any other way to transform it? Can Generalized Linear Model work with such skewed data (I'm not very familiar with it)? Here are the data: Here are two examples of my distributions: 
 The distribution you get is good news, not bad. The distribution is close to symmetric on a logarithmic scale. That means that we don't expect the distribution to be problematic to deal with on that logarithmic scale. Note further that few methods expect the outcome or response variable to have a marginal normal distribution. Regression certainly doesn't. An approximately symmetric distribution like this will be well behaved. That doesn't rule out surprises or complications arising from other variables in your data, but we have no precise information on those variables. Further, why did you add 1 before taking logarithms? Was it because there are some zeros in your data? Know that generalized linear models with logarithmic link have made that fudge unnecessary. That's too new an idea for some fields to have caught up, as the key work was published as recently as 1972. Generalized linear models with logarithmic link just expect that means are positive, and that doesn't oblige all values to be positive. Not only do generalized linear models not have problems with skewed responses; dealing with those skewed responses using appropriate links such as the logarithm is arguably one of their main benefits. NB: General linear models and generalized linear models are not the same family. EDIT: I plotted the data. It can all go on one graph, but I fall short at offering a model as I have no idea what kind of model makes sense. I chose a square root scale to pull in the outliers (wilder values) a bit. That's arbitrary, except in so far as it copes cleanly with the zeros, as 0 maps to 0 without fudge. There is one A standing outside the others at bottom right. The ATs fall into two groups, perhaps. The Ts fall into two groups, perhaps. Perhaps the zero responses are qualitatively different as well as quantitatively. It's tempting to note that two apparent anomalies are for points with zero response. (A small merit of the transformation is that the zeros stand out. That's clearer on a quantile plot than a histogram, so I give a quantile plot too. The quantile plot below shows the distribution of the roots of the Response, but labels it according to the raw Response value. Histograms often obscure fine structure in data.) Does any of those convey some biological meaning or message? It's likely that any analysis ignoring that fine structure might obscure as much as it clarifies. To summarize so far: Mild skewness in your data can be handled by a mild transformation. Your bigger problem is identifying what model makes sense for your data. 
 [This question inspired by work by Jason Thornton et al (see https://cryptome.org/2012/05/person-large-area-spy.pdf , Equation (4))] I am interested in modelling a distribution over the HSV color space (basically $[0,1] \times [0,1] \times [0,2\pi)$ with wraparound for the final angular component): given sampled colour measurements, I would like to estimate the parameters for the distribution. Are there known/studied classical distributions applicable to this space (or on $R^2\times S^1$), for which we know how to estimate parameters sensibly? [Besides following the approach by Thornton, it seems one can embed the HSV data into a 4-dimensional space, by using ideas from https://en.wikipedia.org/wiki/Mean_of_circular_quantities , and then building and possibly marginalizing a 4-dimensional Gaussian, but I'd like to know if there is a better approach known, rather than devising ad-hoc approaches unnecessarily.] 
 I have three data sets that, when joined, have O(320) independent variables for a classification problem. Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous. I'm at a loss as to how to proceed. How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable? How do you decide between variables? What calculations can you go on to supplement your gut and experience? How do you avoid throwing away significant variables? A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant. How to accomplish that? Should I just toss all of it into a logistic regression and see what happens, without any forethought? More detail in response to comments: Classification. Many more observations than columns. Yes, big oh notation meaning approximately. Linear model at first. Also interested in boosted models in addition to logistic regression. 
 Yes, you are right that $\lambda$ is . When you fit the Weibull model with the output is even more confusing. Suppose you have done , to fit a single Weibull model to some data with no covariates. The shape parameter of the fitted Weibull model is and the scale parameter is . When I say "shape" and "scale" I am referring to the parameterization that is used by the R functions such as and Wikipedia (Wikipedia calls this the "standard parameterization"). Obviously uses "shape" and "scale" with completely different meanings. The parameterization that uses for the Weibull distribution is explained in (but only in the Examples section, so I didn't notice it for a long time). 
 The key thing to do in this case is to properly account for the feature selection as a part of the modeling process. This means that inside of your CV loop, you'll apply the feature selection process to just the training data, and then to the testing data separately. This is done to minimize information leakage between training and testing. Failure to do this can result in overly-optimistic results and spurious findings. Elastic net regression allows one to fit a linear model, perform feature selection and control for correlation among variables all at once. The R package supports GLMs (i.e. includes logistic regression as an option) with sparse matrices. This will be a good first-pass linear model of the data. However, be aware that the features selected by a linear model may have little to do with what's important in a nonlinear model (cf this answer of mine ). Next steps are models that are particularly apt at handling sparsity in feature vectors. Factorization machines were conceived to solve this particular task, but they are naturally a bit more complicated than linear models.(1) Linear SVMs are another option. Inner products of sparse vectors are cheap to compute. There are even packages like that work in the primal SVM problem, which means that the complexity is driven by the number of features rather than the number of observations. This can be a nice property, but keep in mind that just because a tool is efficient, it's not necessarily the right one for the job. I'm often hesitant to assume that methods like PCA will solve my problems because it's not "$y$-aware," by which I mean that the projection may not contain any useful information about the target of the modeling process. In some cases, it will even discard or destroy the only useful information! On the other hand, some people insist that its application is usually good enough for their needs: your mileage may vary. PLS is a regression method to perform "$y$-aware" regression. I'm not terribly familiar with boosted models, so I'm afraid I can't help there. (1) Steffen Rendle "Factorization Machines" 
 I have 4 continuous variables as my predictors (say, cognitive ability and three components of self-esteem) and the 3 two-way interactions between cognition and each self-esteem component. I am using a binary logit model with 9 fixed trials for the dependent variable. My problem has to do with further understanding the nature of the interactions. Is there a way to plot them or make numerical analyses? I can only find explanations for regression with two predictors and one interaction. Thank you!!! 
 I think you can refer to the two-way ANOVA table regarding to your question. I find one webpage for you in this case. two-way ANOVA link In the following two-way ANOVA table, $$SS_A=b\times n \sum_{i=1}^a(\bar{y}_{i..}-\bar{y}_\cdots)^2$$ $$SS_B=a\times n \sum_{j=1}^b(\bar{y}_{.j.}-\bar{y}_\cdots)^2$$ $$SS_{AB}= n \sum_{i=1}^a\sum_{j=1}^b(\bar{y}_{ij.}-\bar{y}_{i..}-\bar{y}_{.j.}+\bar{y}_\cdots)^2$$ $$SS_{E}= \sum_{k=1}^n\sum_{i=1}^a\sum_{j=1}^b(\bar{y}_{ijk}-\bar{y}_\cdots)^2$$ where $$\bar{y}_{i..} = \frac{\sum_{j=1}^b\sum_{k=1}^ny_{ijk}}{b\times{}n} $$ and other means have the similar meanings; $a, b$ are the number of levels in each factor; $i,j$ correspond to the level index of each factor, while $k$ is the index for the repetition. In the design provided, you will have $i=\{1,2\};j = \{1,\cdots,4\}; k=\{1, \cdots , 5\}$; $a=2;b=2$. Suppose that $i$ is the Blend type index, $j$ is the food source, then $$\bar{y}_{1..} = \frac{\sum_{j=1}^4\sum_{k=1}^5y_{ijk}}{4\times{}5} $$ is the mean for the first Blend type, i.e., the average of the first 5 rows in the data table. The total sum of squares consists of four parts, $SS_T = SS_A + SS_B + SS_{AB} + SS_{E} $, with each part seen as the effect of the variance caused by the corresponding factor(interaction). By comparing the variance allocated to the factor (consider this as the variance by the introduction of a new level, then the ratio $MS_X$ can be seen as the variance on each "level") the with the average variance caused by the error$MS_E$, we can get the idea that whether the variance introduced by that factor $X$(the subscript in $SS\mbox{ or }MS$) is significant. In other words, the ratio $F_0=\frac{MS_X}{MS_E}$ is the testing for whether factor $X$ is significant or not. Back to your question, you need to average the data factor-wise. You will need to get the mean for each level of factor $A$(the Blend type), $\bar{y}_{i..}$,the mean for each level of factor $B$(the food source), $\bar{y}_{.j.}$, and the average of all the measurements $\bar{y}_\cdots$ to analysis the variance caused by each factor and the effect of their interaction. 
 I think that's always a struggle. If you are using R the best thing that I have seen for basic data exploration is . If you show your data frame then I can show you how to use it. I shouldn't have assumed that you were using R, sorry. If you were using R you would read the data into a structure and set the field names to something you like. You would do something like: But if you are not at all familiar with R then it is probably best to go with the software that you know. But if you do want to dip into R then this function is a nice way to explore this kind of data visually. 
 The 'cross-entropy' method of derivative-free reinforcement learning is defined as follows: I've come across two problems here : Namely, proving that CEM does not always reach a local maximum, and that it does when weighted against the reward (see below). How would I go about proving this? I've been given the hint that I should derive the lower bounds of the importance sampling estimator, but I don't know how to do this. Here is the problem as written: 
 Spark ASL supports only (user, item, measure) implicit pairs, libfm supports any number of features but no implicit feedback ranking (only classification/regression). Is there good articles/implementations about algorithms that allow any number of features and support implicit feedback? 
 I have some trouble with understand the second equality in the proof of theorem 6; Using the lemma we can just plug in $\delta_{0}-v$ and minimize over that w.r.t $v$, but howcome we have the additional $\delta_{0}$ outside? 
 I regress from my data lnhr on lnwg, first with the default OLS (POLSiid), second with the robust unclustered option (POLShet), third with the cluster robust option (POLSpanel). I understand with the Beta stay the same but I don't get why the standard error, and so the t-value, changes. 
 I have a "real" and estimated HMM model given as $(\pi,\mu, \nu)$ and $(\pi^{\text{est}},\mu^{\text{est}}, \nu^{\text{est}})$, where $\pi$ is initial state distribution of Markov chain, $\mu$ is state transition distribution and $\nu$ is emission distribution. All these distributions are continuous and given as following functions in $R$: $\pi(x)$ is a CDF (I can generate PDF too) $\mu(x\mid x')$ and $\mu(x' \mid x)$ are conditional CDFs for forward and backward transitions (I can obtain PDFs) $\nu(O_t\mid x)$ is conditional PDF for observations $O_t$ given hidden state $x_t$ (I can obtain CDF). The conditional CDF is $$F_{X\mid Y}(x\mid y) = Pr(X\leq x \mid Y=y )=\frac{1}{f_Y(y)}\int_{-\infty}^{x}f_{X,Y}(t,y)dt$$ and is artificially defined to exist anywhere, being $F_X(x)$ where $f_Y(y)$ is too small. The same for conditional density. Suppose I have a sequence of observations $O = \{O_1, O_2,...,O_T\}$. I want to compute any sort of likelihood of this sequence being generated by "real" and estimated model $$ L(O_t\mid \{\pi,\mu,\nu\}) $$ to compare them. This is my question. What have I tried : The advices I found so far look corresponding to a discrete HMM, where we can easily calculate probabilities of hidden states and observations. Google told me that I can use Forward algorithm to compute probability of observations given the model $$P(O_t\mid \{\pi,\mu,\nu\}) $$ that I can use to compare models. Actually, my estimation program (that has generated my estimate $(\pi^{\text{est}},\mu^{\text{est}}, \nu^{\text{est}})$) already uses SIR ( sampling-importance-resampling algorithm ): 1). $\alpha_0 = \pi$ 2). For each $1 \leq t\leq T$: Generate $N$ samples (just big number) from CDF $\alpha_{t-1}$, for each sample $x'$ evaluate conditional CDF $\mu(x\mid x')$ and sample single $x$ from it. Set weights $p_x$ proportional to $\nu(O_t \mid x)$ and generate CDF for $\alpha_t$ from this set $&lt;x_i,p_i&gt;$. So, I tried to approach it straightforward by estimating continuous model on the grid. In discrete case, according to the theory (see in the article above (8.14) or this ): $$P(O \mid \{\pi,\mu,\nu\})=\sum_{i=1}^{M}\hat{\alpha}_T(i).$$ Where probabilities $\alpha_t(j)$ for each state $j$ are computed as: $$\alpha_t(j) = \sum_{i=1}^N\alpha_{t-1}(i)\cdot a_{ij}\cdot b_j(O_t), $$ where $a_{ij}$ is the transition probability of a hidden Markov chain and $b_j(O_t)$ is a probability of observation $O_t$ given state $j$ (state observation likelihood). Straightforward approach to compute $\alpha_T(x)$ using formula with densities requires $T$ nested integrals (if it is correct): $$ \alpha_t(x)=\int_{\mathbb{R}}\alpha_{t-1}(x')f_{\mu}(x \mid x')\nu(O_t \mid x)dx' $$ I have evaluated conditional densities $\mu(x \mid x')$ and $\nu(O_t \mid x)$ on the grid $300\times 300$ and $300\times T$ respectively, i.e. took the values of $x$ from $-2$ to $4$ with the step $0.02$ and computed conditional density values. So, my attempt is where and are values of respective densities on the grid with $\Delta x = 0.02$. But this gives me very big value that cannot be probability. This seems to be mathematically correct result of such an expression, because it is related to integrating conditional densities over their condition , which means that I made a mistake somewhere earlier. EDIT: I have already(yesterday) determined that my model belongs to General State Space models, and Particle filter seems to be a particular case. Formulas I found so far are very close to what I need and the model setup seems similar to mine, but the resulting equations ( one , two ) are a bit confusing to me and until now I couldn't understand how to implement them in the code. @matthew-graves's answer gave me a very useful link to Wikipedia article on particle filter (see paragraph "Unbiased particle estimates of likelihood functions") . According to it, my likelihood should look like (lets forget at the moment about initial distribution): $$L(O_1,...,O_T)=p(O_1,...,O_T) = \prod_{t=1}^T p(O_t \mid O_1,...,O_{t-1})$$ where $O_t$ are my observations. The probabilities in the product are estimated as $$\hat{p}(O_t \mid O_1,...,O_{t-1}) = \frac{1}{N} \sum_{i=1}^N \nu(O_t\mid x_t^i)$$ where $\nu(O_t \mid x)$ is the conditional PDF for observations and $x_t^i$ are $N$ samples from the posterior distribution $$p(x_t \mid O_1,...,O_t)$$ which seems to exactly correspond to the distribution $\alpha_t$ that is computed in my algorithm (see quote) and for which I have CDF and can easily sample from. The supporting theory that I have for my algorithm defines $\alpha_t$ as: $$\alpha_t(x)=P(x_t=x \mid O_1,...,O_t,\{(\pi^{\text{est}},\mu^{\text{est}}, \nu^{\text{est}})\}) $$ So now it seems I can at least estimate likelihood for $\{(\pi^{\text{est}},\mu^{\text{est}}, \nu^{\text{est}})\}$, however it is still a bit unclear to me how to use it for initial model. Results With this approach, I get values of magnitude like 1e-17 or even 1e-30 for both real and estimated model. I don't know whether it is correct, I found even lesser values here . Maybe the algorithm is correct, but I don't see the growth of likelihood for iterative models $(\pi^{\text{est}},\mu^{\text{est}}, \nu^{\text{est}})$ and don't know whether it is the problem of my algorithm of the likelihood is computed incorrectly. Searching for the answer, I keep question still open and welcome any suggestions. EDIT 2. The problem was in my model - incorrect computation of backward (smoothing) distribution. After fix, model can detect the difference between two data sets: Good source with likelihood formula . P.S. You don't need $\mathcal{O}(N^2)$ formula for smoothing probabilities! You can use less precise, but workable FFBSa-GDW that just multiplies old weights by the emission density: $w_{t\rightarrow t+1, \ \text{backward}}^{i} \sim w_{t, \ \text{forward}}^{i}\cdot \nu(O_{t+1} \mid x_t^{i})$ and then normalizes them. Thanks to everybody and good luck! 
 I have a glmm model $y \sim b_1 * b_2 * b_3 + random$ where $b_i$ are the fixed effects. I am using DIC to compare models and select the best fitting model. I also have some options in setting up my model. For example, $b_1$ is the average spring temperature in the individuals first 5 years of life. This can be calculated in two ways: Approach 1 - stringent: Average temperature is calculated only in cases where all five years have data, such that if an individual was born in the year $x_0$, we would need information on temperature for years $x_0$ through $x_4$ to get the 5 year average. Approach 2 - relaxed: Average temperature is calculated for all cases where at least one of the 5 years has data recorded. Therefore, if we had the temperature for years $x_0$, $x_1$ and $x_4$ the score for $x_0$ individuals would be $\frac{(x_0 + x_1 + x_4)}{3}$, and for $x_1$ individuals $\frac{(x_1 + x_4)}{2}$ etc.. Can I use DIC based model selection to choose approach? The random effects and all other model structure is the same, it's just a case of substituting fixed effect $b_1$. Because the data is more stringently selected in approach 1 it greatly reduces the sample size, but it does mean the samples are more precise: does the different sample size in either model have impacts on my decision to use DIC based model selection methods? Judging from the answer on my previous question I suspect the answer is that I can make this comparison. However, it would be really excellent to get a citable source on this. 
 I am new to the topic of whitening transformation. In financial time series studies on long memory in data, I have seen that researchers apply an AR(p) model to detrended return series in order to retrieve "white" residuals (white noise residuals). The whitening (or pre-whitening) is done a single detrended return series with stationary increments. The common whitening methods are: Mahalanobis or ZCA whitening, Cholesky whitening and PCA whitening. Which whitening method describes the approach mentioned above (using an AR(p) model)? Secondly, in case the method above is PCA whitening. What is the link between between PCA analysis and AR(p) model? Is there a link between coefficients in the AR model and PCA? 
 Let $f_{X,Y}(x,y) = c \cdot \mathbb1(0&lt;x&lt;y&lt;1)$, where $\mathbb1$ is the indicator function $$ \mathbb1(x) = \begin{cases} 1, \text{ if $x$ is true }\\ 0, \text{ if $x$ is false. } \end{cases} $$ So $$ \int_0^1 \int_0^1 f_{X,Y}(x,y) dy dx = \int_0^1 \int_0^1 c \cdot \mathbb1(0&lt;x&lt;y&lt;1) dy dx = 1, $$ which implies $$ c = \frac1{\int_0^1 \int_0^1 \mathbb1(0&lt;x&lt;y&lt;1) dy dx} $$ Now $$ \int_0^1 \int_0^1 \mathbb1(0&lt;x&lt;y&lt;1) dy dx = \iint_{0&lt;x&lt;y&lt;1} dx dy = \int_0^1 \int_x^1 dy dx = \int_0^1 \int_0^y dx dy. $$ The last expression is easier to get $$ \int_0^1 \int_0^y dx dy = \int_0^1 y dy = \left.\frac{y^2}{2}\right\vert_0^1 = \frac12, $$ therefore $$ c = \frac{1}{\iint_{0&lt;x&lt;y&lt;1} dx dy} = 2 $$ which makes $f_{X,Y}(x,y) = 2\cdot\mathbb{1}(0&lt;x&lt;y&lt;1)$. Now, what you asked and a little more $$ f_X(x) = \int_0^1 f_{X,Y}(x,y) dy = 2 \int_0^1 \mathbb{1}(0&lt;x&lt;y&lt;1) dy = 2 \int_x^1 dy = 2 (1-x) \\ f_y(y) = \int_0^1 f_{X,Y}(x,y) dx = 2 \int_0^1 \mathbb{1}(0&lt;x&lt;y&lt;1) dx = 2 \int_0^y dx = 2 y \\ E(X) = 2 \int_0^1 x (1-x) dx = 2 \int_0^1 x - x^2 dx = 2 \left.\left(\frac{x^2}{2}-\frac{x^3}{3}\right)\right\vert_0^1 = 2 \left(\frac12 - \frac13\right) = \frac13 \\ E(Y) = 2 \int_0^1 y y dy = 2 \int_0^1 y^2 dy = 2 \left.\frac{y^3}{3}\right\vert_0^1 = \frac23 \\ E(X^2) = 2 \int_0^1 x^2 (1-x) dx = 2 \int_0^1 x^2 - x^3 dx = 2 \left.\left(\frac{x^3}{3}-\frac{x^4}{4}\right)\right\vert_0^1 = 2 \left(\frac13 - \frac14\right) = \frac16 \\ E(Y^2) = 2 \int_0^1 y^2 y dy = 2 \int_0^1 y^3 dy = 2 \left.\frac{y^4}{4}\right\vert_0^1 = \frac12 \\ Var(X) = E(X^2) - E(X)^2 = \frac16-\frac19 = \frac1{18} \\ Var(Y) = E(Y^2) - E(Y)^2 = \frac12-\frac49 = \frac1{18} \\ f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_y(y)} = \frac{\mathbb1(0&lt;x&lt;y&lt;1)}y = \frac{\mathbb1(0&lt;x&lt;y)}y \\ f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_x(x)} = \frac{\mathbb1(0&lt;x&lt;y&lt;1)}{1-x} = \frac{\mathbb1(x&lt;y&lt;1)}{1-x} \\ E(X|Y=y) = \int_0^1 x \frac{\mathbb1(0&lt;x&lt;y)}{y} dx = \frac1y \int_0^1 x \mathbb1(0&lt;x&lt;y) dx = \frac1y \int_0^y x dx = \frac1y \left. \frac{x^2}2 \right\vert_0^y = \frac1y \frac{y^2}2 = \frac y2 $$ 
 I have 2 datasets. One is time series data of sale of homes by region by type: These data were manually entered by realtors and given to us by a data feed. We also get another data set of registered homes from the county. However there is a 2-month lag for this data set. It is structured as follows: The weight is calculated as: Retail Transactions/Registered values. So, assuming the weight is correct, I have to predict the Registered values for 201604 and 201605. I have tried regression, time series analysis, etc., but I wasn't able to get to the threshold weight. Is this a candidate for neural net time series? Is there anything else I can do assuming the weight is correct. Assuming it is incorrect, what is the best approach to determine what sliding window of values to use from the transactions to accurately predict the registered values? 
 You may start to look at the examples from BNLEARN here Learn a BN model can be viewed as two steps Step 1 is getting the structure / getting conditional independence for your random variables. Step 2 is getting the parameters for a given structure. Structure learning and Parameter learning two chapters clearly tell you what to do. In addition, the data set included in this R package is very similar to your data. 
 Wanting to work out the statistical power of a repeated-measures t.test. This requires an estimate of the SD of differences within pairs (the correlation between measures for a given person is what powers-up repeated measures over independent samples t.test). What I know is the test-retest reliability of the test (.8) and the SD of scores on the test (15). Question: What is the formula to convert these into the standard deviation of within-subject differences? Is it: i.e., sqrt(twice the unreliability) * the test SD? 
 Sorry the title is so vague - I don't know how else to ask this. Essentially, what is a real-life example showing why: $\frac{_aC_k}{_nC_k}=\frac{_aP_k}{_nP_k}, where \ a&lt;n$ is true? 
 With Matlab, I specified 9 ARMA(p,q)-GARCH(1,1) models and fitted all of them to monthly return data (I used GARCH(1,1) for every model but changed the ARMA order). Here is a very small example For all of the 9 models I got the following warning message: Warning: Linear inequality constraints are active; standard errors may be inaccurate. In arima/estimate (line 1100) Why could this happen and what does it mean? (I only have 351 returns. Is this too less?) 
 What does $S$ mean in estimates like $$s^2 = \frac{S(x_1^2)}{n} - \left(\frac{S(x_1)}{n}\right)^2 = \frac{S(x_1^2)}{n} - \frac{S(x_1^2)}{n} - 2\frac{S(x_1 x_2)}{n^2}$$ (for context look "The probable error of a mean", Student, Biometrica) And what does it mean in $$\sigma_1 = \frac{1}{n} \sqrt\frac{\pi}{2} S(|x-\overline x|)$$ $$\sigma_2 = \sqrt{\frac{1}{n} S(x-\overline x)^2}$$ (look Fisher, "One the Mathematical Foundations of Theoretical Statistics") 
 I am trying to estimate the regression model, say standard linear model, with the error term having a Pareto distribution instead of normal. Although it is fairly straightforward to construct the maximum likelihood function, it is practically not easy because we do know how to obtain the estimate for the minimum of the error term as it is an unobserved variable. What I can think of to do it is to estimate the coefficient and Pareto parameters with the grid of predetermined the minimum threshold Pareto parameters and choose the one with the largest likelihood value. Is that how people practically do that? The model I have in mind is: $y_i=\beta_0 + \beta_1 x_i + \epsilon_i$ where $\epsilon_i \sim Pareto(k,\alpha)$ where $\epsilon_i&gt;0$, and $\epsilon_i &gt;k&gt;0$. So, you would estimate $\beta_0$, $\beta_1$, $k$ and $\alpha$. 
 I was suddenly bombarded with a new topic at the end of my dissertation which other student was supposed to do and its related to statistics which was not my field till now but I am reluctantly entering it. So please dont mind if the question here is rather stupid. The problem I have is: I need to rank a set of items based on "validity". The item ranked 1 in this case would be more "valid" than the item which is ranked 2 and so on. To decide the ranks, there are 9 different criteria with different ranges. Each criterion measures something different and on a different scale. However, I make sure that each criterion is positively related to validity. Higher the "score" more the "validity". For example, Criteria_1 : Scores presence of an element "a" in item x as 0.5, Criteria_2 : Scores presence of another element "b" in the item x as 1.0, Criteria_3 : Scores presence of another element "c" in the item x as 0.7, ..., Criteria_N : Scores presence of another element "z" in the item x as 5.0 Some criteria just score the presence of absence of an element with "1.0" or "0.0", respectively. To make a summary score like Criteria_Sum, I need to add up all these criteria. BUT to do it I need to make the apples and pears comparable. In this case should I "normalize" the scores or "standardize" them? If the answer is "either" why "normalize" or why "standardize"? For further information: The items to be ranked in this case are graphs and we need to check if the graphs are valid or not valid. If the connections between nodes and edges are valid or not valid. 
 When fitting with two correlated independent variables without regularisation, you can get two coefficients offsetting each other, eg one large positive and the other large negative. Is there a specific name for this effect? 
 I am working with from Python library , a function which implements linear classification with a Stochastic Gradient Descent (SGD) algorithm. The function can be tuned to mimic a Support Vector Machine (SVM) by setting a hinge loss function and a L2 penalty function . My train data set consists on approximately $n=100,000$ samples with $159$ features (some features are categorical and have been binarized, some other features are quantitative and have been scaled to interval $[0,1]$). The data has two possible labels $\{0,1\}$, but the data set is highly unbalanced $-$ approximately only $4.5\%$ of samples are labelled $0$. Hence I have introduced class weights into my , with the weight assigned to class $i$ corresponding to the fraction of the data labelled with $j$, $(i,j) \in \{0,1\}, i \not= j$ $-$ so class $0$ has weight $95.5\%$. I also mention that the of the SGD algorithm is set to . Now, here is my issue: I wanted to test the convergence of the algorithm, so I set equal to to obtain the which the algorithm computes at each iteration $-$ for information, the algorithm shuffles the data then goes through all of it, repeating this operation $n_{iter}$ times, such that it makes $n \times n_{iter}$ computations; the is equal to the accumulated average loss and is given at the end of each iteration or epoch. Then, I train my machine through a $10$-fold cross-validation procedure, such that for each cross-validation fold I have a final computed by $-$ which as explained corresponds to an average of $n \times n_{iter}$ losses. Simultaneously, for each cross-validation fold I compute the score (accuracy rate) of my classifier on the training data. I have done this for $n_{iter}=10,100,500,1000$, I show results above: Manifestly, the algorithm converges with respect to the hinge loss, which is $\max[0,1-y\,f(\mathbf{x})]$ where $f$ corresponds to the trained machine, however the accuracy rate, which is $1_{\{sign(f(\mathbf{x}))=sign(y)\}}$, seems to have a rather erratic trajectory with respect to $n_{iter}$. My question is: is this normal, or at least possible? Intuitively, I would say that it is mathematically possible to have these results, however I do not know if this is normal or if it indicates that something is going on with my code. Has someone already experienced this phenomenon? Could someone shed more light on this? P.S.: I know accuracy rates are poor, however this is due to the quality of the data. [EDIT] The above computations have been undertaken for some value $\alpha^*$ of parameter that I had already found to be optimal by cross-validation, setting $n_{iter}=100$. I wanted to test the sensibility of the training accuracy rate for $\alpha^*$ to the number of SGD's iterations $n_{iter}$. 
 Other than computational ease/requirements - are there reasons to band continuous variables? It seems to be a thing at my work place where everyone would split continuous data into 20-ish categories either by a normal distribution or a equal split, depending on what would give the best-looking one way graph with the response variable. Is this normal? I don't really see why I would want to do this at all (other than computational limitations, but even then, why only 20-some groups?)- since it feels like manipulating my data to look like how I want it to look, and I can't imagine that whatever CI or p-value that I'd get from this would be accurate. 
 I think that in practice I would use user20160's answer, but I'd like to offer a few warnings and a possible alternative. Warning 1: The definition $T_{X\rightarrow Y} / H(Y_t | Y_{t-1:t-L})$ only works for discrete random variables. If you you have continuous random variables, then $H(Y_t | Y_{t-1:t-L})$ could be negative and your quantity would no longer be between zero and one. Warning 2: Suppose that $H(Y_t | Y_{t-1:t-L}) = \epsilon$ and $H(Y_t | Y_{t-1:t-L},X_{t-1:t-L}) = 0$. You will get a score of 1. In the limit of $\epsilon\rightarrow 0$, you continue to have a score of 1 even though, in some sense, the amount of information transferred is going to zero. Alternative 1: If you are using discrete random variables, note that $0 \leq T_{X\rightarrow Y} \leq H(Y) \leq \log k$, where $k$ is the number of discrete possibilities $Y$ can take. Then if you define $T_{X\rightarrow Y} / log k$, you still get something between zero and one, but it will only be one if $Y$ is maximally uncertain and $X$ is a perfect predictor. This would bypass warning 2, if that is a concern in your case. Alternative 2: For the continuous case you could define your score as $T_{X\rightarrow Y} / I(Y_t ; Y_{t-1:t-L},X_{t-1:t-L})$. The denominator is always non-negative and $T_{X\rightarrow Y} \leq I(Y_t ; Y_{t-1:t-L},X_{t-1:t-L})$ (you could see that using the chain rule, e.g.). You would get one if $I(Y_t ; Y_{t-1:t-L}) = 0$ which could loosely be interpreted as saying all the information that can be predicted about $Y_t$ comes from $X$, and not from $Y$'s past. 
 Based on a random sample of size n,denote the $\alpha$-level critical region for a hypothesis testing problem $H_0$ vs. $H_1$ as $C_{\alpha}$,if we change the level to $\alpha_1&lt;\alpha$,then can it be said that $C_{\alpha_1}\subseteq C_{\alpha}$ or $C_{\alpha}\subseteq C_{\alpha_1}?$which one will be true? As we know that if the rejection of null hypothesis is not so serious,one can take a high value as the level of a test.now if we take the level of a test a small value i.e.,the probability of rejecting the true null hypothesis is considered as a serious error,does that imply that the new critical region will be a subset of the previous? 
 If you are interested in modeling the differences between group and pre and post event (time) then those should be fixed effects in your model. As you are not interested in subject specifically, but different individuals cause variation in your response variable, it would be a great candidate for a random effect intercept (1|subject). If you really only have three (although maybe that was just the example you used), that is not really enough levels to include it as a random effect and you should include it as a fixed effect instead. Here is a great discussion on fixed and random effects: https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/ which includes a decision tree for choosing between whether to include something as fixed or random. 
 I am comparing lasso implementation in liblinear and glmnet and I can see one of the coefficients from liblinear fit is larger than 1 , while for the same data set, glmnet coefficients are all between -1 and 1. Since the data is normalized, I was curious if coefficients in these models can be outside the range of [-1, 1] and if not, does it mean something is wrong ? e.g the optimization has not converged ? I appreciate more insight, 
 I have a logistic regression in which i transformed geographical distance measured in km using a natural log. I've have run the regression, and now i am having trouble how to interpret the findings. I have found a b-value of .21 and an odds ratio of 1.23 (it is statistically significant) Do i need to convert these to orginial values using e^x? or do i need to do something different? Also would it be better to use log10? Thanks for your time 
 Your data set is too small. So It is very hard to estimate and forecast the values with such a small data set by using methods as auto regressions and others. Either way, you may use simple linear regression on Retail Transactions/Registered values, But It will hard to say about the accuracy of prediction. The data set should be large enough for forecasting. 
 I want to study the bias in a recommender systems.So,in each iteration,the recommender systems update the model using the coming data(new ratings) from users.and then, the RS recommend a top N items to each user and so on... So,we can see the process as a closed feedback loop between users and the RS.we can see also that in each iteration, users are influenced by the current model in the types and variety of items that they are likely to discover(for example items that are more often recommended at the top of the list are more rated more often by the users, which will in turn affect the next batch of data). So, I want to know what metric can capture this bias (in data used to update the model) in each iteration in order to study its influence on the RS as iterations increase. 
 As you said, with $L_2$ regularization, the update is: \begin{equation} w \leftarrow w - \eta_t\lambda w - \eta_t y_t x_t 1_{\{...\}} \end{equation} The first two terms corresponds to a simple scaling. It is updated with the line: where the weight are not really updated: to be fast, only a scaling coefficients is updated. The last term of the update is done with the line: The key element is that if the update is sparse, we don't want to update all the weights. The second step only apply the non-zero updates, but the first step would affects all the weights. To avoid that, only a scaling coefficient is updated. You might want to check the code of the weight vector . 
 I think you hit on the main reason people tend to do this (in the context of linear models or glms) here depending on what would give the best-looking one way graph with the response variable. Especially in the context of a linear model, binning a continuous variable will introduce some non-linearity into the fit of that variable, allowing the model to adapt to a non-linear trend in that variable. Another way to say this, when a pure linear fit in a certain variable produces considerable bias, binning the variable into multiple range indicators and fitting a parameter for each range will lower this bias. Of course, you are introducing many more parameters for your model to determine, so there is an associated increase in the variance of your model (its sensitivity to the data) which has the opposite effect of harming your out of sample perfomance. If the non-linearity in the trend is severe enough, the lowering of this bias will be the dominant effect, and you will end up with a more predictive model. A subtle point is that, while the bias can be lowered for the model as a whole, there maybe considerable remaining local bias. If the true underlying effect changes rapidly in a single of your buckets, you will have a case where the bias is lower at the center of mass of the bucket, but at the edges of the bucket the model will be quite biased. With that said this is still generally considered a poor practice (from a statistical or modeling standpoint) as there are better options available. Often introducing a quadratic term into the fit can be effective. You can easily discover whether this will be the case with some exploratory data analysis and plotting. I generally stay away from adding cubic terms though, and instead go to If the non-linearity is quite severe, consider using some natural cubic splines. These fit pwicewise cubic functions, with the constraint that the cubic pieces must match up smoothly. You can either set down the transition points (knots) by hand, as informed by more data exploration, or use an algorithm like gam to set them down automatically. Of course, when using these techniques it's extremely important to make sure that they old up well on out of sample data, either with an explicit hold out, or by using cross validation. Setting aside statistical reasoning for a moment, there may be context dependent reasons this is done in your industry. For example, the model may need to be approved by regulators or consumed by non-technical entities. It's up to you to weigh these concerns and state your case one way or the other. 
 General question: In a generalized method of moments estimation could the covariance matrix of the moment conditions be ill-conditioned and therefore the inverse not computable? Background on my model: I am estimating a random coefficient logit model of demand formulated as a mathematical programm with equilibrium constraints (MPEC) (see Random Coefficients Logit using the MPEC algorithm . There the structural parameters of consumer demand are estimated using simulated GMM. Essentially, I build a model that predicts market shares for products based on the mean valuations of product characteristics (e.g. price, color, size, ...) and the individual deviations from this mean valuation. I then try to find the parameter values that minimize some metric between observed and predicted market shares. Because price is endogenous the metric is not simply the sum of least squares but rather the metric is defined by the instrumental variables and the GMM objective function. This problem cannot be solved analytically so numerical methods are used. So finding the global minimum requires me to repeatedly solve this problem using different starting values of the parameters I want to estimate. For some values of the starting value I cannot compute the inverse of covariance matrix of moment conditions (efficient estimate of the weighting matrix for two-step GMM). For other starting values this problem does not arise. Details on my issue: In my specification for product demand has many indicator variables (300+). Many of these indicators are interactions between indicator variables and are therefore often zero. The matrix of independent variable X (dimension N x K with N observations and K independent variables) also contains "normal" continuous variables but only 5. Therefore the matrix of included and excluded instruments Z (N x L where L is number of instruments) also contains the same amount of zero-one indicator variables. Now when I try to calculate the covariance-matrix of the moment conditions (to get an efficient estimate of optimal weighting matrix $\hat{W}$) i.e. $\frac{1}{n} Z'\hat\Omega Z$ with $\hat\Omega = blkdiag(\hat{u}^2_1 ... \hat{u}^2_n )$ and $\hat{u}$ the residuals from the first stage GMM estimate Matlab will tell me that this matrix is nearly singular and therefore the inverse might be imprecisely calculated. Now note, that Matlab does not tell me hat this matrix is singular. Therefore, the problem is not that I have perfect multicollinaerity due to complete indicators (dummy variable trap). Is this to be expected with an instrument matrix that includes many indicator variables that are mostly zero for an observation? Therefore is this some kind of numerical problem? Or what else could be the problem? 
 I've figured it out. The second line comes from the "chain rule" of probability which states that: $$P(A, B, \dots, Z) = P(A |B,\dots,Z) \times P(B|C,\dots,Z) \times \dots \times P(Y|Z) \times P(Z)$$ In fact, this can be proven by expanding each term in the equality: $$P(A, B, ..., Z) = \frac{P(A |B,\dots,Z)}{P(B,\dots,Z)} \times \frac{P(B |C,\dots,Z)}{P(C,\dots,Z)} \times \dots \times \frac{P(Y|Z)}{P(Z)} \times P(Z)$$ and we can see that the terms cancel out. My main confusion stemmed from the fact that I didn't know that the commas were in fact a notation for the intersection. This post clarifies it: Now, by common convention, a list of events is interpreted as the intersection (AND) of those events, such that $P(E_1,E_2) = P(E_1 \cap E_2)$ or, using logical connectives instead of set-theory ones, $P(E_1 \land E_2)$. However, that convention is by no means universal, so if you want to be sure to avoid ambiguity, you should explicitly use $\cap$ (or $\land$) to denote the intersection of events. 
 I have a multinomial response variable and a multinomial "independent" variable. Is there an equivalent statistics or method for calculating the variance explained by the independent variable? 
 As explained at The Number of Exponential Summands in a Fixed Interval is Poisson , each of the variables $X_i$ is the waiting time for the first point to appear in a Poisson process of rate $\lambda$ starting at time $\mu$. It is obvious from the characteristic properties of a Poisson process that a union of Poisson processes of rates $\lambda_1, \lambda_2, \ldots, \lambda_n$ is just a Poisson process of rate $\lambda_1+\lambda_2+\cdots+\lambda_n$, equal to $n\lambda$ in this case. Therefore the expected waiting time to the first point among them all must be $1/(n\lambda)$. After the first time $X_{(1)}$ is encountered, there remain samples from the $n-1$ other processes. Since they are all independent, and the chance of new points appearing is independent of any previous points that have appeared, it's the same situation all over again: the starting time is now $X_{(1)}$ instead of $\mu$ and $n$ has decreased to $n-1$, that's all. Consequently the expected time to wait for the next point, which is $X_{(2)}-X_{(1)}$, is $1/(\lambda(n-1))$. (And so it continues: the expectation of $X_{(j+1)}-X_{(j)}$ is $1/(\lambda(n-j))$ for $j=1, 2, ..., n-1$.) In this figure a realization of a Poisson process of rate $\lambda=2$ is shown for each of $n=5$ independent variables indexed $1$ through $5$. The union of the points is plotted with ticks at the bottom: they form a realization of a Poisson process of rate $n\lambda=10$. The first points in each process are highlighted with solid dots (and shown with longer ticks at the bottom): these form the sample $(X_1, X_2, \ldots, X_n)$; all the hollow dots are superfluous but are shown to illustrate the concepts. The expected time to wait for the very first point overall is $1/(n\lambda)=1/10$ because the combined rate is $n\lambda$. After it is encountered, four processes remain. Thus, their combined rate is $(n-1)\lambda=8$ and the expected waiting time to the first point among them is $1/((n-1)\lambda)=1/8$. 
 The best way to interpret it is to plot the predicted probabilities as a function of distance on the original scale, with confidence bands. The following would do this using the R package: To make the fit more flexible (assuming the sample size supports this) use a restricted cubic spline in the distance: 
 You can code a multinomial model as a variation on a binomial model and use the 'logit' link. To handle multinomial models without recoding, you can use the MCMCglmm package. See the answer to a similar question here . 
 First off, I didn't know whether to post this here or on SO. I tossed a coin and ended here. Second, I'm trying to get a grip on the function in the MASS library. I've read the manual on and (which uses to compute its "vector of bandwidths for x and y directions"). In the manual page for it says that that function is "A well-supported rule-of-thumb for choosing the bandwidth of a Gaussian kernel density estimator". Why would I want to use this function specific? Also, I'm doing a project on cell data and spatial data (i.e. GPS coordinates of phones and the cell tower they've been on). How well does work with specifically this kind of data and why? Are there alternatives? Lastly, I've found that the MASS implementation of multiplies its result by 4 "for compatibility reasons". I found that dividing the result of by 4 gives me a better-looking image (i.e. I can see roads and highway interchanges, not so much when I use the result as-is). 
 If each measured variable has only one loading on it, it's the square of the standardized loading. If a measured variable has two loadings, and the latent variables are not correlated, then $R^2$ is the sum of the squared loadings. 
 I developed Poisson and Binomial GLMMs following the steps described in the excellent book from Zuur et al. (2009) Mixed Effects Models and Extensions in Ecology with R and I'm now ready to include my results into manuscripts to be submitted for publication. However, I'm struggling to find published studies in marine biology using non-normal GLMMs to use as a guideline to write my Methods and Results sections. What scientific publications would be good guidelines as to how to report non-normal GLMMs in the field of marine biology? 
 My problem in brief is as follows. A case control study was carried out to determine the association between alcohol drinking and eye cancer. The findings are stated as. 552(121) exposed and diseas +ve, 498(110) exposed but diseas -ve, 448(99) non-exposed but diseas +ve, 1502(330) non-exposed but diseas -ve, N.B the numbers in the bracket indicates the number of cigaette smokers. The question here is how can i find the odds ratio for alcohol drinkers despite the numbers given in the bracket 
 I'm making a couple of logistic regression based predictive models and intend to compare them and see which is "best". "best" here is obviously ill-defined, but as I'm looking for common metrics for evaluating the model performance I've came across different uses of GoF. For instance, on this wiki page R^2s are GoF, while here R^2s are measures of "predictive power". I also read somewhere that ROC is a GoF measure. So my question is, is there a distinction between GoF and predictive power? And is discrimination something that can be under either? Also, where does proper score functions, c-statistic and specificity/sensitivity fall under? 
 What you can do is build an experimental design that tells you whether age and gender are effect modifiers: that is, do they predict how well your treatment will work? You can still use a crossover design, and you'll have (for each participant) a difference $Y_i = Y_{trt, i} - Y_{pla, i}$. You then have essentially an observational dataset where you can regress $Y_i$ on factors like age and gender. But, beware subgroup analysis: there's a major multiple comparison issue there, and false-positive horror stories abound. Try to pre-specify a few tests of particular interest to you. Also, in a crossover design, you need a suitable wash-out period or other scheme to limit the possibility of treatment effects persisting from one arm to the next. 
 Below is an excerpt from chapter 13 of Fox, Negrete-Yankelevich, and Sosa 2015 (excerpt on Google Books ) describing my (somewhat informed :-) ) opinions on reporting GLMM results. The particular example excerpted here is using data from McKeon et al. 2012 "Multiple defender effects: synergistic coral defense by mutualist crustaceans" ( Oecologia , so not a marine biology journal , but at least a marine biology topic ...) More generally, a Google Scholar search on "lme4 glmer marine biology" should get you some reasonable examples ... Graphical summaries of statistical analyses that display the model coefficients and their uncertainty, or that overlay model predictions and their uncertainties on the original data, are important (Gelman et al. 2002). However, you also need to summarize the results in words. This summary should include the magnitudes and confidence intervals of the fixed effects; the magnitude of the among-group variation for each random effect, whether it is of primary interest or not; and possibly the confidence intervals of the among-group variation (if the random effects are included because they are part of the design, you should not test the null hypothesis that they are zero). If you are interested in the partitioning of variance across levels, report among-group variation as random-effect variances, or proportions of variance (see the grouse tick example below). If you are more interested in the fixed effects, report among-group variation as random-effect standard deviations, as these are directly comparable to the corresponding fixed effects. The following are sample reports for the four worked examples; appendix 13A shows the technical details of deriving these results. The results from all the combinations of estimation and inference methods in this chapter are summarized in Figure 13.3. Coral symbionts: For the analysis done here (logit link, one-way comparison of crab/shrimp/both to control) we could quote either the fixed-effect parameter estimates (clarifying to the reader that these are differences between treatments and the baseline control treatment, on the logit or log-odds scale), or the changes in predation probability from one group to another. Taking the first approach: “Crab and shrimp treatments had similar effects (–3.8 log-odds decrease in predation probability for crab, –4.4 for shrimp); the dual-symbiont treatment had an even larger effect (–5.5 units), but although the presence of any symbiont caused a significant drop in predation probability relative to the control (Wald p-value 0.0013; parametric bootstrap p-value &lt; 0.003), none of the symbiont treatments differed significantly from each other (likelihood ratio test p = 0.27, parametric bootstrap test (N = 220) p = 0.23); in particular, two symbionts did not have significantly greater protective effects than one (Wald and PB p-values both ≈ 0.15). The among-block standard deviation in log-odds of predation was 3.4, nearly as large as the symbiont effect.” (McKeon et al. (2012) present slightly different conclusions based on a model with a log rather than a logit link.) Alternately, one could quote the predicted predation probabilities for each group, which might be more understandable for an ecological audience. 
 I ran into a practical sampling question. I have 24 strata but I can only select 20 stores due to budget/practical constraints. What's the best way to select these 20 stores and how to correct it in the final data analysis? 
 I use the ADF and KPSS to test for stationarity / non-stationarity of price increments in financial time series. The two test applied provide different results for low lags, but the same result for higher lags: Lag = 1: KPSS rejects H(0) is stationary, however ADF rejects H(0) is a unit root in favour for H(1) is stationarity. Lag = 2: same results as for lag 1. Lag = 3: both tests indicate stationarity for price increments. Lag = 4: both tests indicate stationarity for price increments. Lag = 5: both tests indicate stationarity for price increments. Are the increments of prices of this financial time seris stationary or not? The estimation of the generalized Hurst exponent (I use the algorithm provided by Tomaso Aste (2003)) requires that price increments are stationary. What do the results above mean with respect to this requirement? 
 Assuming you are showing the complete test output you seem to have included 4 variables (series) in your test, and from your description I'm guessing that it's one spot price series and three different futures prices. The way to read this is from top to bottom (this is not gretl-specific), so the conclusion of the test sequence is either r=3 or r=4, depending on your significance level. (Also abstracting from potential small-sample issues, since you haven't shown the sample size.) The case r=4 would mean that each of the 4 series is stationary (relative to the included deterministic terms that you haven't shown, either), which may or may not be a reasonable outcome. The case r=3 would mean that there are 3 (linearly independent) cointegration relationships, not just one. A natural follow-up hypothesis after r=3 would be that for any series pair x,y the difference x-y would be I(0), so the matrix of cointegration vectors could be written as a 3-dim identity matrix on top with a fourth bottom row of (-1, -1, -1). But this does not follow from the pure Johansen test results, the unit coefficients are just a reasonable theoretical possibility (which can be tested in gretl). 
 I would advise you to reduce your number of strata. For example, if you have 24 different geographies (i.e. New York, San Francisco, Washington D.C., ...) then you may want to build strata by North/South/East/West and whether there are over or under 5,000,000 people. If you currently stratify on multiple variables (i.e. age, sex, income) then consider reducing the number of variables you use or collapsing categories. The benefits to using strata include being able to come up with point estimates and estimates of uncertainty within each stratum. If you do not have at least two observations within a stratum, you will not be able to estimate the uncertainty within that strata. 
 Simple question: I have a sequence of 10 proportions $(p_1,\dots,p_{10})$ (indexed by time), $p_j\in(0,1)$, that seems to have an increasing trend. How can I test this trend? 
 In linear approximation, we approximate the value of a state as a linear combination of some feature vector and a vector of weights, i.e. $\hat v(s, a) = \textbf{s}\cdot\textbf{w}$ for a feature vector $\textbf{s}$ and weights $\textbf{w}$ What Silver's getting at in this is that, in the discrete case, we can construct a feature vector $\textbf{s}$ as a list of dummy variables, each an indicator corresponding to a discrete state. This feature vector $\textbf{s}$ will have a zero for all entries except one, which will be one. Each $w_i \in \bf{w}$ will then correspond to a single state $s_i$, and $\hat v$ will just return that weight as the value of $s_i$. Under this representation, you can view a lookup table value function as a special case of a linear approximation. 
 One way to look at this issue is that goodness of fit is training error and predictive accuracy is test error. ("Predictive power" is not a very precise term.) That is, goodness of fit is how well a model can "predict" data points you've already used to estimate its parameters, whereas predictive accuracy is how well a model can predict new data points, for which it hasn't yet seen the true value of the dependent variable. Many of the same metrics, such as root mean square error, can be used to quantify goodness of fit as well as predictive accuracy; what distinguishes the two cases is whether the model has been trained with the data in question. Which is more important? Personally, I care a lot more about predictive accuracy. This tells you how useful the model would be for predicting unseen data in the future. Goodness of fit is what you should pay attention to if you think of the model as purely descriptive, as providing a summary of the data, rather than predictive. To be clear, the model with the best fit may not be the most predictively accurate, and vice versa, so there's a real choice to be made here. Now, often, data analysis is done for explanatory reasons, where the researcher isn't interested in describing the data or predicting new observations so much as making inferences about the true underlying data-generating process, that is, the explanation for the data. Whether goodness of fit or predictive accuracy is better for this is unclear, not least because neither does a particularly good job of saying how accurate the model is as an explanation. My opinion is that goodness of fit is better, but it's clear that mindlessly trying to optimize goodness of fit, without regard for content-specific issues, won't get you to good explanations fast. Explanation is ultimately a less statistical and more scientific concept than goodness of fit or predictive accuracy. 
 I am training a variational auto-encoder for a task of my interest. Recently I got to know from a colleague of mine in the lab that VAEs MUST learn a standard multivariate Gaussian distribution (zero-mean, diagonal covariance). However, from the reconstructions anyone can say that the VAE is working properly. But when I pass an example into the encoder the Z values are not necessarily around 0. Some individual z's might be even larger than 4 or 5 or some might be less than -4 or -5. So all of them are not close to 1 necessarily. So I wonder, am I doing something wrong that the forward propagation does not yield standard MVN distribution over Zs? Thanks 
 There is a test called the Mann-Kendall test that can assess if a particular (monotonic, or one-directional) trend exists. If you can conclude that no serial correlation exists, that the observations at time $i$ are representative of the conditions at time $i$, and that the sampling, measurements, etc. are unbiased and representative of the populations over time. Although the article here doesn't mention testing proportions (just general random variables $x_1,\ldots,x_n$), I am confident that the test still works. There may be an issue if some of your observations are $0$ or $1$, but you wrote above that $p_i\in(0,1)$ so that should be avoided. 
 Welcome to CV. I've assumed that you've already looked at the graph of the proportions against time. The simplest approach would be to regress the proportions against time as the single predictor, assuming time is coded in an analyzable format, e.g., as in a trend from 1 to 10. This would give you the strength of a linear or deterministic relationship as well as its sign or direction. For curvilinear relationships, e.g., quadratic, introduce time-squared along with time. You don't have many observations so you can't fit too many more parameters with this method. You don't indicate what the unit of time is for your data...e.g., is it daily, weekly, monthly, quarterly, etc. This would be useful information. This CV thread discusses a similar question about trends... Describing trend magnitude or detecting trends Among the recommendations was one from @whuber regarding building a Loess model for seasonal decomposition ... In addition, with so few observations many of the more advanced models and tests simply won't work, e.g., tests for unit root, stationarity, arima, etc... 
 Standardizing or normalizing predictors doesn't affect the range of the coefficients, which in linear regression and most of its descendants (such as the lasso) is always $(-∞, ∞)$. You're confusing the range of the predictors with the range of the coefficients. 
 I am looking for a good pedagogical example use of feature selection for model building. The purpose is to expose students to some very basic methods for feature selection, in the context of boolean classification, as well as the notion of model building -- and to apply it to a real-world data set that someone might care about. Let me distinguish prediction vs model building . One use of feature selection is for prediction , where the sole goal is to classify (predict the class of instances) in whatever way we can, and we don't care about the model itself; feature selection can be helpful for that by allowing one to focus on relevant attributes. In contrast, in model building , we care about the model as a object of interest in its own right. For instance, perhaps there is some scientific meaning that can be derived from the results of feature selection: maybe knowing that some features are relevant and others aren't might help build a scientific model. I want to teach feature selection and also give an example where they see classification used not just for prediction but also for model building. Students will have been previously exposed to boolean classification and the $k$-nearest neighbors classifier, but not other more sophisticated classification methods. I would like to find a concrete example application I can use for teaching feature selection, and that highlights model building as a motivation. Ideally, the application would have a few characteristics: Involves a boolean classification task. One where the classification and model building task is well-motivated and potentially of interest to students (e.g., perhaps from some scientific or real-world application), and where it's easy to explain why we might be interested in model building. There is a data set available, so students can play with it and be exposed to working with real data and see an example of what one can learn from feature selection. An application where feature selection is effective (i.e., some features are highly relevant and improve prediction, and others are irrelevant) Ideally, there exists a clean data set without too many complications (e.g., no missing feature values) and of modest size (e.g., at most tens or hundreds of features), and suitable for use with a $k$-nearest neighbors classifier. Is there an example application that might be suitable for these purposes? 
 I've been starting to play around with autoencoders for feature extraction and dimensionality reduction, and am wondering how critical input feature definitions are for success. For example, some of my features record how often "events" have a certain property, with the total number of events varying between cases. I'm wondering if it would make a difference to represent that as the number of events that have the property, or the fraction of events which have the property, given that I'll have another feature which represents the total number of events. To put it another way, how flexible are autoencoders in doing functional transformations of the input features? Can they (effectively) multiply or divide? Do log transforms? (etc.) Is there a particular network architecture which makes this easier (e.g. minimal number of layers, particular activation functions, etc.)? If, in practice, they do have difficulty with non-linear feature transformations, are there any rules of thumb on how to choose feature representations to best facilitate dimensionality reduction? 
 Usually I know how to do these problems, but this one stumped me. This is what the problem says: "Let $D$ denote the event that a person selected randomly from a population has a disease, and suppose $P(D) = 0.0002$. Let $T$ denote the event that the screening test is positive for a person selected at random. Suppose the manufacturer claims that the rate of false positives is $0.02$ and the rate of false negatives is $0.01$. Compute $P(T)$." So $P(D^c \mid T) = 0.02 \implies P(D \mid T) = 0.98$ And $P(D \mid T^c) = 0.01 \implies P(D^c \mid T^c) = 0.99$ $P(T) = P(T \mid D)P(D) + P(T \mid D^c)P(D^c)$, but I don't know how to find $P(T \mid D)$ or $P(T \mid D^c)$. $P(T \mid D) = \frac{P(D \mid T)P(T)}{P(D)}$, but this equation has two unknowns. Any help would be much appreciated. 
 In a regression setting, imagine you have a very highly statistically significant $(p&lt;0.01)$ effect of covariate $x$ on a binary variable $y$. However the variable $y$ has many zeros and only a few ones. The sample size can be seen as large. How does knowing that the there are many zeros affect your interpretation of the statistical significance of the coefficient, if at all? Put differently, should the information about the small sample size of ones make me more skeptical about significance in this regression? 
 Along @whuber's suggestion lines, you can create a two by two matrix with either probabilities: $$\begin{array}{c|cc|c} &amp;T &amp; T^c \\ \hline D &amp; P(DT) &amp; P(DT^c) &amp; P(D) \\D^c &amp;P(D^cT) &amp; P(D^cT^c) &amp;P(D^c) \\ \hline &amp; P(T) &amp; P(T^c) &amp; 1 \end{array} $$ Or equivalently, frequency counts out of a million people $$\begin{array}{c|cc|c} &amp;T &amp; T^c \\ \hline D &amp; n(DT) &amp; n(DT^c) &amp; n(D) \\D^c &amp;n(D^cT) &amp; n(D^cT^c) &amp;n(D^c) \\ \hline &amp; n(T) &amp; n(T^c) &amp; 1,000,000 \end{array} $$ And then use your knowledge of probability to fill out the table. Edit/Update: The problem is that the false positive rate is $P(T|D^c)$ not $P(D^c|T)$. The false positive rate is the probability of testing positive given that you don't have the disease. $$ P(T|D^c) = .02$$ The false negative rate is the probability of testing negative given that you do have the disease. $$ P(T^c | D) = .01 $$ Hence we have $P(T | D) = .99$. Now going through some algebra/probability stuff: $$ \begin{align*} P(T)&amp;=P(TD^c) + P(TD)\\ &amp;=P(T|D^c)P(D^c)+P(T|D)P(D)\\ &amp;=P(T|D^c)(1-P(D))+P(T|D)P(D)\\ &amp;=.02(1 - .0002) + .99 \cdot .0002\\ &amp;=.020194 \end{align*} $$ Which matches WHuber Note: previously I had given you the identity $P(D)=P(D|T^c)P(T^c)+P(D|T)P(T)$, which is true but not useful in this situation. 
 I was looking at this paper and at one point it states Does this mean that if your corpus is too small to produce good word vectors, you can just bump up the number of training iterations and the quality would improve? 
 I have observed a vector of quantities $\vec y$. I wish to use these to constrain a vector of initial conditions $\vec x$ that are related to $\vec y$ through a non-linear (numerically evaluated) function $f$, i.e. I want to find $$\hat x = \underset{\vec x}{\arg\min} \; \left[ f(\vec x) - \vec y \right]^2.$$ However, $\vec y$ has been measured with uncertainties $\vec \sigma$. Therefore, I should minimize something like a reduced $\chi^2$: $$\hat x = \underset{\vec x}{\arg\min} \; \sum {\frac{\left[ f(\vec x) - \vec y \right]^2}{\sigma^2}}.$$ However, I fear the following two situations: $f$ is nonlinear, so two elements of $f(\vec x)$ could provide identical information, and then my minimization would be biased towards that redundant information. The values $\vec y$ have been measured with covariance, but this covariance only accounts for the measurement error and does not account for the covariance in the model $f$. If one of the variables is measured much more precisely than the other variables, then it will effectively be the only thing being fit by the minimization procedure. Are there any ways around 1 and 2? 
 I fitted a lasso logistic regression using glmnet. I use a pretty small dataset with only 51 (28/23) observations. I want to compare the model fit of two possible variable combinations. Only control variables Control variables + linguistic predictors Both models are comparable regarding explained deviance with best lambdas (1.:17% | 2.:16% dev. explained from null model). Now I want also compare the mean cross validated error at the best lambdas. Again both models are pretty close (1.: 1.304177 | 2.: 1.324639). My questions are: 1.) What exactly measures this score? Is it RMSE as measured in linear regression? 2.) From a predictive perspective: Is such a score either good or bad? (I would guess it is not the best predicitve model on earth) 3.) What would a good score look like? 
 From what you've told me, I would almost certainly predict that if you: (1) ran standard two stage least squares estimator and (2) tried to compute robust standard errors, then some of the standard errors for your coefficients would either be reported as zero or correctly reported as blank/uncomputable. My guess at what is happening is that some of your coefficients are exactly identified, hence a heteroskedastic robust covariance matrix cannot be computed for those coefficients. Go to invert that to compute your optimal weighting matrix and Matlab will tell you that the matrix singular or near singular. Some background From what you're telling me, matrix is full column rank. What you're also telling me is that is less than full column rank (where and is your residuals). That is: $$ \underbrace{Z = \left[\begin{array}{ccc} z_{1,1} &amp; z_{1,2} &amp; z_{z,3} \\ z_{2,1} &amp; z_{2,2} &amp; z_{2,3} \\ \ldots &amp; \ldots &amp; \ldots \end{array} \right]}_{\text{full column rank}} \quad \quad \underbrace{A = \left[\begin{array}{ccc} z_{1,1}u_1 &amp; z_{1,2}u_1 &amp; z_{z,3}u_1 \\ z_{2,1}u_2 &amp; z_{2,2}u_2 &amp; z_{2,3}u_2 \\ \ldots &amp; \ldots &amp; \ldots \end{array} \right]}_{\text{rank deficient?}}$$ The square of the singular values of the matrix A on the right are singular values of $Z'\Omega Z$, that is, $Z' \Omega Z$ has zeros (or near zeros) for singular values if and only if A has zeros or near zeros. (Basically: effective rank = # of singular values above some threshold ) Something I would do is singular value decompositions on both $Z$ and $A$ (or Z'*Z and A'*A, basically same thing for these purposes). Singular values near zero are indicative of a matrix, that from a numerical linear algebra, is near rank deficient. Does $Z$ already have problems without Matlab spitting out an error? Or do the singular values only go to zero (or near zero) once multiplying by the residuals $u_i$? An educated guess at what might be happening... Imagine you have 3 observations: $$ Z = \left[ \begin{array}{cc} 1 &amp; 0 \\ 1 &amp; 1 \\ 1 &amp; 1 \end{array} \right]$$ Z is full column rank because of observation 1. But if the residual $u_1$ for observation 1 is zero... $$ A = \left[\begin{array}{cc} z_{11}u_1 &amp; z_{12}u_1 \\ z_{21} u_2 &amp; z_{22} u_2 \\ z_{31}u_3 &amp;z_{32}u_3 \end{array}\right] = \left[\begin{array}{cc} 0 &amp; 0 \\ u_2 &amp; u_2 \\ u_3 &amp; u_3 \end{array}\right]$$ Then $A$ is column rank 1 and $Z'\Omega Z = A'A$ will be rank deficient. Intuitively what would be happening is that if some of your initial estimates are exactly identified the estimated residual is zero and you can't form an estimate of the variance or covariance! There are 0 leftover degrees of freedom. Mechanically, what gets pumped out is an estimate of zero, then you go to invert that, and Matlab blows up by telling you the matrix is singular or close to singular. For reference, some MATLAB code I've written/tested to do what I imagine you're trying to do: Warning, this code does NOT check for rank deficiency. Let be your endogenous regressors, be your exogenous regressors (constant goes here), and be your instruments. Initial setup: Calculate your basic ingredients so you don't need to repeat these expensive operations: Get your initial estimates (you will use for weighting matrix): Get residuals: Calculate weighting matrix inverse Calculate your estimate b: 
 I am reading the book Artificial Intelligence a Modern Approach and I have trouble understanding why the SVM needs to keep support vectors. From the book: SVMs are a nonparametric method -- they retain training examples an potentially need to store them all. On the other hand, in practice they often end up retaining only a small fraction of the number of examples . And then: A final important property is that the weights associated with each data point are zero except for the support vectors -- the points closest to the separator. Because there are usually many fewer support vectors than examples SVMs gain some of the advantages of parametric models. Source: Artificial Intelligence a Modern Approach p746 As the SVMs separator is defined by a hyperplane w.x + b = 0 we only need to know w and b to make predictions. Why should it keep all the support vectors? 
 I would like to use random forest regression for prediction of schizophrenia-related continuous measures from genetic data. However, I have siblings in my data which would still be problematic with RFR right? Would one solution be to ensure that related individuals do not get split over bootstrap sample and OOB per tree? If so, is anyone aware of a way to accomplish this? Ideal would be if this fits with my current use of randomForestSRC package. 
 My question is which resampling technique has gained the more popularity Bootstrapping or permutation tests? Bootstrapping is mostly about generating large sample standard errors or confidence intervals; permutation tests as the name suggests are mostly about testing. (Each can be adapted to be used for the other task though.) How would we judge popularity? If we look at fields like psychology and education we can find plenty of use of rank based tests like Wilcoxon-Mann-Whitney, the signed rank test, rank-correlation tests and so on. These are all permutation tests (on the other hand there are many instances where permutation tests of the original data could be used instead but are usually not). In some other application areas, permutation tests would rarely be used, but the varying popularity across application areas sometimes says more about the local culture of whichever area than usefulness. easier to implement? In many cases - especially simpler ones - they're almost exactly equally easy -- it's essentially the difference between sampling with replacement and sampling without replacement. In some of the more complex cases, bootstrapping is easier to do because (looking at it from the testing point of view) it operates under the alternative rather than the null (at least naive implementations will be -- doing it so that it works well may be much more complicated). Exact permutation tests can be difficult in the more complex cases because a suitable exchangable quantity may be unobservable -- often a nearly-exchangeable quantity may be substituted at the price of exactness (and of being truly distribution-free). Bootstrapping essentially gives up on the corresponding exactness criterion (exact coverage of intervals) from the outset, and instead focuses on trying to get reasonably good coverage in large samples (sometimes with less success than may be understood; if you haven't checked, don't assume your bootstrap gives the coverage you expect it to). Permutation tests can work on small samples (though limited choice of significance levels can sometimes be a problem with very small samples), while the bootstrap is a large-sample technique (if you use it with small samples, in many cases the results may not be very useful). I rarely see them as competitors on the same problem, and have used them on (different) real problems -- often there will be a natural choice of which to look at. There are benefits to both, but neither in a panacaea. If you're hoping to reduce learning effort by focusing on only one of them you're likely to be disappointed -- both are essential parts of the resampling toolbox. 
 I came across this paper about Naive Bayes that states [Naive Bayes] is based on another common simplifying assumption: the values of numeric attributes are normally distributed within each class. Is that true? Does Naive Bayes require that you assume continuous predictors are normal? From this answer and my experience, I thought that you can use any distribution to describe your predictors, normal or otherwise. Am I missing something? Is the paper just badly worded, e.g. it should say "is commonly based on a normality assumption" instead of "is based on a normality assumption?" 
 For a paper I’m running analyses on factors influencing the rate of adoption of a reporting practice, i.e., the dependent variable is the rate of adopting a specific sustainability reporting standard. Time is operationalized in years (due to much of my data being available at this interval) for a period of 8 consecutive years (hence discrete time intervals). From economic literature on diffusion of management practices (more specifically reporting) I have identified a number of continuous variables that either represent intrinsic factors, or extrinsic factors; say ties (whether they be social or physical). In order to scrutinize whether these factors have an effect on the hazard rate, I want to run an extended Cox regression model in SPSS (23). This is my first with survival analyses / Cox regression and I have some questions that I very much hope I can get feedback on:) The dataset is structured per individual (in my case company), with a time variable indicating time of adoption or time of disappearance from the data set (0 to 7), and a status variable indicating 1 for adoption and 0 otherwise (i.e. censored). For each continuous independent variable I’ve created a ‘subvariable’ per year, e.g. assets06, assets07… until assets13. What I’ve understood so far is that these represent segmented time-dependent covariates*. In that sense I’ve computed a variable as follows: Is this a correct way of structuring the data at hand? Is this indeed the way to go for computing a time dependent covariate? * IBM source on computing time dependent variables . 
 On its own, Naive Bayes does not assume the normal distribution. The heart of Naive Bayes is the heroic conditional independence assumption: $$P(x \mid X, C) = P(x \mid C)$$ Gaussian Naive Bayes assumes the normal distribution... 
 I like to have control over the objects I create, even when they might be arbitrary. Consider, then, that all possible $n\times n$ covariance matrices $\Sigma$ can be expressed in the form $$\Sigma= P^\prime\ \text{Diagonal}(\sigma_1,\sigma_2,\ldots, \sigma_n)\ P$$ where $P$ is an orthogonal matrix and $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n \ge 0$. Geometrically this describes a covariance structure with a range of principal components of sizes $\sigma_i$. These components point in the directions of the rows of $P$. See the figures at Making sense of principal component analysis, eigenvectors &amp; eigenvalues for examples with $n=3$. Setting the $\sigma_i$ will set the magnitudes of the covariances and their relative sizes, thereby determining any desired ellipsoidal shape. The rows of $P$ orient the axes of the shape as you prefer. One algebraic and computing benefit of this approach is that when $\sigma_n \gt 0$, $\Sigma$ is readily inverted (which is a common operation on covariance matrices): $$\Sigma^{-1} = P^\prime\ \text{Diagonal}(1/\sigma_1, 1/\sigma_2, \ldots, 1/\sigma_n)\ P.$$ Don't care about the directions, but only about the ranges of sizes of the the $\sigma_i$? That's fine: you can easily generate a random orthogonal matrix. Just wrap $n^2$ iid standard Normal values into a square matrix and then orthogonalize it. It will almost surely work (provided $n$ isn't huge). The QR decomposition will do that, as in this code This works because the $n$-variate multinormal distribution so generated is "elliptical": it is invariant under all rotations and reflections (through the origin). Thus, all orthogonal matrices are generated uniformly, as argued at How to generate uniformly distributed points on the surface of the 3-d unit sphere? . A quick way to obtain $\Sigma$ from $P$ and the $\sigma_i$, once you have specified or created them, uses and exploits 's re-use of arrays in arithmetic operations, as in this example with $\sigma=(\sigma_1, \ldots, \sigma_5) = (5,4,3,2,1)$: As a check, the Singular Value decomposition should return both $\sigma$ and $P^\prime$. You may inspect it with the command The inverse of of course is obtained merely by changing the multiplication by $\sigma$ into a division: You may verify this by viewing , which should be the $n\times n$ identity matrix. A generalized inverse (essential for regression calculations) is obtained by replacing any $\sigma_i \ne 0$ by $1/\sigma_i$, exactly as above, but keeping any zeros among the $\sigma_i$ as they were. 
 I have a task to estimate expected run time (in seconds) of a tool. The tool is essentially a black box which appears to run to completion in amount of time which seems to be non-deterministic. In other words, run time given the same input varies slightly from run to run. The tool in addition to input accepts a positive integer as a parameter: 1, 2, 3, and so on. I would like to plot the distribution of the run times for a range of parameters. One parameter - one distribution plot. I would like to get the answers to the following questions: What is the mean run time for a fixed input parameter for the tool? What is the distribution or uncertainty of run times for each fixed input parameter? What is the best model to address the questions 1. and 2. The most straightforward way would be to create a histogram and compute mean for each. I am curious whether this problem can be cast into a Bayesian model. I know that some processes like time, rates usually are naturally modeled with distributions like Poisson (for integer) or Negative Binomial (for real) distributions. My prior knowledge is that individual measurements, or tool run times, are iid. I also know that run time should increase with increased value of parameter for the tool. For each parameter value for the tool (1,2,3,...,10) I have recorded 100 run time measurements. Essentially, I have ten files with 100 records in each. I am writing the code in Python using PyMC. I am curious and willing to learn how to specify the model that would best fit my problem. 
 I have about 100,000 products in my database. I have files with products in different formats which are some variation of "Apple iPhone 6s 64GB Gray $100". I need to find the same product in another format in my database and link them. For example: Match the string "Mobile phones Apple Apple iPhone 6 4G 64GB silver EU MG4H2__/A 0 100.00 €" from file I need it to link with "Apple iPhone 6 64GB silver" in my database. So is there some advantages to use machine learning for recognize and linking products? 
 After some more research I believe a GLMM is the appropriate solution to handling levels of a variable that are insignificant, as this essentially credibility weights the coefficient for each level of the variable. This paper provides a good description (See 9.1): 
 Question 1: Let’s say I want to predict SAT scores, and I have a hypothesis that IQ is a better predictor of SAT scores than weight is. What is the method that would allow me to reject the null hypothesis that IQ and weight are equally good predictors? (I'm assuming the answer is multiple regression, but I'm open to ideas!) Question 2: I still want to predict SAT scores, but now my hypothesis is that the % of time you spend studying math is a better predictor of SAT score than the % of time you spend studying language. Both probably make significant contributions, but I predict that you get more bang for your buck out of math time than language time. What’s the method that would allow me to reject the null hypothesis that %math and %language are equally good predictors of SAT score? The added complication here, as you’ve no doubt realized, is that %math and %language aren’t completely independent. If you devote 100% to math, you can’t devote 100% to English. On the other hand, they’re not totally dependent either: if you spent 30% of your study time on math, I don’t know really know how much you spent on language- I just know it’s not more than 70%. Is multiple regression an option here? What's the alternative? Question 3: Whatever the answers to 1 and 2 are, what are the alternative approaches in the case of heteroscedastic data? Question 4: Whatever the answers to 1, 2, and 3 are, can they also handle binary outcomes (i.e. logistic regression)? 
 Is the sampling distribution of an unbiased estimator symmetrically centered around the true value of the parameter? Why? Why not? Intuitively I think the question above is true (since I can use the approximation to a Normal Distribution) but I suppose there could be a counterexample. 
 1) For logistic regression use type.measure="class" or "auc" depending on whether it is a binomial or a multinomial classification. 2) Plot the two models using a ROC curve (use ROCR package) and compare the area under the curve as shown below. 3) A good score would depend on the baseline you are comparing with. If your baseline is random guessing you are comparing against the purple line. 
 I have seen some questions here about what it means in layman terms, but these are too layman for for my purpose here. I am trying to mathematically understand what does the AIC score mean. But at the same time, I don't want a rigor proof that would make me not see the more important points. For example, if this was calculus, I would be happy with infinitesimals, and if this was probability theory, I would be happy without measure theory. My attempt by reading here , and some notation sugar of my own, $\text{AIC}_{m,D}$ is the AIC criterion of model $m$ on dataset $D$ as follows: $$ \text{AIC}_{m,D} = 2k_m - 2 \ln(L_{m,D}) $$ where $k_m$ is the number of parameters of model $m$, and $L_{m,D}$ is the maximum likelihood function value of model $m$ on dataset $D$. Here is my understanding of what the above implies: $$ m = \underset{\theta}{\text{arg max}\,} \Pr(D|\theta) $$ This way: $k_m$ is the number of parameters of $m$. $L_{m,D} = \Pr(D|m) = \mathcal{L}(m|D)$. Let's now rewrite AIC: $$\begin{split} \text{AIC}_{m,D} =&amp; 2k_m - 2 \ln(L_{m,D})\\ =&amp; 2k_m - 2 \ln(\Pr(D|m))\\ =&amp; 2k_m - 2 \log_e(\Pr(D|m))\\ \end{split}$$ Obviously, $\Pr(D|m)$ is the probability of observing dataset $D$ under model $m$. So the better the model $m$ fits the dataset $D$, the larger $\Pr(D|m)$ becomes, and thus smaller the term $-2\log_e(\Pr(D|m))$ becomes. So clearly AIC rewards models that fit their datasets (because smaller $\text{AIC}_{m,D}$ is better). On the other hand, the term $2k_m$ clearly punishes models with more parameters by making $\text{AIC}_{m,D}$ larger. In other words, AIC seems to be a measure that: Rewards accurate models (those that fit $D$ better) logarithmically. E.g. it rewards increase in fitness from $0.4$ to $0.5$ more than it rewards the increase in fitness from $0.8$ to $0.9$. This is shown in the figure below. Rewards reduction in parameters linearly. So decrease in parameters from $9$ down to $8$ is rewarded as much as it rewards the decrease from $2$ down to $1$. In other words (again), AIC defines a trade-off between the importance of simplicity and the importance of fitness . In other words (again), AIC seems to suggest that: The importance of fitness diminishes. But the importance of simplicity never diminishes but is rather always constantly important. Q1: But a question is: why should we care about this specific fitness-simplicity trade-off? Q2: Why $2k$ and why $2 \log_e(\ldots)$? Why not just: $$\begin{split} \text{AIC}_{m,D} =&amp; 2k_m - 2 \ln(L_{m,D})\\ =&amp; 2(k_m - \ln(L_{m,D}))\\ \frac{\text{AIC}_{m,D}}{2} =&amp; k_m - \ln(L_{m,D})\\ \text{AIC}_{m,D,\text{SIMPLE}} =&amp; k_m - \ln(L_{m,D})\\ \end{split}$$ i.e. $\text{AIC}_{m,D,\text{SIMPLE}}$ should in y view be equally useful to $\text{AIC}_{m,D}$ and should be able to serve for relatively comparing different models (it's just not scaled by $2$; do we need this?). Q3: How does this relate to information theory? Could someone derive this from an information theoretical start? 
 I have a data set of participants with four sets of test results, two pre- intervention and two post. So far I've been averaging the two test scores and using a paired t-test to compare the results, is this the best way of doing it or is there a better way? 
 I have written a function that evaluates the log-likelihood of Zero-inflated Beta Binomial data: I've tried using 'optim' to get a maximum likelihood estimate from my simulated data, but so far the results are giving me a convergence parameter of 0 (which I take to mean that the algorithm hasn't converged) A few concerns/questions I have as someone who has very little understanding of numerical optimisation : 1) So far I've tried the 'BFGS' algorithm, and I'm not sure if this is the right approach. Are there more appropriate gradient-free methods to evaluate the maximum likelihood? 2) I am getting a host of warnings after running optim which seems to come from the log-gamma (lgamma function in R ) and log-beta (lbeta in R ) acting on an argument which evaluates to zero ( this happens whenever mu = 1 or rho = 0 ). Since these parameters are evaluated using a sigmoid transformation, I think this is an floating point representation error ( not sure if that is the technical term, but my computer does not have enough memory to represent the value that the sigmoid transformation actually evaluates to ?). Is there a quick fix for this where I can avoid constraint optimisation ? 3) I have tried constraint optimisation ( linear constraints on Gamma only ), and it fixes the warnings and errors, but still no convergence. I have no idea what to do from here. Is constrOptim with linear constraints not appropriate for this problem ? What could the reasons be for lack of convergence ? Would specifying the Jacobian and Hessian give me convergence ? Also, constrOptim uses the Nelder-Mead algorithm if the gradient is set to NULL. Could this be causing the trouble ? Is there a better gradient free optimisation algorithm more suited to my purposes ? I hope that's not too much to ask in one post. Any help would be greatly appreciated, I have been stuck with this for far longer than I'd care to admit. 
 Please can someone provide an accessible interpretation of the parameter estimates from a discrete weibull regression model, e.g in R: produces: I am trying to understand how one would communicate the results to a layman in an analogous way that one might interpret the parameter estimates from linear (unit increase in x is assoc with beta increase in y) or poisson (mulitplicative) etc. 
 I believe that my estimation is fine It is not. Since you accept that all regressors are "endogenous" (i.e. correlated with the error term), and you don't have valid instruments for all of them, then your estimator is inconsistent (for all coefficients, since endogeneity contaminates everything): this means that you don't really know what you are actually estimating. At least by the current prevailing consensus on the matter, any interpretation of the coefficients will (should?) not even be heard to begin with. But I think I understand what your question is and the answer is "no, you cannot have in the same relation some effects correlational only, and some effects causal". Now, make the assumption that the conditional expectation function of the log-dependent variable with respect to the three regressors is linear: $$E(\ln y\mid \mathbf x) =\gamma_0 + x_1\gamma_1+ \log (x_2)\gamma_2 +x_3\gamma_3$$ Note that I used different symbols for the unknown coefficients. If you estimate by Ordinary Least Squares this relation, you will always get consistent estimates of the gamma coefficients (if indeed the conditional expectation function is linear). These are all "correlational" effects, as you write, because they only postulate a pure statistical relation between the variables. But the gamma coefficients by design have different values than the beta coefficients in your initial regression. What this have to do with your question? Well, by saying that your regressors are "endogenous", and more importantly, by saying that this is a problem, you reveal that you are interested in estimating something like the following relationship: $$\log\left(Y\right)=\beta_0 + x_1\beta_1+\log (x_2)\beta_2 +x_3\beta_3 +(x^*\beta_4+u)$$ where $$E\big[\log\left(Y\right)\mid \{\mathbf x, x^*\}\big]=\beta_0 + x_1\beta_1+\log (x_2)\beta_2 +x_3\beta_3 +x^*\beta_4$$ but $x^*$ is unavailable, and correlated with the other regressors. So if you have valid instruments for all the regressors, your IV/2SLS estimator would indeed estimate consistently the first four betas. But the betas also are seen to represent a statistical relationship: causality cannot enter the picture through this route. 
 You can certainly use the Law of Iterated Expectations "with an absolute value" -if this wasn't so it would mean that LIE cannot be used with non-negative random variables, which is wrong, it certainly can. Simply define $$Z \equiv |Y-b_0-b_1X|$$ and you can of course write $$E[Z]=E[E(Z|X)]$$ 
 I'm just starting to learn about Restricted Boltzmann Machines (RBMs). So far I understand that they try to predict/output the inputs that are fed to them. If the RBM is trained on input where each record is a vector, then the RBM should likewise learn to output a vector. While reading about RBMs, I keep coming across references to an "energy function", which I don't really understand, but my guess is that it represents the formula of a trained RBM. While reading through Geoffrey Hinton's tutorial ( ) I came across the following paragraph: What confuses me about this paragraph is that it seems to suggest that the energy function outputs a scalar not a vector, which is inconsistent with my understanding above. Does it not actually output a scalar? Does an energy function not actually represent the architecture of a trained RBM and the means of calculating an output/prediction? Also, the article goes on to state that the variables a and b correspond to biases. If they're actually biases, why are they being multiplied with the vectors v and h ? Aren't biases always supposed to be added only? 
 I have a dataset with clustering results from $K$=3 to 70 clusters for $n$ observations. My original dataset is an $n \times m$ matrix with $n$ rows (one for each observation) and $m$ features. I am using these data along with my clustering results to compute various metrics such as SSE, Silhouette coeff, internal density, and others to determine the number of clusters, $K$. I am also trying a kernel based approach to find an elbow for what $K$ to pick from my dataset, by first creating a kernel matrix and then computing the above metrics. I have tried a Gaussian kernel with different sigma squared values ranging from 0.1 to 2. When sigma squared equals 0.3 I start to see a clear elbow. This result happened when I was playing with the distance parameter of gaussian kernel. I tried this in python from a previous post as follows: My question is, if changing the distance to cosine a valid strategy as commented in my code above, to compute the kernel matrix and then use this matrix to determine a suitable K from my dataset, or we have to stick to euclidean distance only? 
 Hi I am using e1071 package for SVM. I am using the tune() function to find the best parameters. It seems to me the default prediction function used is RMSE. However, I am having a binary classification model here. May I know how it is possible to use a function that will help me to use mean of classification error as prediction function in this case? Here is the code how I would find the classification accuracy: 
 So I have data for two groups with 5 subjects in each and a score for each of them. The anova test will show that there is no significant difference. It is now asking me to make a minimal change in order to produce a significant difference between the two groups, but keep the number of subjects and within condition variances the same. Then, I would have to make the results non significant again but keep the number of subjects and change I made before to make it significant constant. Would it make sense to say that adjusting the difference among means would make the F-test significant initially, and then adjusting the within condition variances would make it non significant again? I'm not totally sure how to adjust my data to meet these changes, however. The scores for group 1 are 3, 2, 3, 1, 5 and group 2 are 5,4,6,7,3. I would appreciate some advice on how to approach this. Thank you. 
 I have a sequence of events and I want to learn discrete probabilities of transitions between them and to predict next item in a sequence. Can you point me on good papers and Python or R package for this topic? 
 In one of my econometrics assignments, we were asked to consider the effect of measurement error in the dependent variable of a simple linear regression. And I was just wondering, under what circumstances can a confidence interval not be constructed? Obviously, if there's no way to estimate a parameter or the variance then you can't have one, but I was wondering if there was a more formal way to think about this situation. 
 Its false, consider $X_1,\dots,X_n \sim \text{Poisson}(\lambda)$ Then $\bar{X}$ is unbiased for $\lambda$, but $\bar{X}\sim \frac{1}{n}\cdot\text{Poisson}(n\lambda)$, and the Poisson is a right skewed distribution.(Of course, the 1/n just scales the distribution, it doesn't change the skew.) 
 If you are modelling the time between events, this can be thought of as being exponentially distributed, i.e a Poisson distribution can be thought of as being the number of events, while the exponential as the time between these events, https://en.wikipedia.org/wiki/Exponential_distribution . Based on this, $ y=time_B−time_A $ probably follows an Exponential distribution. 
 I am working on a study to quantify average working hours for doctors. However, when I leave it empty for respondents to fill up, it remains unfilled. Changing it into categories as above yield better responses. (categories of working hours; 1 to 10, 11 to 20, 21 to 30) Now is it possible for me to derive a mean from these categorical data? n1 x midpoint of category 1 + n2 x midpoint category 2 ..... divide by total n 
 I have performed a binary logistic regression using RStudio software with whether or not a player is re-contracted or not as the dependent variable. There are 7 predictor variables with 3 of these dichotomous (0,1). It was suggested to me that generating a ROC curve based on the full model and models with single predictors only may be useful in interpreting the sensitivity of my results. My question is in relation to my dichotomous variables - is a ROC curve still useful for these variables? and am I able to use "cut off" indicators on the curve of the full model if it includes dichotomous variables? Thankyou 
 No, I would not consider that to be valid. The problem is that the mean of the true values in each category is not likely to be the midpoint. For example, there are probably many more people who would answer 10 hours than one hour - so the average hours worked will be more than 5.5, but you are assuming that the mean is 5.5. Hence your estimate will be biased. What you could do is consider it to be a scale with a weird non-linear transformation - saying something like "On a scale where 1 = 1 to 10, 2 = 11-20 ... the mean score was 1.8." But if you only have three categories, you can just say "22% of people worked 1-10 hours, 43% worked 11-20 hours ..." Unless there's a very good reason that you need a mean, I would do that. 
 It's somewhat misleading to just lump this in with ordinal data; I'd called it "binned data" though formally it's interval-censored data (and there are a variety of other terms that might be used). You can certainly talk about the population mean (since the underlying scale really does have a mean) and how to estimate it, bringing in what is understood about the underlying variable to help figure out ways to estimate it well from the bin-counts and bin-boundaries. While it's common to use the mid-point in such cases it's not always the best possible option. However, one can get some idea of how biased that might be under some set of assumptions so it's possible to get a sense of whether it really matters all that much. Where the underlying density is decreasing, the correct "midpoint" to use would be left of half way, and if the underlying density is increasing, the correct "midpoint" to use would be right of half way. If you can come up with a plausible distributional model for the underlying variable, the mean can be estimated from the binned data via maximum likelihood (for example). Even in the absence of any model at all, one can place limits on the mean, since the lowest the mean can be is when all the values are at the low end of each interval and highest when they're all up at the high end of each interval. [Even if the upper category is seemingly open-ended, there's still likely an effective upper bound on hours worked. e.g. it's simply impossible to work 25 hours in a day or 169 hours in a week, even if you never need to eat or sleep. Likely there's some other substantially lower bound beyond which nobody can go for one reason or another.] 
 Let me try to answer on two levels. 1) How to compare rankings ? To compare the full rankings of the two systems you need a) the correct ranking, or a ground truth ranking and b) you need a measure that tells how tow rankings are similar. This question Can I compare ordinal rankings (and if so, how)? on CV discusses 2 metrics to compare rankings. Notice that in this case you will be comparing the ranking of system 1 with the ground truth and then the ranking of system 2 to the ground truth, for each query or user. Then you will have something similar to the figure in your question, but instead of P@5, you may have the Kendal tau or the Spearman rho. But you may have settled in using P@5, which is not really comparing the full ranking of the two systems, but only how right they are on the first 5 entries. It is OK, if the publications in your area use P@5, you should use P@5. Which brings us to the second level, given that you have the sequence of measures (either Kendal tau, or Spearman rho or P@5) for System 1 and for System 2, for each query, how you should perform the test 2) How to perform a significance test on two sequences of measures for System 1 and System 2? Here, @dmontaner is basically correct and describes the mechanics of using the functions on R, but I will be a little bit more incisive. If you are going to use a t-test than you should use this solution proposed by @dmontaner your data is DEFINITELY paired by the query (or user). Methodologically you should not use the option unless you have theoretical (not empirical) reasons to believe that your system is better than the competition. But there is an issue of whether you should or not use t-test. If researchers in your area use t-test, if that paper you pointed out was a good paper in a good conference, that by all means use the t-test. But the t-test makes assumptions on your data that may not be true. For example, the t-test assumes your data is more or less distributed normally, and this is more important the less data you have. If you have less than 30 queries, that your data should be distributed very similar to a normal. But P@5 is usually not normally distributed (at least because it is limited to the 0-1 range). The same can be said about the other two measures (tau e rho). In this case you should use the Wilcoxon signed rank test . Fortunately there is a R function for that, and you only call 
 This works for me. Instead of using nlme or lmer, I use mgcv::gamm and specify a gaussian link as follows: The estimation results are close to nlme/lmer mixed models and is applicable for most of the cases. In fact, the HLM/mixed model is a specific case of GAMM. 
 How are convolutional networks better than simple neural networks?Is the Convolution operation only difference between Conv. neural networks and deep networks? 
 I know there are many neural network for classification and regression problems. I myself use caret package in R for those problem. But now I am looking for a neural network which can be used for ranking the prediction values. For example consider the widely used iris flower dataset which I myself used initially for my studies. The dataset has 50 samples of data for three species Iris setosa, Iris virginica and Iris versicolor. Currently we can train this data set using any classification algorithm and predict a absolute result Instead of this absolute result I want a ranking with prediction accuracy with cut off (eg:&gt;50% accuracy) Can It be done using nerual network in R or any other machine learning system? 
 I have to plot distribution function for 2 causes of deaths of Byar Dataset. In Byar dataset of clustMD, events range from 1 to 8, for several causes of death.I am using the following code: This gives me a curve, but it is not corresponding to two causes of deaths. Would survfit(Surv(time,event=="1"|event=="2"|event=="3")~1,data=A) give me plot for 2 causes of death, where event=1 is alive, and other two are causes of deaths. 
 Here is the generalized formula for Fleiss' kappa: $$r_{ik}^\star = \sum_{l=1}^q w_{kl}r_{il}$$ $$p_o = \frac{1}{n'}\sum_{i=1}^{n'}\sum_{k=1}^{q}\frac{r_{ik}(r_{ik}^\star-1)}{r_i(r_i-1)}$$ $$\pi_k = \frac{1}{n}\sum_{i=1}^n\frac{r_{ik}}{r_i}$$ $$p_c = \sum_{k,l}^q w_{kl} \pi_k \pi_l$$ $$\widehat{\kappa} = \frac{p_o-p_c}{1-p_c}$$ where $q$ is the total number of categories, $w_{kl}$ is the weight associated with two raters assigning an item to categories $k$ and $l$, $r_{il}$ is the number of raters that assigned item $i$ to category $l$, $n'$ is the number of items that were coded by two or more raters, $r_{ik}$ is the number of raters that assigned item $i$ to category $k$, $r_i$ is the number of raters that assigned item $i$ to any category, and $n$ is the total number of items. Here is the formula for calculating interval weights: $$w_{kl} = \begin{cases}1-\frac{|x_k-x_l|}{x_{max}-x_{min}} &amp; \text{if } k \neq l\\1 &amp; \text{if }k=l\end{cases}$$ where the weight of any two categories $k$ and $l$ is equal to $1$ minus the distance between these categories divided by the maximum distance between any two possible categories. I have more information on my mReliability website, including the mSCOTTPI function which will calculate the Fleiss' kappa coefficent in MATLAB. Or if you prefer R, you can use the fleiss.kappa.raw function from the agree.coeff3.raw.r package, albeit with less documentation. 
 On the movielens dataset, I used SVD to find U, s, and V matrices. Then performed the dimensional reduction by elimination of everything corresponding to lower valued eigen values( upto a threshhold). How do I now get the prediction matrix. From what I have been trying, either the formula given in research papers, I couldn`t understand it or the accuracy comes out just to be very low, because the prediction matrix obtained was almost nowhere related to the original matrix. 
 I have a vector of numbers and I am trying to fit the data by Generalized Inverse Gaussian Distribution . My goal is to estimate the parameters $ a,b,p $ which appears in the pdf function. As in the above wiki page, we know that the pdf function, called $f$, satisfies the following equation: $$ f(x)(x(ax-2p+2) - b)+2x^2f'(x)=0 $$ My questions are: Can I use this equation to estimate the parameters $a,b,p$? Does there exist other ways to approximate the parameters more accurately? 
 I am new to stats. I have a straightforward ball drawing situation. If I have certain red balls (r) among blue balls in a pool (total: N), and I sample the balls with replacement for a certain times (N') to create a second pool. I then sample the second pool for third smaller pool (N'') with replacement again. How do I calculate the probability of picking up the red ball more than once in the third pool? Thanks. N 
 If you integrate out the differential equation $$\int_0^\infty \{f(x)[x(ax-2p+2) - b]+2x^2f'(x)\}\text{d}x=0$$ you get a moment equation $$\begin{align*}&amp;\int_0^\infty [x(ax-2p+2) - b]f(x)\text{d}x+\int_0^\infty 2x^2f'(x)\}\text{d}x\\ &amp;=\int_0^\infty [x(ax-2p+2) - b]f(x)\text{d}x-4\int_0^\infty xf(x)\}\text{d}x\\ &amp;=\int_0^\infty [x(ax-2p+2-4) - b]f(x)\text{d} =0\end{align*}$$ which can be used in a system of moment equations, but you need two further equations. 
 The Restricted Boltzmann Machine is an Energy - based model. The energy function produces a scalar value which basically corresponds to the configuration of the model and it is an indicator of the probability of the model being in that configuration. If the model is configured to favor low energy, then configurations leading to low energy will have a higher probability. Learning the model means looking for configurations that modify the shape of the energy function to drive to low energy configurations. As far as the biases go you basically calculate the dot product between the biases and the corresponding units (visible or hidden) to calculate their contribution to the energy function. 
 I have a data set in which there are data from April 2010 to Dec 2010 ( 9 Months) Jan 2011 to Dec 2011 (12 Months ) Jan 2012 to Dec 2012 (12 Months) Jan 2013 to Dec 2013 (12 Months) Jan 2014 to Dec 2014 (12 Months) Jan 2015 to Dec 2015 (12 Months) Jan 2016 to Mar 2016 (3 Months) Each year includes crime year in each month. I want to plot a graph showing crime rate from 2010 to 2016 and display how it increases or decreases. I want to know how would I normalize my data so that these missing months does not effect my visualization . Am I doing it right by normalizing ? Please suggest any other aprroach you have . But please note I have to plot it year vs crime rate rather than some other time function like month /vs crime rate. 
 As in my question i will highlight the bread, so the model based covariance, belonging to the sandwich estimator of Generalized Estimation Equations Bread= $\sum\limits_{i=1}^n D_i^T V_i^{-1}D_i$, where $D_i$ generally describes $\frac{\partial\mu_i}{\partial\beta}$ and $V_i$ is the working correlation matrix. This is what you normally see in everywhere, but with that the bread is increasing depending on sample size $n$. Why is there no $n^{-1}$ s.t. Bread= $n^{-1}\sum\limits_{i=1}^n D_i^T V_i^{-1}D_i$? this would average the "Bread" over all $n, or am I missing something? 
 I am an enthusiast of programming and machine learning. Only a few months back I started learning about machine learning programming. Like many who don't have a quantitative science background I also started learning about ML by tinkering with the algorithms and datasets in the widely used ML package(caret R). A while back I read a blog in which the author talks about usage of linear regression in ML. If I am remembering correct he talked about how all machine learning in the end uses some kind of "linear regression"(not sure whether he used this exact term) even for linear or non-linear problems. That time I didn't understood what he meant by that. My understanding of using machine learning for non-linear data is to use a non linear algorithm to separate the data. This was my thinking Let's say to classify linear data we used linear equation $y=mx+c$ and for non linear data we use non-linear equation say $y=sin(x)$ This image is taken from sikit learn website of support vector machine. In SVM we used different kernels for ML purpose. So my initial thinking was linear kernel separates the data using a linear function and RBF kernel uses a non-linear function to separate the data. But then I saw this blog where the author talks about Neural networks. To classify this non linear problem the neural network transforms the data in such a way that in the end we can use simple linear seperation to the transformed data like this My question is whether all machine learning algorithms in the end uses a linear separation to classifiction(linear /non-linear dataset)? 
 I'm trying to fit a regression to explain the number of homicides in each district of a city. Although I know that my data follows a Poisson distribution, I tried to fit an OLS like this: $log(y+1) = \alpha + \beta X + \epsilon $ Then, I also tried (of course!) a Poisson regression. The problem is that I have better results in the OLS regression: the pseudo-$R^2$ is higher (0.71 vs 0.57) and the RMSE as well (3.8 vs 8.88. Standardized to have the same unit). Why? Is it normal? What's wrong on using the OLS no matter what the distribution of the data is? edit Following the suggestions of kjetil b halvorsen and others, I fitted the data through two models: OLS and Negative Binomial GLM (NB). I started with all the features I have, then I recursively removed one by one the features which were not significant. OLS is $\sqrt{\frac{crime}{area}} = \alpha + \beta X + \epsilon $ with weights = $area$. The NB predicts the number of crime, with the district's area as offset. OLS residuals: NB residuals So the RMSE is lower in the OLS but it seems that the residuals are not so Normal.... 
 Googeling for your question brought me to a thread on the Gensim user group where that question was asked. That in turn links to a paper titled An Empirical Evaluation of Models of Text Document Similarity containing a partial answer to your question: The first global weighting function we considered normalized each word using the local weighting function, the second was an inverse docu- ment frequency measure, and the third global was an entropy measure. More details are provided by Pincombe (2004). And The results of these analyses are shown in Figure 4. It is clear that altering the local weighting function makes relatively little difference but that changing the global weighting function does make a difference. Entropy global weighting is generally superior to normalized weighting, and both are better than the inverse document frequency function. For the 50 document corpus, performance is best when there is no dimensionality reduction in the representation (i.e., when all 50 factors are used thus reducing LSA to a weighted vector space model). Peak perfor- mance for the extended 364 document corpus is better and is achieved when between 100 and 200 factors are used. Figure 4: Correlations between the human similarity measures and nine LSA similarity models, for each of four situations corresponding to (a) the 50 document corpus; (b) the 50 document without stopwords; (c) the 364 document corpus; (b) the 364 document without stopwords. The nine similarity models consider every pairing of the binary (‘bin’), logarithmic (‘log’) and term frequency (‘tf’) local weighting functions with the entropy (‘ent’), normalized (‘nml’) and inverse document frequency (‘idf’) global weighting functions. The dashed lines shows the inter-rater correlation. So this in turn references Pincombe (2004) - a Comparison of Human and Latent Semantic Analysis (LSA) Judgements of Pairwise Document Similarities for a News Corpus . Checking there, this paper contains far more detail on the topic (I will omit more figures, as they are mostly similar), but comes to a very similar conclusion: Overall, the two best correlations with human judgements of pairwise document similarity are achieved using log-entropy weighting on stopped and backgrounded text. This is consistent with the literature where log-entropy weighting has performed best in information recall (Dumais, 1991) and text categorisation (Nakov et al., 2001). More controversial are the relative performances of the normal and idf global weighting schemes. The results showed that the use of idf as the global-weight produced correlations with human pairwise judgements that were uniformly worse than those achieved using entropy or normal global-weights in similar situations. In an information recall study (Dumais, 1991) idf weighting outperformed normal weighting. The same is true for most local weighting schemes in a text identification study (Nakov et al., 2001) although this ordering of global weighting function performance did occur for term-frequency local weighting. And The choice of the global weighting function affects the correlations more than any other characteristic. The use of idf global weighting produces correlations with human pairwise judgments that are uniformly worse than those achieved using entropy or normal global-weights in similar situations. Variations in global weights have much more effect on the level of correlation with human pairwise judgments than do variations in local weights. So it appears log-entropy seems to work better information retrieval tasks, while you might want rely on TF-IDF for the more semantics-heavy information extraction/classification tasks where you will be using far more features. That being said, the TF-IDF measure has many knobs to tune (Sublinear TF and DFs or not? - see Nakov et al., 2001 Weight functions impact on LSA performance ) and your results with TF-IDF will vary greatly with respect to the exact implementation. Overall, I'd say it makes intrinsically sense that log(TF)-Entropy should perform best, given that (the probability-based) entropy captures more "information" about the term across your documents than (the "binary") DF does. 
 I'm interested in the method proposed by Lange, Vansteelandt et al. for mediation analysis. In their examples, they use a dichotomous or categorical exposures. Could anyone indicate any reference for mediation analysis with continuous exposure and specially how to generate weights in this case. Many thanks! Reference: Theis Lange, Stijn Vansteelandt, and Maarten Bekaert; A Simple Unified Approach for Estimating Natural Direct and Indirect Effects ; American Journal of Epidemiology 2011. 
 I know how to compute the cophenetic correlation coefficient. But I have trouble in understanding cophenetic correlation coefficient. Why it can be used to measure how well a hiearachical clustering fits the data ? Thanks. 
 How can I cluster household data which has binary variables like owns_car, rented_house (which all had answers in yes/no and being converted to 0/1). My data has 86 dimensions and about 3 lakh rows. Which subspace clustering do you suggest? 
 I am dealing with a problem where I have about short pieces of text (around 500 characters) and about 70 class labels. Currently each text is only assigned to one class, but we'd like to extend it so we can have multiple labels on a piece of text. The current label is selected by a user. There is quite a skew in the distribution of the texts amongst the classes and thus using a single classifier proves difficult. What I would like to do is have a binary classifier for each class, which returns the probability that the text belongs to this class so that we can tag the text with the top n class labels which show a probability of the text belonging to them over p. Essentially this is an ensemble of binary classifiers, where each classifier says yes or no with some certainty whether the text could belong to that class. I have created a single classifier using the caret package, but I am not sure how to go about creating an ensemble of binary classifiers. Currently I take all the documents in a class, generate a TDM and then combine all these separate class TDMs into a single large TDM which I use for training. How can I modify this current scheme to rather provide individual classification models for each class, with a probability as the output? Is it possible to have binary classifiers which are only trained on positive data, and the negative data is inferred? 
 You are right that the SVM decision function $w \cdot x + b$ depends only on $w$ and $b$, however it can be shown that $w$ can be expressed as a sum of support vectors . You can consult Stanford's or MIT's course notes, pages 12 and 5 respectively, but basically it can be shown with optimization theory that the optimal weight vector $w^*$ can be written in the following form: $$ w^*=\sum_{i=1}^{n}{\alpha_i^*y_ix_i} $$ where $\{(x_i,y_i)\}_i$ is your data $-$ $x_i$ are the attributes, $y_i$ the labels and $a_i$ Lagrangian coefficients. Further, it can be shown that only a fraction of the $\alpha_i$'s are non zero; vectors $(x_i,y_i)$ for which $\alpha_i&gt;0$ are called support vectors , so that is why you need to store all of them and only them to make further predictions $-$ check page 13 of Stanford's course; check also page 7 of MIT's course. So if $N_{sv}$ is the number of support vectors, $\{(x_i^{sv},y_i^{sv})\}_i$ the support vectors and $\alpha_i^{sv}$ their corresponding alphas $-$ which are all strictly positive $-$ we can write the optimal $w^*$ as follows: $$ w^*=\sum_{i=1}^{N_{sv}}{\alpha_i^{\,sv\,*}y_i^{\,sv}x_i^{\,sv}} $$ The optimal decision function is then: $$ f_{SVM}^*(x) = \sum_{i=1}^{N_{sv}}{\alpha_i^{\,sv\,*}y_i^{\,sv}(x_i^{\,sv}\cdot x)}+b^* $$ 
 When looking for methods to perform classification on longitudinal data, I stumbled upon Longitudinal support vector classifiers , which seeks to find not only the hyperplane parameters $\alpha$ but also the temporal trend parameters $\beta$, such that the data collected for an individual $s$ at time points $1,2,...,T$ is given by: $\mathbf{x_s}=\mathbf{x_{s,1}}+\beta_1\mathbf{x_{s,2}}+...+\beta_{T-1}\mathbf{x_{s,T}}$, where $\mathbf{x_{s,k}}$ is a $1 \times p$ vector of parameters measured for individual $s$ at time point $k$. My question is: when using such method, am I defining an overall trend for parameters, regardless of the individuals? And if so, how stringent is this assumption when the dataset is composed (of course) by individuals that belong to different classes whose parameters are expected to vary on time differently for each class? 
 So I am looking at a dataset for auctions. I have all the bidders who won something, and how much they bid. This auction takes place every year. I wanna see if the same people come back every year to participate in the auction. I thought that maybe doing a histogram to compare frequencies might summarize the data nicely. However, because my data has a time component involved, and I would like to see the frequency for all years, is there any other way to find out if the same individual repeats itself in the data at a later date? 
 I hope to fit the spatial autoregressive model : $$ y= \gamma_1 Wy + \gamma_2 By + X\beta +\epsilon. \quad (1) $$ where $W, B$ are different weight matrices. However, every references I've found only mention the SAR model that estimates one autocorrelation, i.e. $$ y= \gamma_1 Wy + X\beta +\epsilon ,$$ which can be fitted by using lagsarlm function in the R package spdep . Therefore, I wonder if I could fit the model (1). Any comments would be helpful. Thanks. 
 For my thesis I want to use a multivariate elastic net. So I've got multiple dependent variables, which I want to estimate simultaneously with Elastic Net. Sort of like the idea of SUR. SUR stands for Seemingly Unrelated Regressions, which uses the idea of normal univariate regression (with one dependent variable y1) and extends it to a multivariate regression, with multiple dependent variables y1, ... , yn. This method might be able to capture the correlation between dependent variables in some cases, which improves the forecasts. Normally you estimate this system of equations with Feasible Generalized Least Squares (FGLS). I want to estimate this system of equations with the Elastic Net, as it has been shown to improve the MSE of your forecast. Does anyone know if this is possible/has been done before? I can't find any papers about it. In case there are no papers I have to derive all the statistical properties for this method myself, is this doable in a time span of ~2.5 weeks? 
 Given a time series, I have to detect two types of events: 1) "medium" decrease 2) "high" increase Event detection should be "fast enough". I used quotation marks as I'd like to set different trigger thresholds for the two events which can be calibrated after some tests. I've researched on google and this forum and one common approach is the double (or triple) crossover method. Given two moving averages (I though exponential are more suitable in my case since are more reactive) EMA_4 and EMA_8 (calculated on 4 and 8 periods respectively), I thought I can differentiate the "medium"/"high" cases by looking for the following disequalities for each t in {1,2,...}: 1) EMA_4(t) &lt; EMA_8(t) + Tm 2) EMA_4(t) &gt; EMA_8(t) + Th with Th &gt; Tm &gt; 0. I'm not sure that crossover methods are the best and solid approaches, and I was wondering if I can use something better. I know the existence of low-pass filters , (kernel) smoothing and I've the feeling they might be used someway... 
 Some algorithms use a hyperplane (i.e. linear function) to separate the data. A prominent example is logistic regression. Others use a hyperplane to separate the data after a nonlinear transformation (e.g. neural networks and support vector machines with nonlinear kernels). In this case, the decision boundary is nonlinear in the original data space, but linear in the feature space into which the data are mapped. In the case of SVMs, the kernel formulation defines this mapping implicitly. Other algorithms use multiple splitting hyperplanes in local regions of data space (e.g. decision trees). In this case, the decision boundary is piecewise linear (but nonlinear overall). However, other algorithms have nonlinear decision boundaries, and are not formulated in terms of hyperplanes. A prominent example is k nearest neighbors classification. Ensemble classifiers (e.g. produced by boosting or bagging other classifiers) are generally nonlinear. 
 I am not entirely sure where (the need for) the double integrals comes from (maybe you can elaborate), but anyway, here is some hopefully related information: The first line seems to be related to the derivation of the expected value of the KDE, reproduced below: \begin{align*} E\left[ \widehat{f}(x)\right] =&amp; \int k(\psi )f(x+h\psi )d\psi \\ = &amp; \int \left( f(x)+(h\psi )f^{(1)}(x)+\frac{(h\psi )^{2}}{2}% f^{(2)}(x)+O(h^{3})\right) k(\psi )d\psi \\ =&amp; f(x)\int k(\psi )d\psi +hf^{(1)}(x)\int \psi k(\psi )d\psi \\ &amp;+\frac{h^{2}}{2}f^{(2)}(x)\int \psi ^{2}k(\psi )d\psi +O(h^{3}) \\ =&amp; f(x)\kappa _{0}(k)+hf^{(1)}(x)\kappa _{1}(k)+\frac{h^{2}}{2}% f^{(2)}(x)\kappa _{2}(k)+O(h^{3}) \\ =&amp; f(x)+\frac{h^{2}}{2}f^{(2)}(x)\kappa _{2}(k)+O(h^{3}), \end{align*} where the $O(h^3)$ term comes from bounding the final term in the integral over the Taylor expansion of $f$. $$ (1/3!)h^3\left|\int f^{(3)}(\tilde x)\psi^3k(\psi)d\psi\right|\leq Ch^3\int\left|\psi^3k(\psi)\right|d\psi=O(h^3), $$ where $C$ is some positive constant and $\tilde x$ lies between $x$ and $x+h\psi$. For this to work, $f$ must be three times differentiable. Moreover, a bounded kernel $k$ ensures that $\int\left|\psi^3k(\psi)\right|d\psi&lt;\infty$. 
 Lets say we have data set with an and a variable. We have an effect size from a linear model called of called . Now we have a simulated 1000 new data sets according to a process we care about, to ask whether the observed relationship between and is more than we might expect according to the simulation process. To do this we therefore do 1000 linear models of for each of the 1000 simulated data sets and so have 1000 new values. Now to compare our observed to the simulated values to get a p value we could do any of these pretty much equivalent options Is this correct to compare raw beta values to raw simulated values ? Or do we have to convert to a first? i.e. I feel like this shouldn't matter but the results change from some toying... 
 I am using Label Propagation in skicit-learn for finding labels for the unknown ones My Input is 'data_list' containing 400 000 sentences in sanskrit language like : ['tatra yad tad mahABAga SaMkara arDa kAyin', 'gam sa bala SrImant duryoDana ariMdama', 'Sigru pattra Bava SAka rucya vAta kaPa apaha'] I gave this 'data_list' as a parameter to method of fit_transform of Tfidf vectorizer and got a sparse matrix 'X' Link : X = fit_transform(data_list) I then gave this matrix 'X' to TruncatedSVD to obtain a matrix 'X_reduced' which is dimensionally reduced Now to use the 'fit' function of Label Spreading I need to give two parameters 1)the above matrix X_reduced and 2) Matrix with labels Link : For creating Labels, I have a list of words (these are the words that occur in above sanskrit sentences) with name 'unique_words' which has labels of '1' or '0' and '-1' for all other words not in the list The problem I am facing is X_reduced has no. of rows (n_samples) = no. of sanskrit sentences i.e., no. of elements in the above 'data_list' and I should create a labels matrix of same numbers of rows as X_reduced So how to create labels matrix to give as a parameter to 'fit' function of Label Spreading in skicit-learn 
 AFAIK it's because SVMs are often used together with kernels. if we don't use kernels, it is sufficient to only store the decision boundary $wx+b=0$, then SVM will become a parametric method. kernels can be thought of as mapping the input to an implicit feature space, of which the dimensionality can be very high (even infinite in the case of Gaussian kernels). in this case storing the decision boundary on the high dimensional space would be inefficient (or impossible if there're infinite dimensions). if we decompose the decision boundary as a function of some support vectors (as shown in Daneel Olivaw's answer) then it would be independent of the dimensionality of the "implicit feature space". such that it can work with any type of kernels. 
 What criteria make you decide to use random forest over decision tree ? How do we decide when decision tree is not sufficient ? 
 I am testing two variables for cointegration and after implemeting test for structural break I found there are 2 structural breaks. So, in the Johansen test I need to add dummy variables (following the Johansen et al. 2000 research). And here I need help - what dummy variables I need to create and how to create them in orer to be able to conduct the Johansen test (ca.jo()). Initially, I was thinking that I need just two dummy variables: (1) zero until the first structural break occurs, ones until the second break occurs and again zero afterwards: (2) zeroe until the second break occurs and one afterwards. And in the ca.jo function in dumvar I add the two dummy variables. But is does seem to be more complicated. I found in a very useful blogspot: But some people adrgue that the Eviews implementation is not really correct (besides I do not have access to Eviews, I am using R): " We have to have the dummy-trend interaction inside the levels matrix (i.e., the error correction matrix) and not as an exogenous series." Additionally, I know the critical values change with dummy variables, but for this I have found solution. 
 my question is can i run linear regression model using summarized counts/frequencies? For example, my dependent variable is total number of people who is aware of a specific TV show and my independent variable is total number of people who said that the specific program is good.The total number of TV programs is 100. If yes, how do i interpret it? Is it the same as with the normal GLM? Thanks in advance. 
 In some literature it is possible to find the Generalized Instrumental Variable Estimator (GIVE), but this is not always well-specified. Is this estimator equivalent to an estimator found using the Generalized Method of Moments (GMM) under conditional homoskedasticity assumption, so that is GIVE equal to the 2SLS Estimator? 
 I want to know how to perform a power analysis in the following case to know if my sample is good enough or not. In my faculty, we have decided to apply a new lecturing methodology to a set of students, and we hypothesized that if we apply this methodology, then more than half of the students will have a drop in their marks. However, we were only be able to test this methodology in a group of 30 students, because another classroom of approximately the same number of students was not able to follow this new methodology. We have also applied an h-test in our group, but we have received the critique that the sample size might be too small for such a text. How to tackle this situation? 
 I'll provide a sketch of the derivation. Omitting the bias term (since anyways we take derivatives later), the recursion looks like: $$\mathbf{h_{t+1}}=tanh(\mathbf{ U x_{t}+W h_{t}}) $$ where the $tanh$ is taken elementwise. Now, since $\mathbf{h_{t}}$ and $\mathbf{h_{t+1}}$ are vectors, the derivative $\frac{\partial \mathbf{h_{t+1}}}{\partial \mathbf{h_{t}}}$ is a Jacobian . Notice that if $y=tanh(x), dy/dx=1-tanh^2(x)=1-y^2$. Let's see how a single element of the Jacobian looks like. Assume that the hidden layers are of dimension $n$ Now $h_{t+1, i}=tanh(\sum_{k=1} ^{n} w_{ik}h_{t,k}+ g(x))$. Here $g(x)$ stands for some function of x. Hence $\frac{\partial h_{t+1, i}}{\partial h_{t, j}}$ = $1-h_{t+1,i}^{2}w_{i,j}$. This corresponds to element in position $(i,j)$ of the Jacobian (from #1 above). This multiplier $1-h_{t+1,i}^{2}$ applies to each element of the $i^{th}$ row of the Jacobian. From basic linear algebra you can show that pre-multiplying a matrix $A$ by a diagonal matrix $diag(b_{i,i})$ is equivalent to scaling up row $i$ of matrix $A$ by $b_{i,i}$ Hence the term $diag(1-\mathbf{h_{t+1}^{2}})$ occurs as a premultiplier to the matrix $\mathbf{W}$ 
 You can use a generalised linear model to fit count data. This is sometimes called Poisson regression. You use a log link. I assume by GLM you meant the general linear model not the generalised linear model. You do not say what software you are planning on using but these models are available in any respectable package. 
 Now you have the data the concept of power becomes much less relevant as it is more of a planning issue. You can carry out your test and see what happens. It may well be that you cannot detect even quite a scientifically meaningful difference with what you have but that is life. You ask what you should do and I am afraid if the anonymous critic persists your only remedy may be to run the experiment on more classes. 
 You could look at Appropriate R package/function for applying HMM to categorical sequence data? for some hints to get you started. Although it is old the advice should help you find R packages. 
 I am having problem deciding wheter to define a variable as a random effect to include in our logistic regression model. Any help on this subject would be most appreciated. In this model, we are trying to determine the effect of region of residence on different endpoints after hospital admissions. We have included several demographic and hospital variables as fixed effects as well as region. Due to the fact that most people from regions are hospitalized in their regional hospital, each hospital is highly correlated with region. Is it appropriate to use hospital (as a categorical variable) as a random effect to control for the potential bias in the varying quality of care between hospitals? 
 I am wondering how the confidence interval for the Area under the Curve statistic (ROC curves) is derived. I have heard that the AUC can be assumed to be normally distributed, but I am looking for a proof of this statement or a derivation of the confidence intervals 
 How can you plot the logits of a logistic regression to show an interaction effect? My variables are all significant constant: -1.12 technological diversity B= .99 expB=2.69 Involvement B=1.16 expB=3.19 technological diversity x involvement: B= -1.33 expB=0.26 Thank you guys. 
 I ran a decision tree on some data with over 1 million of observations, a binary dependent variable and a few dozens of independent variables, some partially correlated. I got weird results. The sensitivity is 4%, while the specificity is 97%! I am trying to understand why this has happened. I mean, if the tree can't classify, I expect it to be as good as a coin, i.e., 50% in both measures, not 4% and 97%. What have I done wrong, should I have used the Z transformation for all variables, or should I have omitted the correlated variables? I used the CHIAD method which is default on SPSS. Will appreciate your advice. 
 I am trying to implement the Pegasos algorithm for large scale SVM training . I'm following the main paper Pegasos . Everything worked fine but the results are quite disappointing. The code: I commented in the code which I think is enough to understand the meaning of each line. And for the algorithm, refer to Fig.2 of the paper given. And the result after training is this. Sometimes the result is even worse. Where did I go wrong. Any help is appreciated. 
 Is it possible to evaluate the following integrals? $\int_0^1 y^{m-1}(1-y)^{n-1}\Phi\left(\Phi^{-1}(y)+\mu\right)dy$, where $m, n, \mu$ are constants and $\Phi(.)$ is the normal CDF Thank you 
 From what I've seen in (supervised) machine learning the general idea is to work with some training set $(\mathbb{x_i},y_i)$ and learn the $y_i$ outputs without over-training. In the case of regression this is a continuous problem. However, in the previous definition of my training examples (and in a lot of definitions I see) the $y_i$ is single valued. So I was wondering if it were possible / if there are any good machine learning algorithms currently which can work on continuous vector value output spaces? To put it more simply, imagine I have a set of inputs, which I regress to a particular output. We can call this model $H_1$. I have new set of inputs, and a new set of outputs, which results in a new model, $H_2$, and so on until I make up to $H_n$ models. I want this set of regression functions to by my training data set , so that if I have new input, I will be able to make an educated guess on what this new model, $H_{n+1}$ should be. I can only think of possibly extending Boosting to this scenario (if it hasn't already been done before), perhaps using a Gaussian Process, or maybe even some Bayesian parametric method where I integrate over all the models? I don't have much experience using entire regression functions as training inputs, so I'm wondering if someone on here could educate me on the topic / suggest plenty of algorithms and point in the direction of some good sources. 
 I have seen published papers include "exogenous controls" in their instrumental variables regression. Can someone explain: What is meant by "exogenous" in this case? The purpose of these "controls" when one is substantively interested in a consistent estimate of a single independent variable? The only way I see this making sense is if you know the data generating process well enough to know that conditional on the control variable, the instrument satisfies the $Cov(Z,Y) = 0$ exclusion restriction. This seems incredibly unlikely in practice (at least in the social science). 
 Under the assumption that you're looking at similar data across the different regressions, you're describing Bayesian hierarchical modeling. The classic example of this is the Eight Schools data: students from each school are pre-tested in PSAT scores. Then each of the students' schools provide PSAT coaching to them. Then the students take the PSAT again. Was one coaching experience more effective than the others? How much do the students' scores improve (if at all)? A hierarchical analysis of the PSAT scores can answer this question. Using the posterior predictive density from the model and some pre-coaching data for a hypothetical ninth school, we can make inferences about how effective coaching might be for this new batch of students. Hierarchical models are premised on the idea that regression coefficients themselves have some distribution, e.g. $\beta\sim\mathcal{N}(\mu_\beta,\sigma^2_\beta)$ would imply that the coefficients $\beta$ are normal deviates with a mean and variance parameter. Gelman's Bayesian Data Analysis is a great resource for getting started with hierarchical models. 
 If you are mainly interested in differences in time and Group, those variables should be treated as fixed effects. As you want to take into account, "multiple measurements from each subject ", I suggest to allow these effects to vary across subjects. This mean that you also model them as random effects: 
 Allow me to comment on the statement: “... fitting k parameters to n &lt; k observations is just not going to happen.” In chemometrics we are often interested in predictive models, and the situation k &gt;&gt; n is frequently encountered (e.g. in spectroscopic data). This problem is typically solved simply by projecting the observations to a lower dimensional subspace a, where a &lt; n, before the regression (e.g. Principal Component Regression). Using Partial Least Squares Regression the projection and regression are performed simultaneously favoring quality of prediction. The methods mentioned find optimal pseudo-inverses to a (singular) covariance or correlation matrix, e.g. by singular value decomposition. Experience shows that predictive performance of multivariate models increases when noisy variables are removed. So even if we - in a meaningful way - are able to estimate k parameters having only n equations (n &lt; k), we strive for parsimonious models. For that purpose, variable selection becomes relevant, and much chemometric literature are devoted to this subject. While prediction is an important objective, the projection methods at the same time offers valuable insight into e.g. patterns in data and relevance of variables. This is facilitated mainly by diverse model-plots, e.g. scores, loadings, residuals, etc... Chemometric technology is used extensively e.g. in the industry where reliable and accurate predictions really count. 
 You could look at RL Toolkit, which gives examples of visualizations for mountain car and grid worlds. 
 This might be a overly simple question, but I just want to confirm that the multi:softmax objective in xgboost, what it really means is that it uses a softmax function with categorical cross entropy loss? Thank you :) 
 I'm not sure this question is better suited for Cross Validated or Stack Overflow, please don't hesitate to tell me if I ask it on the wrong forum. I'd appreciate some help interpreting what shows the result of plot.gam on a GAM object with random effects, obtained with gamm4. I'll try to give a reproductible example. I'll take an invented example : we have several species of rabbits in several forests (ecologist spotted). We'd juste like to visualize what is the evolution of rabbit size in time, averaged on all forests and on all species. In other words, we'd like to see if there is a global pattern in the evolution of size, independently of location or species. Let's generate a dataset : We get a data set with sizes for 3 species (X,Y,Z), between 1991 and 2008, in six forests (A,B,C,D,E,F). We get this graph from : Intuitively, I'd like to say that this plot plot represents the "average" evolution of rabbit size in time, when we remove forest and species effect. Though, I'm totally new to GAM and GAMM. So my question is : is it really what this is about ? Or am I wrong to use this approach ? I was wondering so because here, my goal is more to visualize the data than to make inferences, so I'm not sure if this is a good idea to use this method. 
 I can fit the data with non-linear function, namely Mechanistic Growth curve from the JMP library. See the example of the fit in the next figure. The fit equation is BA = a(1 - b e -c *PA ). BA represents a parameter within sheet metal forming domain. I also now that this fit depends on the three parameters: P 1 - tooling dimension, P 2 - tooling dimension, and P 3 - plate thickness. Is it possible to couple the parameters of the fit curve (a, b, and c) and (P 1 , P 2 , and P 3 )? I want to have equation in a form of BA = a(P 1 , P 2 , P 3 )(1 - b(P 1 , P 2 , P 3 ) exp -c(P 1 , P 2 , P 3 )*PA ). What I've done so far is, I obtained the a, b, and c for every possible combination of P 1 , P 2 , and P 3 . P 1 , P 2 , and P 3 are not nominal parameters, however I've done full factorial analysis for all available tooling dimensions and thicknesses. I don't see where I should go further. Is there a way to obtain this kind of equation or during the non-linear fit or after obtainment of parameters? 
 I might be wrong, but I don't see how we can replace the dot products with kernels without turning it into the dual problem. the kernels map the input implicitly to a feature space, so $x$ becomes $\phi(x)$, the loss function becomes $J(\mathbf{w}, b) = C {\displaystyle \sum\limits_{i=1}^{m} max\left(0, 1 - y^{(i)} (\mathbf{w}^t \cdot \phi(\mathbf{x}^{(i)}) + b)\right)} \quad + \quad \dfrac{1}{2} \mathbf{w}^t \cdot \mathbf{w}$ if Gaussian kernel is applied, then $\phi(\mathbf{x}^{(i)})$ will have ifinite dimensions, so will $\mathbf{w}$. It seems difficult to optimize a vector of infinite dimensions using gradient descent. 
 Given a sample of data, it is always possible to construct non-parametric (i.e. confidence intervals which do not assume that the data are related to any particular probability distribution) confidence intervals about properties such as the median and the range of the data. So in the sense of there being CI's for some useful properties of a data sample, these always exist. However, in the sense of there being CI's for specific properties assuming particular distributions, they may not exist. For example, if a series of data points are generated from a Cauchy distribution, calculating the mean will provide little or no useful information on the parameters of the distribution because the Cauchy distribution's mean is undefined, as is its variance. Hence, you can't calculate the confidence interval for the mean or variance for a sample of random data generated by the Cauchy distribution because they don't exist. The reason that the mean, for example, doesn't exist for the Cauchy distribution is that it is mathematically undefined. This answer explains why. Essentially if there is a parameter which exists and can be estimated with real numbers, it will be possible to calculate a confidence interval, at least by bootstrapping if all else fails. 
 I have been reading about likelihood ratio tests and came across some "real world" examples. I am confused about this example I found at the following website: https://evomics.org/resources/likelihood-ratio-test/ It has two examples of likelihood ratio tests, I'll paste one below: Since a molecular clock only allows a single rate, this is the simpler, hierarchically nested, null model. In testing a molecular clock, the degrees of freedom work out to be s-2, where s is the number of taxa in the phylogeny HKY85+clock -lnL=7573.81 HKY85 -lnL=7568.56 Then, LR = 2 (7573.81 – 7568.56) = 10.50 degrees of freedom = s-2 = 5-2 = 3 critical value (P = 0.05) = 7.82 The null hypothesis, that the rate of evolution is homogeneous among all branches in the phylogeny, is rejected. Rates of substitution significantly vary among branches and a molecular clock is inappropriate. In this example (as well as the other at the link), isn't the likelihood ratio test being performed backwards? Shouldn't the null be subtracted from the alternative? Forgive my ignorance, but maybe that doesn't matter, but it seems like it would to me as it completely makes the original equation of ratios backwards. In this situation, wouldn't that make the result incorrect? Sorry for this confusion and naivety! Thanks for any advice and clarification. 
 I'm working with three level models in Stata. Example of one would be: What I'm interested in, are the regional level estimates of random effects, which I get using: Now for each state I get the random part of the regression equation, which I understand as 'deviation from overall mean': However, I'm struggling with correct interpretation of this region specific values. What does the 2.07 deviation from the mean really tells me? Can I refer it somehow to original values of the outcome (remember that in this case it's log transformed)? Is there any transfomration of the data or results helpful in that? Or alternative way of summarizing variability across states? Even more difficult is interpretation of the difference of state-level random effects across models with different degrees of adjustment. For instance unadjusted model estimates can be obtained as: Now state's 6 adjusted estimates of random effects go from in unadjusted model to in adjusted. How could that change be interpreted? 
 In data mining problems it is common to do variable transformation, sometimes doing pairwise combinations like A/(B+1), B/(A+1) or A*B. Now, let's say after performing transformations the two best variables are exactly A/B and B/A. I would expect to see a high correlation between A/B and B/A, which is not the case in a particular problem I am addressing. First, why is this happening? Different units of measurement are making the transformed variables have outliers? Second, can I still use both variables in a predictive model, or should I drop one even if correlations between them are low? 
 I would like to compare outlier detection algorithms. I am not sure if area under roc or under precision recall curve is the measure to use. A quick test in matlab gives me strange results. I try to get the ROC and PR values for a perfect classification: Why is the area under PR = 0? Shouldn't it be 1? The AUC ROC also gives the expected result if evaluated with a constant classifier which is 0.5: So why is the area under PR curve considered better to compare outlier detection algorithms? Edit: I do not aim for a matlab specific answer to my question. This is just a MWE for you. I just wonder why so many publications recommend the AUC PR to be better suited than ROC for imbalanced datasets as their are typical for outlier detection. My quick proof of concept above seems to indicate the opposite. 
 I have conducted a mate-choice experiment where one test animal can choose between two stimulus animals at the same time. Thus, the response towards one stimulus animal is always dependent of which other stimulus animal it was paired with. Stimulus animals belong to two groups (treated/untreated, variable name Stimulusgroup) and in every mate choice experiment one stimulus animal was from the treated group, the other from the untreated group. Due to a lack of animals, I have had to re-use some of the stimulus animals. Test animals were used only once with two stimulus animals each. Hence, I believe that I need to use the random effects "TestanimalID" or - in other words - "ExperimentID" (which is the same as every test animal was used only once) as well as the random effect "StimulusanimalID" as only the stimulus animals were sometimes re-used. Hence I was aiming to model: lme(Response ~ Stimulusgroup, random=~1|StimulusanimalID/TestanimalID, data=x, method="ML") Someone pointed out to me that this is incorrect because not every test animal was paired with every stimulus animal and I have a much sparser dependency matrix. It was recommended I use one of the pdMat structures in the nlme package. However, I have no clue how to do that. Could you help me out? Thank you for your help, it's greatly appreciated! PS: If possible I would also like to add a third random effect - in specific Test animal family (I have sometimes used test animals from the same family). 
 AUC can be viewed as Wilcoxon-Mann-Whitney Test . And here is some demo, where for the R code I posted, I first calculate AUC, then use Wilcoxon-Mann-Whitney Test to calculate the number. Then verify both numbers are the same which is 0.911332. For a hypothesis testing, it is not hard to derive confidence interval. Right? Also I do not remember it Wilcoxon-Mann-Whitney Test requires normal distribution. 
 First, with such data I would expect overdispersion (if you don't know what that is, see ). That would have to be addressed with a Poisson glm, but is not an issue with usual linear regression. As said in a comment, with a poisson glm you want to include $\log(\text{DistrictSize})$ as an offset, with a linear regression you will need to use as response variable $\frac{\text{Nr. homicides}}{\text{District Size}}$. One possible reason for the discrepancy of results is that you have treated this problem differently in the two cases. You could post here some plots of results, like residual plots, so we can see what is happening. Or you could post your data as a table in the original post .... could be interesting to have a look. Another issue is the transformation you used with the linear regression. The usual variance stabilizing transformation used with count data is the square root, not the logarithm. Another issue is the choice of transformation used with linear regression. When using as response $Y_i/x_i$, you wil need weighted linear regression. Assuming as an approximation that $Y_i \sim \text{Poisson}(\lambda x_i)$, we have $$ \DeclareMathOperator{\E}{\mathbb{E}} \DeclareMathOperator{\V}{\mathbb{V}} \E \frac{Y_i}{x_i} \propto \lambda \\ \V \frac{Y_i}{x_i} \propto x_i^{-1} $$ So you should use weighted linear regression with $x_i$ as weight. A simple analysis shows that, as an approximation, the same weights are appropriate with $\sqrt{Y_i/x_i}$ or $\log (Y_i/x_i +1)$ as responses. 
 I need to study which variables influence the change in a score after a treatment. The problem is that the score (from 0 to 18) is actually the sum of six 4-levels (0 to 3) ordinal variables which represent the severity of a group of symptoms. Which regression can help me model this outcome? Given the non-continuous nature of such dependent variable, an ordinal regression should be appropriate. But since we have 19 possible levels for the outcome maybe it's not the best choice (it's more a feeling, I would be glad if someone could formally explain me why). So maybe OLS linear regression is ok in this setting, or even better quasi-poisson, since the value can only be positive. By the way, I know that summing up ordinal scores doesn't make really sense, but I need the have just one model, not one for each fo the six symptoms variables. If you have alternative designs to suggest for this kind of problem please do suggest it. 
 I have trouble in coming out with a straightforward way to know which one is better in K-means when clustering considering SSE(squared sum error). Thanks. 
 I've implemented an artificial recurrent neural network and want to start training it on a variety of tasks. I've extensive searching online and haven't found a satisfactory answer of how the algorithm can autonomously determine when to terminate training. So far I'm doing a check if the past errors are below a certain hardcoded threshold, which is definitely not universal to all tasks. Perhaps there is some probabilistic interpretation of the problem so I can terminate on some universal probability of the derivative of the network decreasing by some order of magnitude? I could throw together some statistical measure of this sort, but I'm not sure of its pitfalls and perhaps something better has been developed by researchers. 
 I suspect that part of the problem may lie in your choice of performance metric. If you measure test performance using RMSE then training the model to minimise the MSE matches the test criterion, giving a hint as to what is considered important. You may find that if you measure the test performance using the negative log-likelihood of the test set using a Poisson likelihood that the Poisson model works better (as might be expected). This may be a minor issue compared with the other issues raised, but it might be a useful sanity check. 
 Im trying different linkage methods for my hierarchical clustering problem. Now I would like to evaluate which one works better. Is this as easy as just just comparing the two Dunn's index values? Or should I take other factors into consideration? 
 If my understanding is right, you have six measurements for each subject/patient. Then these outcomes from the six measurements may be highly correlated. On the top of my head, you may want to try multivariable regression. See here 
 Anyone here who could describe the usage of adjbox() ? Is it used for very skewed distributions to have a more generous range like the boxplot? What is the formula like mean+1.5*IQR in boxplots? Is it appropriate for analysing outliers of skewed distributions? 
 I am having trouble with the common method factor technique (or CFA marker technique). I can't seem to make it work (Podsakoff, 2012). Here's my (lavaan) code: And I get these warnings: Why is this model not identified? 
 No. Think about decision trees. With many splits, it is a non-linear decision boundary. Another example is the boosting model, it aggregates many "weak classifiers" and the final decision boundary is not linear. You can think about it is a complicated code/algorithm to make the final prediction. Finally, think about K Nearest Neighbors (KNN). It is also not a linear decision function at the end layer. Here are three visualizations in 2D space (Tree, Bossting and KNN from top to bottom). The ground truth is 2 spirals represent two classes, and the left subplot is the predictions from the model and the right subplot is the decision boundaries from the model. 
 I conducted a Wilcoxon rank sum test with continuity correction, which produces W = 7195.5, p-value = 0.07094 . This lets me confidently reject the null hypothesis of no true location shift at a confidence level &lt; 0.1 My question on this is relatively general: Based on my descriptive graphs, i.e. boxplots, jitter plots, it is not immediately visible which of the two groups I am comparing has the higher/lower median/mean. Since I know that the Wilcoxon test compares pseudo-medians, I am not sure which way would be appropriate to answer the question which of the two groups of which I know are significantly different w.r.t. the outcome variable, is is greater than the other. I see the following possibilities: Conduct a one-sided test and see which is significant. Look at the means/medians within the two groups and whichever is greater is the one which is significantly greater. Are these two possibilities valid in order to answer my question? Possibility 2 can be problematic, if the mean of group A is bigger than the mean of group B, but this is different for the median. 
 i'm analyzing the results from a 20 patients prospective study and what i'm trying to do is to correct the odds ratio for bad outcome associated with the presence of atrial fibrillation for the severity of the patients conditions (measured by SOFA score). In order to do so i performed binary logistic regression with, as dependent variable the outcome of the patient and, as covariates i put the intersection between atrial fibrillation and the severity score SOFA. The numbers seems fine since the ExpB that comes out is 1,208, which is consistent with previous letterature about the topic, but i'm not sure about the procedure i used to obtain it. Could you please confirm me i used the test appropriately? 
 I have read some papers on forecasting time series with double seasonality (e. g. hourly data with daily and weekly seasonality). I understand that double seasonal ARIMA can be used for that purpose. However, some researchers instead create a different data set for each weekday and then fit a single seasonal ARIMA model to each data set (e. g. the SARIMA model estimated by a Kalman filter in Lippi et al. (2013): Short-term traffic flow forecasting: An experimental comparison of time-series analysis and supervised learning. IEEE Transactions on Intelligent Transportation Systems). The forecasting results of such an approach seem to be good in most papers but I have not found a theoretical foundation for doing that. It seems weird to me because it disturbs the pattern for the non-seasonal components of the model. For example, the lagged value for the AR(1)-component for the first value of each day is then the value of the last time point on the same weekday from the last week and not the value directly preceding the current time point. Does anyone have a theoretical justification for such an approach (or a reference to paper discussing this issue)? 
 I have a data set with missing values in the IVs. I intend to use MI and in particular PMM for the numerical variables. One of them is very skewed and has many 0s so I can't log tranform it. My question is does PMM assume normality of the variable being imputed, and if so what can I do in this situation? 
 My groups are control and alcoholics. I want to measure them based on an experiment that has 2 conditions: go (press button after x when you see o followed by x) and no go(when o appears but any letter but x appears, no button but brain response still gets recorded). The peak amplitudes I want to measure are tied to a certain stimuli so that's why I take the peak rather than a mean. Does a paired t test still make sense? I'd also like to measure reaction time but this would just be another paired t test? 
 A colleague has a dataset from a before and after study looking at a continuous outcome that was measured in individuals before and after an intervention. An unknown but not insubstantial proportion of the individuals were only measured before or after the intervention. However, due to an oversight following changes to the study design due to ethics they have only now realised that there is no way to link the before and after measurements that were made to the same individuals, i.e. there is no person ID. Consequently, the data consists of a mix of paired and independent samples, but can only be treated as independent samples. Are there any reasonable ways to mitigate this problem or address it, e.g. via some kind of sensitivity analysis? 
 Let IG denote Inverse-Gamma distribution Inverse-Gamma . If $X\sim IG(\alpha,1)$ and $Y\sim IG(\beta,1)$. Show that $\frac{X}{X+Y}\sim Beta(\alpha,\beta)$ I tried with jacobian transformation taking $Z=\frac{X}{X+Y}$ and $W=Y$ then $$X=\frac{WZ}{1-Z};Y=W$$ $$\frac{\partial(z,w)}{\partial(x,y)}=\frac{1}{(1-z)^2}$$ $$f_{z,w}(z,w)=f_x(\frac{wz}{1-z})f_y(w)(1-z)^2$$ $$f_{z,w}(z,w)=\frac{1}{\Gamma{(a)}}(\frac{wz}{1-z})^{-\alpha-1}e^{-\frac{(1-z)}{wz}}\frac{1}{\Gamma{(\beta)}}w^{-\beta-1}e^{-\frac{1}{w}}(1-z)^2$$ but I'm stuck, in some place I read that $\frac{X}{X+Y}$ is a type 3 Beta distribution, but I can't show that. 
 I'm trying to build a time series model based on a cumulative variable that never decreases. I'm interested in knowing when the observable will reach a certain value (i.e., when it will intersect with the blue line in the image below). The orange line is fixed to the last known data point and increases based on the average of the last 5 observables. The red line is not fixed and represents a linear fit based on the last 5 observables. This seems problematic because in Time Period 108 in the graph, the predicted value is less than the observable in the previous time period, which will never physically happen. The green line is not fixed and represents a linear fit based on all observables. I'm wondering if someone can suggest an alternative/better approach to modelling this type of situation. EDIT / MORE INFO: The time periods (e.g., 101, 102, etc.) represent 2-week intervals. The cumulative information is the number of items completed. E.g., at the end of time period 107, there were approximately 200 total items completed. At the beginning of time period 106, there were about 150 items completed. So between time points 106 and 107, approximately 50 items were completed. I'm interested in when approximately 410 items are completed (represented by the blue line). The number of items completed starts at 0. Time period 101 is actually the first time period, it's just 101 has a separate (shouldn't be relevant for this post) meaning to our business. It can basically be treated as a label. I see no particular reason why the number of items completed should follow an S-shape curve would. 
 I am doing research in spread spectrum communication, and many papers frequently use the terms Periodic Cross-Correlation and Aperiodic Cross-Correlation. However, I cannot find a clear definition of these terms and their differences between them. Also, what is the definition of Odd Cross-Correlation? Could one share their thoughts on that and maybe share some books/papers? 
 If we are given a list of $n$ independent measurments $\{0,0,1,0,1, \dots, 0,1,1,0,1\}$ of binomially distributed random variable $X$, we can use the sign test to check, whether the difference between numbers of $1$s and $0$s is statistically significant, i.e., whether we can reject the null hypothesis that the parameter $p$ in $\text{Binomial}(n, p)$ equals $1/2$. Since the distribution (under the null hypothesis) of $X$ is known and the probabilities $P(|X - pn|&gt;k)$ are not hard to compute using computer, why it is so often mentioned that z-test can be used for obtaining p-values in the sign test (when $n$ is big enough), while, at the same time, the exact procedure is not even described? 
 Kmeans has many local minima (cluster results) and it is possible that certain local minima have higher or lower or equal Loss with others. In your picture, it is possible in all three ways. You can just calculate the loss the check. In sum, from the figure, it is not easy to tell. From theory, it is possible to have larger, smaller or equal loss values. 
 I need to implement the metropolis algorithm to solve the example titled Cheating among students here . In summary the aim is to estimate the frequency of students cheating in an exam. The experiment is as follows: Each student is asked to toss a coin. If the result is heads then student will answer honestly if he/she cheated. If the result is tails the student will toss a second coin and answer "yes, I cheated" if it lands heads and "no, I didn't" if it lands tails. The experiment involved $N=100$ trials and the interviewer got $X=35$ "yes" answers. I need to find the frequency of cheating using a raw implementation of the metropolis algorithm. To that end I have identified the following variables in the experiment: $FC \sim Bernoulli(\theta=0.5)$. The probability distribution for the first coin. $SC \sim Bernoulli(\theta=0.5)$. The probability distribution for the second coin. $TA \sim Bernoulli(\theta=P)$. The probability of a true answer. That is, a student honestly answering. At this point is where I start to get lost. I think $P$ is the probability that I am interested in when using metropolis. However, it depends on the two other probabilities $FC$ and $SC$. Moreover, I would say it depends on a new random variable $Z = FC \cdot TA + (1-FC)\cdot SC$ that depends on the two others. My first question is: Am I right until this point in my reasoning?. If so, from what I understand, metropolis proposes a value at each iteration. The probability of this value is computed taking into account the distributions listed above. So, does that mean that at each point I have to evaluate the probability of the proposed value according to $Z$?. 
 I have one realization $x$ drawn from any random distribution. I also have a large set $X$ of other (iid) draws from the very same distribution. Now I'm interested in the distribution of the rank of $x$ in $X$. My intuition is that this would be uniform, but I couldn't find a proof of that. Background : In the context of a Fisher re-randomization test, I'd like to obtain a p-value for the null that a number is drawn from a certain distribution. I can draw from that distribution and was now thinking I could just do another 1000 draws, look at the relative rank of my number in the distribution of these draws, and use that as a p-value: $$p^{rightsided} = rrank(x,X)$$ $$p^{leftsided} = 1-rrank(x,X)$$ $$p^{twosided} = 1-|rrank(x,X)-0.5|*2$$ (where $rrank(x,X)$ is the relative rank of $x$ in $X$) This would obviously only have power against the alternative that the original number stems from a distribution with more extreme values being more likely. 
 Given 3 continuous, normally distributed, independent, time-series vectors ( X , Y , Z ) and 1 continuous, Poisson distributed, dependant, time-series vector ( A ). How can I rank the independent vectors in terms of how much they influence ( A )? Baring in mind there will be a delay (lag), of an unknown time period, in changes X , Y , Z causing changes in A . My initial thought was cross-correlation, but I'm struggling to evaluate my results to determine which vector has the most influence and whether that influence is significant. 
 I am running a binomial logistic regression with a logit link function in R. My response is factorial [0/1] and I have two multilevel factorial predictors - let's call them a and b where a has 4 factor levels (a1,a2,a3,a4) and b has 9 factor levels(b1,b2...b9). Therefore: mod &lt;- glm(y~a+b, family=binomial(logit),data=pretend) summary(mod) The model output would then show all the information about the model as well as the coefficients. There is a factor level for both a and b missing (a1 and b1) from the summary output. I understand that it is constrained in the "intercept" of the model. I have read that if I want to remove the intercept term and see the estimates for those factor levels I can just add -1 to the model formula, i.e.: mod2 &lt;- glm(y~a+b-1, family=binomial(logit),data=pretend) summary(mod2) In the new model (mod2) the intercept term is then gone and variable a's factor-level a1 is given amongst the list of coefficients. But, variable b's factor-level b1 is still missing and given that there is no intercept term anymore, how can I interpret the odds-ratio for that factor level then? Could someone please explain to me how to get the coefficient for b1 too and why this is happening? Thank you. 
 I'm currently doing a translation and adaptation of a questionnaire composed of 36 items. After finishing the translation, synthesis and back-translation steps, 40 people randomly selected from a population of undergraduates were asked to rate how well the could understand what was being asked, with responses being given on a scale of 1 to 6. So what I'm testing is the clarity of the instrument, namely how comprehensible the questions are. This is my hypothesis: if the instrument has good comprehensibility, the average score for item comprehension will be high and the correlation of scores given by raters to each item will be low (indicating that when there is misunderstanding, it is more probably due to random noise than to bad question design). So, I'm interested in knowing both the average scores and the level of agreement among raters. For that, I'll run 2 tests: (1) for each question, calculate for the mean of ratings given by all judges. (if mean rating is high, probably the question has good comprehensibility, or there is some other confounder like social desirability). (2) calculate the Intraclass Correlation Coefficient (ICC) for the consistency presented by the raters, using a two-way random effects average-measures model. (if ICC is low, probably there's no agreement; if ICC is high, probably there is good agreement) My doubt here is with the hypothesis testing: If I understand well, a high ICC (alternative hypothesis, r &gt; 0) would indicate that raters generally agree with how well they can understand each item. If that is the case, it would be plausible that the low rating is due to the item itself and not som random variation. On the other hand, a low ICC (null hypothesis, r = 0) would indicate that raters disagree on their measures and if there are variations in the scores given by the items, we could not say that it is due to item or rater characteristics. Thus we might be seeing no more than random variations. Would that interpratation of the ICC be incorrect? Here's the data I got from irr::icc in R: Single Score Intraclass Correlation Model: twoway Type : agreement Subjects = 36 Raters = 33 ICC(A,1) = -0.000293 F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 F(35,1084) = 0.977 , p = 0.507 95%-Confidence Interval for ICC Population Values: -0.005 And here the means and std. deviation for each question: Mean Std. Dev 5.84 .495 5.79 .528 5.87 .414 5.89 .388 5.87 .578 5.81 .569 5.95 .324 5.82 .563 5.76 .634 5.68 .904 5.82 .865 5.82 .865 5.97 .162 5.74 .724 5.87 .811 5.87 .811 5.86 .536 5.97 .162 5.79 .875 5.87 .529 5.89 .649 5.97 .162 5.87 .811 5.84 .594 5.87 .811 5.89 .388 5.79 .875 5.82 .865 5.87 .811 5.67 1.195 5.68 1.068 5.84 .823 5.82 .652 5.92 .359 5.89 .649 5.84 .718 
 I would recommend the following: "An Introduction to Statistical Learning" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It is freely available as a PDF and a series of video lectures . "Python Machine Learning" by Sebastian Raschka. It is very well-written, good combination of explanations and code, and the author is responsive. "Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies", by John D. Kelleher, Brian Mac Namee and Aoife D'Arcy For more advanced treatments: "The Elements of Statistical Learning: Data Mining, Inference, and Prediction" (as mentioned previously). "Bayesian Reasoning and Machine Learning", by David Barber. "Machine Learning", by Tom Mitchell. "Machine Learning: A Probabilistic Perspective", by Kevin P. Murphy. "Pattern Recognition and Machine Learning", by Christopher Bishop. 
 There is a data set that has yearly data for mean values of the number of ice creams eaten for 5 persons. The data set has values for 5 persons and 5 years(5x5 panel). Suppose due to some theory it is assumed that mean value for the sample should be 50 units of ice creams (null hypothesis). How to perform a hypothesis test for this sample. Please help . Thank you. 
 Let me add this highly readable often-forgotten text, Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2012). Pattern classification. John Wiley &amp; Sons. 
 If we apply a transformation $\phi$ to all input weight vectors ($\mathbf{x}^{(i)}$), we get the following cost function: $J(\mathbf{w}, b) = C {\displaystyle \sum\limits_{i=1}^{m} max\left(0, 1 - y^{(i)} (\mathbf{w}^t \cdot \phi(\mathbf{x}^{(i)}) + b)\right)} \quad + \quad \dfrac{1}{2} \mathbf{w}^t \cdot \mathbf{w}$ The kernel trick replaces $\phi(\mathbf{u})^t \cdot \phi(\mathbf{v})$ by $K(\mathbf{u}, \mathbf{v})$. Since the weight vector $\mathbf{w}$ is not transformed, the kernel trick cannot be applied to the cost function above . The cost function above corresponds to the primal form of the SVM objective: $\underset{\mathbf{w}, b, \mathbf{\zeta}}\min{C \sum\limits_{i=1}^m{\zeta^{(i)}} + \dfrac{1}{2}\mathbf{w}^t \cdot \mathbf{w}}$ subject to $y^{(i)}(\mathbf{w}^t \cdot \phi(\mathbf{x}^{(i)}) + b) \ge 1 - \zeta^{(i)})$ and $\zeta^{(i)} \ge 0$ for $i=1, \cdots, m$ The dual form is: $\underset{\mathbf{\alpha}}\min{\dfrac{1}{2}\mathbf{\alpha}^t \cdot \mathbf{Q} \cdot \mathbf{\alpha} - \mathbf{1}^t \cdot \mathbf{\alpha}}$ subject to $\mathbf{y}^t \cdot \mathbf{\alpha} = 0$ and $0 \le \alpha_i \le C$ for $i = 1, 2, \cdots, m$ where $\mathbf{1}$ is a vector full of 1s and $\mathbf{Q}$ is an $m \times m$ matrix with elements $Q_{ij} = y^{(i)} y^{(j)} \phi(\mathbf{x}^{(i)})^t \cdot \phi(\mathbf{x}^{(j)})$. Now we can use the kernel trick by computing $Q_{ij}$ like so: $Q_{ij} = y^{(i)} y^{(j)} K(\mathbf{x}^{(i)}, \mathbf{x}^{(j)})$ So the kernel trick can only be used on the dual form of the SVM problem (plus some other algorithms such as logistic regression). Now you can use off-the-shelf Quadratic Programming libraries to solve this problem, or use Lagrangian multipliers to get an unconstrained function (the dual cost function), then search for a minimum using Gradient Descent or any other optimization technique. One of the most efficient approach seems to be the SMO algorithm implemented by the library (for kernelized SVM). 
 This is for homework, so rnbinom doesn't help I've created two functions: combr (combinations with repetitions) and another called negbinom. This is combr: And this is negbinom: Negbinom generates a number. How do I create a function in order to generate "n" random numbers by recalling negbinom "n" times. Or similarly, how do I create my own "rnbinom"? 
 As long as the specific $x$ is prespecified, its rank should be uniform. Thats because all possible permutations of $n$ iid variables are equally probable. 
 I'm experimenting with the gradient boosting machine algorithm via the package in R. Using a small college admissions dataset, I ran the following code: and found to my surprise that the model's cross-validation accuracy decreased rather than increased as the number of boosting iterations increased, reaching a minimum accuracy of about .59 at ~450,000 iterations. Did I incorrectly implement the GBM algorithm? EDIT: Following Underminer's suggestion, I've rerun the above code but focused on running 100 to 5,000 boosting iterations: The resulting plot shows that the accuracy actually peaks at nearly .705 at ~1,800 iterations: What's curious is that the accuracy didn't plateau at ~.70 but instead declined following 5,000 iterations. 
 I have a rather conceptual question. I am investigating concordance between two response levels, namely mental and genital sexual arousal, in women. We are interested in estimating the degree of concordance between these two levels of response. Two analyse concordance, we measure both levels of response over a period of 10 minutes and create 10-sec (or 30-sec) bins for mental and genital arousal. Multilevel-modeling is the appropriate approach, because levels of physical arousal are quite different between women. Some colleagues of mine have argued that the SMALLER the b value for genital arousal predicting subjective arousal (and vice versa), the greater sexual concordance is. Their argument is that a small b value indicates that less change in the predictor is necessary for 1 unit change in the outcome variable. Therefore, a smaller b value might indicate that, for example, the same change in genital arousal is related to lesser change in subjective arousal -&gt; meaning that the women got better in detecting genital arousal. I cannot really grasps if this is a reasonable approach. What do you think? It was my impression that the greater the b value the stronger the association between our predictors and outcome variables is, leading to stronger concordance related to greater b values. Best, Julia 
 It is true that your data is not Normally distributed (which I presume is why you also ran a Poisson regression) but your data is likely not a Poisson distribution either. The Poisson distribution assumes that the mean and the variance are the same, which is likely not the case (as mentioned in other answers - you can capture this discrepancy and incorporate it into the model). Since your data isn't really a perfect fit for either model, it makes sense that OLS may perform better. Another thing to note is that the ordinary least squares estimates are robust to non-Normality which may be why you're getting a reasonable model. The Gauss-Markov Theorem tells us that OLS coefficients estimates are the best (in terms of mean squared error) linear unbiased estimators (BLUE) under the following assumptions, The errors have a mean of zero The observations are uncorrelated The errors have have constant variance There is no assumption of Normality here so your data can very well be reasonable for this model! With that being said, I would look into a Poisson model with an over-dispersion parameter baked in there and you should get better results. 
 I am looking for literature (guidelines) which discuss the consequences of unbalanced designs on running a (two-way) within-subject ANOVA and pros/cons of various counter-measures. I came up with fragmented anecdotes ranging from "dropping subjects with missing data" (without further qualification of missing) to "rebalancing" of all sorts, Type III SS, and substituting an mixed-effect model for an ANOVA. However, I miss some authoritative literature references. I searched for previous answers, and despite some useful hints on given designs, there are no pointers to methodical and research literature provided. I am asking for the latter explicitly. Any reference would be much appreciated! 
 As far as I'm aware, there is no established convergence criteria that work in all cases. Keep in mind that you're dealing with a non-convex optimization problem where many, actually an unknown number of local minima may exist across a large stretch of the search space. I presume you're using some sort of gradient descent algorithm, so the optimum you find is only one of many solutions. There is no point of putting in a huge effort just to land precisely on one local optimum that you don't even know how good it is. My advice is to pick many random starts and run gradient descent to a small gradient neighborhood, then pick the best solution. 
 1) "The distribution of Y within each group is normally distributed.” It’s the same thing as Y|X and in this context, it’s the same as saying the residuals are normally distributed." (is it appropriate to say "distribution is distributed"?) (Source: Why do we care so much about normally distributed error terms (and homoskedasticity) in linear regression when we don&#39;t have to? ) 2) Normality of residuals vs sample data; what about t-tests? (Scrotchi's answer) So for a t-test Normality verification, residuals or the individual groups' outcomes may be analyzed. Why then in the earlier mentioned book the author analyzes residuals by group , instead of looking at the overall residual plot? Can someone please comment whether this is correct too? 
 I have implemented the gradient decent algorithm for the SVM (linear case). I would like to check if my implementation is correct. I see 3 possible solutions: Write some unit tests for the code Check the results using another tool (python libs) Check if the estimated parameters have the expected distribution. For the latter approach, I would like to know if there are any papers that provide some hint on the confidence interval of the estimated parameters of the SVM (it would be great for the case when kernel trick is applied). 
 I have to understand what are the main differences between the concept of empirical risk and the one of Bayes risk. I have the following idea now: empirical risk mathematically speaking is $\frac{1}{n} \sum_{i = 1}^n L(x_{i},y_{i},f(x_{i}))$ where $L$ is the loss function with $x_{i}$ that is a data point, $y_{i}$ that is the label for $x_{i}$ and $f$ that is the estimator function. It is the average value of the loss function applied on a certain set of training data. Bayes risk: mathematically is $R^{*}_{L,P}(x) = inf_{f: X\rightarrow R} \int L(x_{i},y_{i},f(x_{i})) dP(x,y) $ where $dP(x,y)$ is the joint distribution. However I think that it is just the greater function that minimizes the real risk by definition of extreme inferior. I would like to know, conceptually, what are the differences between them. What do they mean individually on data? 
 I want to use Kendall in order to measure the distance between rankings A and B. How do we deal with different ranges? For example, as shown below: let's say we have the following rankings A and B - Number 7 in A is not available in B and number 8 in B is not available in A. How do we measure the distance between these rankings using Kendall tau? What I meant by this is, let's say Person A ranks America as number 1, UK as number 2, Germany as number 3, France as number 4, Brazil as number 5 and Italy as number 6. In the other hand, Person B ranks UK as number 1, France as number 2, America as number 3, Spain as number 4, Brazil as number 5 and Germany as number 6 
 before doing data analysis, I have to find public data sources first. The goal is to find housing services related data, or people data related to these housing services, in different regions of Metro Vancouver. Especially focus on those housing aims at improve individual well being or community well being. I searched for many lists of this type of services, checking their websites, social media or reading their reports, but could not find public data for my data analysis. I have also checked Canada Council, Statistics Canada, they don't have specific data for regions in Metro Vancouver... Do you have any suggestions to find this type of data contains Metro Vancouver regional info? 
 The gbm package has a function to estimate the optimal # of iterations (= # of trees, or # of basis functions), You don't need caret's train for that. 
 We want to test for the existence of a correlation between a variable $X$ and a second variable $Y=ab$ ($a$ and $b$ being independent variables representing observational data). We know that the variable $X$ is correlated with the variable $Z=ac$ ($a$ as described above, and $c$ a different independent variable, representing observational data as well). How can one test for the presence of the $X vs Y$ correlation while accounting and controlling for the presence of $X vs Z$ (which one considers to have been secured)? As a subsidiary question, should the correlation testing be done simultaneously, or can be done independently? 
 For SVM the objective is convex, which means you will have a unique solution for each problem setting. There are many SVM libraries available, why not compare your results with the libraries' output? For SVM your predictor is $$ f(\mathbf x)=\sum_i \alpha_iy_iK(x,x_i) $$ And you are trying to use algorithms to get all the $\alpha$. You can directly compare the $\alpha$ got from your algorithm with the $\alpha$ from the libraries. Also if your interest is trying to verify your gradient decent works well, you can use many optimization toolboxes to optimize your objective function and compare results. Your optimization problem should be $$ \max_\alpha \sum_i\alpha_i-\frac 1 2 \sum_i\sum_j \alpha_i \alpha_j y_i y_j K(\mathbf x_i,\mathbf x_j) $$ You can calculate the Kernel matrix and feed it into a solver to get all $\alpha$ Many R solver can detect if the problem is convex and if the result is converge. Please check this task view for details. 
 Let $X_1,X_2,...$ be a sequence of i.i.d. random variables having p.d.f as, $f(x)=\frac{1}{B(6,4)}x^5(1-x)^3,0&lt;x&lt;1.$ Let $Y_i=\frac{X_i}{1-X_i}$ and $U_n=\frac{1}{n}\sum Y_i.$We have to find,if the distribution of $\sqrt n\frac{(U_n-2)}{\alpha}$ converges to $N(0,1)$ as $n\rightarrow\infty$,what should be the possible value of $\alpha.$ I tried in this way, $X\backsim B(6,4)$$\rightarrow Y_i=\frac{X_i}{1-X_i}\backsim B(6,4)$ of 2nd kind. Now we have to find $E(U_n)$ and $V(U_n)$,as,$\frac{(U_n-E(U_n))}{\sqrt {V(U_n)}}\rightarrow N(0,1)$ when $n\rightarrow{\infty}$ From the question,it seems that $E(U_n)$ should be $2$.But integrating $f(x),$ I am getting $E(X_n)=E(U_n)=\frac{1}{840}$ which is quite impossible in this case. How can we get the required result? 
 In general, boosting error can increase with the number of iterations, specifically when the data is noisy (e.g. mislabeled cases). This could be your issue, but I wouldn't be able to say without knowing more about your data Basically, boosting can 'focus' on correctly predicting cases that contain misinformation, and in the process, deteriorate the average performance on other cases that are more substantive. This link ( Boosting and Noise ) shows a better description than I can provide of the issue. This paper ( Random Classification Noise ) by Long and Servedio provides more technical details of the issue. 
 What you have displayed is a classic example of overfitting. The small uptick in error comes from poorer performance on the validation portion of your cross-validated data set. More iterations should nearly always improve the error on the training set, but the opposite is true for the validation/test set. 
 I am trying to estimate the regression model, say standard linear model, with the error term having a Pareto distribution instead of normal. Although it is fairly straightforward to construct the maximum likelihood function, it is practically not easy because we do know how to obtain the estimate for the minimum of the error term as it is an unobserved variable. What I can think of to do it is to estimate the coefficient and Pareto parameters with the grid of predetermined the minimum threshold Pareto parameters and choose the one with the largest likelihood value. Is that how people practically do that? The model I have in mind is: $y_i=\beta_0 + \beta_1 x_i + \epsilon_i$ where $\epsilon_i \sim Pareto(k,\alpha)$ where $\epsilon_i&gt;0$, and $\epsilon_i &gt;k&gt;0$. So, you would estimate $\beta_0$, $\beta_1$, $k$ and $\alpha$. 
 How do I create a function in R in order to generate "n" random variables with a negative binomial distribution? This is for homework, so rnbinom doesn't help. 
 Here is my solution from first principles: $$P\left(z' \leq z\ \left|\ z' = \frac{x}{x+y},\ x \sim X,\ y \sim Y\right. \right) = P\left(\frac{x}{x+y} \leq z\right) = $$ $$ \int dP_{X}(x) \int dP_{Y}(y)\ I\left[ \frac{x}{x+y} \leq z\right] = \iint dx\ dy \ f_{X}(x)\ f_{Y}(y)\ \theta\left(y \frac{z} {1-z} - x\right),$$ where $\theta(x)$ is a Heaviside Step function . Differentiating $P(z)$ by $z$ one can obtain that $$f_{Z}(z) = \frac{d}{dz} P(z' &lt; z) = \int dx\ f_{X}(x) \int dy\ f_{Y}(y)\ \delta\left(y \frac{z} {1-z} - x\right) \frac{y}{(1-z)^2},$$ where $\delta(x)$ is a Dirac Delta function . So integrating over $x$ one could see that $$f_{Z}(z) = \int dy \ f_{Y}(y)\ f_X\left(y \frac{z}{1-z}\right) \frac{y}{(1-z)^2} =$$ $$\frac{1}{\Gamma(\alpha)\Gamma(\beta)} (1-z)^{-2} \int dy\ y^{-\beta -1}e^{-\frac{1}{y}}\cdot y^{-\alpha - 1} z^{-\alpha -1} (1-z)^{\alpha+1} e^{-\frac{1-z}{yz}},$$ and after simplification and integration by $y$ $$f_{Z}(z) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} (1-z)^{\alpha-1} z^{\beta - 1}$$ which is probability density of beta distribution $Beta(\beta, \alpha).$ P.S. You can avoid using of Step/Delta functions by specifying precise limits of the inner integral, but I find that way less handy. 
 You may want to read up on Lagrangian duality. Many convex solvers will in some sense give you a certificate of optimality in the form of giving you both $x$ and the value of the multipliers $\lambda$. Let $L(x, \lambda)$ be the Lagrangian: Trivially, for any $\hat{x}$ and $\hat{\lambda}$: $$ \min_x L(x, \hat{\lambda}) \leq L(\hat{x}, \hat{\lambda}) \leq \max_\lambda L(\hat{x}, \lambda) $$ Let us define: $$g(\lambda) = \min_x L(x, \lambda) \quad \quad f(x) = \max_\lambda L(x, \lambda) $$ $f(x)$ gives an upper bound on the optimal value of L. $g(\lambda)$ gives a lower bound on the optimal value of $L$. If $f(x) = g(\lambda)$, then $x$ must be a solution to the optimization problem! It turns out that for convex problems that satisfy some kind of constraint qualification (eg. Slater's condition), that there exist $x$ and $\lambda$ such that $g(\lambda) = f(x)$, that is, strong duality (aka the saddle point property) holds. Weak duality: (requires essentially nothing) Since this is true for any $\hat{x}$ and $\hat{\lambda}$, it holds for specific values: $$ \max_\lambda \min_x L(x, \lambda) \leq \min_x \max_\lambda L(x, \lambda)$$ Strong duality: (requires stronger conditions, eg. convex problem + Slater's condition) $$ \max_\lambda \min_x L(x, \lambda) = \min_x \max_\lambda L(x, \lambda)$$ 
 In Python I would do this the following way: Then you get the weights how much each original component contributes into the new component in four .txt files. You can take the genes that have an absolute value above an arbitrary threshold. 
 Unlike Spearman's rho, Kendall's tau doesn't actually require assigning numerical rankings to entries. Instead, it functions off of concordant and discordant pairs. For example, say you have the following rankings An example of a concordant pair would be America and Germany. If you ignore everything else, both A and B rank the two in the same order. An example of a discordant pair would be America and the UK. Here the order of ranking differs between the two people. A ranks America before the UK, but B ranks the UK before America. You don't need a numeric ranking or even a full global ranking in order to compute this - you just need a relative ranking for each pair of entries. Also, as you're just looking at the two entries, what's happening with other entries doesn't matter. So how do you deal with missing entries? Probably the easiest way is just to ignore them. That is, when you're counting the number of concordant and discordant pairs you just throw out any pairs involving those items. Since Kendall's tau isn't sensitive to the absolute ranking, throwing out an entry shouldn't greatly effect the correlation, so long as we don't affect the relative ranking of the other entries. The one trick here would be that the total number of entries (the "$n$" in the Kendall's tau formula) would be the post-censoring number of entries - the number of entries which are common to both. (Such that the $n(n-1)/2$ is still counting the total number of pairs considered.) For my table above this would be 4. Another approach you could use is to treat the missing entry as a "tie" toward everything. The rationale being that you can't tell if a missing entry is ranked higher or lower than any other entry, so you can compromise by calling it a tie. Given that Kendall's tau is computed pairwise, there's no technical issue with considering an entry to be tied with all of the other entries. If you've settled on counting it as a tie, you then have to decide which of the various tie-handling methods for Kendall's tau you're going to use. When doing so, keep in mind that your $n$ will be the total number of possible entries, including the unpaired entries on both. For my table above, this would be 6. Personally, I would suggest going the route of just ignoring unmatched entries, especially if there are other possible entries you're also ignoring from both sides. (e.g. Should we also include Burkina Faso in the ranking, despite no one mentioning it?) I'd only go with the ties approach if you deliberately want to penalize the correlation ranking because of the unmatched pairs. 
 There is a dataset with about 20 dimensional input vectors and binary output vector. There are about 100K training examples. When trained using logistic regression the accuracy on training set is about 50% and trained using random forest classifier(100 trees) the accuracy on training set is 100%. It is a general notion that random forest classifiers are less prone to overfitting. But in this particular case, the test accuracy of the logistic regression classifier is much higher than the random forest classifier. What can we infer about the distribution of the dataset from this observation? 
 Codes to reproduce a similar result, without grid search, 
 The problem here is not that your random effect and your response variable are somewhat correlated, its that they are perfectly correlated. The model estimates a baseline level of the dependent variable for each random effect in the model. So, here the levels of your random effect can explain ALL of the variation in your response variable. Generally, when using something like zip code as a random effect in the context of disease, people are analyzing occurrence rates, given some baseline rate within that community modeled by the random effect, as a function of predictors that may influence that occurrence rate. Or, you might analyze the effect of income and water source on lead levels in blood, while using zip code of those people as a random effect, to account for the fact that there are other environmental factors that amy be geographically unique but unobserved that drive variation in baseline lead levels in blood. In this example, given these data, the incorporation of zip code as a random effect is inappropriate, as it cannot give you baseline information on this binary outcome. Your dependent variable and random effect are perfectly correlated, and this is why you are having trouble. 
 I apologize if this question is very simple but I have found a lot of information on how to interpret log transformed variables ( , Interpretation of log transformed predictor ) but not on logit transformations and I am confused. I have a linear mixed effect model in which the dependent variable is the proportion of home range overlap (logit transformed), and two predictor variables, home range size and season (untransformed): logit(overlap proportion) ~ 1 + Home range size + season + 1|indiv_code I want to know how to interpret the effect of the home range size on home range overlap. The beta coefficient of home range size is -0.00​19829​. My question then is: ​Can I interpret the effect of home range size as I would do in any logistic regression? And if so, could I then say that for every unit increase in home range size there w​ill be​ ​a ​99.80% increase in the odds of ​home range ​overlap, ​given that exp(-0.0019829) = 0.9980​ (99.80%​)​. This was my understanding after reading about how to interpret the effect of continuous variables in logistic regression ( ) Many thanks! 
 The cdf of the Negative Binomial distribution $\mathrm{Neg}(m,p)$ is available as , which means $\mathbb{P}(X\le x;n,p)$. You can alternatively recode it as $$\mathbb{P}(X\le x;n,p)=\sum_{i=1}^x {n+i-1 \choose i-1} p^i (1-p)^{n-i}$$ The distribution can be generated from the pdf $F$ by the inverse pdf method: namely, generate a uniform variate $U$ and, when $U=u$, return $F^-(u)$ as a simulation where $F^-$ denotes the generalised inverse of $F$: $$F^-(u)=\max\{k\in\mathbb{N};\ F(k)\le u\}$$ This means you generate a uniform variate with value $u$ and check when the pdf gets above $u$. 
 No, you should not "unnormalize"! When body weight is already in your model, including other (metabolic) variables as a total, will, in a sense, represent body weight multiple times in the model (and this could well lead to problems with multicollinearity, as one might well expect that different (imperfect) measures of body weight will be highly correlated. So, when body weight is in the model, all other such variables should be expressed relative to body weight. 
 For a unit change in home range size, the odds change by a factor of exp(-0.0019829)=0.9980 so the overlap expressed (in terms of the odds) decreases by 1 - 0.998 = 0.2%. 
 I am reading about the Deep Belief Network from here . As per my understanding, DBN can be thought of multiple layers of RBM stacked on each other, and in order to train a DBN, we need to train each RBM at a time (for example using Contrastive Divergence). However, one thing that, I couldn't understand is the usefulness of using tied weights. It seems to me that propagating the 'learning' from one layer to the higher level has some thing to do with the tied weight, though it is not clear to me how. Can someone please put some light on this? 
 You might need these papers: Multivariate Cauchy EDA Optimisation There is a detailed explanation of the difference between Gaussian &amp; Cauchy distribution in Chapter 2, along with empirical results and other analyses. How effective is Cauchy-EDA in high dimensions? Further reading for Cauchy distribution used in EDA published this year. Hope it helps. 
 I am quite new to machine learning, and I am curious as to whether other developers publish their trained classifiers (with trained weights) online for others to test out and use. I know there are a lot on Mathworks, however these are not trained, and I am currently trying to find a good classifier (for the MNIST database) with maybe a &gt; 99% success rate. I have trained one myself but have capped around 98.77%. Is there maybe some online repository (like mathworks) that I am unaware of that has these? Thanks. 
 There are lots of choices of pseudo $R^2$'s. Lots of them are very flawed. Generally speaking, there's usually no reason that the $R^2$ produced from OLS will be a comparable value to a given pseudo $R^2$; rather, pseudo $R^2$'s are typically used for comparing models of the same distributional family. 
 I am having two different data sets. First data-set is ratings given by experts for particular product (sample size of 200) and the second data-set is ratings given by users of that product (Sample size of 450). Now, I would like to know the inter-dependencies between user and expert ratings? How can I check the same using SPSS? If someone know the answer, please let me know. 
 Is there a way to measure the effect individual predictors have on an outcome for a Random Forest Regressor? If there's not something similar to a regression coefficient, is there a way to utilize importance measures (MSE,etc.) as a sort of proxy measure? 
 Either can be better This depends on the scales of your axes. If x is [0;1] and y is [0;100000] then the upper is better. If x is [0;100000] and y is [0;1] then the lower is better. Unfortunately, k-means is very sensitive to scale. 
 If you're only looking at one product, there is no reason to do a correlation. You don't have anything to link the expert reviews to the user reviews. (for instance, how would you do a scatter plot of experts on one axis and users on the other). Think about doing a simple t-test to test if the magnitudes of the two review types are not the same. 
 I have read (for example, here ) that the Epanechnikov kernel is optimal, at least in a theoretical sense, when doing kernel density estimation. If this is true, then why does the Gaussian show up so frequently as the default kernel, or in many cases the only kernel, in density estimation libraries? 
 I am using HMM-GMM tool for Matlab by Kevin Murphy.My Frame work has a feature extraction Algorithm (MFCC,Spectrogram) followed by a HMM-GMM classifier. I am trying to set the the number of states and number of Gaussians in each state. I am wondering changing the window size on feature extraction algorithm(also the overlapping ratio) would have any effects on the decision of number of states and number of Gaussians? to my best knowledge, number of HMM states related to the quasi-stationary duration of input signal(for example it is 16 or 32 ms for speech), but how about if i don't know that quasi-stationary interval for a particular animal call and/or My input seems non-stationary? Does anybody knows an easy way to test or fix the following parameters and also show the relation in between them? Window size/overlapping ratio, Number of states, Number of Gaussians i am trying to find an easy way to fix those parameters. If you believe there is no relation in between window size and HMM-GMM parameters, then Does any body knows how i can apply BIC to determine those parameters. I made searches but couldn't find a reply that has been accepted to be helpful or correct. 
 Is there a test in Matlab if a time series satisfies properties of being a fractional Brownian motion? 
 The Gaussian kernel is used for example in density estimation through derivatives: $$\frac{d^if}{dx^i}(x)\approx \frac{1}{bandwidth}\sum_{j=1}^N \frac{d^ik}{dx^i}(X_j,x)$$ This is because the Epanechnikov kernel has 3 derivatives before it's identically zero, unlike the Gaussian which has infinitely many (nonzero) derivatives. See section 2.10 in your link for more examples. 
 I am struggling with a statistical method to assess whether two populations have "changed" over time. In my case I have a clonal bacteria species, some strains with a mutation conferring resistance to drugs and others without this mutation (e.g. susceptible). Under drug therapy, one would expect a small clonal subpopulation with resistance to eventually dominate the population (because everything else is eventually killed off). In my case, I will first attempt some test tube experiments. Obviously one can set up a highly controlled experiment where the starting conditions is 1% resistance of a single mutation and 99% the susceptible clonal strain, adding drugs, and watching over time. In this case I was going to try a non-parametric test for trend such as Mann-Kendall and then using Sen-Slope estimation. However, I want to also look at this situation in 'nature' where there are multiple mutations and I am struggling to conceptualize how to analyze my data. Also, to make things worse, I will instead of being able to count population size, unfortunately my genetic tests will only give me the proportion of the sample (0-100%) for each mutation. The closest analogous situation I can find is Clonal Interference (see bottom image below which comes from https://en.wikipedia.org/wiki/Clonal_interference ). Any suggestions? 
 The study focus is mediational effects of spirituality on the relations between engagement and wellbeing. Rather than the mediator decreasing the relations between engagement and wellbeing when it entered the equation, it increased the magnitude of the coeffs between the predictor and outcome. What is the explanation for this? 
 in a derivation for the gradient of a collaborative filtering system (similar to Probabilistic Matrix Factorization), I got to the following expression for the gradient of a latent vector $\mathbf{u}_i$: $\frac{\partial}{\partial \mathbf{u}_i} = 2\sum_{j=1}^M \left[ (\mathbf{u}_i^T\mathbf{v}_j - R_{ij})\mathbf{v}_j \right] + \lambda_u (\mathbf{u}_i - \boldsymbol \mu_u)$ The gradient is a sum of terms, hence I can apply SGD! But I wonder, how should I treat the term that is outside the summation? Should I estimate $M$ and then add it to the individual $j$-gradients multiplied by $1/M$? Should I use it only every $M$ updates? Should I use it randomly or with a precise schema? What would your choice be, and why? thanks for the hints. 
 I'm trying to understand some basics of neural nets and am trying to build one that can predict the function y = 2*x in R: library(caret) library(nnet) v1 &lt;- 1:1000 v2 &lt;- v1*2 df &lt;- data.frame(v1, v2) mod &lt;- train(v2 ~ v1, data=df, method="nnet") predict(mod, df) I end up getting all 1's for the answers. I'm able to predict other, more complex functions using a neural net, but, for some reason, I can't predict this function. Does anyone happen to know why that might be? Thank you! Bill 
 The key is the word "deep" in deep learning. Someone (forgot ref) in the 80s proved that all non-linear functions could be approximated by a single layer neural network with, of course, a sufficiently large number of hidden units. I think this result probably discouraged people from seeking deeper network in the earlier era. But depth of the network is what proved to be the crucial element in hierarchical representation that drives the success of many of today's applications. 
 I want to use the R implementation of t-distributed stochastic neighbor embedding to sort out a large number of sets of numbers of cDNA reads from different cell lines. I am interested in the degree of clustering of both cells and genes. I have a table of reads with columns being cells and rows being genes, and the table values are expressed in a standardized way. Unfortunately, not all genes in all cells have identifiable reads that match them, so there are lots of zeros in the table. The zeros have many sources, and I would like to confine the analysis to cases in which there are attributable reads. Can be persuaded to ignore the zero values? I have tried turning them into and without success. It may be that requires a completely regular data matrix. 
 EDITED I am not an expert (so someone correct me if I am wrong) but when you have a model with random effects, you get to choose which parameters will be treated as random. Usually your intercept will always be random (but you can make it fixed) and you can also specify if you want your slope to be random or not. The terms authors use to explain what is going on on their models/methods may vary but that is the bottom line --- So in summary to answer your question, yes, you can have models with only a random intercept, only a random slope, or both random intercept and slope (and just for the record, you can run all three types for the same dataset and find pretty similar results, depending on how complex your data/model is). 
 One of the most famous capabilities of random forest is the ease to estimate the importance of a predictor. In fact, this is called "variable importance" in Leo Breiman's original paper. In the R package randomForest, after a model is fitted, you can get the variable importance by mod$importance which gives you a matrix of two columns. There are as many rows as the number of predictors. For any predictor row, the first column gives you an estimate of how much worse in terms of accuracy your model would have been had you omitted this predictor; the second column is the increase in MSE had this predictor not included in the model. 
 I have no idea how to analyze this dataset. I am asking if two genotypes, T and M, respond differently to a treatment, E2 (I also have a control, CON). All 36 animals were given both E2 and CON in a counterbalanced order and then their behavior was measured. The behavior was scored once every 30 seconds for 10 minutes as "yes" or "no" (coded "1" or "0"). I am interested in knowing if the treatment affects the genotypes differently and whether this effect changes over time. I have tried running the ANOVA (see below) as a non-parametric test on my count data, but the data are not normally distributed and the results do not make sense. Therefore, I think that a mixed effects model is right. In this analysis, I think my fixed effects should be genotype and treatment. The random effects should be animal, treatment, and time. I also noted sex and age, but I'm not sure that matters for this analysis. The datafile is set up such that each line is a separate observation for each animal, every 30 seconds. See below: I have tried modeling the data using lme in R but I am not sure that I am nesting the random factors properly because the df for "genotype" is 28, but I only have 2 genotypes (so it should be df=1). This is my model: I am also concerned that the count data are not normally distributed. Should I use a GLM instead? If so, does that change the random effects? Please help. I have not seen any examples like this in any of the R books I have seen or on this blog. Thanks in advance!!! 
 You need to specify "linout=TRUE" in the nnet, function as the default is logistic output. i.e.: 
 The problem was that I was failing to specify the "linout" parameter as TRUE. Thanks to Alex R. for noticing this! 
 Beverage caps are blowing off bottles, we want to compare rates for different colour bottles. The test is simply pop or not and we get a failure rate (as a percentage)for each population. I want to know how to establish whether a different rate based on the sample is significantly different from another. I can't use T-test or Chi squared because this is not a variable, so there is no mean or std deviation. How do I do this? 
 In the 'nlme' R package, for instance, I ran the following models: And then I compared them with an ANOVA: I am confused about how to interpret that output. Correct me if I am wrong, but in model1 I allow my intercept and slope to be random, as opposed to model2, where only my intercept has a random effect. Model1 AIC is better than model2, but model2 BIC is better than model1. However, the anova table tells me that the models are different from each other. With that in mind, I wonder which one is the best model and how could I justify that? 
 I don't know if there's a direct routine but, there are spectral and chi-square tests for fractional brownian motion that are fairly straightforward to implement. See for example section 3.2 of this: Keep in mind that spectral tests don't preclude your signal from having similar spectrum to fractional brownian motion but being something wholly different (although this is a bit esoteric). 
 Chan-Vese Algorithm is an unsupervised image segmentation method. It finds the boundary curve $C$ by minimizing the object function $$ F(c_1, c_2, C) = \mu \cdot \text{Length}(C) + \nu \cdot \text{Area}(inside(C)) + \lambda_1 \int_{inside(C)} (u_0(x,y) - c_1)^2 dxdy + \lambda_2 \int_{outside(C)} (u_0(x,y) - c_2)^2dxdy $$ However, if I already have training data with accurate segmentation, can I improve the performance by learning? I am thinking adding a term with parameters $\boldsymbol{w}$ in $F(c_1,c_2,C,\boldsymbol{w})$, first train a model $\boldsymbol{w}^\star$ with given images and $C$ and then use $F(c_1,c_2,C,\boldsymbol{w}^\star)$ to do segmentation. Is it feasible? How can I design such an object function? Any idea? 
 I have developed an algorithm to detect micro events in sleep. These events have duration of a couple seconds, and in my data set each subject has around 100 of these for a full night being ~6-8 hours. Gold standard right now is manual scoring, and humans are incredibly poor at doing so. The events are difficult to find, and scoring outcome is very different from expert to expert, and even the same expert looking at the same data twice will have different outcome. Luckily, I am in possession of a small data set (2 hours) of one patient that has been scored by over 5,000 different experts. This is very rare. I was thinking that I would use this as some sort of final validation of the algorithm. However, I am trying to figure out how to compare them. This dataset has the following form: Each "epoch" is 30 seconds of data, and the number indicates how many voters have found events within that epoch. There is a very strong bias towards 0 events, which is very natural since they are difficult to find. First, I thought of doing some kind of weighted average for comparison, i.e. There can be a different amount of voters from epoch to epoch. Now, looking at the weighted averages I could derive that I am a little more likely to find an event within epoch 3, but looking at the fact that 595 people found 2 events, compared to the background levels of 2 events, I will believe that I am more likely to have between 1 or 2 events in this epoch. The weighted average will not find this because there are so many people not finding the events. Let me put it this way: There are a lot of people who are bad at scoring these events, but why should they ruin the party for the few people who are good at it? Unfortunately, we can not track the different voters. Can I go about this in a more probabilistic manner, taking the apriori bias toward 0 events into account? I guess what I am trying to say, is that I would value a positive value (more than 0 events) higher than the absence of an event. A deviation from apriori distribution, that is. This is the different distributions: 
 For which phenomenon Convolution Neural Networks work, and for which doesn't and where is a border? If it means some type of convergence then in which sense - pointwise, uniform, in L2 norm. CNN as I understand mean "closure" os such things: Input variables Compoistion OR system cascading OR functional graph, use termin which you like "Convolution node", which is in fact do discrte convolution with some small kernel (3x3) and do some extra bias. (i.e. update each pixel in the imput image via block multiplication w^T x+b ) Max pooling operation – take a maximum value of blocks of images 2x2 "ReLU" block. I don't know why it was called so, but it evaluates value of “ramp” function. 
 Let's say in each of ten very different schools, one half of students is offered free lessons in physics and the other is not. After the next examination phase, the physics skills of the two groups should be compared by their marks. I guess the first thing would be a two-way ANOVA: mark ~ group * school But how could I test in which of the schools the lessons induced a significant difference (given that the effect of group was significant)? Tukey HSD seems to compare all combinations of means, even though 10 tests (one for each school) would be sufficient. Should I run pairwise tests (e.g. t-test) for each school with adjustments for multiple testing? 
 I have fit a poisson regression model to my count data. The distribution of my dependent variable followed Poisson distribution and mean is almost equal to variance. But when I fit a Poisson regression model, some of goodness of fit test like deviance and pearson chi-square test showed that was not the case though my model fits reasonable well to the data and the profiles seem like the way its supposed to. I read on one of the pages that deviance test is wrong about 94% of the time. Is there any other way to asses the validity of the model? 
 Suppose we are given a set of points $(x_i,y_i)$ for $i=1,\cdots,n$ with $x_1&lt;x_2\cdots&lt;x_n$. The usual form of linear interpolation partitions $[x_1,x_n]$ into a grid of $k$ equally spaced points and then interpolates between each pair of grid points.Sometimes, we specify the grid points manually (i.e. not equally spaced). Note that we are using OLS distance for error. I would like to know the technical term (and R package) for the following variant: the grid points are $learned$. In other words, we specify that we would like to use exactly $k$ lines, for a total of $k+1$ grid points (with first and last grid point equal to $x_1,x_n)$. The interpolation then optimizes over the location of grid points. While the OLS is easy to write down for this, it looks like there will be strange boundary effects as the grid points are moved across each $x_i$. Is there a nice solution to this problem, perhaps by smoothing the distance between $y_i$ and whatever line is below it? 
 One of the most important things to check before going for a Poisson model is whether your data exhibit overdispersion. Remember Poisson is very special in that it has one parameter. If the variance of your data is significantly greater than the mean, your model is misspecified. Popular alternative is the negative binomial model, but think before you jump there. 
 I am going to examine the impact of interest rate, inflation, housing expenditures on the ratio of daily consumption to income . I would like to use Error Correction Model . The unit root test shows most variables are I(1). But the dependent variable, ratio of daily consumption to income is I(0), is that means the ECM model is not reasonable because there is no deviation in an I(0) series? Is there any other model I could use? Thank you. 
 Not all books do this, of course -- many books specifically for statistics students, for example, discuss this properly. However, you're correct that a lot of basic books designed for students in various application areas do this. (They also tend to have a long list of other strange flaws.) [I'll avoid mentioning some of those application areas by name here. But you also sometimes see it in books written by some statisticians for general introductory statistics subjects] In effect, you're asking us to speculate why authors of some elementary books do certain things, which is likely to reduce your question to an opinion-based one unless some of the authors concerned turn up and explain why. Nevertheless I'll make an attempt to offer some explanation. From observation over several "generations" of textbooks* I think there are * I suppose I'd consider a textbook generation to be a bit under a decade -- about the time it often takes for a widely used "standard" book to be replaced by new one rather than using another edition of the old one. The real answers will vary from book to book. Here are two explanations that I am quite confident cover some of the cases I've observed. A lot of the time no thought at all will have gone into this - it will be presented as a z-test because that's what was in the textbook the author used when they did statistics. [I often work in a particular application area that does a similar thing with other statistical issues (though they get the binomial/sign test right), simply rehashing what the previous generation did, as they in turn copied what their own predecessors did without any real thought as to why the original choices were made that way back in the first half of last century. In that case it's very interesting to trace back and find the original papers some of the traditions taught to students have grown from - many of the reasons for those choices were no longer relevant by about 30 years ago. The authors of those papers would presumably scorn the lack of scholarship that followed them.] Occasionally some authors of elementary methods books will deliberately choose to avoid teaching the binomial distribution** and thereby go straight to normal approximation. [I'm not sure this is necessarily a good choice (I think the binomial is easier to understand than the normal), but you can't put everything into an elementary methods course, so it might at least be argued to be reasonable in that case. **(though that may happen less since logistic regression has become more widely used -- people may want to mention the binomial earlier) 
 I would like to use Fourier terms to model seasonality in an ARIMA model. The reason for using Fourier terms instead of a seasonal ARIMA model is that the frequency of the time series is very high (672) and that I want to model some special days as if they were different weekdays (e. g. I want to treat Easter Monday as if it was a Sunday). I first wanted to do that by using seasonal dummies but 671 seasonal dummies are probably to much. Thus, I want to use Fourier terms which I would adjust for the special days to get the correct regressors. Now, I have two questions: Does anybody have a good reference for using fourier terms as regressors in ARIMA models? I only find online references like blogs (e. g. ) but no paper or book I could cite. Does anybody have comments on whether this approach is useful or not? Note: I have to use ARIMA models, so I do not need suggestions regarding alternative methods. 
 
 I emailed to the author, Roger Bivand, who has developed the spdep package to ask this question. Prof.Bivand, who kindly answered and gave me more suggestions, said : "There are no such fitting functions implemented anywhere, to the best of my knowledge." Thanks for anyone giving interest on this question. 
 I have a dataset of traffic count at several intersections at various dates. Most of the intersections were counted only once. I want to know if there is a significant daily and monthly variability in peak hour volumes. So I ran an ANOVA model in R that look like this: Looking at p-values, I then conclude whether or not there is a significant variability among day or month or year. Is that an acceptable method to achieve that? 
 Since I know that the Wilcoxon test compares pseudo-medians Not quite. The Wilcoxon signed rank test compares the one-sample Hodges-Lehmann statistic (median-of-within-sample-pairwise-averages, equivalently median of Walsh averages, or pseudomedian) to 0. But the rank-sum test compares the two-sample Hodges-Lehmann statistic (the median of between-sample pairwise differences as described in the second paragraph under "Definition" at the link) to zero -- it does not compare two one-sample pseudomedians. Based on my descriptive graphs, i.e. boxplots, jitter plots, it is not immediately visible which of the two groups I am comparing has the higher/lower median/mean. You seem to be assuming that the difference-in-mean and the difference-in-median will behave like the median-pairwise-difference (in the sense that if one is different the other two will be and in the same direction). This is not necessarily the case. A population (or indeed, a sample) can have any of the three be different in some given direction while one or both the others are not different or even arranged in the opposite direction. I am not sure which way would be appropriate to answer the question which of the two groups of which I know are significantly different w.r.t. the outcome variable, is is greater than the other. If you're asking "which group did the rank sum test think had higher location?" (i.e. what difference caused the rejection?), then compute the median pairwise difference. This is a simple calculation and most decent stats packages will offer the calculation (at least as an option relating to a confidence interval for the difference) with the rank sum test. If you're asking "in what direction do the medians differ" or "in what direction do the means differ" then you won't necessarily have an answer consistent with the rank sum test -- if you care about one of those, test that instead. If you assume that the distributions are the same up to a possible location shift under the alternative, and if you assume population means exist (generally a quite reasonable assumption) then you've already made the necessary assumption to attribute the direction of difference to the direction the rank sum test looked at. Conduct a one-sided test and see which is significant. You could, if you do it at half the significance level, but it would seem to be a fairly involved way to go about finding what can be obtained via a simple sample calculation. If you don't have a convenient way to do the calculation otherwise, it should work just fine. Look at the means/medians within the two groups and whichever is greater is the one which is significantly greater. You could find mean and median both have group 1 greater but the rank sum rejected in the other direction . Or you could have mean greater in one direction and median greater in the other direction. Or you could have both group-means and both group-medians be equal even though the rank-sum test rejected. So this would not be a generally good choice. Here's an example in R. First, adding the confidence interval calculation gets the location difference estimate, then calculating it directly from the sample. This assumes there's already data (in and ): So this tells us that y tends to be larger (as measured by the rank-sum statistic) than x (since the x-y differences tend to be negative) An aside on this bit: This let's me confidently reject the null hypothesis of no true location shift at a confidence level &lt; 0.1 There's a few thing wrong there. That doesn't really let us "confidently" do anything, and 0.1 would be your significance level, not a confidence level. If I wanted to speak with some sort of confidence about an effect, I'd tend to be looking at effect sizes, confidence intervals and I'd at least want some (before-seeing-the-data) sense of the power against an anticipated/useful effect-size. 
 I am trying to run an optimization on a ROI function with a large matrix (~9,000 rows and 15 columns) as an input. A sample of my data structure is below: Before calculating ROI, I apply some transformations to the calls, samples, sp, and web variables: My ROI function uses these functions and variables to create a scalar output: I would like to optimize my roi function and find values for each and every record of the calls, samples, sp, and web variables. All other variables need to be fixed. Starting constraints would be that these variables all must be non-negative, but more may be added later. I did some research online and came up with the following code to optimize, but am getting Optimization Function: Any help and guidance on where I went wrong would be greatly appreciated. Thanks! 
 I have a scenario I believe could benefit from some of the statistical models used in ML but I need a little guidance. Or someone to tell me I'm way off base with my idea. The scenario: I have distinct sets (thousands of sets) with thousands of settings in each set. The sets contain the same setting definitions but these settings may have different values. What I would like to do, is detect settings in set for whose value is suspect. e.g: I would like to develop statistical methods (which I believe are used with ML applications) to tell me that setting305 in set3 is suspect given that typically, when all the other settings have the values specified, the value for setting305 is usually 6 and not 1. The challenge as I see it is as follows: Figure out which settings affect which other settings (I have no clue how to do this) Use the results from step 1 as features to determine likelihoods of values - could I use clustering here? From 2 see which values are outside the clusters and report as likely error Has this sort of problem been solved, maybe someone knows of an example similar to this I could learn from - or am I barking up the wrong tree? 
 p-values are to some extend based on the readers or the analysts perspective, but the general rule is, the closer your value is to zero the more evidence you have against $H_0$. R uses a star rating system according to commonly used significant (cut-off points) levels,so as to help the readers decision. For example the three stars next to the p-value of ExIndex suggest that there is very strong evidence against the hypothesis that $\beta_1 = 0$, while on the other hand the one star next to the p-value of the intercept suggests that the hypothesis that $\beta_0 = 0 $ might be rejected at a cut of point of 5%, while is not rejected at 1% and 0.1% cut of point. It is worth noting that things like practical and statistical significant are to be taken into account when dealing with p-values. 
 As you guessed, you just need a Wald test. You can approach this problem in different ways: you can run multiple regressions on the split and combined samples and compute the test statistic by hand; or you can just use interaction and conduct a joint test for significance. For details on how to implement this in Stata please refer to and 
 Hello all and thanks for taking the time to read this. I'm closing out the last few sections of statistics in Khan Academy, but there is a problem that is really bugging me. The problem reads like this. Marvin is supposed to buy a light for his moped, but he's trying to argue the case that it will be less expensive without it. A light for his moped will cost $150. He compiles accident stats into a table like so: Time of Accident|Cost |Probability ----------------|------|----------- Morning |$2000 | 10% ----------------|------|----------- Dusk |$4000 | 15% ----------------|------|----------- Night |$2000 | 20% It then goes on to say that it doesn't matter if he has the light during the day or dusk, but it will prevent him from having an accident at night. I would have gotten the problem wrong anyway because I thought the final value was going to be negative, but here was my thinking: Expected cost of accident with purchase of moped light [P(morning accident) * (cost of morning accident + cost of light)] + [P(dusk accident) * (cost of dusk accident + cost of light)] + [P(night accident) * (cost of night accident + cost of light)] Plug in the values like so: [.1 * (2000 + 150)] + [.15 * (4000 + 150)] + [0 * (2000 + 150)] So obviously I got this wrong, but I'm not sure why. Khan gave the answer as this: [.1 * (2000)] + [.15 * (4000)] + [150] I'm baffled by this! Isn't the probability of a night accident zero?! And why isn't the cost of the light factored into the other accidents? Can anyone help me out with this? Any help would be much appreciated! I'm including a screen shot, hopefully you can read it well enough. EDIT Thanks to Matt Gunn for his post, I was able to work through the problem again. My problem was the fact that I did not account for the possibility of not getting into an accident. This should have been my thinking from the start. [P(morning accident) * (cost of morning accident + cost of light)] + [P(dusk accident) * (cost of dusk accident + cost of light)] + [P(night accident) * (cost of night accident + cost of light)] + [P(NO ACCIDENT!) * (cost of no accident + cost of light)] Plugging the values back in like so gives: [.1 * (2000 + 150)] + [.15 * (4000 + 150)] + [0 * (2000 + 150)] + [.75 * (0 + 150)] Which simplifies to: (.1 * 2000) + (.15 * 4000) + [150(.1 + .15 +.75)] And finally gives the answer of 950. So, the cost of the light was factored into the cost of an accident, I just wasn't able to see that Khan Academy had skipped the steps where that value was factored back out. Thanks again for the help! 
 I don't know R, but it looks like the input parameters to your function, roi, need to be a single vector argument of all parameters. Per the documentation "fn A function to be minimized (or maximized), with first argument the vector of parameters over which minimization is to take place." See and Edit - As an additional matter: It dropped out in my answer composition, but the comment by @ashokrags reminded me about start &lt;- start[start&gt;=0] appearing to possibly eliminate some entries from start, although I don't know if start actually wound up getting shortened and causing a problem in your case. 
 Let me begin with a part-of-speech tagging task. The ultimate goal: given a sentence, what is the most probable part-of-speech tag for each word in the sentence? We want to answer this question by learning a HMM. Define the state space of observable variable as $D$ (dictionary) which is the collection of all possible words. Define the state space of hidden state variables as $T$ (tag). Then we can learn the transition probabilities between the tags and the emission probability from a tag to a word with EM algorithm. Here comes my question: how can we know $t_1$ is verb or noun or preposition? In that sense, how can we reason the meaning of the hidden state variables $t_1, t_2, ..., t_{|T|}$? 
 I'm detecting vehicle honking from environmental sounds (engine noise, speech music, siren etc.) based on binary classification. Common audio features (spectral flux, centroid, mfcc, harmonicity etc.) seemed not doing well --- we get model with nice sensitivity but the problem is that the false-alarm rate is high. Failure cases includes sirens and speech. Any features that describes most common vehicle honking sounds with discrimination from similar sounds for example, siren or speech? Thanks. 
 The tests are not generally the same, but in this case they are and will be for any two parameter model. This is because the t-test on the summary table compares the the full model $$ y = \beta_0 + \beta_1 x ...(1)$$ with the model that $y = \beta_1 x$ for testing $\beta_0 = 0 $ and then for testing $\beta_1 = 0 $ it compares $y = \beta_0 $ with (1). While the F test in the anova table compares $y = 0$ with $y = \beta_0$ for testing that $\beta_0 = 0$ and then compares $y= \beta_0$ with $y = \beta_0 +\beta_1 x$ for testing that $\beta_1 = 0$. Thus the two tests here gives almost the same results, but for the case where there are multiple explanatory variables the results will differ, and the difference will be more apparent if the variables are correlated, another difference is that the anova table F-test may differ with different ordering of the explanatory variables thus anova table is more preferable if the is suspect of correlation between the explanatory variables (suspect that they may explain the same variation in the response), while the t-test is best as an first step of assessment , hope that answers all your questions. 
 So I am working on a NLP problem, and I am having trouble interpreting results from my Python program. I have read a bunch of documents and extracted a bunch of terms and words which I think are important. Let's call this the 'gold standard'. I have run a bunch of NLP algorithms on the same set of documents which extract terms as well. Running NLTK's precision, recall, f-measure, and jaccard's distance functions on the two sets of terms produces the following results: Positive precision: 0.4896755162241888 Positive recall: 0.3811710677382319 Positive f-measure: 0.42866365397030337 Jaccard distance: 0.7271980279375514 So the algorithms performed terribly :( But why is Jaccard distance telling me that the two sets are over 70% similar? 
 In your formulation (adding the cost of the light to the cost of an accident), Marvin will only pay for the light if he gets in an accident... That's generally not how the purchase of vehicle parts work :P You could either: Do what they did (i.e. add 150 to the expected cost of an accident). Do what you did but add an additional .75 * 150 to pay for the light in the 75 percent of the time you don't get an accident. Of course, both (1) and (2) are equivalent since: $$.1*150 + .15*150 + .75*150 = (.1 + .15 + .75) * 150 = 150$$ 
 Outliers can be bad for boosting because boosting builds each tree on previous trees' residuals/errors. Outliers will have much larger residuals than non-outliers, so gradient boosting will focus a disproportionate amount of its attention on those points. 
 I will quickly address the general use of aov. When using aov in R, type I sum of squares are used. These are sequential, which means the order of variables will affect the results if the design is unbalanced (see here: ). Type III sum of squares are sometimes preferred when there is an interaction and type II when there is not a significant interaction. This can be done in the car package with the function Anova (notice the capital A). This may be why your anova results did not make sense. Now to address the question about mixed effect models. I would first recommend lme4, as I think the formula specification is easier to understand. For instance, the random effect would be + (1|animal/time/treatment). In regards to the degrees of freedom, it is not necessarily the case that your model is wrong. Douglas Bates, the author of lme4, has wrote extensively about the difficulties in calculating degrees of freedom in mixed models ( https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html ). This has also been discussed on this site ( getting degrees of freedom from lmer ). Because of this, the lme4 package does not provide p-values and, in order to calculate a p-value, extra steps are necessary such as sampling from the posterior. I am not sure if nlme is still being maintained, but it wouldn't hurt to email the authors. In the event that the model is right, the tricky part will be interpreting the estimates ( Interpreting the regression output from a mixed model when interactions between categorical variables are included ). The reference category (i.e, the intercept) is going to be the first level of each factor. From what you have provided, this would be the first time point (I assume time is categorical because random effects are always factors), treatment = CON, and genotype = M. The p-value that is significant, for instance, is comparing time to this reference category. The question is whether this is a meaningful comparison? Using a package for Bayesian multilevel models, for instance brms or rstanarm ( ), you could add posterior estimates together and use simple subtraction to obtain contrasts at each level of the factors. This might not have been much help towards your initial question, but specification of random effects will generally change the estimate little unless there is great variation between levels of a random effect. Additionally random effects are not always straight forward ( Minimum number of levels for a random effects factor? ) or easy to define ( What is the difference between fixed effect, random effect and mixed effect models? ). If you still cannot get an answer to your question about random effects, you can try a sensitivity analysis. For instance, animal ID should be included as a random effect but the others are open to debate. You could check whether the estimates (eg, coefficients and confidence intervals) change drastically by only nesting some of the variables. If they do not, this would provide confidence in your model and you could mention the potential problem with the random effects in the discussion of your paper. For a more rigorous approach, you could use a likelihood ratio test comparing models that differ in regards to random effects ( Likelihood ratio tests on linear mixed effect models ). You can even use this test to determine whether time is significant. For instance, compare models that differ only in the inclusion of time. Another option would be to use a gee, generalized estimating equation (r packages: gee &amp; geepack), which might be appropriate here because the correlations between outcomes do not need to be correctly specified. The method is robust to "unknown" correlations. This is also ideal when samples are small (see here: ; https://en.wikipedia.org/wiki/Generalized_estimating_equation ). In regards to using different distributions, you could try glmer in the lme4 package with a negative binomial or Poisson distribution. The assumptions of a Poisson distribution are often violated (variance and mean must be close to equal). When there is over dispersion (variance is larger than the mean), the negative binomial distribution is preferred. Since you have 20 potential yes/no's, you should include the number of times possible as an offset which would model the counts as rates. I hope this information can be of use for the manuscript! 
 I'm working on a project to forecast the distribution of a baseball player's "At Bats per game" (a baseball statistic w/ integer domain) using a player's position in his team's batting order as a predictor (1st up to bat, 2nd up to bat etc). I went with my initial thought, which was to train a logistic regression on historical data w/ a poisson assumption, plug in the batting order for the game I want to predict and just nab the predicted lambda from the model without actually attempting to predict a value. I then decided to plot the density of my "fitted" poisson model over the empirical density from my data. Every time I do this the poisson distribution has much more variance than the empirical density. I have several thousand data points so I suspect my empirical density should be pretty good. I've attached a picture of the empirical density(proportion of all at bats for players with a batting order = 2) plotted against a poisson with lambda predicted for batting order = 2 from my glm. Is there another technique I can use to better predict a distribution of count data? I'm assuming a negative binomial glm will give roughly the same results. I know it's somewhat taboo but it seems like I could get a much better fit for the distribution using a normal distribution from an ols. Thanks for any advice! 
 Tree-based methods like Adaboost can produce a list of relative variable importances that you can then use to rank-order your variables. Variable importance is measured by how much error the variable reduced each time it was used in a tree's split/branch. In your case, you could feed the top n-ranked variables into your SVM. In practice, I've seen modelers narrow down a list of 2,000+ to 10-15 using this method. Here is a more thorough explanation of how the CART algorithm determines variable importances: https://www.salford-systems.com/blog/dan-steinberg/what-is-the-variable-importance-measure 
 As far as I know the two formulas you gave are pretty much the standard initialization. I had done a literature review a while ago, I copied it below if interested. [1] addresses the question: First, weights shouldn't be set to zeros in order to break the symmetry when backprogragating: Biases can generally be initialized to zero but weights need to be initialized carefully to break the symmetry between hidden units of the same layer. Because different output units receive different gradient signals, this symmetry breaking issue does not concern the output weights (into the output units), which can therefore also be set to zero. Some initialization strategies: [2] and [3] recommend scaling by the inverse of the square root of the fan-in Glorot and Bengio (2010) and the Deep Learning Tutorials use a combination of the fan-in and fan-out: for sigmoid units: sample a Uniform(-r, r) with $r=\sqrt{\frac{6}{\text{fan-in}+\text{fan-out}}}$ (fan-in is the number of inputs of the unit). for hyperbolic tangent units: sample a Uniform(-r, r) with $r=4 \sqrt{\frac{6}{\text{fan-in}+\text{fan-out}}}$ (fan-in is the number of inputs of the unit). in the case of RBMs, a zero-mean Gaussian with a small standard deviation around 0.1 or 0.01 works well (Hinton, 2010) to initialize the weights. Orthogonal random matrix initialization, i.e. then use as your initialization matrix. Also, unsupervised pre-training may help in some situations: An important choice is whether one should use unsupervised pre-training (and which unsupervised feature learning algorithm to use) in order to initialize parameters. In most settings we have found unsupervised pre-training to help and very rarely to hurt, but of course that implies additional training time and additional hyper-parameters. Some ANN libraries also have some interesting lists, e.g. Lasagne : [1] Bengio, Yoshua. " Practical recommendations for gradient-based training of deep architectures. " Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478. [2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade . [3] Glorot, Xavier, and Yoshua Bengio. " Understanding the difficulty of training deep feedforward neural networks ." International conference on artificial intelligence and statistics. 2010. 
 I have a piece of data. There are three groups without balancer, apache and HAProxy. Virtual users and Total sample are parameters. Example 1 virtual user makes 7 request such that the throughput of APAche is 19.4. Now I want a simple statistical way to check if there is a relation in througput. I want to see that are they equal or are they not equal? If this is complex then we can just test between two groups such as between APACHE and HAPROXY. in my mind correlation test could be ideal .. Not so good at statics .. Many regards. 
 In my lecture notes it is stated that sum of all positive signed ranks (defined in Wilcoxon signed rank test) = the number of Walsh averages that are greater than median How can I prove this statement? 
 Q: I wondered if anyone could offer a mathematical proof or similar, that, on average, delete-50% jackknife samples are not inherently more “independent,” in terms of information content, than bootstrap samples? Context: Someone in my department as a rule does not use bootstrapping to estimate the distribution of a parameter, instead preferring the delete-m jackknife (specifically the delete-50% jackknife). This person maintains that jackknife samples will on average be more statistically independent of each other than bootstrap samples, in part because bootstrapping is performed with replacement. I am fairly certain that this is not the case, based upon both simulations and my intuition, but I am looking for a mathematical proof demonstrating this. Simulation Results: For a simulation, I generated an n = 100 observations random normal vector and drew 1,000 bootstrap samples (each length n , uniform distribution, with replacement) and 1,000 delete-50% jackknife samples (each length n /2, uniform distribution, without replacement). I then calculated the pairwise mutual information between all bootstrap samples and separately, between all jackknife samples. I found that the bootstrap ( mean MI = .2152, SD = .036) on average had significantly lower mutual information between samples on average than the delete-50% jackknife samples did ( mean MI = .2244, SD = .054) ( t (998,998) = 42.4, p &lt; .001). I have also seen in other simulations that the delete-50% jackknife and bootstrap tend to give very similar results for confidence interval calculations for a number of parameters. However, delete &lt; 50% jackknife methods don’t seem to perform as well in general (i.e., overly narrow CI’s, anti-conservative). Some background for the reasoning behind the assertion that delete-50% jackknife samples are more “independent” than bootstrap samples: Probability Calculations: There is a straight-forward way to derive the expected proportion of unique observations contained within a bootstrap sample. This is known as the .632+ rule in the literature (Efron &amp; Tibshirani, 1997): the probability of drawing a given observation on a single bootstrap draw equals 1/ n , since it’s from a uniform probability distribution. So the probability of not drawing a given observation on a single draw equals 1 – (1/ n ). The probability of not drawing a given observation in the whole bootstrap sample equals (1 - (1/ n ))^ n , since each draw is an independent event. This means that the probability of drawing a given observation at least once in the bootstrap sample = 1 - (1 - (1/ n ))^ n . The limit of this equation as n approaches infinity equals approximately .632 (Efron &amp; Tibshirani, 1997), i.e., 1 - e ^-1. Therefore, the expected proportion of unique observations drawn in a given bootstrap sample ≅ .632 ∙ n . This means that there is an approximate .6322^2 = 0.3994 probability of observations overlapping at least once between two bootstrap samples. I’ve validated this using simulations, drawing 1,000 bootstrap samples from an n = 100 observations random normal variable, and comparing the number of intersecting indices between every pairwise bootstrap sample. On average, 39.997 indices intersected, with an expected number based on the above of n ∙ (1 - ( 1 - (1/ n ) )^ n )^2 = 40.19, for n = 100. In the case of the delete-50% jackknife: there is a .5 probability of selecting a given number for a single jackknife sample, and therefore a .5^2 = .25 probability of selecting the same number in two jackknife samples. Therefore, the expected number of observation intersections in delete-50% jackknife samples, for n = 100, is .25 ∙ n . Indeed, in a similar simulation, I get an average intersection number of 25.0001. From these figures, it seems to the person in my department that, given 25% intersection (delete-50% jackknife) &lt; 40% intersection (bootstrap), there is more “independence” among delete-50% jackknife samples than bootstrap samples. However, I realize that this is neglecting the additional independent information contained in random combinations of the n ∙ (1 - .632) expected number of non-unique observations in a bootstrap sample. Unfortunately, I do not know how to calculate the expected “level of independence” or unique information, with respect to the additional 36.8% of non-unique observations on average in bootstrap samples. References Efron, B. (1979). Bootstrap methods: Another look at jackknife. Annual Statistics . 7 (2), 1-2. Efron, B., &amp; Efron, B. (1982). The jackknife, the bootstrap and other resampling plans (Vol. 38). Philadelphia: Society for industrial and applied mathematics. Efron, B., &amp; Tibshirani, R. (1997). Improvements on cross-validation: the 632+ bootstrap method. Journal of the American Statistical Association , 92(438), 548-560. 
 Let's say for a text mining problem (e.g creating a predictive model using text analysis), using a feature selection method (e.g TF-IDF) we come up with 1000 features/words/tokens. Is there some principals that suggest what number of features we should use? What I currently do is creating the model based on all 1000 features, then reducing the number of features until reaching the highest accuracy when doing train and test. Is it the correct way to deal with number of features or is it somehow fishing a model which might result in a non-robust model? 
 You can assume that number of icecreams eaten per person in each year follows a normal distribution such as $N(\mu_1,\sigma_1^{2})..,N(\mu_5,\sigma_5^{2})$,where $\mu_1,...,\mu_5$ and $\sigma_1^{2},...,\sigma_5^{2}$ are population means and variances respectively.Then for each of the 5 years you will get 5 samples from the normal populations.So,in this case,your required test is $H_0:\mu_1=\mu_2=...=\mu_5=50$ against $\mu_{i}$'s are not equal. Now you draw a random sample of 5 persons for each year.Let $x_{i1},x_{i2},...,x_{i5}$ be the random sample drawn from the ith population. By likelihood ratio test you can get the required test statistic as, $\lambda_x(\theta)=(\frac{1}{1+\frac{SSB}{SSW}})^{\frac{n}{2}}$,$\sum n_{i}=n,i=1(1)5.$ where, $SSB=\frac{1}{5}\sum\sum(\bar x_{i0}-\bar x_{00})^2$ $SSW=\frac{1}{5}\sum\sum(\bar x_{ij}-\bar x_{i0})^2$,$\bar x_{i0}=\frac{\sum x{ij}}{n_i}$ and $\bar x_{00}=\frac{\sum\sum x{ij}}{\sum n_i}$ As,$\lambda_{x}(\theta)$ is a decreasing function of $\frac{SSB}{SSW}$,we reject $H_0$ when $\lambda(x)&lt;C$ i,e,. $T=\frac{MSB}{MSW}&gt;C'$,$MSB=\frac{SSB}{5-1}$,$,MSW=\frac{SSW}{n-5}$,where $T\backsim F_{(k-1,n-k)},$under $H_0.$ Under $H_0,C'=F_{\alpha;5-1,n-5}$,$\alpha$ is the desired level of significance. 
 Both Lasso and Elastic Net are efficient methods to perform variable or feature selection in high-dimensional data settings (much more variables than patients or samples; e.g., 20,000 genes and 500 tumor samples). It has been shown (by Hastie and others) that Elastic Net can outperform Lasso when the data is highly correlated. Lasso may just select one of the correlated variables and does not care which one is selected. This can be a problem when one wants to validate the selected variables in an independent dataset. The variable selected by Lasso may not be the best predictor among all correlated variables. Elastic Net solves this problem by averaging highly correlated variables. 
 Two cases: supervised POS tagging: no need for EM, you can simply count to learn the emission and transition probabilities (2). Each hidden state is mapped to one single POS. unsupervised POS tagging: the Baum-Welch (EM) is typically used learn the emission and transition probabilities. To map the hidden states to actual POS, it depends what information regarding the actual POS you have. You can look at the literature on POS induction to see how they map hidden states with gold POS tags, e.g. see (1) Section 3 Evaluation Measures. (1) Christodoulopoulos, Christos, Sharon Goldwater, and Mark Steedman. " Two decades of unsupervised POS induction: How far have we come? ." In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 575-584. Association for Computational Linguistics, 2010. (2) Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin. Draft of February 19, 2015. Chapter 9 Part-of-Speech Tagging https://web.stanford.edu/~jurafsky/slp3/9.pdf 

In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.
 I am very new to this field and I want to learn forecasting of stock price using R. Please let me know which are the step should I follow? If someone know tutorial links for forecasting in R then it will be good. Thanks in advance. 
 0 down vote favorite I have some words which have labels of '1' or '0' and '-1' for the words that I don't have labels. And when I run Label Propagation algorithm of sklearn,it is even changing my labelled data I even set the Clamping factor, alpha to 1.0. 'words' is a list of words : Input 'labels' is a list of integers for each word : with labels '1' or '0' or '-1' 
 I'm having trouble finding a solution to the following problem: Assume that there are $N$ observations and each observation is associated with $K$ probabilities. For each $i \in N$, the probabilities sum to 1. Given the probabilities for each individual, I want to calculate the probability that the first option occurs more across the $N$ individuals than all other options. I can bootstrap the answer, but as $N$ increases, this becomes an unnecessarily difficult exercise. Can anyone think of theory or point me to a paper that might have this result? 
 I have proposed a model that is the modified ANN to estimate flood at ungauged site. In order to test the performance of the model, i want to design the simulation for ungauged estimation problem. Did anyone had done the simulation at ungauged site which objectively to test the model performance in estimation? 
 USE CASE Use R to fit/train a binary classification model, then interpret the model for the purpose of manual calculating classifications in Excel, not R. MODEL COEFFICIENTS QUESTIONS (1) what is the classification formula from the fit model in example code below named ' model1 '?. (is it formula A, B or Neither)? (2) how does ' model1 ' determine if class == 1 vs. 2? Formula A: class(Species{1:2}) = (-31.938998) + (-7.501714 * [PetalLength]) + (63.670583 * [PetalWidth]) Formula B: class(Species{1:2}) = 1.346075e-14 + (5.521371e-04 * [PetalLength]) + (4.485211e+27 * [PetalWidth]) R CODE EXAMPLE 
 In other words, let's say we have a data representation as in the image below, which is generated from the PCA, the projection of the data onto the first two PCs. As it's shown in the 2-D space, the data is not separable. Can that be a good indication to say that this data is not separable? What are the other metrics that could be used to figure out data separation? Any suggestions would be appreciated. 
 I have 36 hours of speech data along with transcription. I'm planning to have 7 second audio segments, because I don't know any better. Suggestions are welcome. These segments will be passed through a recurrent neural network. 
 I have a problem with the concept of false discovery rate. Assuming I have a panel of 50 anticancer drugs for which their IC50 values are reported for 30 cell lines. On the other hand, I have a modeling framework which allows me to predict IC50 for those drugs. I use Spearman correlations here (so basically, one p-value for each drug); the question is: should I use FDR to correct p-values? What does it mean when I use FDR here or not? Thanks [EDITED] The experimental data is something like this: My modeling (mathematical) approach resulted in a set of IC50 (similar to the above results, with the difference that these results are simulated/predicted and not experimental). To compare the simulation results with experimental ones, I use Spearman correlations: So, should I correct for multiple hypothesis? 
 It appears to me that this approach is sufficiently intuitive that many people must have looked at it, but I can't locate a useful reference in my bib file, either. Searching for "Fourier ARIMA" or similar at the International Journal of Forecasting (IJF) does not yield anything very useful. Ludlow &amp; Enders (2000, IJF ) do combine ARIMA and Fourier terms, but not as regressors in the way you envisage. A similar search at Google Scholar turns up a couple thousand hits that you would need to refine. This older paper seems to use this approach (so it's been around for thirty years at least), but I'm not sure you want to cite it. I'd say this approach is eminently useful. Rob Hyndman seems to agree: Forecasting with long seasonal periods and Forecasting weekly data . I see that you have to use ARIMA models (why?), but note that he writes that TBATS performs comparably well. Rob's recent update to the package is also relevant. (Don't disregard these because they are "just blog entries". Rob Hyndman is one of the forecasting gurus, highly active in the community, and the Chief Editor of the IJF . I'd trust anything he blogs more than much of what other people publish in journals.) 
 This is probably the first time I have seen underdispersion in practice. Usually the problem is that observations are over dispersed with respect to the model. Now, if data are underdispersed compared to a Poisson distribution (with equal mean and variance), I'd immediately think of a binomial distribution , where the variance $np(1-p)$ is always smaller than the mean $np$. You could do a "kind of ad hoc binomial regression ": parameterize $p=\frac{1}{1+e^{-\beta x}}$ with your regressors $x$ as in the logistic case, to constrain $0&lt;p&lt;1$. Set up the log-likelihood and estimate $\beta$. Do this separately for $n=1, 2, \dots$, yielding estimates $\hat{\beta}^{(n)}$. Pick the $(n, \hat{\beta}^{(n)})$ combination that yields the highest overall log-likelihood. If your predictors are truly discrete and can only take on a finite number of values, you may instead want to group your data and estimate separate $(\hat{n}_x, \hat{\beta}_x)$ for each observed value of $x$ (then simply $\hat{p}_x = \frac{1}{1+e^{-\hat{\beta}_x}}$). This is extremely ad hoc . Typically, in binomial regression , your $n$ is set a priori , and you are only interested in estimating your $p$ - here, you'd have to estimate both, and the problem is that there is no good interpretation of $\hat{n}$ (nor of $\hat{p}$). The only two good points this approach would have would be that (a) it's inherently discrete, in contrast to the normal approach that you mention, and (b) it should be able to capture the underdispersion. It still won't capture the negative correlation between innings (which you could potentially take into account by taking previous at-bats into your regressors), though. But it might improve on your model so far. 
 I ran a study using multiple logistic regression with repeated measures (3 predictors- 2 are dichotomous and 1 is continuous, 64 observations per participant), and now I want to compute an effect size and a required sample size for a second study. How can I do that? Specifically, is there any way of figuring out the required sample size to achieve a given power to detect two-way (and if possible- three way) interactions in multiple logistic regression with repeated measures? Thanks 
 First look at the plot of your time series data to have an idea about break point. If there seems to be a single structural break (also require literature support), you can test it by using ' Chow breakpoint test ' and if there seems to be multiple structural breaks (also require literature support), you can test the exact breakpoint by using ' Bai-Perron muliple breakpoint test '. In Eviews, the following steps can be followed: Select data - view - graph - basic graph - line &amp; symbol - OK Visualize, if there is any break point. At what time is it present (rough estimate) quick - estimate equation - enter your equation - Ok View - stability diagnostic - Chow breakpoint test (if single break) - enter date (which you've taken from graph) - click ok - interpret the result View - stability diagnostic - multiple breakpoint test (if more than one break) - select method, maximum breaks(which you've taken from graph), trimming % and sig level as per your requirement - click ok - interpret the result 
 Housing starts = 50 - 7 Mortgage rate + 7 Q1 + 9 Q2 + 7 Q3 Adjusted R²: 0.74 All variables significants is this activity seasonal? why? If t-stat for Q3 was 1.23 instead of 4.19 how would that affect seasonality arguments? How we can modify the model to chck if we have overall increase in housing starts over considered 4 years? 
 Every time series with a trend component is necessarily a non-stationary series . Non-trended series may or may not be stationary. First plot your time series (if required logged series) to visualize the presence of trend. If there is an intuition for presence of trend , it means the series is not mean reverting , hence non-stationary . To test for stationarity statistically, you can apply unit-root test . For unit root test ADF method is most common as generally, there is auto-correlation in the series. If there is auto-correlation as well as heteroskedasticity in the series, select phillip perron method . Use intercept or intercept + trend or none option as suggested by plot. If null hypothesis is rejected means series has no unit root, so it is stationary. If not, use difference or transformation to make it stationary. 
 In R I have where gives Now is simply the day (and is in order). is the promotion-value for the day. It is simply the number of times an advertisement has been on television. is the number of new users we got that day. This plot shows us the number of good new users we get for a promotion for each day. For example for promotion value 90 we have a day where we got 8 new good users and a day where we got 14 new good users. I want to investigate the impact the promotion-value has on new users (). Since we have a count process I thought it would be best to make a poisson regression model. When we type we get But when I plot the fitted values for the model we get this but it does not look like a good fit. We also want to find a sweet spot, ie a point where the increase of don't have an impact on the increase of that much. Is the another better approach to solve this? Thanks. 
 This is easier to do using matrix notation. Your first, true model (model 1) is in matrix notation $$ Y = X \beta + E $$ Where we assume the error terms in $E$ is iid dstributed with zero mean and constant variance $\sigma^2$. The overparametrized model (model 2) is in matrix form $$ \DeclareMathOperator{\V}{\mathbb{V}} Y = X \beta + Z \gamma + E $$ with the same specification of $E$ and all the components og $\gamma$ has the true value of zero. The estimator of $\beta$ in model 1 is $\hat{\beta} = (X^T X)^{-1}X^T Y$ with variance matrix $\V \hat{\beta} = \sigma^2 (X^T X)^{-1}$. In model 2 the parameter estimator becomes $$ \widehat{\begin{pmatrix}\beta \\ \gamma \end{pmatrix}} = ([X \colon Z]^T [X \colon Z])^{-1} [X \colon Z]^T Y = \\ \begin{pmatrix} X^T X &amp; X^T Z \\ Z^T X &amp; Z^T Z \end{pmatrix}^{-1} \begin{pmatrix} X^T Y \\ Z^T Y \end{pmatrix} $$ Now, we are only interested in the first component of this, let us write that $\hat{\beta}_2$ ("$\beta$ estimated from model 2). Using formulas for the inverse of a block matrix (see https://en.wikipedia.org/wiki/Block_matrix ) we can write the variance matrix of these estimator as $$ \tag{*} \V \hat{\beta}_2 = \sigma^2 (X^T X)^{-1} + \sigma^2 (X^T X)^{-1} X^T Z (Z^T Z - Z^T H Z)^{-1} Z^T X (X^T X)^{-1} $$ where $H=X^T (X^T X)^{-1} X$ is the hat matrix for model 1. Now, we can see that the matrix in the second term here is positive definite (or, if the matrix $Z$ is not of full rank, positive semidefinite) so that the diagonal terms are positive, showing that the variance of each component of $\hat{\beta}_2$ is larger than the variance of the corresponding component of $\hat{\beta}$ (estimator from model 1). EDIT Here some more details as asked for. The block inverse formula we use is $$ \begin{pmatrix} A &amp; B \\ C &amp; D \end{pmatrix}^{-1} = \begin{pmatrix} A^{-1}+A^{-1} B (D -CA^{-1}B)^{-1} C A^{-1} &amp; \cdot \\ \cdot &amp; \cdot \end{pmatrix} $$ (where I have only given the part of the formula that we actually use). Now in that formula you use $$ A=X^T X, B= X^T Z, C= Z^T X, D= Z^T Z $$ which will give the results. EDIT To respond to the second question in comments: How can we conclude that the second term in (*) is positive definite? Note thet it has the general form $A B A^T$ where $B$ is positive definite, that is enough! Since $B$ is positive definite we have the cholesky decomposistion $B= C C^T$, using that we have $A B A^T = A C C^T A^T = AC (AC)^T$, and using that we can verify positive definiteness, let $x \not= 0$ be any vector, we get $$ x^T A B A^T x = x^T AC (AC)^T x = ((AC)^T x)^T ((AC)^T x) &gt; 0 $$ since $y^T y&gt;0$ for any nonzero vector. To use this we must show that the "middle matrix" $(Z^T Z - Z^T H Z)^{-1}$ is positive definite. First, the inverse of a positive definite matrix is positive definite so we must show that $Z^T Z - Z^T H Z$ is positive definite. Now $ Z^T Z - Z^T H Z = Z^T (I-H) Z = Z^T (I_H) (I-H) Z = ((I-H)Z)^T (I-H)Z$ and now this can be shown to be positive definite by the same "trick" we used above. Here we have used that $H$ is an orthogonal proyector matrix, so $I-H$ also is, so $I-H = (I-H)^2$. 
 Situation is as follows: I have much unsupervised data, some images (about 50k) and much less regression data (about 5k) - images of the same class but with some number I want to predict. I want to improve network performance by pre-train conv layers with first kind of data that I can't label by hand with that numbers but that data is pretty similar. Can it helps in my case? 
 I'm attempting to specify several random effects in a . In specific, I have conducted an experiment where I have stimulated several test animals with other animals that belong to two groups (treated/untreated stimulus animals - the variable specifying stimulus treatment is called Stimulusgroup). Due to a lack of animals, I have had to re-use some of the stimulus animals. Test animals were used only once with two stimulus animals each - hence I have two values per test animal (Response to Stimulus animal 1 and response to Stimulus animal two). Hence I need to include both and . Therefore my model looks like this: However, when running this model I receive the following error code: number of levels of each grouping factor must be &lt; number of observations I would like to know if there is a way to still conduct the analysis as the reviewers of my paper have explicitly asked for inclusion of these two random effects in my models. I have heard that one may use in this case but I did not find anything online. Thanks for your help! Yamper 
 you can use dummyVars in R. it will automatically create different columns based on number of levels. Post which you can use cbind and attach it to you original data. 
 What is the appropriate way to encode and handle regressors that are nested? For example, a feature "Husband's age" is nested in the feature "Is married" in a sense that it's not defined unless "Is married" = 1. Should "Husband's age" be set to 0 or NA for those who are not married? Should we omit "Husband's age" from the model entirely and only use the interaction term "Husband's age"*"Is married"? I guess this situation must be quite common, but I don't even know how to google it. 
 I'm implementing a Bayesian Cormack-Jolly-Seber mark-recapture model in JAGS in the M-array format (based on code from Kery &amp; Schaub's book "Bayesian population analysis using winbugs"). I am struggling with how to account for the fact that the time intervals in my data are not equal. Specifically we have 11 years with yearly census data, then a gap of 2 years, followed by another 6 years of yearly census data followed by a gap of 5 years. I need to account for these different time intervals between surveys, but don't know exactly where (or how) to do this. Any tips highly appreciated! Here's my model code (as implemented in JAGS) for a model with a simple linear time effect on survival (year) and a constant resighting estimate: 
 Use continuous data for DV as forcing it to categorical (2 categories -addicted or not addicted) leads to information loss For 8 (or even 5) predictors, sample size is very less (You may need more sophisticated techniques to handle such data) Either try to gather more data (recommended) Or try to use dimension reduction techniques (such as PCA) for variable selection (Literature review may also help/support) Try using both of the above mentioned options 
 I am comparing dog breeds with a specific disease in my study to a population of registered dogs (no disease information known). The population of registered dogs is not a true reference population, so I can't perform Chi square analysis or anything to look at breed predispositions, but I was wondering if I could still use the data to look at potential overrepresentations? At the moment, I'm comparing proportions of breeds in the study with the proportion that are registered. Example: Staffordshire Bull Terriers in study = 14/117 (12%) Staffordshire Bull Terriers in registered population = 6698/100674 (6.7%) Is it possible to compare them, and see if the proportion of Staffies in my study is significantly different to the 6.7% in the registered population? 
 I am analyzing an experimental data set based on the trait values under drought condition. The experiment was carried out on three separate drought environments. First, I have done a single experiment analysis for each environment to extract the entry means. I have also extracted the degrees of freedom and standard error from single analysis. Now, I have combined the entry means from each environment to run a combine anova. The model is: $Y=meu+Genotype+Environment+Genotype*Environment+Error$. My F-test for the main effect, genotype, is non-significant, while the interaction the term is significant at the 10% level and main effect, environment, is significant at the 5% level. Looking at the means for grain yield of each genotype, I have found some differences which are higher than my LSD value at 5%. So I performed a pairwise comparison test and found significance differences at 5%. Now, I am wondering how can that happen. Is it possible that the F-test is non significant and pairwise comparisons are significant? I would be very grateful if someone can explain me this issue with references. 
 I have the following weight update rule for a multiclass (with k classes) logistic regression with l2 regularization optimized using stochastic gradient descent: $$ w_i = w_i - \eta * softmax_i(prediction) * w_i + \eta \lambda w_i $$ Can you suggest some way how I can include cost-sensitivity with respect to the misclassification, please? I read about using a cost-matrix. For example, in the case of 4 classes, something like: $$ \begin{pmatrix} 0 &amp; 1 &amp; 2 &amp; 3\\ 1 &amp; 0 &amp; 1 &amp; 2\\ 2 &amp; 1 &amp; 0 &amp; 1\\ 3 &amp; 2 &amp; 1 &amp; 0 \end{pmatrix} $$ Unfortunately, I could not understand how this should be incorporated. Thank you! EDIT 1: Thanks to lmo for the friendly hint that my questions is unclear and also sorry about that. I will try to explain what I mean by cost sensitivity. I am using a multiclass softmax regression. This means that for each possible label a set of weights are trained by the expression I wrote earlier. When training I want to penalize more the predictions that are further away from the actually label. Meaning, if for example I have 5 possible labels (0 to 4). When the have a actual label 4, and two predictions one stating 1 and another one 3, I want to penalize the first one more than the second one, since it was less closer to actual label. 
 Foreword: I'm working on a non-econometrics related project where we're trying to use econometrics toolboxes in order to avoid re-inventing the wheel. I'm a bit overwhelmed by it and am trying to cut the parts I don't understand into small independent pieces, so that I can ask them here as a question. Background: Basically I have 6 independent "data sources" that I'm modelling by 6 AR processes running in parallel, each giving 8 Ki samples. The data is stored in a 8Ki x 6 matrix. These data sources are recorded by 31 sensors. I multiplied the 8Ki x 6 matrix with a 6x31 matrix which models how the 6 sources are perceived by the sensors. In the end, I get a 8 Ki x 31 matrix with the data as received by the sensors. Now I'm trying to estimate parameters for the 6 AR processes from the data gathered by the sensors. Question: When I'm setting up ARMA processes whose parameters are supposed to be estimated from the sensor data I have, should they be endogenous or exogenous variables? I did read this question, which was enlightening, but the matrix I'm using to mix 6 original time-series into 31 channels is creating a bit of confusion for me. 
 Test and training set should be combined as one set and then change the levels of the training set. My codes are: This works in any cases where the number of levels in test are more or less than training. 
 I think if I got your question right.. you have two sets of animals one used just once and the other used twice . I think you can just use animal as a factor and fit response =(1|animal) in the lmer syntax. You will have to switch to the lme4 package since, if my memory serves me right, me requires that you have more than one observation per group. Lmer lets you fit observation level random effects. Perhaps @benbolker might jump in and give you some hints here. EDIT: After reading the comments below.. this is my understanding of the experimental design ( will edit if it needs refinement). For each test animal there are two observations associated with different stimulus animals. Some of these stimulus animals are re-used.. that is some test animals had encounters with the same stimulus animals. OP originally wanted to fit a nested model with testAnimalID nested within stimulusID. Nested effects can also be considered as maineffect+interaction , so in this case it would be something like . The first part will account for correlated observations, however the second part will be equivalent to observation level random effects not covariance among observations from the same testAnimalID. I agree with @rolands comment above that it would be simpler to just fit a random effect for each id i.e . 
 I am using R version 3.30 on windows OS. I have come across a business problem in marketing of channel attribution that's when I found the aforementioned package. But I wonder, is there a way to validate the results we get from the markov model. How can we make sure that the results we are getting are the actual attribution and they'll hold with the new data. The model tells us 3 things: Transition Probabilities Removal Effects Attribution What metrics one can check to ensure the robustness of the model? The help file compares the heuristic models with the markov model but what measure to be used in order to comment that markov model should be used and the attribution through its use is the genuine attribution? 
 Very late answer but you might find this question , and this post helpful. Personally, I would just run a linear model with the categories represented as (ordered) integers, and examine the coefficient. There is also some nice guidance in the BMJ stats series . 
 I have 18 variables (measured in a Likert-scale) which should according to previous studies measure the same thing, let's say intelligence. I want to calculate the reliability of the sum variable of these 18 variables. I am concerned that Cronbach's alpha will not work for my data. Nearly all of these 18 variables' distributions are skewed (= non-normal) * the variables does not correlate with each other that much (pearson's rho &lt; 0.3 for most of the variables). The variances of the variables differ (from 0.1 to 1.9). Sample size is quite small, N = 109. My questions are: Can I calculate Cronbach's alpha in this situation? I'm not that confident about what are those special cases when alpha is a "ok" coefficient for reliability. Is there some other methods I could use instead? Factor analysis? I have read about Tarkkonen's rho, but I'm not familiar with it at all. Is the non-normality or non-correlation a problem? If so, would a variable transformation (eg. the inverse/square-root) to make distributions more normal help? 
 I have been reading about time series comparison and haven't really found an answer to my question. I have around 1000 stores which I have clustered based on trend over time and identified 8 groups of stores. I then selected mediods from each cluster (basically a store that is representative of a cluster) and ran arima on 8 different time series to understand how different are parameters of estimates. I had also run arima on an overall chain level and now I want to compare the 8 different model parameters (of 8 clusters) with model parameters of the overall chain model. Now I am not sure how to quantify which group of stores have an upward/downward trend relative to the overall chain. Any help much appreciated? Few more information - Clusters were created based on trend over time (Basically sales was scaled and trend was identified as Sales in Wn - Sales in Wn-1 and this was measured over a period of 52 weeks on a weekly series. And arima was run on log transformation of sales across a period of 104 weeks on a weekly series for each of the cluster mediod and at chain level. Thanks, Rohit 
 I want to predict future demand based on data I have. I hava already used ARIMA model but it is not giving very high accuracy. So should I also try NN or ARIMA is better? 
 If I have a sample data set of 5000 points with many features and I have to generate a dataset with say 1 million data points using the sample data. It is like oversampling the sample data to generate many synthetic out-of-sample data points. The out-of-sample data must reflect the distributions satisfied by the sample data. The data here is of telecom type where we have various usage data from users. Is there any techniques available for this? Can SMOTE ( https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/node6.html ) be applied for this problem? 
 I have the following data set with mixed variable types There are many more variables and the task is to predict . Now, I have imputed the data using package as follows to fill missing values: Now, I have divided into two sets: and using package as follows: Now, I tried to use on the data and used the model to predict output on data. But the accuracy is not great. I want to use Clustering but I can't use as the variables are Factor. Can you please tell which algorithms are suitable for such problem and how to handle Factor variable? 
 I am a statistician. I'm pretty good with the concepts of topics like Linear &amp; Logistic Regression &amp; Time Series. But in order to run data I need to learn the R language. Since, having no programming background makes it difficult for me to understand it. How can I easily learn and construct commands in R software? What could help me with achieving that? 
 I have a very specific problem, as I've done a somewhat complicated analysis for my Master's thesis (deadline in two weeks!). First, I've done Latent Class Analysis with Gibbs sampling (paper I've followed available here ) on several hundred sites with presence/absence data. As I understand it, the resulting Latent Classes represent probabilities, but I have other external and continuous variables that I'd like to use to predict these probabilities. In my results, I have four different Latent Classes that I want to predict separately from each other. Because there are quite a few zeros in the probabilities, I decided against Beta Regression, and instead I'm trying to utilise quasibinomial GLM in R. Now, the issue I'm having is that I don't really understand how I should be interpreting the odds ratios (I've transformed them from the coefficients) in this case. As I understand it, one-unit increase in the predictor variables results in the change of odds that a site belongs to a Latent Class, but as the y-axis represents probabilities, does this interpretation make sense? Should I instead be saying that as X increases, the odds that any given site's probability of belonging to a Latent Class changes by the amount indicated by odds ratio? To me, the latter way of saying it is very clumsy and confusing, as the reader could easily interpret this as the probability increasing in terms of points (so from 0.6 probability to say 0.7 probability if odds ratio is 1.1). Any help would be much appreciated. 
 I'm building a units based type for C++ which describes probabilities (using boost::units library.) While I know that the values of probabilities are dimensionless, I think it makes some sense in the same way that angles (another dimensionless quantity) are often given units. My question is given that percent is a reasonable unit for parts per 100, is there some such concept for ? Following the etymology of (from per centum) I arrived at per unus (latin for one) =&gt; perun. However, I would prefer not making one up. 
 I have read that RMSE of calibration/validation/cross validation is frequently used for model selection (e.g., for ANN), but can lead to over-fitting because the prediction error represents the decreasing bias more than the increasing model variance. Empirically, I often observe the RMSE increase with number of model parameters after a certain point, but why does this not sufficiently characterize the variance? 
 I usually use the term "proportion". 
 My knowledge of stats is fairly basic, so you please bear with me! I'm trying to calculate the CDF for the vertical displacement of a (light, small) object floating in a wave tank. I have experimental data which suggests that the displacement can be approximated to a normal distribution, however I am also required to provide an analytical solution, as the experimental data is not reliable enough. My approach so far has been to use the Rayleigh Distribution to calculate the probability of any given wave being greater than an amplitude (h). $$P(H&gt;h) = e^{-2\left(\frac{h^2}{h_s^2}\right)}$$ where $$ h_s = 2\sigma $$ Separately, I have approximated each individual wave to a sin wave of constant frequency. I have calculated the CDF of a sin wave (of amplitude H) to be: $$P(X&gt;x) = \frac{1}{2}-\frac{sin^{-1}(\frac{x}{H})}{\pi}$$ (where x is the vertical displacement of the object) The part I do not understand, is how I combine these two to get the probability of exceeding a certain displacement for any wave. Instinctively, I suspect I am supposed to multiply the two and integrate for h between x and inf, but so far I haven't found an answer which is anywhere near what I am expecting, so I may be way off! Thanks in advance! 
 I developing a kernel density estimate in Java for a control and test sample population given a certain treatment of the data. I am wondering the best way to test the similarity of the distributions to obtain a p-value corresponding to whether the treatment had a significant effect on the control population. I have researched this and I know I can do a K-S test, but that requires me to get a CDF from the density estimate before running the test. I've also found multiple sources that talk about finding the integrated square error of the two distributions, but I can't find a best practice on how to do this (frequency of points being used in the calculation, etc.) or how to get a p-value once the integral is calculated. Any help on this would be greatly appreciated. 
 I am investigating the predictors for adverse events (AEs) from a sample of 400 patients recieving an injection therapy. The outcome AEs is binary and I will use a multiple logistic regression model: AEs = diagnosis + region + structure (R code: ) The levels of each of the variables are: Then I came to think about: 1) Is it a problem, that some combinations are impossible/will not be present in the dataset? E.g. ’other’ will never be combined with ’hip’, ’elbow’ will never be combined with ’joint’ etc. 2) If not; I have considered adding interactions between all the predictors, because it clinically makes sense. However, will 1) be a problem for the model in that case (I am aware of that I will not get any estimate for e.g. ’elbox:joint’ etc., but does it harm the model?). (R code: ) 
 I have the two sets of data; $A = $ 1.5010, 1.3118, 1.4219, 1.1650, 1.2720, 1.5421, 1.4211, 1.1832, 1.5378 1.4357, 1.2707, 1.2411, 1.4833, 1.5578 and $B = $ 1.4039, 1.1912, 1.3596, 1.4168, 1.6269, 1.4635, 1.2787, 1.5260, 1.8160, 1.6103, 1.5315, 1.7100, 1.5181, 1.4049 The data is meant to represent the amount of negative words people use on a discussion forum before and after a political election. The higher value, the more negative the forum post is. $A$ is two weeks before the event, and $B$ is two weeks after, where $A_7$ is day 7, for example. So I made a two sampled paired t-test in R where $$H_0: \mu_{A} - \mu_{B} = 0$$ $$H_a: \mu_{A} - \mu_{B} &lt; 0$$ And I get a p-value = $0.04325$, so I reject null with 95% certainty, since $\alpha$ = $.05$ With this, I claim to say that there has been an increase of negative words since the mean is different, and seems to be larger two weeks after the election. How do I calculate the actual percental increase? Would I take the diffrence between $A_i$ and $B_i$ divided by $B_i$ and then cumulative their sum? Any help would be wonderful! 
 You could take one of the many, freely available online course on R. For example: R Programming by coursera.org , or Free Introduction to R by datacamp.org Both should be fairly easy going even if you don't have any background in programming. They also touch practical aspects with small integrated assignments, so you will actually write some R with most of them. And of course those two courses are just examples, there are tons of other courses that will fulfill this criteria. You might also want to take a look at the list of free online courses at RDataMining - some of those might be interesting to you as well. 
 A percent is just another way to represent any fraction by scaling it to be out of 100. You could do this for your scenario since something like .25/1 is obviously 25% of 1. It seems like there isn't a special term since there doesn't need to be. 
 I have imputed my data set using mice but when I use lrm(from rms package) on the compelted data set complete(imp,1) I keep getting Error in X[, mmcolnames, drop = FALSE] : subscript out of bounds My original data worked fine with lrm. Is there a way to make it work? 
 Without knowing more about $W$ and $B$ (or why you would theoretically want to include two different weight matrices), I can't be sure that this directly answers your question. But, Spatial Durbin and Spatial Durbin-Watson models can be estimated with maximum likelihood, without having to use the spdep package. And there are many options in R for MLE. You might also consider some form of a two-stage or simultaneous equation model. For example, fit the model first with with $W$, and then use your predicted values from this in the second model with $B$. But again, the right specification will depend on your theory of the relationship between $y$, $By$, and $Wy$. 
 For clarity: The sample Walsh averages are the pairwise averages $(x_i+x_j)/2$, $i=1,2,...n,$ $j=1,...,i$. The median of the Walsh averages is the Hodges-Lehmann estimator, also called the the pseudo-median. Here's some (hopefully useful) hints to get you started -- which is basically one of several ways to just arrange the calculation methodically, but it makes it easier to see the connections between the two calculations. Here the observations are from a single sample (in the case of a paired test the observations are the pair-differences, at which point we're dealing with a single sample of pair-differences). Further assume that the $X$'s are continuous (so for example, there are no tied ranks and no Walsh-averages at 0) Consider without loss of generality that we're comparing to a specified median of zero. Let $X_i = S_i M_i$ where $M_i=|X_i|$ and $S_i = \mathop{\mathrm{sgn}}(X_i)$ (and similarly for $j$, when we deal with pairs of observations). Then let $R_i = \mathop{\mathrm{rank}}(M_i)$. We now have some notation in place to describe the basic components of the signed rank test. Now order the $X$-values from smallest magnitude to largest magnitude (i.e. sort them by the $M$-values). (It helps to play with a small numerical example. Consider data values $1.0, -2.4, 3.6$ say -- these have deliberately been ordered as just described) Write an $n \times n$ table with row and column headings being the ordered $X_i$ values (you may like to write a small $(S_i,R_i)$ under the columns). Inside the table, for values on or above the main diagonal, put a "+" if the Walsh average of the corresponding row- and column- $X$-values is above 0 and "-" if it's below it. For each column, count how many "+" values there are. Note that looking down a column we're just seeing $X_i$ compared with each other value that is no larger in magnitude than $X_i$ (i.e. with each observation to it's left in the ordered list, plus itself). For the column labelled $X_i$, the count will either be positive or $0$. What determines which of the two things it is? Now note the connection between the column "+" count and $R_i$. Hopefully you should be able to work your way to an argument from that. Here's the example I mentioned above. Note that if X's are 1.0, -2.4, 3.6 then the signed ranks are +1, -2 and +3: It should be clear that if $S_i$ is $-1$ then there are no "+" terms in the column of positive Walsh-averages, but if $S_i=+1$ then there are $R_i$ positive Walsh averages. What you need to do is make this observation a bit more formal and then argue a small step to the needed result from that. 
 For the last few days I have been trying to design a neural net to colorize images. Right now my dataset consists of 116k images. I have tried a number of approaches all involving constitutional nets. Right now the best network I can come up with is the following. It accepts a gray scale image at the input and outputs the A and B channels of the LAB colorspace at the output. This network according to the summary has about 5.5 million parameters. I am not really sure if this is too low or too high. After the first epoch of training on a random sample of 16,000 images I can tell that the network is starting to get the correct colors, grass looks kind of green and skin looks kind of pink/red. I then left it to train overnight and unfortunately when I tested it in the morning all the images were a dull brown color. I also noticed that the loss function had not been decreasing for the last few epochs. I'm just looking for any general feedback or advice. Do I just need to train longer? Do I need to make the network deeper / shallower? 
 I can attest to Coursera's R Programming and Practical Machine Learning courses, as I've taken both. Once comfortable, read through the hard or free online copy of Hadley Wickham's Advanced R. This should give you a solid understanding of R programming. Although, if you need a solid foundation of object-oriented programming in general, I'd try sites like CodeAcademy or CodeSchool mentioned below. 
 I'm especially interested in the following case: let's say we have a Gaussian kernel $K$ with bandwidth $\sigma$ and RKHS $\mathcal{H}$ and a set $H = \{ \forall h \in \mathcal{H} : ||h||_K \leq \Lambda\}$. Now we take another Gaussian kernel $K'$ with $\sigma'$ and RKHS $\mathcal{H}'$ and a set $H' = \{ \forall h' \in \mathcal{H}' : ||h'||_{K'} \leq \Lambda\}$. Now if $\sigma &gt; \sigma'$, will then $H \in H'$? Or $\mathcal{H} \in \mathcal{H}'$? Since $\sigma'$ is smaller, $K'$ smoothes less, and thus my intuition is that it can reconstruct any function $h \in \mathcal{H}$. But I find this very difficult to show... 
 I do not have any formal training in programming or Statistics, at least not to a sufficient enough level. I am trying to teach myself and stats, and going to write what so far I found helpful. So the answer is not based on broad knowledge and therefore inherently personal. If you're a complete beginner, then watching videos would be useful. But since watching videos is more passive than learning from reading/studying a book, etc., videos are not as effective as books or other written materials. Second, most of the video tutorials are shallow, i.e. , they do not dig deep enough to be practically useful. Nonetheless, video tutorials are great to begin with. Other answers already mentioned most popular and recognized video tutorials. Below is a list of some written materials I would recommend: The Art of R Programming by Norman Matloff is an excellent beginner's book. The book is freely available here . It starts with basic syntax, data types and the like. It then deals with statistical fundamentals and then programming aspects of the language. If you think deeper treatment of the language is needed, I would recommend more advanced Software for Data Analysis Programming with R by John Chambers. An Introduction to Statistical Learning: with Applications in R deals with somewhat advanced statistical techniques (that are quintessential for more advanced topics) in . online/free materials: An Introduction to R on CRAN R language for programmers by John D. Cook Quick-R Advanced R by Hadley Wickham Though might not be recommended for absolute beginners, learning to visualize data with ggplot2 and manipulate with dplyr is certainly worth it. Introduction to ggplot2 by Norman Matloff, and a fairly comprehensive cheat-sheet by RStudio. Introduction to dplyr , and cheat-sheet . 
 Your model should be fine. You will find when you fit the interaction (your point 2) that glm will warn you that it has had to drop a term from the model (I have forgotten the exact warning message) but it is a warning, not an error. The only thing to beware of is if for some combination either all patients had an AE or, more optimistically, none of them as then the coefficient for that combination will try to go to infinity, a problem know as separation (also known as Hauck-Donner effect). Try it and see. 
 Proportion or fraction (if out of one) or percentage (if out of 100) seem pretty innocuous and easy to interpret. Other bases may work too, like parts-per-million, depending on your intended domain. For example, some fields report outcomes as X per 100,000 patients or parts per million. You could label them as relative frequencies (or something like ), though this (obviously) has an frequentist slant and might be a little bit odd if the probability is being used to describe something like a degree of belief. On the other hand, if you're explicitly encoding subjective beliefs, you could call them credences (or maybe for short). There isn't anything weird about calling the units probability , in the same way we often label an angle as being "in radians" even though angles are, in fact, dimensionless. You could coin your own term, though I have a strong but viseral dislike of "perunus"--it just looks weird. It's emtymologically tempting, but I would strongly suggest not overloading probit either--it's confusing. In some situations, it might be more natural to use odds instead of probability, which you could label as odds or odds ratio . I suppose there's also the Hartley or ban/deciban too, though these aren't quite probabilities (and have a very old-fashioned, Bletchley-park sort of feeling to them). Finally, make sure that you're distinguishing between probabilities (which are dimensionless) and probability densities, which have units of $\frac{1}{\text{whatever}}$ (and, obviously, may be larger than one). 
 Disclaimers: This is a question about the model, not the software. While I was exposed to boosting in my undergraduate program (2007) I have been using boosted trees on and off for the last ~4 years. I consider myself an explorer who has a lot to learn, not an expert, although I can do some neat things with the tools. Is there any "major" gradient boosted CART model whose general behavior is decent, whose premise is not decently enveloped in the following paragraphs? To be clear I am not referring to bagging (CART models in parallel), nor to a model whose weak learner is not some sort of classification and regression tree. Linear (cubist), logistic, spline, glm, and gam are "not on the menu". CART models that I know are primarily greedy, split on one axis at a time using a metric (Gini impurity, variance, entropy) and have bounds like min elements per leaf, max depth, and some version of pruning control. GBM approaches that I know are usually built around some version of Adaboost (Freund &amp; Schapiro). They use the output error from one tree to compute weights associated with rows, as the training input for the next tree. This is "CART models in series". Stochastic gradient boosting is about selecting the rows. Is there an approach to the "CART model in Series" that has fair strength, whose important parts is not covered by this? Some References: list of boosting models ( link ) Decision tree learning ( wikipedia , category ) CRAN ( gbm , xgboost , h2o ) 
 I have a set of distributions corresponding to predictions for how each of hundreds of players will perform. I am looking to identify the distinct distributions of players. In other words, I'm looking to identify the distinct distributions in a group of distributions . I know can perform clustering on a vector, e.g.: However, my data are a series of vectors (i.e., distributions)---one vector for each player, e.g.: How can I perform clustering to identify the distinct clusters of players based on their distributions, focusing on differences in their means, rather than their variances. I don't want to just cluster the mean values, though, because I want to take into account the variances to know whether their means are in the same or in a different cluster (e.g., high variability in two players' distributions may indicate that two players with different means are in the same cluster). Ideally, I'd like two players with the same mean and different variability distributions to be in the same cluster. Is there a way to do this using the or another package in R? I've considered doing pairwise t-tests, but this seems that it would be heavily dependent on the sample size in each distribution (which I'd rather it not be too dependent on sample size, if possible). I've also considered comparisons based on effect size (Cohen's d). I'm not sure what other options there are (e.g., Tukey's HSD, hierarchical clustering, etc.) 
 I am trying to determine the habitat of a species of dolphin. My data is highly zero-inflated, so I chose hurdle and zero-inflated negative binomial models to analyze it. I used the pscl package in R to run a suite of models with different combinations of the explanatory (environmental) variables. I have selected two models (using AIC, k-fold cross validation, likelihood ratio tests, and AUC), one that models the zero part best and one that models the count part best. These models only retained a few variables and only three ended up being significant. There are two results I'd really like to obtain, but I'm having trouble finding a method to get them. One would be a plot of the results against the original explanatory variables, just the significant ones. I'm new to statistics, so I first tried to plot the fitted responses against the original variables, but it didn't agree with the model results, so I figured that was the wrong way to go about it. The other is the "optimal" values of the significant environmental variables (e.g. dolphins are most often found in waters of x1-x2 meters depth). This is the really important metric, but I can't find anything explaining how to determine this. Does anyone have a method or a resource I can look through to try to get at these things? Turbidity and chlorophyll a have been binned into low, medium, and high due to instrument issues. I'm not sure if there's a better way to paste in the data, so if there is, please let me know and I'll fix it so it's easier to use. 
 First things first, your outcome is either yes or no so I would strongly recommend using a logistic mixed model. Using such model you can estimate the probability of observing the behavior for a given genotype and treatment over time. In a counterbalanced design, you also have to consider the order in which treatments were delivered so you'll have to include that in your model. using you should be able to do something like this: You can also allow for random slopes for by modifying the random portion to look like this: If you don't expect the time to have a significant effect i.e. the effect of the treatment doesn't change with time, an alternative (possibly simpler) analysis could be performed. You would have to collapse your dataset to the animal level and calculate the proportion of yes as your outcome and adding the number of time points to the argument . Hope this helps. By the way, this page is a very useful resource for longitudinal-mixed-model specification. 
 I have a study design setting where each patient received a treatment and was assed again after 30 days. the dependent variables are a set of symptoms and clinical values. I thought of a design where the scores of such symptoms are evaluated before and after treatment for each patient. Since all patients received the treatment I don't think that using the other variables as covariates makes sense causally. But instead I suppose it would be interesting to evaluate if and how other variables could influence the effect of the treatment. So these are the questions: Should I consider the other covariates only as interacting with the treatmente (R notation: ), or also as independent variables (R notation: )? As a random effect I would of course the subject (R lme4 notation: ) Should I include a random slope? if yes which factor should I consider for the random slope? the treatment (R lme4 notation: ) 
 I am trying to use mixed-effect modeling to analyze brain wave data from two groups of participants when they were presented with two distinct stimulus. The data points (scalp voltage) were gathered from the same set of 9 nearby channels from each participant. And so I have the following factors: : the dependent variable : the between-participant/within-item variable for groups A and B : the within-participant variable (note there are exactly only 2 items, P and Q) : identifying each participant across the two groups : identifying each channel (note that data from these channels in a nearby region tend to display similar patterns in the same participant) The hypothesis is that only group B will show difference between P and Q (i.e., there should be an interaction effect). So I established a mixed-effect model using the lme4 package in R: Questions: I'm not sure if it is reasonable to add in participant as a random effect, because it is related to group and seems to weaken the effects of group. Because the data from nearby channels of the same participant tend to be correlated, I'm not sure if modeling participant and channel as crossed random effects is all right. But meanwhile it seems also strange if I treat channel as nested within participant, because they are the same set of channels across participants. When the interaction term is significant, how should post-hoc comparisons be done (e.g., differences between groups A and B for P)? I saw suggestions for t-tests, lsmeans, glht, or for more complicated methods such as breaking down the model and subsetting the data: But especially here comparing only between two groups while modeling participant as a random effect seems detrimental to the effects. And I'm not sure if it is really OK to do so. On the other hand, because the data still contain non-independent data points, I'm not sure if simply using t-tests is all right. Will non-parametric tests (e.g., Wilcoxon tests) do in such cases? I suppose I don't need to model item as a random effect because there are only two of them, one for each level, right? 
 I think you do not need to do anything with it. Essentially you are asking how to deal with "correlated" features, i.e., two random variables $X_1$ and $X_2$ they are not independent where $P(X_1,X_2)!=P(X_1)P(X_2)$. This case is very normal in most real world problems. For example, you want to predict a person's income, it is very common to use both age and education level as features, and age and education level are not independent, e.g., higher education usually indicates the person is older. Another example would be predicting a person's healthy status. It is very common to use both height and weight. Although we know height and weight has really strong correlation. In your example, the only difference is you have discrete random variables and the two examples I mentioned above are continuous random variables. For your case, Let's use $X_1$ to represent spouse's age and $X_2$ for is married. You will have, say $P(X_1|X_2=1)$ is normal distribution, while $P(X_1|X_2=0)=0$. As well as you do not have highly correlated features, such as identical features, e.g., $P(X_1)=P(X_2)$ / "weight in lb and weight in kg", you would not have too much problems. If you do have "collinearity problem", you can drop some "repetitive" features or use Principal component analysis or use regularization 
 I have means and standard deviations on a given variable, $X$, for six independent samples of unequal sample sizes: Group 1: $SD = 0.96 , n = 290$ Group 2: $SD = 1.082, n = 250$ Group 3: $SD = 0.70, n = 250$ Group 4: $SD = 0.794, n = 310$ Group 5: $SD = 0.819, n = 290$ Group 6: $SD = 0.691, n = 190$ total sample size = $1,580$ I am asked to report the the average standard deviation based on these values, so, in essence, I am looking to compute the average standard deviation based on six independent datasets - that is, an average of six separate standard deviations ( not the combined standard deviation for the total sample). Can I just calculate the weigthed mean of all six standard deviations, with the weights for each sample given its sample size divided by total sample size of all six samples combined? So, in essence, producing the following: average standard deviation = $\frac{[(\frac{290}{1,580}\cdot0.96) + (\frac{250}{1,580}\cdot1.082)+(\frac{250}{1,580}\cdot0.70)+..continued for all six groups..)]}{6}?$ Thank you! 
 I am looking at one algorithm(KERNEL SPECTRAL CLUSTERING).Steps: 1. compute the training eigenvectors 2. let A be matrix containg vectors as columns 3. binarize A and let the code-book be composed be composed by the k encodings Q=sign(A) with most occurences Theory is in the paper(page 341) Why? How? Is there any inverse operation from binarized matrix to the original one? 
 I am modeling wood properties variation of several individual within the same tree species using multilevel lme(). I specified a first model as : with 'radius' nested within 'ind'. As 'id' is a longitudinal variable according to 'dist2' within 'radius' within 'ind', I want to add an AR(1) autocorrelation error term. So, I wrote a second model by updating the first one with : Is it a good way to include autocorrelation error structure in my model? This specification is also possible and gives slightly different results : But I don't clearly understand the difference between both specifications. Could you help me ? Thank you. Romain. 
 I'm currently trying to improve on a classifier. The current method used is a neural network, and the method I've found to be better is a random forest (or even just a single tree). With 40 trees, the classification is much better than the neural network. However, it takes 40 minutes(using 4 parallel workers due to running out of memory) to classify a large block of data; whereas, the neural network takes ~5 minutes(using 8 parallel workers). Is there a way to improve the speed of prediction? And does anyone know the reason for this huge slow down? I'm guessing it is due to the number of trees, and also the number of workers I can use. MATLAB was used to create and run both the network and the forest. 40 features, 13 outputs, training set size: ~800,000, individual block size: ~500x500, whole file to be classfied: 1+GB along with other files containing more information The data is not sparse. 
 I found when using two sample ranksum test, Matlab's ranksum gives different p value from R's wilcox_test (seems to be related to unequal sample size), I'm not capable of delving into their implementations. Anyone has a clue? Thanks. Matlab side: results: p = 0.0952 R side: results: p = 0.04762 What's also confusing to me, when the p values are not the same, R's p is usually half of that from Matlab. Whey 'a' and 'b' have equal sample size, R and Matlab produce the same results. 
 We are performing a RCT with 4 groups (1 control + 3 experimental), where we measured physical activity (PA) in 3 moments (at baseline, post-test and follow-up). Although we have done stratified randomization, the groups had different PA at baseline. So, we want to input this variable as a covariate , but also as within-subjects variable, because PA (baseline) it could be a confounding variable and PA (baseline, post-test and follow-up) are our dependents variables. Thus it make sense? In a repeated-measures ANCOVA , how can we do it, since when we input PA in SPSS as within-subjects variable it disappears from our variables (making it impossible to put PA as covariate too)? 
 Quick question regarding adjustung p-values.... I preformed a logistic mixed effects regression model and used Wald likelihood ratio test to select model parameters. I received comments back from a journal reviewer stating that I should use Holm-Bonferroni correction for p-values due to multiple comparisons.... I did not think that adjusted p-values were required when performing mixed effects logistic regression.... am I incorrect in my assumption? any comments/suggestions greatly appreciated, thanks 
 There is an easy rule of thumb for ANOVA: Unequality in sample sizes deteriorates the power. The reason is that the power depends mostly on the variance of the effect estimator, i.e. the mean differences between the groups. These mean differences have the least variance (given a total sample size) if all the sample sizes are equal. 
 In a colony all families have at least one child.The probability that a randomly chosen family from this colony has exactly $k$ children is $(0.5)^k;k=1,2,...$ A child is either a male or a female with equal probability.What will be the probability that such a family consists of atleast one male and atleast one female child? Well,I started the problem in this way,may be I am wrong. $A_i$:The $ith$ family has no male and female child. So, we have to compute $P(\bigcap_{i=1}^\infty A_i)^c$ $P(\bigcap_{i=1}^\infty A_i)^c=1-P(\bigcup_{i=1}^\infty A_i)=1-(\sum P[A_i]-\sum(P(A_{i1}\bigcap A_{i2})+...+P(\bigcap A_i))$ Now I am unable to compute the probabilites. May be my approach is not right.Can anyone please give some hints about this problem? 
 Let's break this down into three possibilities: Only male children Only female children A mixture of male and female children The three groups sum to 1 (because all families have at least one child), and so if we compute the probability of the first two groups, we can subtract that from one and get our answer. This is even easier because the two are symmetric--the probability of only having male children is the same as the probability of only having female children. So how would we get the probability of only having sons? Start with the probability of having a family with exactly one son (and zero daughters). The chance that the family size ($F$) is 1 is: $P(F=k)=0.5^k \to P(F=1)=0.5$ The probability that a family of size 1 has only sons ($S=F$) is: $P(S=k|F=k)=0.5^k \to P(S=1|F=1)=0.5$ But we want an unconditional probability, so $P(S=1,F=1)=P(S=1|F=1)P(F=1)=0.25$. Back out to the formulation with $k$, sum over all $k$, and we have half of the complement of our answer. 
 Try to define the probability that you want to obtain in mathematical terms. If you do so the following becomes just calculus. The probability you want to obtain can be seen as an expectation of $I\left[x &gt; x_0\right]$ over joint distribution of $x$ and $h$ where $I$ is an indicator function that is equal to $1$ when $x &gt; x_0$ and to $0$ otherwise: $$P(x &gt; x_0) = \mathop{\mathbb E} _{x, h} I\left[x &gt; x_0\right] = \int dP(x, h)\ I\left[x &gt; x_0\right] = \iint dx\ dh\ p(x, h)\ \theta(x - x_0).$$ In this expression $p(x,h)$ is a probability density of joint distribution of $x$ and $h$, and $\theta(x)$ is a Heaviside Step function . Your probability $P(X &gt; x)$ is actually a conditional probability for given value of $h$ and it's derivative is a probability density of conditional probability of $x$ for given $h$ $p(x \mid h).$ So using definition of conditional probability you can get that $p(x, h)\ dx\ dh = p(x \mid h)\ dx \cdot p(h)\ dh.$ So now you can obtain the probability density functions $$p(x \mid h) = \frac{d}{dx} P(X \leq x \mid h) = \frac{d}{dx}\left(1 - P(X &gt; x \mid h) \right) = -\frac{\partial}{\partial x}P(X &gt; x \mid h),$$ $$p(h) = \frac{d}{dh} P(H \leq h) = -\frac{\partial}{\partial h} P(H &gt; h)$$ and substitute them to the integral: $$P(x &gt; x_0) = \iint dx\ dh\ p(x\mid h)\ p(h)\ \theta(x - x_0)= \int\limits_0^{\infty} dh\ p(h) \int\limits_{x_0}^{\infty} dx\ p(x\mid h) = \\-\int\limits_0^{\infty} dh\ P(X &gt; x_0 \mid h) \frac{\partial}{\partial h} P(H &gt; h).$$ Actually in this particular case there is a more simple way to use conditional probabilities directly: $$P(X &gt; x) = \mathop{\mathbb E}_h P(X &gt; x \mid h) = \int P(X &gt; x\mid h) \ dP(h),$$ but I find the joint distribution/indicator function approach to be more powerful because it allows to infer probability distribution of more complicated relations (e.g. if probability of wave energy $x^2$ is larger than given function of $h$) or introduce new probabilistic variables to the model (e.g. instead of single value of $\sigma$ use random variable sigma from distribution obtained in result of bayesian estimation of $\sigma$ from experimental data) using a single generic framework. 
 Multiple testing corrections are not inherently linked to an particular analytic tool, just to hypothesis testing in general. p.s. I would suggest using something less conservative than the bonferroni correction, especially if you have A LOT of tests. For example, look into controlling the false discovery rate, rather than the family-wise error rate. 
 I'm using lasso logistic regression in order to identify important variables and make inferences. For that I deploy glmnet and repeated cross validation to identify the best tuning parameters lambda. In the first step I build a model including only my control variables. In the second step I add other predictors and compare the selected variables, fit, and performance with the first model. Below are the measures of the two models. controls (CV_error| % deviance): 1.320194 | 15.26 full model (CV_error| % deviance): 1.3705 | 14.97 I identify a best lambda for each of the two models (0.03|0.09). The first column measures the mean cross validation error in binomial deviance. The second column measures % of deviance explained compared to the intercept-only model when I fit the model on the whole data. As far as I know I can use the first column to tell something about the predictive accuracy of the models. The second column can be used to tell something about the performance improvement of the model compared to the null model (comparable to R^2 in linear regression?). As one can see, my full model doesn't perform better than the controls only model, despite other variables are being selected. Even when I create another model where I don't penalize the controls to keep them all in the model, one additional variable is selected by lasso. What would you conlude? Why is there no improvement in performance? Are the predictors useful despite they don't improve the model? 
 I have a question about the function in the GEE package in R. I am a beginner in statistics and the estimates in the output of the model are puzzling me. How can I tell that the estimates are ok and how can I recognise overdispersion? The output looks like this: Call: geeglm(formula = logSUM ~ teplota + radiace.rezidualy + srazky2, family = gaussian, data = PH1, id = den, corstr = "ar1") Coefficients: Estimate Std.err Wald Pr(&gt;|W|) (Intercept) -0.933429 0.515005 3.29 0.070 . teplota 0.163241 0.028613 32.55 1.2e-08 *** radiace.rezidualy -0.000842 0.000397 4.50 0.034 * srazky2 -0.153942 0.037414 16.93 3.9e-05 *** --- Signif. codes: 0 ‘’ 0.001 ‘’ 0.01 ‘’ 0.05 ‘.’ 0.1 ‘ ’ 1 Thank you for any feedback or suggestions. 
 In this formula t is the time of arrivals (random variable) of vehicles at an intersection and W(t) is estimated delay. How can I develop the probability density function of the vehicle delays given this function? 
 Let's say we have datapoints $x_i \in \mathbb{C}^N$, like this: $x_1 = \begin{pmatrix}z_1 &amp; z_2 &amp; ... &amp; z_N \end{pmatrix}^T$ and $x_2=\begin{pmatrix}z_1 &amp; z_2 &amp; ... &amp; z_N \end{pmatrix}^T$. My question is, how to interpret the covariance matrix calculated for the data points $x_i \in \mathbb{C}^N$. I know how to interpret the real-valued diagonal, which is just the variance of the corresponding variable, but the off-diagonal elements are complex numbers. Can someone give an intuition of what these represent. For example the covariance between $z_1$ and $z_2$ yields the number $a+bi$. Then, does $a$ represent the covariance of the real parts of $z_1$ and $z_2$ and $b$ the covariance of the imaginary parts of $z_1$ and $z_2$? 
 Looking only at the CV error (I don't think a training-error figure like your "% deviance" is very helpful, especially for a penalized model like the lasso), I would conclude that, at least around your sample size, the predictors beyond the control variables aren't helpful for prediction. Adding them to the model only worsened predictive accuracy. So, they aren't useful. Why they're not helpful for prediction is of course hard to guess at without knowing the context of this problem. By the way, the right way to compare your models' predictive accuracy to that of a trivial model is not to look at training error, but to compute CV error for the trivial model. 
 I been looking at this for a few days; I cant understand how the pseudo-random generator comes into play, why do we need it? Is it just in order to have an actual $X^{'}=X \mid t$ in a real life situation for the experimenter?? 
 If we have two dichotomous variables variables and run an interaction constructing the means for each cell seems straight forward, but not the standard errors. I would like to use these standard errors to compute if the differences between any two given cells is significantly different. With the following (fake) output from an OLS regression : I interpret the above for the mean . The representing mean for those under 21 and untreated ; representing mean for the treated individuals under 21 ; representing the mean for those 21 and older and untreated ; representing the mean for those for those 21 and older and treated . 1) Is this interpretation of means correct? 2) How are the corresponding standard errors of the means computed 3) Is it statistically acceptable to then compare the means across this 2X2? I can check the above by hands and calculating means, but it becomes unclear when you include controls in the regression. 
 Can you please post the entire code for your model. Without seeing the code, I would say that you might have log coefficients. This is entirely dependent on the link function, however. If you do have log coefficients, in order to get a coefficient that is easy to understand you must exponentiate the estimate: exp(Beta). This will give you a rate ratio. Numbers over 1, for instance 1.1, would mean a 10 % increase in the response variable. Numbers less than 1, for instance .9, would be a decrease. These numbers are related. Divide 1 by 1.1 and you will get .9 and if you divide .9 by 1 you will get 1.1. See here for more information ( https://onlinecourses.science.psu.edu/stat504/node/180 ). 
 Does it make sense to use a deep neural network as some sort of comparison method (distance function) that tells us how much some image is similar to something? For example, let's say we have a super great Genetic Algorithm program that tries to draw faces. Does it make sense to feed results into an already trained neural network and ask it how much it believes the input is a face? Or how good of a face it is? Any hints on how to do something like this (if it's even right) would be greatly appreciated. 
 Since you're interested in the means rather than variances, why not just cluster the means? 
 Both are valid. In applied problems, we never know the true generating model and hence we only have the second option. But in theoretical settings where you do know the true generating model, it can be useful to look at error from the true model as well as error from the observations. So which you should use depends on what you want. 
 Hidden Markov Models are designed with discrete state distributions in mind. Consider instead something like a Kalman Filter , which is built around continuous state distributions. The dynamics are limited to being Gaussian, and so you may want to use the extended Kalman Filter or a particle filter if your situation requires it. 
 $P(W(t)\leq w)=P(r + 1/s-T(1-q/s) \leq w) = P(T\geq -(w-r-1/s)/(1-q/s))$ Which gives use $f_W(w)=\frac{d}{dw} (1-F_T(-(w-r-1/s)/(1-q/s)))$ where $F_T$ is the distribution function of $T$, and $f_W$ is the density function of $W$. You still have to worry about the support and so forth though. If you know the distribution function of $T$ than you know the density of $W$. 
 A few additional points to the ones made in other answers: if you want to consider order (i.e., control-first vs treatment-first), I think that should probably be a fixed effect (the meaning of "control-first" is the same across all individuals). With this kind of crossover trial, you can consider the among-animal variation in the effect of treatment (which greatly increases your power -- this is one reason that people use this kind of design); you can also consider the difference in temporal effects among animals. You might possibly want to simplify the random effects by making the slopes independent of the intercept and treatment effect: (suppress the intercept from the term since it will end up in the other term anyway). 
 Percent change can be extremely misleading for a number of reasons ( e.g. regression to the mean ), even though in the medical literature percent change is used quite frequently to "normalize" clinical data. In a regression model it is better to put the baseline value in the overall model, as opposed to transforming the data to "percent change". However, I'm trying to understand what pitfalls one might encounter if you use percent change in power calculations for prospective studies. Is it better to use absolute values for power calculations? Would using percent change lead to erroneous calculations the same way it would for normalizing clinical data? 
 Let $X_1, X_2, \ldots, X_{3n}$ be a sequence of iid random variables sampled from an alpha stable distribution , with parameters $\alpha = 1.5, \; \beta = 0, \; c = 1.0, \; \mu = 1.0$. Now consider the sequence $Y_1, Y_2, \ldots, Y_{n}$, where $Y_{j+1} = X_{3j+1}X_{3j+2}X_{3j+3} - 1$, for $j=0, \ldots, n-1$. I want to estimate the first percentile of this sequence, i.e., the $0.01-$percentile. My idea is to perform sort of a Monte-Carlo simulation: } Calling the mean of all the sample $0.01-$percentiles computed to be $\hat{\mu}_n$ and their variance $\hat{\sigma}^{2}_{n}$, to compute the appropriate confidence interval for $\mu$, I resort to the Strong form of the Central Limit Theorem : Let $X_1, X_2, \ldots$ be a sequence of iid random variables with $E \left[ X_i \right] = \mu$ and $0 &lt; V \left[ X_i \right] = \sigma^2 &lt; \infty$. Define the sample mean as $\hat{\mu}_n = (1/n) \sum_{i=1}^n X_i$. Then, $(\hat{\mu}_n - \mu) / \sqrt{\sigma^{2}/n}$ has a limiting standard normal distribution, i.e. $$ \frac{\hat{\mu}_n - \mu}{\sqrt{\sigma^{2}/n}} \overset{n \rightarrow \infty} \longrightarrow N(0,1).$$ and Slutksy's theorem to conclude that $$ \sqrt{n} \frac{\hat{\mu}_n - \mu}{\sqrt{\hat{\sigma}^{2}_{n}}} \overset{n \rightarrow \infty} \longrightarrow N(0,1).$$ Then a $(1-\alpha)\times 100\%$-confidence interval for $\mu$ is $$ I_{\alpha} = \left[\hat{\mu}_n - z_{1- \alpha / 2} \sqrt{\frac{\hat{\sigma}^{2}_{n}}{n}} , \hat{\mu}_n + z_{1- \alpha / 2} \sqrt{\frac{\hat{\sigma}^{2}_{n}}{n}} \right], $$ where $z_{1- \alpha / 2}$ is the $(1- \alpha / 2)$-quantile of the standard normal distribution. Questions: 1) Is my approach correct? How can I justify the application of the CLT? I mean, how can I show that the variance is finite? (Do I have to look at the variance of $Y_j$? Because I don't think it is finite...) 2) How can I show that the average of all the sample $0.01-$percentiles computed converges to the true value of the $0.01-$percentile? (I should use order statistics but I'm unsure on how to procceed; references are appreciated.) 
 Very good question, and it depends on whether $Y$ is binary or continuous. For binary $Y$ many speak of % change on the odds scale, e.g. a 25% increase in the odds of an event = an odds ratio of 1.25 and we plug 1.25 into a power calculation (we also need the control group probability). For continuous $Y$ percent change is an improper measure when it is used on individual subjects, because of asymmetry in the way the measure behaves. But percent change on overall summary measures is OK. But we work backwards to solve for the absolute change corresponding to that. For example one might have a 2-sample $t$-test designed to detect a 15% decrease in mean $Y$ from a control group mean of 90 to a mean of $90 \times 0.85 = 76.5$ on the original scale, for a difference of 13.5. We compare 13.5 to SD$(Y)$. 
 Using a Wald likelihood ratio test to select model parameters will invalidate any p-values and confidence intervals, as well as biasing estimates - unless you adjusted for that somehow. Thus, the reviewer is right to highlight this topic, if you had not done some appropriate there - especially so, if the model building is an important part of the paper. Just reporting estimate, CI and p-value after model selection as if not model selection had occured is wrong and misleading. I am not sure that using a Bonferroni-Holm correction is an approach for dealing with that. Perhaps it is possible to use it for that purpose at least to correct the p-values, but it may be extremely conservative. If you then afterwards are performing multiple comparisons across multiple levels of a random effect, then there's some people that would argue that no correction is needed (see e.g. Gelman et al.'s "Why we (usually) don't have to worry about multiple comparisons" article) - but others (perhaps including your reviewer) may disagree. If you are comparing multiple levels of some fixed effects factor, then it is clearer that there is a multiple comparison issue (although some might still argue against the need for type I error correction if this is rather exploratory and you do not want to use a hypothesis testing approach claiming "statistical significance"). 
 Is there an intuition why character based models language bases models are preferred over word based. For example Karpathy builds his language model by predicting the next character in Karpathy Blog . The aspect I am struggling with is that not each combination of characters is a word, so intuitively I would try to predict the next word (or word embedding and calculate squared error). I think this is also used in the sentence embedding proposed by Kiros Skip Thought . So my question is what are advantages and disadvantages for character based language models in comparison to word based models. 
 In most statistical packages I believe Cons (mean for A = 0, T = 0), Cons+T (mean for A = 0, T = 1), Cons+A (mean for A = 1, T = 0) and Cons+A+T+A*T (mean for A = 1, T = 1) would have the interpretations you describe. (2) is complicated, because the estimates are not independent, but most statistical packages will have an option for giving you either the contrasts you want or outputting the estimated covariance matrix for the estimates that let's you calculate it by hand. I must be missing some crucial point regarding the third question - of course one can compare the means across the 2x2 table. Or is the question how to do this, or whether some multiplicity adjustment is needed? 
 "Best" for what? AIC attempts to identify the model that will best predict future data (it minimizes the expected Kullback-Leibler distance, which is a measure of the distance between predicted and actual values of outcomes). AIC is asymptotically equivalent to cross-validation; well-implemented cross-validation [i.e., resampling at the level of groups] will make fewer approximations, and hence will be more reliable although more computationally expensive. BIC attempts to identify the true model (it is an asymptotic approximation of the Bayes factor, which is related to the Bayesian posterior probability of the model) the log-likelihood ratio test attempts to reject the null hypothesis (that the slopes are identical for every group) Since these are all different questions, it's confusing but not horribly surprising when they give different answers, although in extreme cases (the models are very nearly identical, or one model is much better than the other) they generally agree. I would use whichever criterion is more consistent with the general statistical approach you're taking (which ideally you decided on before you started to analyze the data ...) In general AIC is less conservative than BIC (provided you have more than 8 data points), so it's not surprising that AIC chooses the more complex models. You should also be aware that all of these tests are moderately crude approximations in the mixed model case: a null hypothesis on the border of the feasible space violates the assumptions of the derivation of the AIC and LRT, making them generally conservative counting number of observations for the BIC is tricky: do we count groups or observations? for the AIC, are we trying to optimize prediction at the population or the individual level ("level of focus")? These issues are discussed somewhat further, with references, in the GLMM FAQ , specifically the sections on testing significance of random effects and Can I use AIC for mixed models? 
 Suppose I have a random sample $\lbrace X_i, Y_i\rbrace_{i=1}^n$. Assume this sample is such that the Gauss-Markov assumptions are satisfied such that I can construct an OLS estimator where $$\hat{\beta}_1^{OLS} = \frac{\text{Cov}(X,Y)}{\text{Var(X)}}$$ $$\hat{\beta}_0^{OLS} = \bar{Y} - \bar{X} \hat{\beta}_1^{OLS}$$ Now suppose I take my data set and double it, meaning there is an exact copy for each of the $n$ $(X_i,Y_i)$ pairs. My Question How does this affect my ability to use OLS? Is it still consistent and identified? 
 Perhaps one option might be to exponentiate the results: (since outcome is log transformed). And then talk about percentage increases in region, over and above adjustment. So from the adjusted model, for state 1 we have estimate: That would translate to 13.4% decrease of gross state product? 
 I am going to try to give as much information as possible. I have a data base So all binary variables except one with 1,2,3. Therefore I wanted to do a probit model out of it with y113 being endogenous. However, I want to use the lavaan package for SEM because I want to add variables and links to this model later. So I created a model: So this table gives values that don't look too real, however I this is my first attempt at SEM so I am not comfortable with it.So I wanted to know how well fitted my model is : fitMeasures(sem) Still did not expect to see fmin = 0 or chisq = 0 or pvalue = NA, or anything like those. I expect to have made mistakes in my programming. Do these results make sense ? (I wish to help as many people that I can with my question (and your answers) so please help me improve it if needed :) ) If you need anymore information just let me know ! 
 In epidemiology, sometimes we are interested in calculating RRs than ORs. When the rare disease assumption is not valid (i.e. event rate is more than 10%) we cannot interprete odds-ratios as relative risks in logistic regression. In order to calculate risks in this case, we used modified Poisson regression (cf. reference below) with robust error variance. References: Guangyong Zou, A Modified Poisson Regression Approach to Prospective Studies with Binary Data , Am. J. Epidemiol. (2004) 159 (7): 702-706 doi:10.1093/aje/kwh090 How can I estimate relative risk in SAS using proc genmod for common outcomes in cohort studies? Introduction to SAS. UCLA: Statistical Consulting Group. from . 
 Here is an exercise from a past exam of my course in Time Series Analysis Suppose you have observations $(Y_{i,t}; x_{i,t})$ on $n$ units $i = 1, ..., n$ at time $t$, with $t = 1, 2, ...$. Consider a dynamic regression model \begin{gather} Y_{i,t} = \alpha_t + \beta_tx_{i,t} + \epsilon_{i,t}\\ \epsilon_{i,t} \sim N(0, \sigma^2), i = 1, ..., n \end{gather} where $(\alpha_t)$ and $(\beta_t)$ are assumed to be independent AR(1) processes. Write this model as a dynamic linear model (DLM) and discuss dynamic estimation of $(\alpha_t; \beta_t)$, assuming for simplicity that the parameters of the model are known . I had no problem in rewriting the model in a DLM fashion but now, I am having some problems in understanding the question about dynamic estimation of the parameters. I know that using a frequentist approach, OLS is not still valid for the estimation since we are in a dynamic framework. But with DLM, a Bayesian approach may be more appropriate. Any hint? 
 I'm trying to classify (LDA) few samples () in a high dimensional feature space () into 3 classes. First I reduced the dimension of my initial dataset with a PCA, keeping only the first two Eigen vectors. Update: turns out, I was actually using all 11 PCs for the LDA. Then I had a look at the projection of my n x 2 dataset in the LDA space (1st vs 2nd Eigen vector) and I obtained the following: I was quite happy because the LDA found a strong separation between the 3 classes. So I tried a leave-one-out cross validation to evaluate the LDA. I trained the classifier with 11 samples and tested it with the last one, and looped around. The problem is the classifier performs at chance level (30% success rate). I noticed that the LDA space changes drastically between each iteration, depending on the 11 samples used to compute it. Moreover, when I project the tested sample in the corresponding LDA space, it falls quite far away from what should be its group, explaining the poor success rate. My questions are: is it normal that such a (visually) nice separation between classes leads to such a poor classification? Is it due to the small number of samples? Is there anything I can do to improve the situation? 
 I am not yet familiar enough with the theory to give you a very mathematical answer, but intuitively, OLS only cares about proportions in which different cases are present. This makes sense when you recall that OLS chooses the coefficients that minimize the mean of the squared residuals, and the mean reflects purely the proportions of its inputs (in the sense that the mean of (1, 3, 3) is the same as the mean of a dataset with a million 1s and two million 3s). So, doubling the dataset will get you the identical model. Here's an R example, where we generate a random regression problem and notice that the coefficients are unchanged when doubling the data: One run gives me: 
 I want to make timeline in Excel with stacked bars so that each stack within a single bar corresponds to a given time period. As such, instead of having separate bars for all the categories, I want all the date on the UN in one stacked bar, and all the date for the US in another stacked bar etc. The problem is that when making a timeline one needs two columns for each category, both start date and duration. Thanks for any kind of help! 
 Do you have a good reason to do the doubling? It doesn't make much statistical sense, but still it is interesting to see what happens algebraically. In matrix form your linear model is $$ \DeclareMathOperator{\V}{\mathbb{V}} Y = X \beta + E, $$ the least square estimator is $\hat{\beta}_{\text{ols}} = (X^T X)^{-1} X^T Y $ and the variance matrix is $ \V \hat{\beta}_{\text{ols}}= \sigma^2 (X^t X)^{-1} $. "Doubling the data" means that $Y$ is replaced by $\begin{pmatrix} Y \\ Y \end{pmatrix}$ and $X$ is replaced by $\begin{pmatrix} X \\ X \end{pmatrix}$. The ordinary least squares estimator then becomes $$ \left(\begin{pmatrix}X \\ X \end{pmatrix}^T \begin{pmatrix} X \\ X \end{pmatrix} \right )^{-1} \begin{pmatrix} X \\ X \end{pmatrix}^T \begin{pmatrix} Y \\ Y \end{pmatrix} = \\ (x^T X + X^T X)^{-1} (X^T Y + X^T Y ) = (2 X^T X)^{-1} 2 X^T Y = \\ \hat{\beta}_{\text{ols}} $$ so the calculated estimator doesn't change at all. But the calculated variance matrix becomes wrong: Using the same kind of algebra as above, we get the variance matrix $\frac{\sigma^2}{2}(X^T X)^{-1}$, half of the correct value. The reason is that we have calculated as if we still have iid data, which is untrue: the pair of doubled values obvioulsly have a correlation equal to $1.0$. If we take this into account and use weighted least squares correctly, we will find the correct variance matrix. 
 I am working on some similar methods to Lumley's wrapper around Miller's AS274 algorithm, and I can't seem to find a reference for his incremental Huber/White sandwich covariance innovation? The only mention of 'how' it works I can find comes from this talk (page 31). But I'm looking for more details. I get how to create a "$(p+1)^2 \times (p+1)^2$ matrix of products of x and y", but I'm not sure how this gets me the 'meat' I need for the sandwich? I could look at the code, but 1) I'd like to read up more on this implementation anyway, and b) I'm writing a wrapper for AS274, and the default license for most scientific code is not GPL (usually BSD-3), so I'd need to implement the sandwich from 'scratch' so to speak, in order to avoid 's default GPL license. Any thoughts, references, other talks given by Lumley? Frankly, an explanation of his implementation would also do nicely, as then I could probably figure out the required code myself. 
 To get started you'll want to load the forecast package and convert your dependent variable to a time series. For monthly prices you could do the following: For defining more complex frequencies (daily, hourly, multiple) see the following post by Prof. Hyndman. You'll probably want to define something more complex for stock prices depending on your forecast horizon. Next you can easily fit and forecast an ARIMA Model using the and functions from the package. To run a working example of the above on your computer just replace with to use one of the included data sets. Eliminating and will significantly improve run time. 
 I would recommend a two step approach: 1. Imputation / Estimation (replacing missing values with resonable values). This is a interpolation task. 2. Forecasting (predicting future values). This is a extrapolation task. Most forecasting methods require time series without NAs, that's why the imputation step is required. Assuming you have a univariate time series (just one attribute observed over time), an R approach could include the packages imputeTS for imputation forecast for forecasting Both packages provide multiple algorithms, you would have to choose the best one for your specific dataset. I will provide you an example for algortihms, which I think perform good in most of the cases: This is the solution for univariate time series. Beware, AMELIA, which was mentioned in another answer does not work for univariate data. If you have multivariate time series (two or more variables observed over time) the solution would look different. Here imputeTS would not work and now the AMELIA or mice or VIM packages would have to be used. 
 I am trying to understand the relationship between R2 and correlation coefficients between different units in the underlying data. Specifically, I have a balanced panel data set, with N different units. For each of my N units, I have T different observations over time. I am interested in exploring the time series variation only, so I demean each of the N units to be mean zero (over time). I then regress all the of units together on indicator/dummy/factor variables for each time period, e.g., i.time in Stata. This regression gives me an R2 related to the commonality of the time series variation. Suppose I instead were to run the matrix of correlations between my individual demeaned units of observation and get a whole bunch of $rho(i,j)$ (where i and j are units in the cross-sectional dimension). I believe there is a relationship between the R2 of my regression and these individual correlations, but I haven't been able to find this derived in the literature and determine whether this is a recognized property of regression, or not. Can anyone help me? If it helps, here is an example of what I'm trying to understand. Think of N as zip codes around a city, T as daily observations over a year, and the unit of measure as the measured temperature. I want to know how similar temperatures are over time across the city, in the sense of whether the changes over time are similar, even if one side of the city is generally 5 degrees warmer than the other. When I demean the temperatures by zip code and run my regression, I unsurprisingly find that the R2 is rather high, because much of the movement over time is similar across zip codes. But I know some zip codes' temperatures are less variable and less correlated with the others [e.g., 10 waterfront zip codes versus 100 inland zip codes]. I am trying to figure out the math of how the presence of these types of zip codes affect the overall R2 of my regression model. (I am aware that the R2 in my model is the squared correlation between the actual and fitted (i.e., average) values. I'm trying to understand how the R2 relates to the individual correlations of the underlying data themselves.) 
 I would set it up as a regression with number of negative words as the dependent variable, post length and election-indicator as independent variables. To get a "percent change" interpretation, and to avoid nonsensical results (e.g. A model allowing for zero length and positive negative word count), take the natural log of the negative word count and the length. The model would be: ln("negative word count") = b0 + b1 * ln(length) + b2 * "Election-Indicator" Where the third term equals 1 if the observation was taken during electionseason season, 0 otherwise. Your null hypothesis is that b2=0, alt hypothesis is b2&gt;0. Check the significance (both statistical and practical) of b2. Interpreting this is easy. Because we took the log of the dependent variable, we can say that a 1 unit change in the election-indicator variable (I.e. If it is election season) roughly corresponds to a b2% increase in negative words, adjusted for length. Because we took the log of length, we can interpret b1 as a 1 % increase in length corresponds with a b1% increase in negative words. 
 When using forward or backward selection in R, for example, backwardselect &lt;- regsubsets(x = df[,-c(1,2)],y = df[,1], nvma = 10, method="backward") backwardselect &lt;- summary(backwardselect) backwardselect$which Then it gives 10 rows with loads of columns specifying True or False for every variable (105 in my example), then I need to compute the models for the selected covariates. Do I have to see the T/F for every variable in the 10th row? Is there an easier way to select the covariates with True in the last row directly in R? Thank you! 
 I have a dataset of six variables that represent respondents' ranking to six different stimuli. Each respondent ranked this stimuli in order of preference from 1 to 6. This means that each row of a data matrix with these 6 variables can't have two of the same values. I also have a variable that represents respondent's gender. I would like to examine whether there are gender differences in terms of their ranking preferences for each of these 6 stimuli (variables). I am not sure if ordinal regression is an appropriate statistical analysis, since a person's response on one of the variables depends on the his/her responses on the other variables. Any suggestions or insights on what could be an appropriate analysis for such rank ordered data is appreciated! 
 I do not know much about statistics but from my primitive research, I would like to explore how to apply Bayesian statistics in A/B testing. The best Bayesian-based A/B split test graphic calculator I have encountered so far calculates the "Apprx. probability of being best", and uses a simulation with jStats to determine 95% confidence intervals. So far, I can calculate the "Apprx. probability of being best" column in the calculator above with for A/B and . I am not sure how to calculate more than 3 variations but I can live with that. How about credible intervals ? possibility without resorting to running a simulation? I found this function that seemingly calculates what I need from Frequentism and Bayesianism III: Confidence, Credibility, and why Frequentism and Science do not Mix , but I have no idea how to use: What are &amp; ? Thanks for bearing with my primitive understanding of Bayesian statistics. 
 I am using an x13-arima-seats based solution to remove seasonality and detect data trends. However, many of the series I need to observe trends on contain zero and negative values, which my solution cannot currently handle. Could anyone recommend other quality programs or algorithms to use? I would prefer something that is implemented in python or interfaces with python, bash or ruby. 
 I am trying to model the relationship between forest age and individual tree mortality rate. The probability of mortality declines rapidly as forests go from being very young, and then creeps back up as they age (Juveniles more likely to die than intermediate aged trees. After this initial decline in mortality probability, mortality creeps back up as a function of age). Here are some example data generated in R, and a plot of the relationship. I build the example data using two independently parameterized exponential functions, and then summing them together. The outcome looks like this: I'd like to fit a function that could capture this non-linear relationship, so that I could compare the parameters of these fits to different sets of data. Specifically, I'm asking: Which equation could fit an asymmetric curve like this using any statistical software? Can you demonstrate it works using the package, or similar, in R? 
 This is described in your link For any set of $N$ values $D=\{x_i\}^N_{i=1}$, an unbiased estimate of the mean $\mu$ of the distribution is (...) Note here that we've assumed $\sigma_x$ is a known quantity; this could also be estimated from the data along with $\mu$, but here we kept things simple for sake of example. $D$ is vector of observations and $\sigma_x$ is standard deviation that is known in advance, or can be estimated. 
 A tutorial on categorical pca (CATPCA) (Linting et al. 2012) explains that a decision to merge categories of an ordinal variable can be made based on the category quantification ("none of the transformation plots show extreme category quantifications, merging categories is not necessary." P.22). The process of determination is not very clear or intuitive to me. How do you determine whether merging categories is warranted based on transformation plots? SPSS CATPCA produces a separate category quantification output but it is unclear how one gleans this info based on the transformation plots specifically. REFERENCE Linting, Mariëlle, and Anita van der Kooij. "Nonlinear principal components analysis with CATPCA: a tutorial." Journal of personality assessment 94.1 (2012): 12-25. 
 I built a Bayesian A/B testing tool - i.e. one which models A and B having posteriors $Beta(\alpha_i, \beta_i)$ where $\alpha_i, \beta_i$ are updated every iteration. After T iterations, I compare the posterior distributions of A and B and determine which one is larger, lets say, using the mean. How can I incorporate a third (or fourth...) group? Originally I wanted to model it as a Dirichlet, giving each group some $\alpha$ that is updated each iteration. And then after T iterations select the group with the largest $\alpha$. But I think that it's more correct to keep each group A, B, C as a separate Beta (since they are theoretically independent) and just compare the posteriors of their respective distributions. It seems naive to me, but also intuitive. Can someone who has done this please opine? 
 I have a question regarding ordinal logit model. My dependent variable, y, is grouped by known thresholds. For example, 1, 2 and 3 define income less than $ 9,999, income between 10000-39,999, and income greater than or equal 40,000, respectively. After I employed Harrell's graphical method (plot.xmean.ordinaly in rms package of R) to validate the proportional odds assumption, I concluded that the proportionality assumption is violated. Here are my questions: Since the y is grouped by known thresholds, should the proportional odds assumption be complied? If the proportional ordinal logit model is not an ideal approach here, can I use the generalized ordinal model instead? 
 Just like we have $R^2$ in linear regression, How do I find out the 'goodness of fit' of a circle to a given data? Is it wrong to use the $R^2$ formula from linear regression, $R^2 \equiv 1 - \frac{SS_{res}}{SS_{tot}}$ for circular regression? If so, why? Here (or here ) is a sample data for which I would like to find out $R^2$ value with respect to a unit circle. 
 This answer might not be complete enough for your taste, but here goes: The SUTSE model comes from the "structural time series" literature, that is using a state-space formulation. My understanding is that basically you take a univariate formulation, for example the observation equation could be $y_t = m_t + e_t$, and then you suddenly turn the scalar variables into k-dim vectors. Since the k-th components of $m_t$ and $e_t$ only enter the k-th component of $y_t$ these equations of the multivariate system are "unrelated". But they are only "seemingly" unrelated because the covariance matrix of $e_t$ can be non-diagonal, connecting the equations at least a little bit. And similarly for the state equation. On how to do this in R I would refer you to: Giovanni Petris and Sonia Petrone (2011), "State Space Models in R", Journal of Statistical Software May 2011, Volume 41, Issue 4. ( https://www.jstatsoft.org/article/view/v041i04/v41i04.pdf ) Especially section 3.3, where they say "Multivariate state space models can be analyzed in R using package dlm and package KFAS." In contrast, SUR is not really a model, it's an estimator, namely an incarnation of generalized least squares (GLS). What it does is take a system of (linear) equations that could be estimated by classical OLS, but then it also allows the (contemporaneous) cross-equation covariance matrix to be non-diagonal and takes this covariance structure into account to estimate the system more efficiently than with OLS. This system of equations to be estimated with SUR can, but doesn't have to, be a dynamic system with lagged terms. The R package "systemfit" and its documentation should be helpful (but to be honest, I haven't used it myself, I'm not doing these things with R). One thing that SUTSE and SUR seem to have in common is that they work with systems where each "dependent" or "endogenous" variable can only appear in one equation, hence the "seemingly unrelated" label. 
 Cross validation 'runs several times' but it only predicts each case one time. In your example of 10-fold cross-validation on 160 cases, each of the 10 runs (folds) leaves out 10% of the cases (lets say cases #1-16) to be tested on while training on the remaining 90% (#17-160 cases). The trained model tests on the 16 cases in the hold-out sample, and then the process is repeated on a new hold-out sample (e.g. cases #17-32). This process is repeated until each case has been predicted one time. The idea is to never use the same case for both the training and testing phase, which can help with problems associated with over-fitting. 
 Hi I am trying to simulate a Poisson regression here with xi, y generated as following: This matrix has been designed to select variables combination to be included in the regression model: I used the following function to fit all 2^5=32 possible regression models: But I'm not sure if I have done it right as I always get the error message Or glm.fit does not converge. Can anybody please let me know what's been right in this example? Thank you! 
 I have two matched data sets, $A$ and $B$, and I know that $B=a\cdot A$, where $a$ is some constant. Is there a test that would say that $A$ and $B$ have basically equal distribution, just shifted? And what if $a$ is not a constant but a small variation? Both $A$ and $B$ have non-normal distributions. I tried using the Wilcoxon signed rank test but I am not sure if this is appropriate in my situation. Please, let me know if my question is not clear I'll try to clarify. 
 Like Moazzem Hossen, I don't have any formal education in programming or statistics, however I do have a bit of experience (I definitely don't consider myself a developer though) in a few programming languages. I have been considering doing the coursera program, but I've also found some reasonably-priced courses on Udemy as well that focus solely on R and are put together well for people who are starting from zero. Personally, I find the way lesson plans are put together really affect how effectively I learn, so a good teacher is important. I'm not affiliated with this guy at all, but I'm taking this course right now and I like his teaching style and have taken his courses on other topics as well. Good luck! 
 I need to estimate the expectation of a hazard function, $E[h(x)]$. For instance, for the exponential distribution the result is equal $\lambda$ $E[h(x)]=\int_0^\infty \! h(x)f(x)\mathrm{d}x=\lambda *\lambda exp(-\lambda x) \, \mathrm{d}x=\lambda.$ But I am not sure how we can generalized this result for other probability distribution functions with positive support. For instance, Burr distribution or the Generalized gamma distribution. I will use this result in the context of autoregressive conditional duration models. Do you know if there exists some general result for this question? Thanks! 
 I have multiple gene expression profiles, measured at 23 time points with one measurement per time point (there's two at the start, but that's irrelevant for the rest of the question). Due to the method used to get the data, the relative estimated technical replicate error decreases with increasing expression, but it is not a straight-line on a log-log plot (log error v log measurement). I can fit a function to the data (generated by repeated measurements of standards) to give an empirical standard deviation of the error as a function of the level of expression. I want to generate bootstrapped samples from my original series, using this knowledge of the error distribution, in order to compare the fits of two (or more) models. Can I generate bootstrapped samples at those same time points by adding samples from the error distribution to a smoothed version of the original series (with the sd of the error equal to the empirical sd evaluated at the smoothed value)? Secondly, the gene expression data shows a sharp induction (an increase of an order of magnitude or two between two adjacent times), and I want to ensure that the bootstrapped samples have as much of the rapid change as possible. Which would be the best method to ensure that the bootstrapped samples have a similar variation so that the peak is not rounded off too much? I have tried using a Gaussian process on the log-transformed gene expression data to generate samples, but a) the noise of the GP is independent of the log-expression level, which is clearly not the case from the standards data and b) with only one measurement per time point, the induction peak is smoothed out, biasing the estimate of the noise to too high a value. 
 I am doing Locality Sensitive Hashing( LSH ) where I hash based on random sign projections. That is each entry is hashed by drawing K planes in $R^d$ space and I hash them to a bit string. For each plane I do $a^tx_i$ and assign a value of 1 if positive and 0 else. From here (section 2.2) I get that the probability of two vectors being hashed to the same bit is: $$1-\cos^{-1}\left({\frac{x^Ty}{||x|| ~||y||}}\right)$$ Lets call this ps for simplicity. From here we can then calculate the probability of two vectors being hashed to the same bin when using $K$ planes and from that we can calculate the probability of getting at least one collision on $H$ repetitions of the hashing. Let $event$ be at least one collision in $H$ hashes using $K$ planes. $$P(event) = 1-(1-ps^K)^H \tag{1}$$ Assuming x and y are scaled to have mean 0 and sd = 1 then $\frac{x^Ty}{||x|| ~||y||} = \operatorname{cor}(x,y)=\rho$. So now given some known $K$ and $H$, we can calculate the probability that two arbitrary vectors with correlation = $\rho$ have the probability (1) of colliding. For example, with K=6 and H=8, two vectors with correlation 0.8 have P(event) = 0.903. What I am struggling to calculate is instead of calculating the probability based off of $\rho = x$ I want to calculate the collision probability given $\rho \geq x$. That is the probability of a collision given x and y have a greater than or equal correlation than x. 
 Consider a problem, when one is interested in finding influence of variable set {X} on Y (assuming reverse causal relation is infeasible), when confounding influences from a set of variables {Z} are present. I am familiar with two approches to disentangle effect of {Z} on Y from effect of {X}: Partial correlation / residualization in a "fixed" effect model, when the number of covariates $p_Z =$ |{Z}| is small compared to the number of available samples, $p_Z \ll N$, as by fitting multiple regressions one removes degrees of freedom, and given linearly independent factors Z, the degrees of freedom will be depleted as $p_Z$ approaches $N$. Expressing correlation structure of confounding effects as "random" effect, which requires $p_Z &gt; N$ to estimate covariance matrix. Question: What are alternative approaches? Are there approaches that work for $p_Z \approx N$ or all mentioned regimes? Have sparse regression models been applied to this problem? 
 If so then which invariant these hold? In computer vision there are requirement for a good feature to be invariant with respect to following changes: Illumination, rotation, scaling, view point, to support intra-class variability of the figure in the image. Some of "them" have been provided by feature itself, some of this properties have been given by feature descriptor itself. 
 I have a data which which have 17 variables i.e. 17-Dimension data. The Data is a result of Max-Diff exercise which is performed for ranking these 17 attributes and have comparative preference/importance and probability of choice of each attribute. So in summary this data is of around 2000 rows and 17 columns values ranging between 0 and 1. Now I want to create meaningful market segments using this data. Just wondering if anyone can help me in deciding best clustering technique for completing this task. Any suggestion will be highly appreciated. Thanks in advance. 
 I have a data that looks like below and I want to cluster them. What would be the best algorithm to apply for clustering such data. Thanks 
 I'd expect that a sufficiently large convnet could compute any function of the input pixels. Here's an informal "proof": A convolutional layer can pass through all the information in the previous layer, e.g. by using the identity function. Once the information from the original image has been propagated through the convolutional layers to a set of fully-connected layers, we're left with a generic multilayer perceptron, and we can use existing proofs to take us the rest of the way. The convergence guarantees are probably quite weak, for the same reason that they are difficult in the case of other neural networks. I don't see why they should be any weaker than the general case, though. 
 I am studying Quantile Regressions (Koenker, R. and Bassett, G. (1978)). But i cant find a work that uses Quantile Regressions to estimate Conditional Skewness. There is some work/article who did that? Thanks 
 I am trying to find a clustering solution with the help of package in R. The following code has been adapted from the vignette for the package: Every time I change the seed, the output changes. I am evaluating different outputs based on lowest , lowest and minimum . My understanding is is within a given cluster. I tried to find the definition of in the documentation, but couldn't find it. My questions are: How do I set the seed that will give me the best/optimal(?) solution? Are there any general good practices for initializing a seed value? Is my understanding of correct? Thank you! 
 I have frequently been faced with analyzing csv files that are limited in length to approximately 32,000 records in length (overflows at 32,768, from signed integer limits on a 16 bit wide data bus). As a result, I have often had data that spans several hundred files, each containing up to the 32,000 records. Even this 32,000 record file length is variable and random, owing to an automated data collection process whose collection will halt if the process being monitored should stop. Here is the issue -- I can very easily open these two-hundred plus files completely algorithmically and sequentially determine how many data points they contain and some desired summary statitistics. The typical statitistics of interest are the sample size, mean, and standard deviation. Armed with these two-hundred-some-odd summary stats, I wish to extrapolate the mean and standard deviation of the entire sample population collected (up to 200+ times 32,000max). It seems to me that some moments about a mean are somehow involved in that the standard deviation of a hypothetical subset of say 16,000 data points will carry less weight (and therefore have less influence on determining the overall standard deviation), than a data set of 32,000 for example. For that reason, I suspect that variances of each subset somehow play a roll in this, or that a root-mean-square procedure will be able to reconstitute these summary statitistics in a meaningful way to further weight them in deriving the mean and standard deviation of the entire sample set. Your assistance is appreciated. 
 Yes, your interpretation of the coefficients is correct. This gets a bit more tricky / abstract. Yes, you can compare estimated coefficients. Question 2: where do standard errors come from? Refresher: Recall some standard OLS formulas. Estimate of coefficients $\mathbf{b}$: $$ \mathbf{b} = (X'X)^{-1}(X'\mathbf{y}) $$ Regular OLS estimate of the variance of the estimate (i.e. estimate of $Var(\mathbf{b})$): $$ V_{\mathrm{ols}} = (X'X)^{-1} s^2 \quad \quad s^2 = \frac{\boldsymbol{e}'\boldsymbol{e}}{n-k}$$ Regular OLS estimate of the variance of the estimate (i.e. estimate of $Var(\mathbf{b})$): $$ V_{\mathrm{robust}} = (X'X)^{-1} \left(X'\Omega X \right) (X'X)^{-1}\frac{n}{n-k} \quad \quad \Omega = \mathrm{diag}\left(u^2_1, u^2_2, \ldots, u^2_n \right)$$ Special case of indicators: example Imagine we have: 5 observations 2 categories Also for simplicity, let's construct a data matrix with exclusive indicators rather than a constant + a single indicator. (For simplicity, I want the columns of X orthogonal.) Eg. our data matrix $X$ might be: $$ X = \left[ \begin{array}{cc} 1 &amp; 0 \\ 1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 1 \\ 0 &amp; 1 \end{array}\right]$$ That is, the first two observations are category A and 2nd three observations are category B. Hence: $$ X'X = \left[ \begin{array}{cc} 2 &amp; 0 \\ 0 &amp; 3 \end{array} \right] \quad \quad (X'X)^{-1} = \left[ \begin{array}{cc} \tfrac{1}{2} &amp; 0 \\ 0 &amp; \tfrac{1}{3} \end{array} \right]$$ $X'X$ is simply the counts of variables in each category! $X'\mathbf{y}$ sums up observations by category and multiplying by $(X'X)^{-1}$ divides by the counts. $\mathbf{b} = (X'X)^{-1}X'\mathbf{y}$ in this case is simply the category by category mean (i.e. this is your Q1). The regular OLS, homoskedastic standard errors would be $$V_{\mathrm{ols}} = (X'X)^{-1} s^2 = \left[ \begin{array}{cc} \tfrac{s^2}{2} &amp; 0 \\ 0 &amp; \tfrac{s^2}{3} \end{array} \right]$$ That is, it's the estimate of the variance of the residuals divided by the number of observations (we're more certain for categories where we have more observations). The robust covariance matrix is a bit trickier, but in some sense, easier to interpret! Let $u = [u_1, u_2, u_3, u_4, u_5]'$ be a vector of residuals. The robust estimator turns into (dropping the finite sample degrees of freedom correction $\frac{n}{n-k}$ to keep it simpler): $$\begin{align*} V_{\mathrm{robust}} &amp;= \left[ \begin{array}{cc} \tfrac{1}{2} &amp; 0 \\ 0 &amp; \tfrac{1}{3} \end{array} \right] \left[ \begin{array}{cc} u_1^2 + u_2^2 &amp; 0 \\ 0 &amp; u_3^2 + u_4^2 + u_5^2 \end{array} \right] \left[ \begin{array}{cc} \tfrac{1}{2} &amp; 0 \\ 0 &amp; \tfrac{1}{3} \end{array} \right] \\ &amp;= \left[ \begin{array}{cc} \tfrac{1}{2} \left( \frac{u_1^2 + u_2^2 }{2} \right) &amp; 0 \\ 0 &amp; \tfrac{1}{3}\left( \frac{u_4^2 + u_5^2 + u_6^2}{3} \right) \end{array} \right] \end{align*}$$ That is, it's an estimate of the variance for each category estimated separately , divided by the number of observations of that category! Basically, (1) OLS with indicators and robust standard errors in a single regression is equivalent to (2) calculating the mean and standard error for each category separately! Question 3: Most any statistical package will have a way to output matrix $V$ described earlier. Let's say you're interested in some $\mathbf{a}' \mathbf{b}$. Asymptotically, we'll have: $$ \mathbf{a}' \mathbf{b} \sim \mathcal{N}\left(\mathbf{a}' \boldsymbol{\beta}, \mathbf{a}'V\mathbf{a} \right) $$ Example: let's say you're interested in $b_1$ - $b_3$. The standard error would be given by: $$ \sqrt{\left[\begin{array}{ccc}1 &amp; 0 &amp; -1 \end{array} \right] V \left[\begin{array}{c}1 \\ 0 \\ -1 \end{array} \right]} $$ 
 Not sure if this question is still relevant to you or not, but luckily, I think there's a simple solution which involves the use of a technique called panel regression which is used when the question involves modeling data for various products across time. It takes into account the time variant effects (things that change over time, such as trends and seasonality, price, etc.), and at the same time, it takes care of time independent effects (things not influenced by the passing of time, such as different SKUs). Do you have experience with statistical modeling techniques? If so, I would first build a table with your data that looks something like this (we'll keep it simple to start): Column names: SKU#, time, sales, price, quality There's likely other variables influencing your toaster sales, so you'll want to build in variables which control for those factors too, eventually. Time should be something like a trend or something countable which increments up by 1 each period (if you think there's a linear trend to your sales growth). If it's something related to seasons, try adding monthly average weather to the model. I'm not sure what statistical software your company uses, but mine uses STATA. An application most analysts I've encountered have never heard of, unfortunately. Before you run your model, you just have to set what your panels are (time independent variable). In your case, it would be SKU#. But when we run our model used to answer a question very similar to this (predict sales), based on time influenced factors, and other factors unaffected by time, the output (coefficient) for our price variable, which has been cleansed of any influence from other factors included in the model, would state something like: "For every 1 dollar change in price, sales increase by x dollars"; likewise for the quality variable. Essentially, what this model does is this case is produce clean estimates of the variables included in the model, while controlling for the effect of the product (think of it as controlling for the naturally high/low baseline sales levels of each of the SKUs in your data so that you don't end up with messy results in the case that a certain popular SKU has really high sales, but is around the same price as another less popular brand (with equal quality)). We're also doing something similar at my company, which I find very exciting/interesting. Hope this helps. Cheers. 
 Like the title states, I'm interested in learning about the more popular data transformation techniques. I know the internet is abound in this information, but I'd like to hear from those working daily in the data trenches. Beyond not transforming your data, I'm aware of the log/ln transformation which I think is used to normalize skewed data. But even then, why are doing this? and how do we interpret our results? Do we transform them back to non log values? I'm just looking for general knowledge on some of the more popular transformations (and applications) in case I'm working with data, but not aware that it should be transformed or how/why. If this information is already compiled on the web in a neat (not too wordy) website, feel free to share. Or any texts for that matter. I'd say I have an intermediate-minus level of understanding of statistics. Thanks, 
 Where $h_{\theta} = \theta_{0} + \theta_{1}x$, I am trying to minimize $J(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}$ I first transform every sample in the feature (there is only one feature) using: $X_{\alpha} = \frac{X - X_{min}}{X_{max} - X_{min}}$ Then, I run batch gradient descent (I'm implementing regression from an ML standpoint (if I can make the notion of regression approached from a field)), and these are the values of $\theta$ I get: $\theta = [5.670, 2.301] = [\theta_{0}, \theta{1}]$ $X_{min} = 5.0269$ $X_{max} = 22.203$ To test these parameters, when I go to plot, I simply plot $(X, h_{\theta}(X_{\alpha}))$: That is, I take any value $x$, transform it to $x_{a}$, then finds its output $y$ using $h_{\theta}$ and plot $(x,y)$. This does not give the line of best of fit, however. 
 Let's say I have a model that I want to retrieve cross-fold validated probability predictions from. I can divide my data set into n folds, leave one fold out, get probability predictions for each observation in that test. That is, for each fold: SVC.fit(trainFoldsX,trainFoldsY) SVC.predict_probab(testFoldX) The issue is that (obviously) each of my observations will have a single probability prediction based on the left-out fold, and I'm not sure that's appropriate. How do I get cross-fold validated probability predictions from a model? 
 My econometrics professor recently introduced us to the J-test. To perform the J-test, he gave us the following procedure: Estimate $\mathbf{b}^{2SLS}$ and using this, define $\hat{U}_n$ residuals for all $n$ observations in the random sample. Regress $\hat{U}_n$ on $\mathbf{W}_n$ (exogenous variables) and $\mathbf{Z}_n$ (instrumental variables). Define null hypothesis as the instrumental variables are exogenous (not sure how to phrase this mathematically). Compute $J \equiv mF$ where $F$ is the F-statistic corresponding to this test. $J$ is then the J-statistic. My Question Why is $J\sim \chi^2_{m-k}$? I thought $F$ had an $F$ distribution, which itself is the ratio of two $\chi^2$ distributions. Why does multiplying the $F$-statistic by a constant suddenly change it into a completely different distribution? 
 My query is for an assignment where the first question is to determine if the rate of failure on mechanical equipment is different after a change in manufacturer. In order to answer this I have done a likelihood ratio test in R using the -2 log L values and a Chi square distribution. The follow up question is to then provide the likelihood that one of these pieces of equipment made by the new manufacturer fails in a certain number of hours or less using only the data from that manufacturer. I had thought this easy enough and calculated the MLE for the rate but the rate estimated is different from the rate estimated using the full data set (with 2 different rates) and I am completely unsure why? Is this going to be a mistake in my code or a misunderstanding on my part of the background statistics? 
 It's not multiplying by a constant that does it, it's just looking at the asymptotics as your number of observations goes to infinity! Wave your magical asymptotics wand and poof , the $F$ distribution becomes $\chi^2$. The $F$ stat is a ratio of two normalized $\chi^2$ distributions: $$ F_{d_1, d_2} = \frac{\chi^2_{d_1} / d_1}{\chi^2_{d_2} / d_2}$$ In the context of regression, $d_2$ for the classic $F$ stat is $n - k$ (the degrees of freedom of the residuals). As $n$ goes to infinity, the degrees of freedom $d_2$ goes to infinity , and the $F$ distribution converges to a scaled $\chi^2$ distribution. Loose intuition: $\lim_{n \rightarrow \infty} \frac{\chi^2_n}{n} = 1$ and dividing by 1 doesn't do anything. Additional note: A similar thing happens with t-distribution and the normal distribution. T-distribution with infinite degrees of freedom is a normal distribution. You'll sometimes see z-score instead of t-stats etc... 
 Please give a clear explanation of what your problem is, what the data is, and what you want to learn from the data. We need this information to help you determine the best clustering algorithm! A general baseline algorithms is the K-means algorithm (you can find it as a python function in the sci-kit learn package): ) Here, you specify the number of clusters. The algorithm performs 2 steps, iteratively: Assign each point to the closest cluster (measured by the distance to the cluster mean) Recalculate the cluster mean Repeat 1 and 2 until there's no other need. 
 I found this quite confusing from a Udacity course and also got a problem open the course forum. On the following figure, the red curve represented the sample distribution before intervention. After an intervention to every individual in the population, the sample distribution was denoted as the blue curve(area). The sampling sizes were same, but should there be some differences between the shapes of distribution? After all, the population changed, isn't it? Thx for your answers！ 
 I recently came across a blog post ( https://cssanalytics.wordpress.com/2014/02/12/probabilistic-momentum-spreadsheet/ ) that discusses calculating a t-stat as part of their model. However, the formula they use I can't seem to reconcile with the way I think a two sample t-test is calculated. Since I'm not not sure how to do the formatting for math symbols I'll type it in as the authors of the blog post have it in their excel sheet. (the link to the excel sheet is found in the post linked above) I wouldn't expect the author to be wrong. How does this work as a t-stat? 
 Sample mean: $$ \mu = \frac{1}{n} \sum_i x_i $$ Sample variance: $$ \sigma^2 = \frac{1}{n-1} \sum_i (x_i - \mu)^2$$ Sample standard deviation: $$ \sigma = \sqrt{\frac{1}{n-1} \sum_i (x_i - \mu)^2}$$ Estimated variance of the sample mean: $$ v = \frac{\sigma^2}{n}$$ Standard error (i.e. estimated standard deviation of the sample mean) $$ se = \sqrt{v} $$ t stat: $$ \begin{align*} t &amp;= \frac{\mu}{se} \\ &amp; = \frac{\mu}{\sqrt{\sigma^2/ n}} \\ &amp; = \frac{\mu}{\sigma / \sqrt{n}} \\ &amp; = \frac{\mu}{\sigma} \sqrt{n} \end{align*}$$ Which is what they have: 
 Depending on what you need there are two standard problems that this can be considered as, and you might also have a combination of those problems (as discussed at items 3 and 4 below): You've already processed a bunch of files and have for each $\bar{x}_i$, $s_i=\sqrt{V_i}$ and $n_i$ and wish to obtain an overall $\bar{x}$, $s$ and $n$ as if the data had been all in one file (I'll talk hereafter about variances but you can take those back to sds whenever you need them): This problem is that of combining grouped means and variances (and $n$s) into overall statistics. It's dealt with in many posts here. For example, variance formulas for both equal sample sizes and the unequal-sample-size case are given here , of which the (more general) unequal-$n$ case (after putting $n_j$ for $k_j$) is: $Var(X_1,\ldots,X_{n}) = \frac{1}{n-1}\left(\sum_{j=1}^g (n_j-1) V_j + \sum_{j=1}^g n_j (\bar{X}_j - \bar{X})^2\right),$ where $V_i$ is the sample variance of the $i$-th group and $\bar{X}_i$ is the sample mean of the $i$-th group and the overall mean $\bar{X}=\frac{1}{n}\sum_i n_i \bar{X}_i$, where $n=\sum_i n_i$. A derivation is given here (though the notation differs it's not hard to relate the terms) If you need the n-denominator version of variance, that's covered here [I think from the sound of it this case probably covers your needs, but for the sake of completeness, for later readers with a slightly different version of your problem I want to get to case 3 below] You haven't processed any files yet but wish to loop through them one at a time, "online" and have at the end an overall $\bar{x}$, $s$ and $n$ as if the data had been all in one file (though the standard deviation or variance is the only one that may not be obvious). "Online updating" is discussed in a number of posts on site, including this one . This is essentially processing the observations one at a time across all the files, by updating mean and variance every observation. Note the discussion there about saving a few calculation steps if you only need the overall variance at the very end. There's potentially a third problem if you've processed some files in the past (like 1. above) and now wish to combine those summaries and then go on to process additional files that had not been done yet. Those two approaches above can be combined, processing the first part as in 1. but before dividing by $n-1$ you have the SSE that is to be updated in 2. and then updating the rest by going through the remaining files an observation at a time via the approach in 2. You can also adapt an online-updating approach to the situation in 1, where you update the current values by the $i$-th group statistics; I won't detail it unless you need it, but it's easy enough to infer the way it works from the above information. I think this should cover the main situations that could matter on this kind of problem. 
 They are doing a paired sample t test. If you collect your data in pairs $(X_1,Y_1),(X_2,Y_2),\dots,(X_n,Y_n)$ and then you can consider the RVs $d_i=X_i-Y_i,$ $i=1,\dots,n$ . Then, $d_i\sim N(\mu_d=\mu_X-\mu_Y, \sigma^2_d)$, then you have the test stat $$t=\frac{\bar{d}-\mu_0}{s_d/\sqrt{n}}$$ A reference can be found here There is something that is bugging me. Are these data collected over time? If so, this method is completely inappropriate. 
 Time 0 to less than 4 4 to less than 8 8 to less than 12 12 to less than 16 16 to less than 20 20 to less than 24 of students (f) 1 7 15 18 6 3 A; find mean, mode, median, variance, standard deviation, quartile ( q1 &amp;q3), inter quartile range, quartile deviation (SIR) , coeffitient of variations (CV), 90th percentile and 10th percentile (percentile range) B; draw box &amp; wihsker. 
 You are correct in saying that setting the seed is good for creating reproducible results. This is often useful for code debugging or for HW assignments where the teacher wants to make it easier to grade the assignment. However there is no way to 'optimally' set the seed. All setting the seed does is give the random number generator (RNG) a place to start. However the RNG is programmed to avoid 'non-random' output. By choosing a specific seed (or set of seeds) you can create bias in your results. My advice would be to let the random number generator do it's job. In this case the random numbers are being accessed in the kcca() function when it chooses an k initial starting center points for the k-means algorithm. By setting the seed you are essentially telling the k-means algorithm to use the same starting centers each times it runs. The k-means algorithm itself is not-guaranteed to converge to the global optimum so different starting points for the algorithm can lead to different results. 
 I've just had to solve this exact problem. We had a 9 point scale that was changed to a 5 point scale on a tracker going back 10 years. Not only that but some of the statements changed as well. And we were reporting as a form of Net Promoter Score. The solution we used apply's a paired design by asking each respondent a few of the old statements the old way (as well as all the new way). We only asked a couple the old way rather than all of them since this minimises respondent fatigue. We then take each score on the 9 point scale and find it's average on the 5 point score and use this to correct for the scale change AND the statement change. This is quite similar to what is called the "Semantic Judgement of Fixed Word Value" in some papers, but instead of using experts to decide the 'word value' we used respondents actual data. For example, if the average score on the 5 point scale was 1.2 for those respondents who answered 2 on the 9 point scale then to let us directly compare years with different scales on the 5 point scale we would replace all 2's on the 9 point scale with 1.2, then do the same for all the 9 point scores, and proceed as normal. We did a similar thing for reporting NPS. But first we converted the 5 point scale to the NPS scale of 1 (promoter), 0 (passive), -1 (detractor) e.g. if the average on the NPS scale was 0.9 for a 2 on the 9 point scale then we replaced it with 0.9, then do the same for all the 9 point scores, and then calculated NPS normally. To evaluate the effectiveness of this we first compared the 'uncorrected' NPS scores using the 9 and 5 point scales to see if there was actually any problem at all, and then the 'corrected' ones. I haven't got the data yet but will report back when we do! 
 Suppose we have a sample with $n$ discrete categories; e.g., in R: I wonder how to statistically test which category is statistically over-represented in a sample? 
 From the plot of the data it looks like the variable on y axis has only 1 value = 6. If this is the case, then you could use k-means clustering over only the x-axis variable. The reason for this is that the clustering algorithm first creates a distance matrix of each data-point w.r.t. the others. Although various distance metrics are used commonly, for this discussion lets assume you'll be using the Euclidean distance. Since the y-axis does not change, the distance metric would not change whether you include y-axis or not. Hence, there is no point in including it in the clustering. However, if the y-axis variable does vary but the variations are too small to be represented on the plot, then you should scale both x-axis and y-axis variables between 0 to 1 before clustering on them. That would allow you to use the y-axis variable to make clusters. 
 I used the command and got a table in the results window. I want this table in Excel or .dta format. Right now I'm copying pasting this in Excel and then manually separating the data into different cells, which is extremely time- consuming. Is there a easier way? 
 Say I have the following distrubution. And say I now recognise a new element/event which I know to have a probability of 0.4 How should I introduce this element into the existing distribution, and properly update all the other values? I am a complete amateur at statistics and don't even have the stats vocabulary to describe this type of thing to google. 
 I have two sets (“sorts”) $A$ and $B$, and a multiset $R$ whose underlying set is a subset of $A \times B$. I need a clustering algorithm, where each generated cluster consists of two subsets $S \subset A$ and $T \subset B$, and, given $x \in S$ and $y \in T$, then $(x,y)$ is likely to occur “very frequently” in $R$. As usual with clustering algorithms, clusters should be disjoint: given distinct clusters $(S,T)$ and $(S',T')$, then $S \cap S' = T \cap T' = \emptyset$. Any recommendations? 
 I'm attempting to extract t-values from a series of published papers for a meta-analysis. The specific measure of interest is the difference in performance (proportion correct on a memory test) between a Treatment Group and Control Group. Some papers report the results of a t-test between the Treatment and Control groups--exactly what I need. Others however, do not. Instead, those papers only provide Means, SDs, and n. When the experiment is between subjects, that is sufficient for me to calculate the t-statistic. However, when the experiment involves a within-subjects design--although I have Means, SDs, and n for the Treatment and Control conditions, I do not have the SD of the difference scores for each subject (Treatment minus Control). Is there a way that I can estimate what the results of a t-test would be in that case? Further detail: in some of these within-subjects studies, each subject went through a Treatment, Control, and an Alternate Treatment condition. An ANOVA reporting a main effect of condition (3 levels) is reported. Thanks in advance for any assistance! 
 When estimating intractable expectations using MCMC, we usually assume we can evaluate the (unnormalized) target density exactly at any point. I.e. we're interested in evaluating expectations w.r.t. some distribution $\pi$, where $\pi(x) = \frac{1}{\mathcal{Z}} q(x)$, and we can query $q$ exactly and $\mathcal{Z}$ is not needed. What if we can only get noisy estimates of $q(x)$, and what if these estimates have unknown bias? Can we say anything about the correctness of the resulting algorithm, or the bias of the resulting chain? This arises if we would like to apply MCMC to problems where our target is itself an intractable expectation we can only estimate using MCMC "in the inner loop," i.e. when $$q(x) \equiv \mathbb{E}_{z \sim p}[f(z)] \approx \frac{1}{N}\sum_{t=1}^N f(z_t)$$ where $\{z_t\}$ are simulated from a Markov chain whose stationary distribution is $p$. For finite $N$, our "inner loop" estimates will be biased due to our choice of Markov kernel and initial distribution. This is a bit different from what I've seen so far: The pseudo-marginal MCMC examples I've encountered assume you have access to unbiased estimates of $q(x)$ In the "big data" setting, the likelihood function involves a large number of independent terms, so minibatch estimates of the full likelihood are unbiased. E.g. the references here: MCMC sampling with noisy likelihood 
 I have been asked what violates the assumptions of regression analysis, but I don't know how to answer! They said it was either "The error term does not have the normal distribution", "The error term is uncorrelated with an explanatory variable", "The error term has a constant variance", or "The error term has a zero mean". I don't know... can you also explain the answer so I can understand it better? Thanks! 
 I have a panel of countries (N= 30) over four decades (T= 40). I would like to test the impact of a policy by analyzing whether it led to a change in the trend in the outcome variable. Note: The policy was implemented at different times in different countries. I am currently running the following regression but fear it is the wrong one as it is not looking at breaks within each country: where is the outcome of interest in country at time ; is 1 for all years policy is in force and 0 otherwise; is a linear time trend; and is the error term. I have the following questions: 1) Is there a way to test for a single trend break in a panel data, or should I be estimating the above regression for each country separately? 2) If I am to exploit the panel structure of the data, should I also include year FE? 3) I am also concerned that since my N is small and T is large the regular panel data asymptotics may not apply. Am I correct in thinking this way? Please provide any other advice you may have. 
 Yes. You can use (available from SSC) to export your results to .xls (and other formats). Some people prefer to use (also available from SSC) and you can try whichever one works for you. Once you have installed the commands you can read the help files ( installs a family of commands) or for the exact syntax and more details. If you are looking to post the data to a Stata file, please see . 
 This question is not precise but I think the answer you are looking for is the zero conditional mean assumption that says that . Note: you need strict exogeneity to show that OLS is unbiased (a small sample property) but for consistency (a large sample property) the weaker assumptions of zero mean of the error term and errors being uncorrelated with each regressor are sufficient. Obviously this is not the only assumption: for instance, you also need the full rank assumption for consistency but since you question is very generic, I am unable to provide a complete answer. You might find it useful to refer to an introductory econometrics textbook like Wooldridge, J. (2015). Introductory econometrics: A modern approach. Nelson Education. 
 As @amoeba said, it is non-linear in the combination $\mathbf \prod_{i=1}^{n} w_{i}$. Let's see an example where each $w_{i}$ is doubled. Then the new value becomes $2^{n}$ times the old value whereas it should have been just 2 times the old value for a linear function. 
 If you mean a pooled OLS regression by "simple regression" then I am afraid, generally speaking, it is not possible to make the standard errors the same for the two models. The fixed effects estimation method exploits within individual/unit variation and in fact even the point estimates for the two models should be different. 
 The same way you would decide on when to stop for any ML algorithm. What you want is good generalizability . Commonsense dictates that you keep on measuring performance on a held-out dataset. You want the training loss to be small and close to the loss on the held out dataset so you stop when the training loss keeps falling but the loss on held out dataset stagnates. 
 You could consider the coefficient of variation which is defined as the ratio of the standard deviation to the mean. Reference: https://en.wikipedia.org/wiki/Coefficient_of_variation 
 I have an one dimensional toy array X. I want to cluster the data into some numbers of clusters.But when I try to fit my data in scikit-learn K-Means function it shows ValueError: n_samples=1 should be &gt;= n_clusters=3 I think K-Means should work fine for 1-D data though it is not an efficient way to cluster 1-D data.So why it is not working?Are there some other scikit-learn implemented algorithms for clustering of 1-Dimensional data? 
 I'm planning an experiment (described below), and I want to be sure that the experimental design is correct, and I'll have no trouble later on when analyzing the data. I want to know if the temperature has any effect on lipid concentration of fish flesh. I'm planning to test two temperature treatments, and I'm going to set three tanks for each temperature, and I'm going to keep 10 fish and I'm going to sample three fish in each tank (only three for budget reasons). There is going to be just one measurement for each fish at the end of the trial, no time involvement here. My question may be similar to a previous one , but because the experimental design wasn't clearly explained in that question, it's not clear which answer is the appropriate one. Also this may be similar to this other one , but there is no answer. Also, this may be similar to this other one, but there is no answer. I also have read other questions very similar where the factor tank was considered random, but no mention of the subjects within them. I understand that my subjects (fish) are nested within tanks, but I'm not sure if the tank factor is nested within temperature treatment. I also think that tank and subject are random. Using nlme syntax, I think that my model should be like this: But how can I take into account the fact that tank is nested within temperature (in case it is)? In summary, I need to know if tank is nested within temperature if I'm right considering fish as random factor nested within tank, and the syntax to run my model with (or ) taking into account the two points above. Below is the expected structure of the data. Any help will be appreciated. 
 I have a data table that looks like this: Then I converted the table into: My problem is that I have hundreds of thousands of rows (users) and using histogram or pie chart to summarise or explore the data is out of the question because of the large number of observations. I'm trying to understand the areas allocated to each user. Can anyone suggest other ways for me to explore this kind of data? 
 You can use insights from panel data and calculate within (over time) and between (across individuals) standard deviation for the area (I am assuming it is a numeric variable). If you are using Stata, you would first generate a pseudo variable for time-period, then the data and finally use : 
 Is difference-in-difference based on group's internal trait -- socioeconomic status in this case -- appropriate? I'd like to measure the difference in how people behave after a certain shock based on their SES. I have the data on both groups' behavior before the event and also after the event. But all the DID guides deal with cases that measure the effect of an external event -- such as a policy change. In my case, I'm trying to measure how an "internal" trait -- the socioeconomic status -- affects people's reaction to an external change. 
 I guess that your user identifiers are not inherently informative; even if they are, the problem as you say is that you can't show them visually any way. A common set-up in ecology is to count the numbers of individuals of several species and then further to reduce to the numbers of species with so many individuals. Your data reduce further to and I would look at that distribution. I've clearly no idea how your data behave beyond your example, but log scale for one or both variables might help if either quantity extends over two or more orders of magnitude. 
 You would expect equivalence when T = 2. If you have more than 2 years, use the latter approach (the one that relies on fixed effects). Please see for more. Note: is redundant in the newer versions of Stata. 
 I wanted to know, for my intuition, why the name 'kernel' is used for kernel density estimation. I am also curious for the 'kernels' in support vector machine if they are referring to the same name. I always believe that understanding the name can aid in understanding the concept it is trying to give. Any insights welcome. 
 As I understand it, you are still measuring the impact of a policy of an outcome variable. The additional terminology of "internal" vs "external" event/trait is a distraction. If you event is exogenous, you can compare the outcomes in one treatment SES with a control SES. With a DID, you will have to show us that the parallel trends assumption is not violated and that there is nothing else going on at the time the event happened. Please refer to for more. 
 Not really an answer, but I would like to mention that there are not only these two acceptions. For example, have a look at the kernel page on wikipedia . The term is quite appreciated in mathematics, and can mean so many things. Some of them are closely related: For example, heat kernel and kernel-gram matrices of machine learning. But, some others seem not. For example, is there any link between the kernel of a linear map in linear algebra, i.e. between $ker f = \{v \in V|f(v) = 0\}$ and gram matrices in machine learning? It would be interesting if someone mathematically knowledgeable could classify all the acceptions of the wikipedia page (which acceptions are similar and which are not). 
 While there are plenty of discussion of the difference between statistics and stochastic, I didn't find a nice explanation for the difference between statistical and stochastic models. Could anyone try to elaborate more on these two terms? Statistical model Stochastic model 
 Other (non-experimental) approaches are instrumental variables, regression discontinuity and propensity score matching, to name a few. If you can, you should conduct a randomized control trial to estimate the impact of X on Y. You can find a discussion of these approaches in: Angrist, J. D., &amp; Pischke, J. S. (2008). Mostly harmless econometrics: An empiricist's companion. Princeton university press. 
 I'm trying to get the same linear SVM classifier model by using Scikit-Learn's , and classes. I managed to do so (see the code below), but only by manually tweaking the hyperparameter for the class. Both and have the regularization hyperparameter , but the has the regularization hyperparameter . The documentation says that , so I set , but when I use this value, the ends up being a very different model than the and models. If I manually tweak the value of , I can get all models to be approximately the same, but there should be a simple equation to find given . What is it? This code outputs: Note: to make the class output the same result as the class, you have to center the inputs (eg. using the ) since it regularizes the bias term (weird). You also need to set since the default is (weird again). So my question is: how does really relate to in Scikit-Learn? Looking at the equations, the documentation should be right, but in practice it is not. What's going on? 
 Your question is a little broad so I will try to write briefly some of the assumptions statisticians make about the variables used in the analysis. If you need explanation of a particular assumptions, look up CV, and if useful thread not found, post a new question. Consider the regression equation $\hat y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_px_p + \epsilon_i$ and below are the common OLS regression assumptions: Linearity: relationship between dependent and independent variables is linear in nature. You should see from scatter plot of DV vs IV whether the relationship is linear. Various transformations help achieve linearity in case of non-linear relationship. Normality: the variables as well as the unexplained error term, $\epsilon$, are normally distributed (bell shaped). It should be clear from histograms of variables and of error terms (residuals) whether normality assumption holds. A normal probability plot or normal quantile plot of the residuals can be used to check if the distribution of $\epsilon$ is normal. More about normality . Statistical independence of the errors: the error terms, $\epsilon, $ are not correlated with independent variables. Also, there is no correlation between consecutive errors themselves in the case of time series data. Error term has zero-mean: the mean of errors is $0$, i.e. , $E[\epsilon] =0$ Homoscedasticity (constant variance) of the errors: the error term does not vary over time. A plot of residuals versus predicted values should indicate presence of constant variance. See more about homoscedasticity 
 1) Exogenous controls means that . In the context of an IV regression it means that the exclusion restriction is satisfied or . To quote Wooldridge, "In the context of omitted variables, instrument exogeneity means that z should have no partial effect on y (after x and omitted variables have been controlled for), and z should be uncorrelated with the omitted variables." 2) Adding (relevant) controls is always the efficient thing to do because you don't want to be throwing away information. If you have more than one instrument, the linear combination of the instruments is the one that is going to be most "relevant". As for testing whether the instrument is valid, one can do a couple of things (like a test for over-identification) but it doesn't completely guard against all criticism, especially the one you point out. Whether an instrument is good/valid, comes from theory and it is possible even within the social science to find such instruments. For examples and further discussion please refer to Wooldridge, J. (2015). Introductory econometrics: A modern approach. Nelson Education and Angrist, J. D., &amp; Pischke, J. S. (2008). Mostly harmless econometrics: An empiricist's companion. Princeton university press 
 I'm running a linear regression in R. If i have an independent variable with only values 0 (for Male) and 1 (for Male), do i need to convert them to factor or character? What is going to be the impact on my analysis? 
 A factor variable with levels, are represented as binary variables. Thus if this categorical variable is already 0-1 binary, then there is no need to code it as factor variable. The only subtle issue, is the meaning of 0 and 1. Consider the following example: Without coding it into a factor, we have a model matrix: Now, if we code it into a factor, there are two ways of coding: R will drop the first level for contrasting, so the model matrix generated are slightly different: Representations are equivalent, but interpretation of the resulting coefficients will be different. 
 You need to turn the categorical variables into for the regression to deal with them as such. can be used to check the way R thinks about the base level, against which other levels are compared. can be used to change the base level if needed. 
 I am speculating: it sounds like you are trying to cluster the transpose of your array. so it thinks there is a single - very high dimensional sample. Try to see if that is the case and if so, transpose it. 
 If communality = 1, then we have a Heywood case, and if a communality &gt; 1, it is known as an ultra-Heywood case. I read in a SAS manual that an ultra-Heywood case renders a factor solution invalid, and that factor analysts disagree about whether or not a factor solution with a Heywood case can be considered legitimate. I'm particularly interested in the case of a Heywood case within a larger EFA/CFA. Does having a Heywood case (not an ultra-heywood case) renders the whole solution invalid or not? Why? Could someone point me to some literature on this specific topic? 
 Is there a way that I can estimate what the results of a t-test would be in that case? If you know the covariance (or the correlation) between (Treatment,Control) pairs, then yes, because if we let $T_i$ and $C_i$ be the treatment and control values for the $i$-th subject, then $\text{Var}(T_i-C_i)= \text{Var}(T_i)+\text{Var}(C_i)-2\ \text{Cov}(T_i,C_i)$ Since $\text{Var}(T_i)$ and $\text{Var}(C_i)$ are known that leaves only the covariance being needed (and if you know the correlation, the covariance can be computed from the correlation and the two known variances). If you have some reasonable estimate of the correlation (or bounds on it), you can produce a similarly reasonable estimate of the denominator of the t-statistic (or perhaps bounds on it). 
 Does anyone have experience with using generalized estimating equations and the and functions in R? If so, could you recommend a document or tutorial about the these functions and how to interpret and report their outputs? 
 I'm building a predictor for a classification problem. Some of the features are categorical, some are continuous, some are sparse, some are not. Unfortunately, the classes are very imbalanced, with one being over 99% of all the examples. I suppose that there are some features that influence the response, while some are irrelevant. I've tried several approaches to feature selection and different classifiers. Now I want to understand these relations deeper. Among other things I want to see how the dataset size changes the quality of the predictors I have, I want to compare different indicators for the different approaches and "play around" with different properties of the system. So, I want to build a simulation that would reflect the system I'm studying (or actually any similar system). I'll start with some categorical variables: let's say we have , and as (binary) features, and I say that the probability that an example with belongs to class is , for class - , and for class - . Then I generate data based on this probabilities, and apply my classifiers to this data. If an example has both and , I generate class with probability of . I hope this will give me a deeper understanding on the relations between different indicators and dataset size. I'll also see how feature selection performs. Does this approach make sense? Or do I lose a huge amount of information when I make the assumptions above? I don't work with real data, I work with the model that represents my understanding of how the real world process runs. I mean, if I'm able to model the process this way, I should be able to build ideal classifier. 
 My guess is that this is a difference in how the two-sided p-value is computed. In exact tests with non-symmetric distributions (as in your unbalanced example), there are different conceivable ways to accomplish this. See for example the discussion of the two-sided Binomial test and Sign test on Wikipedia: One strategy is compute the left-sided $\mbox{Pr}(S \le s)$ and the right-sided $\mbox{Pr}(S \ge s)$ and then take twice the minimum out of these. My guess is that this is what MATLAB uses. A different approach is to add to the smaller of these p-values the probability of observing an at least as extreme statistic on the other tail. If the test statistic S is standardized (as in the R/coin package) then one can do this via $\mbox{Pr}(S \le -|s|) + \mbox{Pr}(S \ge |s|)$. Finally, one can some over all possible $\mbox{Pr}(S = s_i)$ that are at least as unlikely as the $\mbox{Pr}(S = s)$ for the observed statistic $s$. In your example, we can easily carry out all approaches explicity in R. First, we generate the data, all 126 permutations, and the rank sums (using average ranks). To be able to apply strategy 2, we scale the rank sums (which makes no difference at all for strategies 1 and 3): which replicates the test statistic reported by R/coin. The exact distribution is given by: Then, the first strategy gives this p-value (as reported by MATLAB): The second strategy gives the following p-value (as reported by R/coin): And the third strategy gives While strategy 1 is discussed in the literature - specifically for sign tests and rank tests (see the Wikipedia "Sign test" link above) - something like 2 or 3 is typically preferred (as argued in the Wikipedia "Binomial test" link above). 
 I have been given a project to apply computational intelligence to optimize output of a process. I have been given some 150 parameters, which have several constraints on their numerical values. The parameters are of the following types: RPM of motor input valve opening amount of fuel sent in etc. I have some experience in neural networks, but I am not sure if neural nets are the proper tool to tackle this problem. Is there any genetic algorithm or any other algorithms that can be used with neural nets or independently for such an optimization problem? The problem is as follows: I will be given values for say, some 40 or 50 parameters each day(may be different at different days), now my job is to give a range of, or specific values for the other 100 parameters so as to optimize the target value w.r.t. the given 40 parameters. 
 It shouldn't be too complicated to do. Haven't read the article mentioned, here's my recipe: Variational Auto Encoders Online demo with morphing faces: and https://jmetzen.github.io/2015-11-27/vae.html for teh codez. Basically, this gives you a way to parametrize the 'style' in your case, for example let's say how wide or fuzzy should the brush stroke be. Stuff that depends on the particular style you are trying to emulate. In the example above different 'morphed' or 'imagined' faces are a function of the parameters in the latent space. In the image below that would be what you get by changing stuff at the 'code' level. Here's the basic idea: original image left, stylised version of the same image on the right: Now, in theory, if you would train such a model on a normal image and a stylised image as a target and add convolutions, you should be able to learn the kernel filters that correspond to the type of "brush strokes" that the artist uses. Of course, that means that you need to have a few examples of images in both original and stylized versions. Such a dataset would be nice to donate to the community - if you end up doing this I'd be very keen to see this sort of work. Good luck! The wiki article on auto encoders would be a good starting point: https://en.wikipedia.org/wiki/Autoencoder 
 Let $X_1,\ldots, X_n$ be a random sample from the following distribution: $f(x|\theta)= \frac{4}{\theta}x^3e^{-\frac{x^4}{\theta}}.$ I know that $S = \Sigma X_i^4$ is a complete and sufficient statistic of $\theta$. How can I find a UMVUE for $\tau(\theta)=\frac{1}{\theta}$? 
 I have a very basic question concerning a glmnet plot, but somehow am not able to explain it to myself. Currently, I work through the vignette . I stumbled upon the following example: Note that the minimum lambda found is 0.08307. This value should be represented at the x-axis of the plot at the left dotted vertical line. However, when I calculate, I get $\log(0.08307)= -1.080555$. This is different to (ca.) -2.5 shown by the plot. Any ideas why the plot shows other values? What am I getting wrong here? 
 What is the interpretation of the negative estimate ($b$ value) in an ordered logit regression? That is, an "increase of one unit in the independent variable results in an increase or decrease of the odds ratio of the dependent variable..." For context, my exponentiated $b$ value is 0.869 for feed cost (independent variable) and the dependent variable is income in poultry and it is in ordinal level of measurement. 
 I have resolved my issue. Ranking progress was the difference. First, I rank all the test scores high to low and store the "test score rank" in the person's test results. I then rank the number of test questions answered from most questions answered to least and store that as the "progress rank". Then I simply add the two ranks together and rank their sum for my overall rank. The resulting list is exactly what I was looking for. It produces a list that intuitively seems fair, is simple to explain, and easy to implement. 
 Consider random variable $X \sim \mathrm{Beta}(\alpha,\beta)$ What is the distribution of $Y$ defined by $Y\in \{0,1\}$, $Y=1$ if $X&gt;0.5$ and $Y=0$ otherwise. 
 $Y$ is obviously Bernoulli distributed, with the probability $p=P(Y=1)$ equal to $P(X&gt;0.5)$, i.e. $$\int_{0.5}^1 f(x)\ dx$$ where $f(x)$ is the pdf of the $\mathrm{Beta}(\alpha,\beta)$ distribution. I'm sure you can work it out from here :) 
 In statistics, unless otherwise specified, "log" usually means the natural logarithm. Your mistake was that you calculated the base-10 logarithm instead. The natural log of 0.08307 is about -2.49. 
 I am new in R ,I am studying clustering data, I have applied different algorithms in R ,but with mixed data I am really confusing ,I could not find any good tutorial . I want to apply k-prototype or Fuzzy k-prototype in R. any code or script will be helpful. Many thanks 
 Negative beta would indicate inverse relationship, i.e increase in feed cost leads to decrease in income in poultry. 
 I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket. As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. To do so I have done it this way, but I get a positive effect, so I am not sure about it. So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?) I'd really appreciate any interesting insight and help! :) 
 Could somebody explain this difference just as simple as possible? 
 I have a data set representing interpersonal relations between members of a certain group. I calculated two sets of centrality measures for this population. To calculate values for set A, I used data for all 77 participants (nodes) of my study. For set B, I removed 4 nodes (they have different status than the remaining 73) and re-calculated all measures. Calculating measures for set A and for set B is independent (as far as I can tell), but measures within each set are dependent. I am trying to determine whether excluding those 4 participants with different status will make a difference for my data. When I plot histograms for both distributions they look very much alike, but I would like to be able to use a significance test to quantify this difference rather than "eyeballing". Here are histograms showing the measure distribution without (left) and with (right) those 4 additional nodes: Here is what I thought of so far: I wanted to use the Wilcoxon signed rank test, but the distribution of the differences between the two groups is not symmetrical. I was thinking about the sign test, but observations for each participant are not independent, i.e., measure for one person affects measure for another person. What test would be appropriate in my scenario? 
 As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. Normally, we can use to obtain the logistic models by . For frm, we change to . I noticed we can also use to obtain frm's parameter since it gives the same estimated values. See the following example return, And for , return, The estimated Beta from both are the same, but the difference is the SE values. However, to obtain the correct SE, we have to use as in this post . Now, my questions: What is the difference between these two codes? Is frm about to obtain robust SE? If my understanding is not correct, please give some suggestions. 
 I was reading the book Gaussian Processes for Machine Learning by C. E. Rasmussen &amp; C. K. I. Williams. In the Regression chapter (chapter 2) they teach you how to do a linear regression for data $X \in \mathbb{R}^{n \times d}$ and $y \in\mathbb{R}^{n \times 1}$ in the sense of maximum likelihood with a gaussian prior on the weights and gaussian form assumed for data : $p(y|w,X) \propto \exp(-\frac{1}{2\sigma_{n}^{2}}(y-Xw)^{T}(y-Xw))$ $p(w) \propto \exp(-\frac{1}{2}w^{T}\Sigma_{p}^{-1}w)$ So by direct application of Bayes rule you have : $p(w|y,X) \propto p(y|w,X)p(w)$ $p(x|y,X)\propto \exp(-\frac{1}{2\sigma_{n}^{2}}y^{T}y+\frac{1}{2\sigma_{n}^{2}}y^{T}Xw+\frac{1}{2\sigma_{n}^{2}}w^{T}X^{T}y-\frac{1}{2\sigma_{n}^{2}}w^{T}X^{T}Xw-\frac{1}{2}w^{T}\Sigma_{p}^{-1}w)$ And then by some obscure magic you get: $p(x|Y,X)\propto \exp(-\frac{1}{2}(w-\bar{w})^{T}(\frac{1}{\sigma_{n}^{2}}XX^{T}+\Sigma_{p}^{-1})(w-\bar{w}))$ with $\bar{w}=\sigma_{n}^{-2}(\sigma_{n}^{-2}XX^{T}+\Sigma_{p}^{-1})^{-1}Xy$. I read somewhere it could come from the Woodbury formula indeed the final $\bar{w}$ is of the form $(A+UCV)^{-1}$ but I just cannot figure it out. There must be some simple trick to make sens out of it. EDIT 1 : I got to the result in the case where there is no $\Sigma_{p}$ but I am still at a loss regarding the general case I did something like: $\forall W \in \mathbb{R}^{d \times 1}:$ $(y-Xw)^{T}(y-Xw)=(y-XW+XW-Xw)^{T}(y-XW+XW-Xw)=(y-XW)^{T}(y-XW)+(y-XW)^{T}(X)(W-w)+(W-w)^{T}X^{T}(y-XW)+(W-w)^{T}X^{T}X(W-w)$ As they are all scalar I can transpose the second term which gives: $=||y-XW||_{2}^{2}+2(y-XW)^{T}X(W-w)+(w-W)^{T}X^{T}X(w-W)$ My equality still holds true for $W=\bar{w}$. Then we do not care about the first term of the equality as it is constant w.r.t w, the second term evaluates to 0 if there is no $\Sigma_{p}$ but in the general case I end up with: $p(w|X,y) \propto \exp(-\frac{1}{2\sigma_{n}^{2}}\left(-2(y-X\bar{w})^{T}X(w-\bar{w})+(w-\bar{w})^{T}X^{T}X(w-\bar{w})\right)-\frac{1}{2}(w^{T}\Sigma_{p}^{-1}w))$ And I cannot manage to simplify it... 
 I need to perform binary logistic regression analysis with the explanatory variable of interest fitted with natural cubic spline. Is this handled by function "gam" in R? 
 One of your questions is whether the model is appropriate for the data. Without access to the data this is hard to say. A instead of a as suggested by @PeterFlom in the comments is one natural alternative, though. As for the interpretation I always recommend to compute some hands-on cases to get a feeling what the non-linear logit curve means. If is indeed in meters one could consider a range from 0 to 10 meters and then compute the linear predictor (on the logit scale) and then transform it to probabilities: So you can see that the scoring probability decreases from about 60% to about 50% in this range, i.e., roughly one percentage point per meter. It's been some time that I've been following NBA basketball but this does not seem to be a very plausible model... Nevertheless, if we accept the model and just want to interpret the coefficients we could check either the relative change in the odds per meter which are constant: Thus, the odds of scoring decrease by about 4.3% per meter. The absolute changes in the probabilities are not constant due to the nonlinear logit link. But so close around 50% they are approximately constant and in this case correspond to about 1 percentage point. For example, in comparison to the slope at a distance of 4 meters: 
 The comments are quite accurate, to summarize (and calling $p$ the number of simulateneous workers you have) the complexities should be (depending on the implementations) : Random Forest : $O(n_{trees}*n* \log (n) / p)$ Neural Network : $O(n_{neurons}*size_{neurons}*n/p)$ The speed will also depend on the implementation, the $O$ just gives information about the scalability of the prediction part. The constant term omitted with the $O$ notations can be critical. Indeed, you should expect random forests to be slower than neural networks. To speed things up, you can try : using other libraries (I have never used Matlab's random forest though) reducing the depth of the trees (which will replace the $\log(n)$ by a constant term and allow you to use more workers - but this may harm the accuracy of the classifier) check for duplicate features / constant columns in your data set and remove them (they do not improve accuracy and are responsible for a greater memory usage) [Edit] is your data sparse ? I observed huge speed-ups using the "sparse" representations of the data (as long as the learning algorithms support it) 
 I work with for a retail company. A recurring problem in our meetings is that our mid-level managers usually blame weather for the sales development. I want to find out if this is right or wrong. My plan is to create a regression analysis and find out the coefficient of determination (R-squared). Input so far: Sales per day mm of rain per day hours of sun per day average temperature per day Questions: Is it better to use the actual numbers or should I code the rain, sun and temperature to something, say 0 or 1? What method would be the best approach to tackle this? We have business all over the world, but I'm first going to try it in a specific city. I plan to use Excel, could also use statistical software. 
 The iterative proportional fitting algortihm adjust a matrix to given margins. It is known that the estimates converge to the maximum likelihood estimate. My question is as follows. I have a matrix with cells representing a sample and further, I have the population margins. If I start the algorithm instead of using the sample with a Identitiy matrix, does this affect the interpretation of the results? As, I read the literature IPF converges to the maximum likelihood estimates for any positive matrix. Yet, I also read that the initial values have some effect on the results, which are vanishing after each iteration of IPF. My initial thought would be that the resulting values if I start with an identity matrix represent the unbiased MLE values. 
 so I have a data set of 1500 proteins. I then identify a set of 50 proteins and lets say 40 of these 50 have a certain function. I then draw 1000 random samples of size 50 from my initial 1500 protein data set and look at each sample how many of them have said function. I then get a mean of 27 and a SD of 3 for the 1000 samples. How do I calculate the p-value to proof that in my set of 50 proteins the proteins with a certain function are over represented (compared to my 100 random samples chosen). I assume that my 1000 sample distribution is Gaussian. My first thought was that I use a t-test but if I try that R gives me an error that I have to few observations of one variable. 
 I had a look of may treats and find different pieces here and there but I cant figure out how to do this "simple" comparison. I have 2 counts (obtained with different methods but assuming from same population...this is actually what I am testing...are frequency of prey size overlapping?). Counts are simply frequencies by 0.5 cm of length of prey. In a nutshell, data looks like: Chi-squared would be great because it "equalize" the big difference in n sample but is not possible because I have many zeros. What would you suggest to use for this comparison? p.s. I use R, in case you have some specific suggestion. Many thanks! 
 The most important factors in sales are typically promotions, price changes and markdowns, followed by seasonality (intra-yearly and intra-weekly) or lifecycles, depending on what you sell. (Grocery has yearly seasonality, fashion and consumer electronics have lifecycle effects.) I would thus recommend that you account for these effects first. For instance, you could use Fourier terms to account for seasonality, with day-of-week dummies to account for the fact that retail sales are typically higher on Saturday than on other days of the week. Include prices, price changes and promotions as regressors. You may want to model the residuals using an ARIMA model or similar. Finding a good model for these main drivers can certainly be a whole project all by itself. Then, and only then , you are ready to investigate how much explanatory power weather has on top of all these effects , since your managers presumably know about promotions and seasonality and should not be surprised by them. So you could take the residuals from the above model and regress those on your weather information. (You could also run one big model that includes all covariates simultaneously, and then test the nested models.) I'd recommend that you include your weather information either as-is, or spline-transformed. Splitting, e.g., temperature information into "hot" vs. "not hot" models the effect that sales change abruptly above some specific temperature threshold, and that simply does not happen. Dichotomizing continuous predictors is almost always a bad idea. You can read a lot about this here on CV by searching. Conversely, you could have nonlinear effects of temperature, which you could capture, e.g., by spline transformations. You could probably do a lot of this in Excel, but something like R will be far better. 
 There are many filters doing different tasks for us like edge filters, bar filters. Major filter banks are The Leung-Malik (LM) Filter Bank The Schmid (S) Filter Bank The Maximum Response (MR) Filter Banks We can use these filters for feature extraction, after that we can apply max pooling and other things. The basic thing is that these filters are hand crafted, so they cannot learn weights because we are using fixed filter banks. Contrary to above said techniques for feature extraction, we also have deep learning techniques like neural networks and convolutional networks which learns weights or features over the time. In deep learning we dont tell which features are important rather our network learns itself over the time. Question: In deep learning when we dont tell anything then why is that so that layers closer to input learn the features to detect edges or bars? Why is that so that later layers learn to detect finer details? We did not tell our network to learn edges first, we did not tell our network to learn finer details later. How our network gets to know that it has to learn edge filters in upper layers and other filters in lower layers? Am i missing some basic thing to understand neural networks? Plz correct me. 
 I'm trying to make comparisons between number of incidents that happened in different zip codes. The goal is to figure out whether those incidents happen more frequently in certain areas than others. The problem is that some zip codes are more populated than others and we would expect to see more incidents in more populated areas. I am trying to find a way to normalize the data to control for difference in population size and thus make accurate comparisons between areas that have different population sizes. What is the best way to do this? 
 Since your outcome variable is logSum, you have to exponentiate the coefficients. This is very easy to do in R. For teplota, the exponentiated coefficient would be 1.17732. In r, the code is very straight forward: This means that, for a one unit increase in teplota, there would be a 17 % increase in the outcome variable (see here: .) The clusters are simply the groups of the id variable, in this case den. You have 24 levels of den and the largest group is 16. As for the correlation structure, autogressive AR(1) is generally used for time series data. This is because time points are often correlated with one another. You may not need this. In order to check for autocorrelation, you need to generate an acf plot. I am not sure how to do this using geeglm, so I will use lme4 with the lmer function. If you have autocorrelation, as indicated by the acf plot, you can proceed with your original model. You could explore other packages, for instance, nlme or lme4. I would recommend lme4, but you will have to do a little work to get p-values that you are comfortable with. In the package description, you will have to browse the table of contents for pvalues. If you do not have autocorrelation, my model using lme4 would be appropriate. If this answered your question, please mark your post as answered. If this does not answer your original question, please describe your situation in more detail. 
 If your question is: what is the difference between these two codes? A look at says , and a look at reveals the following description: The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion. This is also what you see in your output. And that is the difference between both models / codes. If your question is: what is the difference between the logistic regression and the fractional response regression? As you correctly identify, the model is a logistic one if your dependent variables are either 0 or 1. Papke and Wooldridge have shown that you can use a GLM of this form for fractions as well for the estimation of the parameters, but you need to compute robust standard errors. This is not required for the logistic regression, and in fact, some people think you should not compute robust standard errors in probit/logit models. Though this is a different debate. The theoretical basis comes from a famous paper by Gourieroux, Monfort, and Trognon in Econometrica in 1984. They show that (under some regularity conditions etc) maximum likelihood parameters obtained by maximizing a likelihood that belongs to the linear exponential family are consistent estimates for parameters belonging to any other likelihood in the linear exponential family. So, in some sense, we are using the logistic distribution here even though it is not exactly the correct one, but the parameters are still consistent for the parameters that we wish to obtain. So, if your question originates from the observation that we are using the very same likelihood function to estimate both logistic and fractional response models, except that we exchange the nature of the dependent variable, then this is the intuition. 
 This is the cost function that seems to be used for training linear SVM classifiers: $J(\mathbf{w}, b) = \dfrac{1}{2} \mathbf{w}^T \cdot \mathbf{w} \quad + \quad C {\displaystyle \sum\limits_{i=1}^{m}max\left(0, 1 - y_i (\mathbf{w}^T \cdot \mathbf{x}_i + b)\right)}$ I can see why it would work, but I am wondering how it was derived from the linear SVM constrained optimization problem (primal form)? $\underset{\mathbf{w}, b}{\operatorname{minimize}}\quad{\frac{1}{2}\mathbf{w}^t \cdot \mathbf{w}} \\ \text{subject to} \quad t^{(i)}(\mathbf{w}^t \cdot \mathbf{x}^{(i)} + b) \ge 1 \quad \text{for } i = 1, 2, \cdots, m$ I suppose it comes from the generalized Lagrangian? $\mathcal{L}(\mathbf{w}, b, \mathbf{\alpha}) = \frac{1}{2}\mathbf{w}^t \cdot \mathbf{w} - \sum\limits_{i=1}^{m}{\alpha^{(i)} \left(t^{(i)}(\mathbf{w}^t \cdot \mathbf{x}^{(i)} + b) - 1\right)} \\ \text{with}\quad \alpha^{(i)} \ge 0 \quad \text{for }i = 1, 2, \cdots, m$ But I don't see how you go from this equation to the hinge loss function $max(0, 1-t)$. Any idea? 
 I have seed and seedling abundance data for several plant species. I am trying to model the effect of my treatment on how many seeds successfully transition into seedlings (seed-to-seedling transition). My goal is to see in which treatment (made of four levels, C, P, I and R) are seeds more likely to establish and grow into seedlings, or see if there is no treatment effect at all. I am trying to do this through the function in the package using a binomial model. I have as a random factor due to a nested design. My response variable, I think, should be successes: the number of seeds that transitioned into seedlings, and failures: the number of seeds that did not transition. However, here is my problem... I measured seeds and seedlings at different scales (.75 m2 seed traps vs 24 m2 seedling plots). In order to compare Seed #s to Seedling #s, I should standardize by the area and get #s per m2. But doing this gives me decimal numbers, which do not seem to work in the model. (I should also note that I have a ton of zeros in my data) Someone recommended using an offset. But I can't figure out how to set up the offset in my data since the columns for my response variable are composed of both seed and seedling data, which were both measured on different scales. Is there anyway I can model what I am trying to do? Or should I just try to figure out a different model? Here is a link to my data as a CSV file: STS15 CSV file And here is a link to the excel file, which contains both standardized and unstandardized values: STS Excel file Here is the formula I am trying: md1&lt;-glmer(cbind(CX_S, CX_F)~Treatment + (1|Site), family=binomial, data=STS15) I get this error message: Warning messages: 1: In eval(expr, envir, enclos) : non-integer counts in a binomial glm! 2: In checkConv(attr(opt, "derivs"), opt$$par, ctrl = control$checkConv, : unable to evaluate scaled gradient 3: In checkConv(attr(opt, "derivs"), opt$$par, ctrl = control$checkConv, : Hessian is numerically singular: parameters are not uniquely determined Please let me know if I can provide any more information. Thanks in advance for any help, I really have been struggling with this model. 
 I've spent several months thinking about the issue of whether or not it is appropriate to apply Rasch models to formative measurement models, and I'm looking to see whether anybody else has considered this issue, and what the answer is. People frequently apply the Rasch model with little consideration given to whether it's appropriate to the measurement model they are working with. I want to begin by stating a few, I would consider, relevant features of a formative model versus a reflective model. In a reflective model, indicators are caused by a latent variable In a formative model, indicators are the cause of the "latent variable" In a reflective model, indicators should all correlate with each other In a formative model, this need not be the case Removing an item from a reflective model may reduce reliability of the final score, but not qualitatively change its interpretation (items are interchangeable) Removing an item from a formative model will change the meaning of the final score (removing earnings from a formative model of SES, will result in a measure of SES that is substantively different) From my thinking about the subject, I've identified at least three issues with using Rasch analyses for formative models. Dimensionality One goal of a Rasch analysis appears to me to be to demonstrate that a measurement model is unidimensional. As the indicators of a formative model need not correlate, and we may in fact want to avoid strongly correlating items, I'd suggest that a formative model may be almost always multidimensional. Since a formative model need not be unidimensional, then why might we care if it is or isnt'? Invariance One property of a measurement model that adequately fits the Rasch model is that the estimate of person abilities that are obtained should remain similar if a different set of items are used. Because what is being measured is a consequence of the items used in a formative model though, this property could never be met. If I change the items then I'm measuring something different and it would be extremely unlikely that people's abilities on that new variable will be the same. Dealing with misfit In a Rasch analysis of a measurement tool, it is not uncommon that if items are found to exhibit particularly harmful misfit then they are removed from the tool. Indeed, the results of Rasch analyses may inform tool development. However, for reasons already suggested this would be dangerous with a formative model (because the variable being measured would now be different). From my survey of the literature, proponents of Rasch only seem to have discussed whether or not a Rasch analysis is sensitive to the causal differences between a formative and reflective model ( e.g., here ). Not whether there's a reason to be applying Rasch to a formative model in the first place. tl:dr Given that formative models may have properties that differ from the sorts of measurement models that Rasch analyses were originally intended for, and given that it's not clear that one could do anything to respond to issues flagged up by a Rasch analysis, is there any reason to apply a Rasch model to a formative measurement model? 
 The Situation We have a set of audio transformations each described by their position in an 80 dimensional feature space, each dimension corresponding to a particular feature of the transformation or the audio signals being transformed. Each of these transforms is also labelled with a semantic descriptor (such as ‘warm’, ‘crunch’ or ‘fuzz’). Some sample data is shown here: What we want to measure is the level to which these semantic descriptors are agreed upon across the set of transforms they label. Intuitively, if all the transforms labelled with are particular descriptor are group together in some meaningful way, in the features space, that descriptor should receive a high agreement score. If the transforms labelled by a particular descriptor are distributed across the entire feature space, the descriptor should receive a low agreement score. A common approach to this is to sum the variances, in each dimension, for all of the transforms with a particular descriptor. The agreement for a descriptor, d, is then given by: $$A(d) = \frac{\log(N_{d})}{\sum_{k=1}^{K} \sigma_{d,k}^{2}}$$ Where $N_{d}$ is the total number of transforms labelled with descriptor $d$, $K$ is the total number of features (dimensions in the features space) and $\sigma_{d,k}^{2}$ is the variance, in feature $k$, for transforms labelled with descriptor $d$. I don’t think this is a particularly good measure for a variety of reasons: The scaling by the log of the number of transforms doesn’t seem very rigorous to me. Clearly the agreement score should increase if there are more transforms within the same region of the space, but this method doesn’t seem statistically sound. It assumes that high variance, in any dimension, is a bad thing and should reduce the agreement score for a descriptor. It may be the case that a particular feature has no influence on the labelling of a transform and as such that feature should have little influence on the agreement score. It assumes that low variance is a good thing and should increase the agreement score. It may be the case that the value of a particular feature is the same for every transform, regardless of the descriptor used to label them. In this cast low variance should not influence the agreement score. It assumes that all features are independent. Summing the variances of each feature individually does not take into account the relationships between features. Below are my thoughts on how we can improve on this measure. Variance vs Margin of Error Firstly I think we should be using something a little more descriptive than just the raw variance of each feature. The margin of error seems more appropriate to me. This would properly take into account the number of transforms and the expected distribution of feature values. Sum of Reciprocals Secondly, I think taking the reciprocal of the sum of variances measures the wrong thing. In the previous equation a high variance in any feature will reduce the agreement score, regardless of how many features exhibit a low variance. Summing the individual reciprocals of the margin of error in each feature makes more sense to me. $$A(d) = \sum_{k=1}^{K} \frac{1}{E_{d,k}}$$ Where $E_{d,k}$ is the margin of error, in feature $k$, for transforms labelled with descriptor $d$. In this way features with a large margin of error have little effect on the overall agreement score, while features with a small margin or error will increase it. Using Principal Components This still suffers from the problem of assuming a small margin of error is due to an agreement between labels. We need to discard any features which have a small variance regardless of their descriptor. This can be done using principal component analysis (PCA) on the entire feature space (including all transforms), and keeping only the first $P$ principal components (PCs). If the transforms labelled with a particular descriptor exhibit low variance in one of these first $P$ PCs it is likely that it is well agreed upon. The problem now is that the PCs all describe different proportions of the variance in the original data. This can be taken into consideration by weighting the margin of error in each PC by the proportion of variance described by that PC. In a PC which describes a large proportion of variance, a low variance is not expected and as such should be given a higher weighting in the agreement score. In a PC which describes a low proportion of variance, a low variance is expected so the weighting should be less. This gives rise to this equation: $$A(d) = \sum_{p=1}^{P} \frac{V_{p}}{E_{d,p}}$$ Where $V_{p}$ is the proportion of variance described by PC $p$ and $E_{d,p}$ is the margin of error, in PC $p$, for transforms labelled with descriptor $d$. Relationships Between PCs One problem still remains that a certain descriptor might exhibit some relationship between two PCs. The original PCA is performed on the entire feature space so only features which are proportional for all transforms will be combined into single PCs. It is possible that two features are only related for transforms with a particular descriptor, in which case they will most likely be split into two separate PCs. This means that, for some descriptors, there may be relationships that exist between PCs which we need to capture in our agreement score. My idea for this is to find the covariance matrix for transforms labelled with a particular descriptor in the first $P$ PCs, then use its eigenvalues and eigenvectors in the calculation of the agreement score. Something like this: $$A(d) = \sum_{i=1}^{P} \frac{V \cdot v_{d,i}}{\lambda_{d,i}}$$ Where $V$ is a vector containing the proportions of variances described by the first $P$ PCs, $v_{i}$ is the $i^{\textrm{th}}$ eigenvector of the covariance matrix for transforms labelled with descriptor $d$ and $\lambda_{d,i}$ is the corresponding eigenvalue. The numerator here is the equivalent of weighting by the PC's proportions of variances as done previously. Taking the dot product of the eigenvector and a vector containing the PC's proportions of variance takes into account the weighted variance described by the PCs to which the eigenvector is not orthogonal. The use of eigenvalues could probably be improved by using a margin of error equivalent, but I'm not sure on what distribution one would use in the calculation of such error measures. Questions So, that is what I have thought about and here are some questions I have about it. Does any of what I have said make any sense whatsoever? Are there any existing statistical measures for this type of thing? I have looked at various inter rater reliability measures but none of them seem to apply. What are your thoughts on the whole eigenvector weighting thing? It seems to make sense to me but maybe there are some gaping holes in my reasoning. Any other suggestions you may have. Thanks! 
 I have a dataset of monthly sales for the past 6 years. Significant attributes in the data set are: I want to experiment with using neural networks for forecasting sales in R for the next 3 months. Using the Last 2 years as Validation data and the previous 4 years as Training. If I were to do that, do I consider Region, Nameplate, Segment and MonthofSale as Inputs? And do a 4 -3-1 network? Or did I get the whole concepts of inputs wrong? Any pointers on this will greatly be appreciated. Thanks &amp; Regards, B 
 For illustration purpose, consider smaller vectors: First, let's say $z$ is a vector of constant values, i.e. , $s_z = 0$ Now will produce a perfect straight line and will return meaning perfect correlation between $x$ and $y$. It is because given an $x$ value, we can accurately predict $y$ value ($y = x + 50$). Again, take random values of $z$ with mean $\bar z = 50$ and standard deviation $s_z = 5$. What this means is that now $y$ depends not only on the randomness of $x$, but also randomness of $z$. Larger SD of $z$ indicates the values are more distant from the mean, $50$. So if we try to predict $y$ from $y = x + 50$, as we did before, our prediction will be a lot less accurate meaning weaker correlation. Speaking of your second question, as it should have been clear by now, SD instead of mean affects the correlation. 
 I have Hit rate data (hit rate being "number of transactions"/"number of visitors") weekly and for several years. I want to compare how the hit rate has changed from year to year and my first idea was to create something where I put the first week of each year to 100%. Then calculate the change and add it to the 100%. You will then be able to compare the years with all of them starting from 100. Now, my final data is showing a steep graph for all and each year. 21.8/8.2 is ~266% increase, however, the "index" show an increase of 286.6%. Can I trust my calculations or what am I'm doing wrong here? NOTE: sorry for the commas. 
 I'm trying to perform an event study around USDA reports for related commodities daily returns. I'm relying on several articles (including, for CMR models, Milonas, 1987) and textbooks (The Econometrics of Finance, chapter 4). As a first approach, I'm using a Constant Mean Return model, meaning that my abnormal returns are $AR_{i,t} = R_{i,t} - \bar{R_{i}} $ where $\bar{R_{i}}$ is the empirical mean computed on another interval. Say I have 1 asset (1 commodity) and $N$ events. What I want to do is to test $H_{0}$ : "my abnormal returns are normally distributed with mean 0" (and if I reject it, then the event has somehow an effect on the asset). This was to give some context, but despite the financial setting my question is really on the methodology :I don't really understand how to test for $H_{0}$ : do I have to consider a sample of $N$ abnormal returns for t = 0 and see if they are normally distributed (my understanding of Milonas' approach, see appendix), or do I have to compute a specific quantity and deduce an answer from it's sole value (my understanding of the approach of the textbook, see p162) ? (or either, or none of the above ?) 
 FIRST APPROACH: Hurst et al. use the following form for a similar problem (as far as I can see): where a, b, and c are parameters. The call to would be along the lines where is the data.frame with your data, including the columns and . The fit does not look very nice, though. SECOND APPROACH: To exploit the known functional relation, you can use constrained optimisation to get a reasonable fit: The important constraints are that the second parameter needs to be negative and the fourth parameter positive. 
 I have a complicated procedure that, given an input $x$, outputs several random samples $S_i(x)$ for $i\in\{1,\dots,k\}$ (each sample consists of $n_i$ points). Each sample $S_i(x)$ follows an unknown distribution $F_i$. I wish to find $x^*$ that will make my samples $S_i(x^*)$ "look as close as possible to" a set of predefined Normal Random Variables $F^*_i =\mathcal{N}(\mu_i, \sigma_i)$. How can I measure this "looking as close as possible to some distribution"? I also wish that this metric weighs equally each $F^*_i$ and that this weighting does not depend on the parameters of $F^*_i$ ($\mu_i$ and $\sigma_i$). Initially I thought of simply calculating the likelihood of the sample, though this would work with discrete distributions, it wouldn't for continuous distributions as the likelihood of a sample is always $0$ (edit: See @amoeba answer as on why this is false). 
 Here is a good resource I use for comparison - their performance on different benchmark datasets. This is an excellent site that does that has an ordered results of most of the noteworthy papers. Take note of the "details" button on the right column, It gives a short description on their testing methodology, for example if an ensemble was used, what type of augmentation, etc. A word of advice, the best thing you can do is try it yourself . From my personal experience, significant part of the performance difference published in deep learning papers has to do with training knowhow - proper choice of hyperparameters and augmentation than the actual architecture. sad but true. 
 Depends on whether you accept other built-in functions which are not specifically for AR processes, but help a lot with generating one. So, firstly use Matlab's function to get a vector of normally distributed i.i.d. random values (with zero mean and unit variance): Then filter this signal with an all-pole filter to get the desired AR process: Here the first corresponds to the filter having no zeros, or in other words not having the moving average part (see ARMA process for more info). is the vector of filter coefficients specifying the poles (for order of 3, we would have e.g. ). 
 Classification is simply a more general term than pattern recognition. In both cases, you have a set of classes $K$ and a collection of observations, where each observation is represented by a set of features. Your job is to find a mapping of features to a member of $K$ such that you minimize some measure/estimate of out-of-sample classification error. In pattern recognition, you are simply using a very complex/large feature space. For example, facial recognition will have an input space equal to the number of pixels in the image. This is no different than classifying a loan application as high or low risk based on several measures of creditworthiness. 
 I thought of calculating the likelihood of the sample, but this would work with discrete distributions. But for continuous distributions the likelihood of a sample is always $0$. This is a misconception. If you have a point $x_i$ and a probability distribution $f(x)$, then the likelihood that this point comes from this distribution is defined as $f(x_i)$. If $f(x)$ is the PDF of a normal distribution, then $f(x)\ne 0$ for any $x$. If you have an iid sample $S=\{x_1, x_2, \ldots, x_n\}$, then the likelihood of it coming from the same distribution is $\prod_{i=1}^nf(x_i).$ I think this should be enough to get you going. Note that if $f(\cdot)$ is normalized to integrate to $1$, then values $f(x_i)$ tend to be small and when multiplied together quickly become close zero. That is why we are usually working with log-likelihood instead of likelihood: $$\log\Big(\prod_{i=1}^nf(x_i)\Big)=\sum_{i=1}^n\log f(x_i).$$ Having said all that, it seems that in your case you should compute some goodness-of-fit statistic and not the likelihood. The difference is illustrated in the first comment by @BrentKerby. 
 The goodness of fit of $S_x$ to $X^*$ can be assessed in a number of ways: the Cramér-von Mises statistic , the Kolmogorov-Smirnov statistic , or chi-square statistic would be common choices, and there are certainly others . If we knew something about the distribution of $S_x$, or about your specific goals or performance criteria, then it might be possible to say more. 
 The probabilities are what are returned by default in the package. If you are using you can simply get the probabilities by using: 
 Suppose $X_1,...,X_n$ are iid from $N(\mu,\sigma^2)$ and let $X_{(i)}$ denote the $i$'th smallest element from $X_1,...,X_n$. How would one be able to upper bound the expected maximum of the ratio between two consecutive elements in $X_{(i)}$? That is, how can you calculate an upperbound on: $$E\left[\max\limits_{i=1,...,n-1}\left(\frac{X_{(i+1)}}{X_{(i)}}\right)\right]$$ The literature that I have been able to find is mostly focused on the ratio between two random variables which results in a ratio distribution for which the pdf for two uncorrelated normal distributions is given here: https://en.wikipedia.org/wiki/Ratio_distribution#Gaussian_ratio_distribution . While this would enable me to upperbound the expected average ratio of $n$ variables I can not see how to generalize this concept to finding the expected maximum ratio of $n$ variables. 
 All in all, a t-test on counts seems to be a weird way to approach the problem at hand. Why should anyone assume, counts were Gaussian? They are not. There is a finite number of proteins and you may simply count how many have "said function". By dividing this number by 1500 you get the probability, that any random protein "has said function". The problem is then easily approached by a binomial test. Let's assume, the probability for any random protein is 68.3% and you have 40 successes in 50 trials, then HTH, Bernhard 
 I was digging to learn how to use function with method. I found this questions, resolved with a basic R example: What are the best R packages for a classification problem with use of Neural networks (I'm copying the code below for convenience). Yet, when I run that code on my Macbook, the first model builds fine, the second -- the one with -- it just crashes R, raising a . I've tried other similar examples using other datasets, but I'm always getting the same exception. What am I doing wrong??? 
 It makes a huge difference. The IPF algorithm basically maintains the odds ratios in the starting values and changes the counts such that they match the desired margins. So if you take the data as starting values, you maintain the odds ratios you found in the data. If you start with all 1s you find the counts that would occur if the two variables are independent. 
 In a school project we would like to predict a binary variable , using ensemble methods(Boosting, stacking ,) ,my problem is that i'm a beginner in this field . we have a large dataset 370 features , and we would like to select the best 20 of them in order to predict correctly our binary variable What are the most used methods to select variables in my case ? can i use Lasso regression and define a sgmoid function to fit my classification problem ? 
 I'm trying to run a two-way ANOVA repeated measurement with (1) lme and (2) ezAnova. The problem is that I get different degrees of freedom of the denominator ( and ). Not sure if I'm using the right syntax. Any ideas why this happens? (1) lme (2) ezANOVA Structure of the data: f: continuous dependent variable subj: identifier (112 different participants) IV_history: Independent categorical variable (4 levels) IV_amount: Independent categorical variable (3 levels) n = 112 * 4 * 3 = 1344 UPDATE After a thorough research I think I figured out the problem. The syntax for the lme term above seems to be wrong. If I adjust the syntax of the lme the F-values (not the degrees of freedom) match with the ezANOVA results above: 
 I understand that this is the dual form of the linear SVM problem (with a hard margin): $J(\mathbf{\alpha}) = \dfrac{1}{2}\sum\limits_{i=1}^{m}{ \sum\limits_{j=1}^{m}{ \alpha_i \alpha_j y_i y_j {\mathbf{x}_i}^t\cdot\mathbf{x}_j } } \quad - \quad \sum\limits_{i=1}^{m}{\alpha_i}\\ \text{with}\quad \alpha^{(i)} \ge 0 \quad \text{for }i = 1, 2, \cdots, m$ I'm wondering how to solve it using (sub)gradient descent? Perhaps I should remove the constraints first? $J_{\text{extended}}(\mathbf{\alpha}) = \dfrac{1}{2}\sum\limits_{i=1}^{m}{ \sum\limits_{j=1}^{m}{ \max(0, \alpha_i) \max(0, \alpha_j) y_i y_j {\mathbf{x}_i}^t\cdot\mathbf{x}_j } } \quad - \quad \sum\limits_{i=1}^{m}{\alpha_i}$ Then I can compute the partial subderivatives: $\dfrac{\partial}{\partial \alpha_k}J_{\text{extended}}(\mathbf{\alpha}) = \begin{cases} \dfrac{1}{2}\sum\limits_{i=1}^{m}\alpha_iy_iy_k{\mathbf{x}_i}^t\cdot\mathbf{x}_k - 1 &amp; \text{ if } \alpha_k \ge 0\\ -1 &amp; if \alpha_k &lt; 0 \end{cases}$ But this does not seem right as the $\alpha_i$ may end up negative. What is the correct approach for this? 
 Supposing $X$ and $Y$ are random variables with a joint bivariate normal distribution and covariance matrix $\Sigma_{XY}$. Consider the following linear combination for constants $A$, $B$ and $C$: $$Z = AX + BY + C$$ This Cross Validated post states that $Y$ will be normally distributed according to: $$ AX+BY+C \sim \mathcal{N}\left[ \left(\begin{matrix}A&amp; B \end{matrix}\right) \left(\begin{matrix}\mu_X\\\mu_Y\end{matrix}\right) + C, \left(\begin{matrix}A &amp; B \end{matrix}\right)\Sigma_{X,Y} \left(\begin{matrix}A^T \\ B^T \end{matrix}\right)\right] $$ However I would like to know how to compute the covariance matrix $\Sigma_{X,Y,Z}$. How can I calculate the correlation between the new variable $Z$ and the old variables $X$ and $Y$ i.e. $corr(Z,X)$ and $corr(Z,Y)$? 
 I have a variable that is a recursive function involving other variables with known distributions (see problem below). Let $b(t+1) = b(t) + C \sqrt{b(t)}$ where I know $C \sim N(1.82, .0298)$ and the initial value of $b$ [$b_{initial} \sim N(.02,0.0036)$]. My observation for updating is a discrete value of $b$ for a certain '$t$' (say $b(1500) = 0.005$) but I need to find posterior distributions for $C$ and $b_{initial}$. Any thoughts or resources that can be directed my way would be extremely helpful. Thanks 
 Here is the scenario: There are a number of students (n), each with a random 9-digit integer student ID number (including zeros). At what n-value would there be a 0.5 probability at least two students having the same "last 3 digits" (identical integers in identical order)? I am not sure how to set this up. 
 What is the best way to perform a goodness to fit test for two discrete univariate distributions knowing only their mean and variance? I don't know the nature of these distributions and I was only able to find tests that require knowing a priori set of samples (e.g. chi-squared test). The only other information I have is that the number of samples used to determine mean and variance for the two distributions is not the same. Would t it help to know how many samples were used in the first place? 
 Suppose that we have fitted a probabilistic model (like GLM) to the data in order to perform predictions using test samples. The question is how to perform such predictions having the conditional distribution of response given data? Should we sample from the conditional distribution and count it as the predicted value? Or we should consider the mean or perhaps the median of the distribution as the predicted value? I have seen a paper where the median is used for prediction but I'm not sure whether it is correct. 
 Machine Learning One the most, if not the most, popular textbooks on machine learning is Hastie, Tibshirani, and Friedman, The Elements of Statistical Learning , which is fully available online (currently 10th printing). It is comparable in scope e.g. to Bishop's Pattern Recognition and ML or Murphy's ML , but those books are not free, while ESL is. Hastie &amp; Tibshirani also wrote freely available An Introduction to Statistical Learning, With Applications in R which is a simpler version of The Elements and focuses on R. In 2015, Hastie &amp; Tibshirani published a new textbook Statistical Learning with Sparsity: The Lasso and Generalizations , also available online. Another freely available all-encompassing machine learning textbook is David Barber's Bayesian Reasoning and Machine Learning . I did not use it myself, but it is widely considered to be an excellent book. Switching now to more specialized topics, there are: Rasmussen &amp; Williams Gaussian Processes for Machine Learning , which is the book on Gaussian processes. Much awaited Goodfellow, Bengio and Courville Deep Learning textbook that is about to be published by MIT Press. It isn't published yet, but the book is already available online. On the official website one can view it in browser but cannot download (as per agreement with the publisher), but it is easy to find a combined PDF e.g. here on github . Csaba Szepesvári, Algorithms for Reinforcement Learning , a concise book on RL. A classical, much more detailed but a bit dated textbook is Sutton &amp; Barto, Reinforcement Learning: An Introduction which is also freely available online but only in a cumbersome HTML format. Boyd and Vandenberghe, Convex Optimization . 
 You can simply calculate proportion of right answers, $$ P = {n_r \over n}, $$ but it's better to add a correction for total number of answers given. One way to correct is to add "dummy" wrong answers (e.g. $10$), so $$ P' = {n_r \over n + 10 }. $$ People with a large number of answers see their modified percentage alters very little from their real percentage, but people with relatively few answers will see their modified percentage move considerably toward low values . This is known as " Bayesian averaging ". In effect, the people with many answers will rank higher than people with the same percentage but fewer answers. 
 this is my first post and I hope you can help. I am trying to extract certain period from a time series. My time series (tsSST) goes from 2011,1,1 to 2015,12,31 and I need to extract the period from 2014,1,1 to 2015,12,31. That is my code and the error I am having: tsSST&lt;-ts(SST[,2], start=c(2011,1,1), frequency=365) tsSST2&lt;-window(tsSST, start=c(2014,1,1),end=c(2015,12,31),frequency=365) Error in window.default(x, ...) : bad value for 'start' If anyone can explain what is needed to be changed I'll appreciated that. 
 I am running Cox Proportional Hazard Model in R, package survival , function coxph() . As I have time-varying covariates, my data is defined as counting process, that is there is one separate data record for each (t1,t2] time interval. So any object i can have multiple records, each for different time interval. Does this mean that I must run the model with correlation structure, i.e. do I have to use cluster(Id) term in the model formula ? I have not seen used cluster(Id) term in some examples that I found, nevertheless I am in doubt, because how otherwise would you define that multiple observations belong to one object only ? I guess if multiple observations belong to one object, we should inform the model that the errors are not independent ? 
 Cosma Shalizi, CMUs ML guru, occasionally updates a draft of a stats book soon to be published by Cambridge Press titled Advanced Data Analysis from an Elementary Point of View . Can't recommend it highly enough... Here's the Table of contents: 
 The expectation is undefined. Let the $X_i$ be iid according to any distribution $F$ with the following property: there exists a positive number $h$ and a positive $\epsilon$ such that $$F(x) - F(0) \ge h x\tag{1}$$ for all $0 \lt x \lt \epsilon$. This property is true of any continuous distribution, such a Normal distribution, whose density $f$ is continuous and nonzero at $0$, for then $F(x) - F(0) = f(0)x + o(x)$, allowing us to take for $h$ any fixed value between $0$ and $f(0)$. To simplify the analysis I will also assume $F(0) \gt 0$ and $1-F(1) \gt 0$, both of which are true for all Normal distributions. (The latter can be assured by rescaling $F$ if necessary. The former is used only to permit a simple underestimate of a probability.) Let $t \gt 1$ and let us overestimate the survival function of the ratio as $$\eqalign{ \Pr\left(\frac{X_{(i+1)}}{X_{(i)}} \gt t\right) &amp;= \Pr(X_{(i+1)} \gt t X_{(i)}) \\ &amp;\gt \Pr(X_{(i+1)}\gt 1,\ X_{(i)} \le 1/t) \\ &amp;\gt \Pr(X_{(i+1)}\gt 1,\ 1/t \ge X_{(i)} \gt 0,\ 0 \ge X_{(i-1)}).}$$ That latter probability is the chance that exactly $n-i$ of the $X_j$ exceed $1$, exactly one lies in the interval $(0,1/t]$, and the remaining $i-1$ (if any) are nonpositive. In terms of $F$ that chance is given by the multinomial expression $$\binom{n}{n-i,1,i-1}(1-F(1))^{n-i}(F(1/t)-F(0))F(0)^{i-1}.$$ When $t \gt 1/\epsilon$, inequality $(1)$ provides a lower bound for this that is proportional to $1/t$, showing that The survival function $S(t)$ of $X_{(i+1)}/X_{(i)}$, has a tail behaving asymptotically as $1/t$: that is, $S(t) = a/t + o(1/t)$ for some positive number $a$. By definition, the expectation of any random variable is the expectation of its positive part $\max(X,0)$ plus the expectation of its negative part $-\max(-X,0)$. Since the positive part of the expectation--if it exists--is the integral of the survival function (from $0$ to $\infty$) and $$\int_0^x S(t) dt = \int_0^x (1/t + o(1/t))dt\; \propto\; \log(x),$$ the positive part of the expectation of $X_{(i+1)}/X_{(i)}$ diverges. The same argument applied to the variables $-X_i$ shows the negative part of the expectation diverges. Thus, the expectation of the ratio isn't even infinite: it is undefined. 
 Probabilistically, the right way to do this is a posterior joint distribution for C and binitial--if you know b(t) exactly, then you'll get a line of sorts, where if binitial is .022 that implies that C was 1.81, but it binitial was instead .023 then that implies that C was 1.76. Each of those points has a probability associated with it from the prior, and the update just consists of renormalization from the entire 2D space to that line. If you know b(t) inexactly, then you instead need to calculate the probability of that b(t) measurement for everywhere in the distribution, multiply those, and then renormalize. (That is, you're updating with a continuous likelihood, instead of the 0 or 1 likelihood implied by an exact measurement.) For your particular function, it doesn't look like it'll be computationally easy to work with, although it probably is smooth (in the sense that the derivatives with respect to C and binitial are well-behaved). Seeing if you can come up with something where you can shift C by $\Delta$C and get the corresponding $\Delta$binitial shift (or the reverse) looks like it'll be very useful. 
 11000 is an invalid class if your classes are mutually exclusive and if you have only five classes consisting of 10000,01000,00100,00010, and 00001. 11000 is no longer an invalid class once you define your class space to be all digits between 0 and 2^5. Ultimately it's up to you to decide what types of classes you would like to classify. Likewise for inputs: you can use however many inputs you would like as long as you are consistent between training and test/predicting. Your dataset sounds very abstract and that could be what is clouding your thinking. If you're just starting out with machine learning and neural networks, go with a well known dataset first so that you only need to think about the algorithm. A good example is the MNIST handwritten digits dataset. All the classes are digits 0-9 (10 total) and so there is no confusion about classes falling outside of 0,1,2,3,4,5,6,7,8,9. Google's Tensorflow library has a good MNIST tutorial here: https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html . 
 I would compare two classifiers (A and B), where B is obtained from A. I would exploit the F-measure computed on two samples (of two independent populations, p1 and p2, respectively): A -&gt; F-measure1 on p1 B -&gt; F-measure2 on p2 Once obtained an F-measure index for the first and the second classifier, I might execute an hypothesis testing in order to measure if index of second classifier (F-measure2) is greater or smaller and such difference is statistically significant. Does it make sense? Can I use the typical hypothesis testing for average of a population, substituting average with F-measure? 
 I am fitting a GAM to data of capture success in trapping sites using the mgcv package in R. The independent variables are proportions of land cover types. Here is the output of my model: Multicollinearity was low in this model as the variance inflation factor of each covariate was &lt; 5. Given that the proportion of wetlands induced autocorrelated residuals, I removed this covariate from the model. However, when I used the model without the proportion of wetlands to do predictions of capture success over large areas, I noted that high values of capture success were present in areas with many wetlands. Is it possible that the model kept the effect of wetlands while this covariate were removed from the model ? 
 Real world datasets are rarely deterministic. In the example you've provided above your neural network will learn that when it sees 01101 there will be a 50% probability of 0 and a 50% probability of 1. Deleting the records will make your neural network overconfident, assuming the records were not accidentally mislabeled by whomever gathered the dataset. 
 Covariance matrix $\Sigma_{xy}$ can be written: $$ \Sigma_{xy} = \left[ \begin{array}{cc} c_{xx} &amp; c_{xy} \\ c_{yx} &amp; c_{yy} \end{array} \right] $$ Where $c_{ab}$ denotes the covariance between $a$ and $b$. (Note symmetric so $c_{ab} = c_{ba}$.) The covariance matrix for $X$,$Y$, and $Z$ would be: $$ \Sigma_{xyz} = \left[ \begin{array}{ccc} c_{xx} &amp; c_{xy} &amp; c_{xz} \\ c_{yx} &amp; c_{yy} &amp; c_{yz} \\ c_{zx} &amp; c_{zy} &amp; c_{zz} \\ \end{array} \right] $$ You need to find the additional terms: $$\begin{align*} c_{zx} &amp;= E[(Z - E[z])(X - E[x])] = \quad ?\\ c_{zy} &amp;= E[(Z - E[z])(Y - E[y])] = \quad ?\\ c_{zz} &amp;= E[(Z - E[z])^2] = \quad ?\\ \end{align*}$$ 
 I'm implementing several heuristic algorithms for my problem and I wanted to see which one works better. The problem itself is NP-complete, so by saying "heuristic" I mean an approximation solution. The problem is that even finding the approximation ratio for these heuristic algorithms is difficult, so my idea is to apply them on a data sets to see which one works better. I don't have data for now, although we can generate data randomly! I'm looking for a distribution model to generate random data sets to find a meaningful comparison of my algorithms! The main question would be how do you know what is the best distribution model? Thanks for your help! 
 My goal is to predict y, but my dependent variable has more than . I dont think multi-nomial model would be a good choice ? Any suggestions or pointers on what modeling methodology I should explore for this problem is much appreciated. Thanks in advance. 
 Predicting a discrete outcome with too many levels is a hard problem. Usually people do one vs. others approach, where you build many models and each model can detect one specific level of the output. Here is why: Think about you have a 100 side-dice, and you know the true distribution . Where $P(S_1)=0.1$, and $P(S_2)=P(S_3)=P(S_{100})=0.9/99=0.009090$. Now what you do with Maximum a posteriori estimation ? You will always guess you get the first side $S_1$, since it has largest probability comparing to others. However you will get wrong $90\%$ of the times!! For details, please check my answers in this post 
 F-test statistic is the ratio of two variance and if the value of F-test statistic equals to 1 or closer to 1 that means that those two variance are equal then we do not reject the null hypothesis.But when we want to check the equality of several mean we perform F-test.Here F-test statistic is the ratio of between mean sum square and within mean sum square. My question is ,why these two variances has to be equal , if they came from same population? How F-test check the equality of several mean? 
 I have been looking online at papers etc. about marginalized denoising autoencoders (mda) and everything I've found so far uses mda for pre-training layers for a classifier such as a support vector machine or logistic regression. Does anyone know of resources that show a use of mda for regression/prediction? I would like to try applying mda for time series prediction. 
 I have a dataset features vs. positions and I would to calculate the correlations of the positions. The data ranges from -1.5 to 0.5. The problem I am facing is that some columns have very limited ranges of values (mostly low negatives), but others have a broad range. When I plot all the data as a heatmap, it is easy to visualize the similarity between the columns. I want compare specific columns and to determine their similarity. When I try to correlate these similar looking columns with limited ranges of values, there is little correlation because it forms a cloud. Is there a way to quantify how similar/dissimilar each of these columns are to each other besides correlation? 
 I need to do a logistic regression that will likely have a lot of zeros. Can someone explain penalized logistic regression to me like I'm dumb? 
 You add a penalty to control properties of the regression coefficients, beyond what the pure likelihood function (i.e. a measure of fit) does. So you optimizie $$ Likelihood + Penalty $$ instead of just maximizing the likelihood. The elastic net penalty penalizes both the absolute value of the coefficients (the “LASSO” penalty), which has advantage of performing automatic variable selection by shrinking irrelevant coefficients to zero, and the squared size of the coefficient (the “ridge” penalty), which has been shown to limit the impact of collinearity. The level of penalization is typically chosen by cross-validation. You can use the R package glmnet to fit elastic net penalized models. 
 A distribution looks like this: modeled by an equation $y=1.0333x^2 - .5382x + 1.6905.$ Find the rate of change (i.e. the slope at that point of the regression equation) at point 6 (the x axis value), and give the standard error of that slope. The data set is this: 
 I'm modeling a sequence of discrete states as a first order, stationary Markov chain. I'd like to calculate a matrix of p values (one for each possible transition). Say the state at time $t$ is $S_t$. Given states $A$ and $B$, I want to test the null hypothesis that $P(S_t=B \mid S_{t-1}=A) \le P(S_t=B)$ against the alternative hypothesis that $P(S_t=B \mid S_{t-1}=A) &gt; P(S_t=B)$. Can anyone suggest a good way to do this? There are a couple hundred states. The 'true' transition matrix of the data-generating process is very sparse, meaning that each state can transition to only a few other states. All states transition to themselves with high probability, but there are no absorbing states. The data contain ~10,000 time points, but some transitions may only be observed several times. So, I'm probably not in the asymptotic regime. Does this rule out parametric tests? The closest method I've found is described in: Vautard et al. (1990) . Statistical significance test for transition matrices of atmospheric Markov chains. They randomly permute the sequence of observed states and estimate transition probabilities from the shuffled data. For each ordered pair of states, they calculate a p value as the fraction of shuffles for which the estimated transition probability exceeds that of the original data. My hesitation about this method is that 1) They don't clearly state a null hypothesis. 2) The permutation destroys temporal dependence between all states. But, I can imagine many more models where some states exhibit temporal dependence and others don't. The method doesn't seem to be counting these cases (but I don't know whether it matters). Another paper: Anderson and Goodman (1957) . Statistical inference about Markov chains. They give a $\chi^2$ test for the hypothesis that particular transition probabilities have particular values. Maybe I could use this to compare $P(S_t=B \mid S_{t-1}=A)$ to the estimated marginal probability of state $B$, but this doesn't seem to take into account the uncertainty of estimating $P(S_t=B)$ from the data. 
 P-value isn't a scale for correlation. For example, you mention "As far as I understand, this means that there is some, but low confidence ( p-value &lt; 0.15 ) for the Autism and Total cases." That's not how to interpret p-value. You need to understand the following: High P values: your data are likely with a true null. Low P values: your data are unlikely with a true null. So the closer you get to 0, your data will prove that your hypothesis is unlikely to be true. so 0.15 means that there is very little chance that the two are correlated. 
 If your problem is well studied and there are standard benchmarks in the literature, you should use them as input to your heuristics. For example, if you are dealing with Boolean SAT problem, here you can find a lot of different Boolean formulae, classified to several distinct types. It is important to use such standardized formulae with different structures and numbers of propositional letters and clauses for evaluating and comparing different (meta)heuristic algorithms because it turns out that completely random formulae are not that useful for this purpose. Most of random SAT instances are easy to solve even with very simple algorithms that are not able to tackle really hard instances (for which we are actually interested in). It is so because randomly created formulae do not have inner structure that makes them "hard" for SAT solvers. Also, there is a phenomenon called phase transition . Let $n$ denote the number of propositional variables in the given formula $F$, and $m$ stand for the number of clauses in a 3-CNF of the $F$. In brief, if the ratio $n/m$ is greater than the critical value $4.2$, there is a high chance that $F$ is not satisfiable, and if $n/m$ is lower than $4.2$, $F$ is almost surely satisfiable. Therefore, the hardest formulae to examine are those where $n/m=4.2$ and you should use such formuale as your benchmarks instances. However, if the problem in question is not well studied and there are no standard benchmark instances, you could use problem instances created with a random generator. In few papers that I've read where this was the case, the authors used uniform probability distribution to create problem instances. For example, when faced with selecting one of the elements $x_1, x_2, \ldots, x_k$, or when the generator had to choose which of the actions $A_1, A_2, \ldots, A_r$ to perform during creation, they were selected with probabilities $1/k$ and $1/r$, respectively. But, those were the first papers where this problem was examined and therefore such approach was acceptable. 
 so ive been selecting features for a regression problem and have obtained a list of the best performing feature sets. (note my list is actually several thousand lines long) 188.493 186.989 [379.45, 0.68, 99.51, 102.71, 109.91, 2.07] 50,12,48 188.352 187.391 [465.3, 0.63, 116.43, 134.18, 104.84, 2.3] 42,36,27 188.007 187.506 [443.08, 0.67, 93.73, 116.96, 110.67, 2.26] 50,42,27 185.867 192.012 [398.89, 0.81, 81.6, 99.44, 124.01, 2.41] 72,53,48 The first number is the MSE on 10foldCV on the training set while optimizing hyperparameters. The second number is the MSE on the test set. Third and fourth items are the hyperparameters and feature sets (not important) My question is: would the best model be the model that performed very best on the test set? or should I also be concerned with how it performed on the training set. For example, my fourth line, performed well on training set but much worse on the test set, while the first line, performed better on the test than the training. Should I be looking for feature sets that perform similar on both training data CV and test? or just take the best model on the test set? Or would it be best to use a combination of models? Any help is greatly appreciated. Thanks 
 I would appreciate assistance with the following: I am running a gamma-GLM model. As part of it, there is a two-way interaction between a categorical (2 levels) and an interval/continuous predictor. The continuous predictor is positively skewed, and non-normal. In my fiend (psychology) we mean-center predictors before computing the interaction. Here is the question: because of skeweness, mean is not a measure of central tendency for the proposed skewed predictor. 1) if I still mean-center it ignoring non-nirmality, would it affect my results (standard errors, CI's)? 2) what other way of centering would you suggest? I read about median-centering as a way to get less biased estimates. Would it make my model better, as median is also not a measure of central tendency (the distribution is positively skewed). Or I can pick a value (say mode) and do mode-centering? Thank you! 
 Say the input is $X$, the output is $Y$, and the fitted model gives the conditional distribution $P(Y \mid X)$. What you're looking to do is obtain a 'point estimate' from this distribution. Typically the mean $E[Y \mid X]$ is used. But, there's no reason you couldn't use the conditional mode, median, or even other quantiles. It just depends on your intended use. One way to resolve the question is if you're specifically interested in one of these features of the distribution. Otherwise, say you're just interested in prediction performance. In that case, you have to specify a loss function , which measures how bad it is to make errors of a given size. You then choose the point estimate that minimizes the loss. For example, the ever-popular squared loss says that badness is proportional to the squared magnitude of the error. The mean is the point estimate that minimizes this loss function. The median minimizes the absolute loss, which says that badness is proportional to the magnitude of the error. These notes may be of interest. 
 I want working on spam filtering and need some email datasets,containing both spam and ham mails. I have found a good source of spam mail dataset,but i also need legitimate mails for training my classifier. Please suggest some online datasets for email ,in which the sender ip is not hidden, not obsfucated and can be clearly seen and used. 
 In my experience, I'd use a model whose performance on training, cross validated and test data are the closest (not large gaps one way or the other) as this would likely be your most generalizable model. You haven't mentioned the size of your test set but generally, if you have only the cross validated and test MSEs as in your example, I'd pick the lowest cross validated MSE and ignore the test MSE. 
 Fishers exact test uses a hypergeometric distribution to compute the p-value when given two classification aspect and there respective group frequencies in a given dataset [ref: https://en.wikipedia.org/wiki/Fisher%27s_exact_test] . My question is how to do this for an uncertain/fuzzy dataset? Where a given class is only partially true for a specific data instance. As an example let's say that Dieting is 60% true for 1 case, 43% true for another and so on in the case of Men. Similar probabilities exist for Women as well. In that case how to compute the p-value as shown in the example of the Wiki reference given? More info: normal p-value computation in Fishers exact test using hypergeomtric distribution is as follows. Given the frequencies, Class1\Class2: Men; Women Diet: a;b Non-Diet: c;d P-value can be computed using, p-value = (a+b choose a)(c+d choose c)/(a+b+c+d choose a+c) 
 If tweaking the software and model architecture doesn't do the trick, there's another interesting approach. Say you have a large ensemble model (like a random forest) that has good prediction performance but slow runtime. It's possible to translate the ensemble model into a more efficient neural net. This paper describes how: Bucila et al. (2006) . Model compression. The idea is to generate synthetic, unlabeled data that mimics the distribution of the training data. Arbitrarily many synthetic data points can be generated (e.g. more than in the original training set). Alternatively, real unlabeled data can be used if a source is readily available. The synthetic data is fed through the ensemble model to generate labels. The synthetic data and labels are then used to train a neural network. Here's a talk by Geoff Hinton describing a similar approach. 
 OK, so I ran an experiment with 2 variables and 2 DVs. The theoretical model is that the interaction of these 2 variables (let's call the interaction X) impacts one of the DVs (Y) through the other DV (M). So we are dealing with mediation. My plan was to run 2X2 ANOVAs on the outcome variables and then use Preacher &amp; Hayes bootstrapping to show mediation. When I run the ANOVA with M (the expected mediator) as the outcome I get the expected significant interaction. However, when I use Y as the the outcome, the interaction and main effects are non-significant. Yet when I run Preacher &amp; Hayes MEDIATE program, with M mediating the effect of X on Y, I find a significant indirect effect (while the total and direct effects of X on Y are non-significant). Now can I make a claim of mediation? Even though in the ANOVA my manipulations did not have a significant effect on Y, but they did have a significant effect on M and the bootstrapping revealed a significant indirect effect of X on Y. So I am not entirely sure how to interpret this. 
 Pick the model that did best on the validation set. Models that do substantially better on the training set than the validation set have overfit and won't generalize well. Randomness will sometimes cause models to do better on a validation set than on a training set, but this is rare. The MSE on the validation set is not the final error though. Important Note: You can't use either the training or validation set to gauge your selected model's accuracy in production -- you'll need a third dataset, called the "test" dataset to do that. As soon as you pick a final model, all previous MSE measurements of that model become biased (error looks lower than it actually is) because of the selection bias that occurs when you pick the model with the lowest MSE. You'll need a third data set, the "test" dataset to gauge the final/selected model's true accuracy. See this question to better understand why you need a third dataset to gauge the final model's accuracy: What is the difference between test set and validation set? . 
 I have this experiment where there are two random vectors $P_1 = (x_1,y_1)$ and $P_2 = (x_2,y_2)$. These two vectors represents two measurements for the location of two nearby points ($10$ meters apart) using two independent sensors. I want to calculate the covariance matrix and correlation coefficient between $P1$ and $P2$ using $10,000$ measurements I have. I know how do this for two random variables but not in the case of two random vectors. 
 Have you considered Matlab's Systems identification toolbox ? If the standard state space modeling functions don't cut it, perhaps the grey box modeling functions will. 
 The increase in false-positive results arises primarily from an overestimation of effect sizes, leading to upward-biased effect sizes that often cannot be reproduced in follow-up studies (‘the winner's curse’). e.g. I have two populations with mean1=1.0, mean2=0.6, SD=0.3. if I draw from these populations n=8 per group and compare the mean of these samples than 50% have adifference of &gt;0.4. However if mean2=0.8, still ~10% of the comparisons have a difference of &gt;0.4. EDIT: Let me rephrase: what I am looking for is a technique which calculates me the risk of ascertainement bias, effect inflation or the winner`s curse. There are some ways for genetic assocition studies (see comments below) however as I am not a statician I can not use the published parameters for my problem... Maybe somebody can answer one simple question: Is the question valid at all? 
 Don't optimize the seed It's okay to loop over seeeds 1 to 10 and use an (unsupervised!) internal measure to choose the "best". But messing around witgh the seed to get the best result is overfittting . You cannot do this on real data. If the seed is important, then the algorithm failed If the results vary a lot, this shows they are not stable, but random . If the method doesn't reliably produce a good result, consider the results to be "not better thsn random". 
 I saw people test it by plotting the residual term and its histogram. I wonder if there is a method in R that does the test. Thank you. 
 Centering a numerical predictor around some value just amounts to reparameterization of the model. The model \begin{equation} y = a + b x + e \end{equation} is equivalent to the model \begin{equation} y = a' + b' (x-x_0) + e \end{equation} The relationship between the two parameterizations can be seen by rewriting the reparameterized model on the same form as the original model: \begin{equation} y = \underbrace{a'-b'x_0}_a + \underbrace{b'}_b x + e. \end{equation} From this we see that $a = a'-b'x_0$ so intercepts in the two models are different but since $b=b'$, the slopes are equal. So any kind of centering around some value $x_0$ does not influence the interpretation of the slope nor estimates of its standard error, confidence intervals or associated hypothesis tests. Similar things holds true if your model includes an interaction between $x$ and some factor $i$. Then \begin{equation} y = a + b x + c_i + d_i x + e \end{equation} is equivalent to the $x_0$-centered model \begin{equation} y = a' + b' (x-x_0) + c'_i + d'_i (x - x_0) + e. \end{equation} which can be rewritten as \begin{equation} y = \underbrace{a' - b' x_0}_{a} + \underbrace{b'}_b x + \underbrace{c'_i - d'_i x_0}_{c_i} + \underbrace{d'_i}_{d_i} x + e. \end{equation} From this we see that $d_i = d'_i$ so the null hypothesis $d_1=d_2=\dots=d_k$ (no interaction in the non-centered model) is equivalent to null hypothesis $d'_1=d'_2=\dots=d'_k$ (no interaction in the $x_0$-centered model). So in terms of significance of the interaction term, mean- and median-centering is of no consequence. Estimates of main effects of the factor $i$, $c'_i$ vs $c_i=c'_i-d'_i x_0$, will be different though and hence their significance will be different too, but you usually test for significance of the main effects before significance of interactions. And without an interaction present, the significance of a factor $i$ is not influenced by any kind of centering of $x$. Only if you believe that a reasonable null hypothesis is that $i$ has no effect on average (for an average value of $x$) given that the interaction is present, do you need to think more carefully about this. 
 I have a theoretical problem, suppose we have a material like marble.. there exist different kinds of marblings, depending on the mineralisation grade of marble, and each of those mineralisations show a different structure of their shapes... here's a picture: each of the structures is defined by parameters like width, elongation, radius, and so on, there exist about 40 different parameters, some may be dependent on each other... the goal is to classify the structures (or type of minerals) -which are the classes (labels)- from the parameters, and of course the other way around, make conclusions to the parameters from the type of minerals... this is not an image detection problem, as the dataset with the parameters does already exist.. but its a classification problem... my question is, how should I approach the problem? of course the first thing is to understand each parameter of the dataset and what it means.. then maybe a correlation analysis of the variables? if there are any variables that are dependent on eachother and can be eliminated? and which variable would you choose to eliminate then? how do I check which of the variables are more meaningful than others? the main problem is probably to guess which classification models is best suited for that sort of task, I think I would start easy by trying out decision trees, Clustering and SVM's ? or random forests, neural networks? what would you think is best suited for that problem and what would you choose? 
 I have a pilot study about testing 2 drugs and a control (placebo) to see which drug is more effective. The design is as follows: Drug 1 is given to 5 random people then the response is measured 5 times for each person. Drug 2 is given to a different 5 random people and the response is measured 5 times for each person The control group: placebo is given to a different 5 random people and the response is measured 5 times for each person. My question is: using this pilot study, how to determine the sample size for each group? I mean how many patients should be in each category (drug 1, drug 2, control, not the measurements). The response is protein. More is worse. 
 You are essentially testing residuals for heteroskedasticity. In R, you can code the Breusch Pagan test (main heteroskedasticity test) using the lmtest package and the bptest () function. You can also use the car package and code the same Breusch Pagan test with the ncvTest () function. 
 The study you cite necessarily went a good deal farther than simply counting up the number of cases, as you did. It took into account the age at which a child received MMR vaccine (counting the child as unvaccinated up until that point), the age at vaccination, the time since vaccination, the year when the vaccine was given, and the age at diagnosis, in a time-dependent model of incidence-rate ratios. It also corrected for the child's gestational age at birth, birth weight, sex, mother's education, and socioeconomic status. Those types of corrections are very important in this type of population health study, as those other factors might also be associated with the outcome of interest (here, risk of autism or other ASD). When those factors were correctly taken into account, the relative risk of "autistic disorder" was nominally lower in those that were vaccinated, by a factor of 0.92; this was not, however, significantly different statistically from a relative risk of 1 (or no difference in risk). Similarly, the relative risk of other ASD was a (non-significant) factor of 0.83 in the vaccinated versus unvaccinated. So the main things you are "missing" is that the study's statistical analysis examined relative risks of developing autism or ASD in time-dependent models, not just the yes/no of whether these occurred, and that other critical variables were properly considered. 
 I'm reading this book by Allen B. Downey and trying to do the exercises I am a bit stuck at this one, 7.4. I tried looking for blogs and stuff like that where people would discuss the solutions, but couldn't find anything Suppose you are the manager of an apartment building with 100 light bulbs in common areas. It is your responsibility to replace light bulbs when they break. On January 1, all 100 bulbs are working. When you inspect them on February 1, you find 3 light bulbs out. If you come back on April 1, how many light bulbs do you expect to find broken? In the previous exercise, you could reasonably assume that an event is equally likely at any time. For light bulbs, the likelihood of failure depends on the age of the bulb. Specifically, old bulbs have an increasing failure rate due to evaporation of the filament. This problem is more open-ended than some; you will have to make modeling decisions. You might want to read about the Weibull distribution ( ). Or you might want to look around for information about light bulb survival curves. The book takes a practical approach, but I'm a bit more comfortable with the theory first. $x$ is the number of lights that will go out after another 2 months (3 total) $data$ will be the information that there were 3 lights out after the first month $\lambda$ and $k$ are the parameter for the Weibull So I got this far $P(x|data)=\sum_{\Lambda,K}{P(x|\lambda, k)P(\lambda,k|data)}$ and $P(\lambda,k|data)=\frac{P(data|\lambda,k)P(\lambda)P(k)}{\sum_{\Lambda,K}{P(data|\lambda, k)P(\lambda)P(k)}}$ I think the math is good, I'm just not sure how to use the data or even if I'm setting $x$ right. The Weibull is the time to failure. So how does having knowledge that 3 light bulbs went out in the first month help me in figuring out the parameters of the distribution? What if 6 went out? What if they were still all good? What if it was 3 out of 200? I'm thinking that I should be looking for something more along the lines of $P(time\ to\ failure &lt; 3\ months|data)$ Any help would be appreciated. Or if anyone know of a place where we can find the solutions that would be perfect. 
 This answer describes three ways to handle the varying sample sizes appropriately: a Generalized Linear Model and two weighted Ordinary Least Squares regressions. In this case all three work well. In general, when some proportions are near $0$ or $1$, the GLM is better. Because the sample sizes are so small compared to the populations (less than ten percent of them), to an excellent approximation the distribution of blue-eyed and non-blue-eyed results in a sample of size $n$ is Binomial (because the samples are random). The other Binomial parameter, $p$, is the true (but unknown) proportion of blue-eyed subjects in the population. Thus, the chance of observing $k$ blue-eyed people is $$\binom{n}{k}p^k(1-p)^{n-k}.\tag{1}$$ Each decade we know $n$ and $k$--those are given by the data--but we don't know $p$. We may estimate it by assuming that the log odds corresponding to $p$ varies by year linearly (at least to a good approximation). This means we assume there are numbers $\beta_0$ and $\beta_1$ such that $$\log(p) - \log(1-p) = \beta_0 + \beta_1 \times \text{Year}.$$ Equivalently, $$p = \frac{1}{1 + e^{-\beta_0-\beta_1\text{Year}}};\ 1-p = \frac{ e^{-\beta_0-\beta_1\text{Year}}}{1 + e^{-\beta_0-\beta_1\text{Year}}}.$$ Plugging this into (1) gives the chance of observing $k$ out of $n$ during a given year $t$ as $$\binom{n}{k} \frac{e^{-(\beta_0+\beta_1t)(n-k)}}{\left(1 + e^{-(\beta_0+\beta_1t)}\right)^n}.\tag{2}$$ Assuming the samples are independently obtained at years $t_1, t_2,$ etc and writing the corresponding sample sizes and counts of blue-eyed subjects as $n_i$ and $k_i$, the probability of the data is the product of the probabilities of the individual results. This product is (by definition) the likelihood of $(\beta_0, \beta_1)$. We may estimate these parameters as the values $(\hat\beta_0, \hat\beta_1)$ that maximize the likelihood; equivalently, they maximize the log likelihood $$\Lambda(\beta_0,\beta_1) = \sum_t \log\left(\binom{n}{k} \frac{e^{-(\beta_0+\beta_1t)(n-k)}}{\left(1 + e^{-(\beta_0+\beta_1t)}\right)^n}\right)\tag{3}$$ obtained from $(2)$. (This simplifies considerably, using rules of logarithms, which is one reason to express the time-proportion relationship in terms of log odds. When all proportions are between $0.2$ and $0.8$, approximately, there is little qualitative difference between using probabilities $p$ or their log odds: the fitted curve will be linear or close to linear, respectively.) $(3)$ is a Binomial Generalized Linear Model . It must be fitted by numerically minimizing $\Lambda$. The procedure in (shown at the end of this post) gives the solution $$(\hat\beta_0, \hat\beta_1)_\text{GLM} = (31.498711, -0.0163568).$$ The data in this figure are plotted with disks whose areas are proportional to the sample sizes. The GLM fit is curvilinear. Shown for comparison, in gray, is the line we would get just by dumping the $(\text{Year},\text{Proportion})$ data shown in the question into an Ordinary Least Squares solver. Both fits are influenced by the greater proportions in earlier years, despite the small sample sizes then. However, the GLM fit does a better job of approximating the proportions in the largest samples obtained in 1970 and 1980. The dotted blue line is described below. By adding a quadratic term we can test the goodness of fit. It significantly improves the GLM fit (although visually the difference is not great), providing evidence that this model does not describe the variation in results well. Looking at the plot indicates the result in 1990 was much lower than the model predicts. An alternative, but comparable, approach is to estimate $p$ individually for each year $t_i$, perhaps as $k_i / n_i$ (although other estimators are possible). A linear regression of the log odds of these estimates against the year, weighted by the sample sizes $n_i$ , or Weighted Least Squares regression, yields $$(\hat\beta_0, \hat\beta_1)_\text{WLS} = (36.12744, -0.018706).$$ The standard errors of these estimates are $15.55$ and $0.00787$, respectively, indicating that the WLS estimates are not significantly different from the Binomial GLM. (The GLM's standard errors are considerably smaller, though: it "knows" these sample sizes are pretty large whereas the linear regression "knows" nothing about the sample sizes at all: it only has a sequence of ten separate observations.) Note that this alternative might not be available if $k_i=n_i$ or $k_i=0$, unless a different estimator of the probabilities is used (which doesn't produce values of $0$ or $1$). Finally, we might simply perform a weighted least squares regression of the raw probability estimates $k/n$ against the year, inversely weighted by an estimate of sample variance. The variance of a Binomial$(n,p)$ variable $X$, re-expressed as a proportion $X/n$ is $p(1-p)/n$. That may be estimated from a sample as $$p(1-p)n \approx \frac{k}{n}\frac{n-k}{n}/n = \frac{k(n-k)}{n^3}.$$ Its result appears in the figure as a dotted blue line. In this case it appears to compromise between the GLM and OLS fits. The following code performed the analyses and produced the figure. 
 The model is $$\mathbb{E}(y) = \beta_0 + \beta_1 x + \beta_2 x^2.$$ Differentiating with respect to $x$ gives the slope at any $x$, $$\frac{d\,\mathbb{E}(y)}{d\,x} = \beta_1 + 2\beta_2 x.$$ Like the model for $y$ itself, this is a linear combination of the parameters $(\beta_0, \beta_1, \beta_2)$. That is key. Obtain estimates of the coefficients, $(\hat\beta_1, \hat\beta_2)$, in any way you like, along with their covariance matrix $$\Sigma=\text{Cov}(\hat\beta_1,\hat\beta_2).$$ Thus, $\Sigma_{ii}$ gives the estimation variance of $\beta_i$ and $\Sigma_{12}=\Sigma_{21}$ gives their covariance. With these in hand, estimate the slope at any given $x$ as $$\widehat{\frac{d\,\mathbb{E}(y)}{d\,x}} = \hat\beta_1 + 2\hat\beta_2 x.$$ Using the standard rules to compute variances of linear combinations, its estimation variance is $$\text{Var}\left(\widehat{\frac{d\,\mathbb{E}(y)}{d\,x}}\right) = \text{Var}(\hat\beta_1 + 2\hat\beta_2 x)= \Sigma_{11} + 4x\Sigma_{12} + 4x^2\Sigma_{22}.\tag{1}$$ Its square root is the standard error of the slope at $x$. This easy calculation of the standard error was possible due to the key observation previously made: the estimated slope is a linear combination of the parameter estimates. Estimate higher derivatives, partial derivatives, and all their variances in a multiple regression model using the same techniques: differentiate, plug in the estimated parameters, and compute the variance. For these data, $\Sigma$ is calculated (in ) to be Using this, I drew one thousand randomly generated tangent lines for $x=6$ (assuming a trivariate normal distribution for $(\hat\beta_0,\hat\beta_1,\hat\beta_2)$) to depict the variance of the slope. Each line was drawn with high transparency. The black bar on the figure is the cumulative effect of all thousand of those tangents. (On top of it is drawn, in red, the estimated tangent itself.) Evidently, the slope is known with some certainty: its variance (by formula $(1)$) is only $0.024591$. Since the intercept of the curve itself is much less certain (its variance is $2.427$), most of these tangents differ only in elevation, not in angle, forming the tightly collimated black bar you see. To show what else can occur, I added independent Normal errors of standard deviation $10$ to each data point and performed the same construction for the base point $x=2$. Now the slope, being much less certain, is manifest as a spreading fan of tangents. 
 I'm looking for an open and relatively current (last year or two) dataset of companies of all sizes by revenue/employee size for testing some models to predict company size/revenue. Does such a set exist in the public domain? 
 At the moment, this is only a partial answer, which is intended to stimulate discussion. In general, AIC, AICc and BIC make specific assumptions concerning likelihood that can sometimes be irrelevant to the physical problems under consideration. For example, let us consider AIC in the context of Tikhonov regularization. There are many criteria that can be applied to selecting smoothing factors for Tikhonov regularization. To use AIC in that context, there is a paper that makes rather specific assumptions as to how to perform that regularization, Information complexity-based regularization parameter selection for solution of ill conditioned inverse problems . In specific, this assumes "In a statistical framework, one solves an ill conditioned inverse problem y = Ax + ε, with A ∈ Rn×m, y ∈ Rn , x ∈ Rm and n m, by specifying a parametric family of conditional probability distributions with densities f (y|A, x), choosing a penalty p (x) that assesses the physical plausibility of the solution, choosing the value of the regularization parameter α , and by using the maximum penalized likelihood (MPL) method....If we consider uncorrelated Gaussian noise with variance $\sigma ^2$ and use the penalty $p(x) =$ some norm, the MPL solution is the same as the Tikhonov (1963) regularized solution." The question then becomes, why should anyone make those assumptions? For the problems I deal with, the residuals are Student's t with low degrees of freedom, and too poorly behaved to make a Gaussian assumption. There is no known solution so that a penalty function cannot be specified, and a goodness of fit criterion will not allow for specifying the parameters that one wishes to extract from the modelling process. Despite all that, an inverse solution can be found using Tikhonov regularization to minimize relative error of whichever parameter one desires to measure as my particular measurement system is a proportional error measurement system. See Tikhonov adaptively regularized gamma variate fitting to assess plasma clearance of inert renal markers . So, yes one can "use" AIC with Tikhonov regularization, that is, as long as one uses smoothing that has nothing to do with the problem and for which no particular parameter values are being extracted. And the purpose of doing that was.....wait for it.....to get AIC. And, then what? Do I get a prize or something? I mean no offense to those using AIC, but it looks more like as solution waiting for a problem to which it applies than a method for understanding solutions. Here is a link that features some regression types , about a dozen or so out of many many others. AIC and BIC are mentioned for only one specific regression type. I think that the more one knows about regression, the less frequently AIC use appears to be appropriate. Certainly one can find many references about AIC use for many problems. However, that pales in comparison to the much larger set of conditions in which AIC use would be intractable to define. I would argue that unless the exact conditions for unbiased ordinary least squares in y are met, then the conditions for AIC are not met. That exact condition includes having no x-axis data uncertainty as applies only for special circumstances, like when exact equal interval time series are plotted. 
 I want to know if neural network (specifically BP network) can handle data with considerable missing attributes (like 50% of the attributes are lost). For instance, I have a set of samples. Each of them has 100 dimensions. However, due to some reason, some dimensions of attribute are missing, and simply has 0.0 values, and each sample has different dimensions lost. I would like to know is it possible to train a neural network on this type of data samples? Additionally, once after a network has been trained, is it possible to generate correct prediction on the testing samples (also has attributes lost as the training samples)? If NN can not deal with this type of data, is there any alternative way can handle this naturally, Thanks. 
 If $X$ and $Y$ are correlated (univariate) normal random variables and $Z = AX+BY+C$, then the linearity of expectation and the bilinearity of the covariance function gives us that \begin{align} E[Z] &amp;= AE[X] + BE[Y] + C,\tag{1}\\ \operatorname{cov}(Z,X) &amp;= \operatorname{cov}(AX+BY+C,X) = A\operatorname{var}(X) + B\operatorname{cov}(Y,X)\\ \operatorname{cov}(Z,Y) &amp;= \operatorname{cov}(AX+BY+C,Y) = B\operatorname{var}(Y) + A\operatorname{cov}(X,Y)\\ \operatorname{var}(Z) &amp;= \operatorname{var}(AX+BY+C) \quad = A^2\operatorname{var}(X) + B^2\operatorname{var}(Y) + 2AB \operatorname{cov}(X,Y), \tag{2}\\ \end{align} but it is not necessarily true that $Z$ is a normal (a.k.a Gaussian) random variable. That $X$ and $Y$ are jointly normal random variables is sufficient to assert that $Z = AX+BY+C$ is a normal random variable. Note that $X$ and $Y$ are not required to be independent; they can be correlated as long as they are jointly normal. For examples of normal random variables $X$ and $Y$ that are not jointly normal and yet their sum $X+Y$ is normal, see the answers to Is joint normality a necessary condition for the sum of normal random variables to be normal? . As pointed out at the end of my own answer there, joint normality means that all linear combinations $aX+bY$ are normal, whereas in the special case being discussed there, only one linear combination $X+Y$ of non-jointly normal random variables is proven to be normal; most other linear combinations are not normal. More generally, if $X$ and $Y$ are (column) $n$-vector random variables with $n\times n$ covariance matrices $\Sigma_{X,X}$, $\Sigma_{Y,Y}$, and $n\times n$ crosscovariance matrix $\Sigma_{X,Y}$, $A$ and $B$ are $m\times n$ nonrandom matrices, and $Z$ and $C$ (column) $m$-vectors, then it is indeed true that \begin{align} E[Z] &amp;= AE[X] + BE[Y] + C &amp;\quad \scriptstyle{\text{compare with } (1)}\\ \Sigma_{Z,Z} &amp;= A\Sigma_{X,X}A^T + B\Sigma_{Y,Y}B^T +2A\Sigma_{X,Y}B^T &amp;\quad \scriptstyle{\text{compare with } (2)}\\ \end{align} but, as in the univariate case, it is not necessarily true that $Z$ is a normal vector (in the sense that the $m$ components $Z_i$ are jointly normal random variables). Once again, joint normality of $(X_1, X_2, \ldots, X_n, Y_1, Y_2, \ldots, Y_n)$ suffices to allow the assertion that $Z$ is a normal random vector. 
 I am trying to fit Negative binomial, Zero Inflated Negative Binomial, Negative Binomial Hurdle, and Random effects negative binomial. Here is the value of AIC for different model: Negative Binomial: 29178 ZINB: 27378.85 Hurdle Model NB: 28525.85 Random effects negative binomial: 31969.2 As far as I know lower the AIC, better the model. Is it true if I compare different types of model? Can I use AIC for comparing Random effect and fixed effect Negative Binomial Regression? Is there any other way to compare models? Thanks in advance. 
 I have the following expectation $$E[x_{t+1} \mathbf{1}_{\{x_{t+1}&gt; z_t\}}]$$ where $x_{t+1}$ is a normally distributed random variable $x_{t+1}\sim N(0,\sigma^2)$, and $\mathbf{1}$ stands for the indicator function. $z_t$ is a function of variables, as in $z_t = f(y_t,p_t)$, which I eventually need to solve for. I need an analytical representation of this expectation. Please let me know if this is unclear or need to add additional info. I am clearly new here. 
 Let $\mathbf{x}$ be a random vector. In matrix notation, the covariance matrix can be expressed as: $$ \Sigma = E\left[\left( \mathbf{x} - E[\mathbf{x}]\right) \left(\mathbf{x} - E[\mathbf{x}]\right)' \right] $$ The sample analogue is: $$ \hat{\Sigma} = \frac{1}{n-1} \sum_i \left( \mathbf{x}_i - \hat{\boldsymbol{\mu}}\right) \left(\mathbf{x}_i - \hat{\boldsymbol{\mu}}\right)' \quad \quad \hat{\boldsymbol{\mu}} = \frac{1}{n} \sum_i \mathbf{x}_i $$ Let $\mathbf{x}_i$ be a $k$ dimensional vector representing the ith observation. Something standard is to put your $n$ observations in an $n$ by $k$ data matrix $X$. $$ X = \left[ \begin{array}{c} \mathbf{x}_1' \\ \mathbf{x}_2' \\ \ldots \\ \mathbf{x}_n' \\ \end{array} \right] $$ You should be able figure out what to do with $X$ to compute the sample covariance matrix. (Eg. what does $X'X$ do...) (Note: bold letters are vectors, upper case are matrices, and lower case are scalars.) Matlab comment: In Matlab, you can easily follow the formulas exactly: make a data matrix $X$, compute $\hat{\boldsymbol{\mu}}$, and compute $\hat{\Sigma}$. There are also built-in functions, and respectively, which will do it for you. 
 You can use a Bayesian technique. This gives you the posterior distribution of the sample means for comparison. 
 I'm making a comparison between the model I build and the data (empirical distribution), using various plots, like this: Histogram and probability densiti function (PDF) CDF CDF at different axis (Weibull Axis) The bule color is the data (empirical distribution), the black and red curve is the model. The problem is that, what color scheme should I chose? I mean, in the 1st plot, I use black to represent model, but in 2nd, and 3rd plot, I use red instead, because it contrasts better with blue dots. I'm afraid this inconsistancy would result in confusion, because there are only 2 type of data (Model vs. Empirical), yet I used 3 colors: UPDATE: After changing the black to red 
 I am working on a project where I calculate the time between two different type of treatments (e.g. A and B). I am trying to assess what factors predicts this duration (measured in days). I use SAS. I have tried multivariable regression of both the duration and the log of duration. Both of these fails the normality of residuals assumption. I have tried poisson model but there is overdispersion. I m not sure what to do now. Can anyone help? 
 This is a rather theoretical answer (it may not be useful practially) that rests on these two assumptions that summarize how I understood your question The system is such that, given $(C, b_0)$ ($b_0 \Leftrightarrow b_{initial}$), the path $b_1, ..., b_t$ is completely determined (deterministic) and given by the dynamic $b_s = b_{s-1} + C\sqrt{b_{s-1}}$. You know the joint prior $f(C, b_0)$ (not only the normal marginals you mentioned). If this is false please comment and I will remove the answer. If not, then the following should hold: We want the update rule $$f(C, b_0|b_u)=\frac{f(C, b_0,b_u)}{f(b_u)}=\frac{f(b_u|C, b_0)f(C, b_0)}{f(b_u)}$$ (in your example, $u = 1500, b_u = 0.005$). First $$b_s = b_{s-1} + C\sqrt{b_{s-1}} = b_{s-2} + C\sqrt{b_{s-2} + C\sqrt{b_{s-2}}}=...=h_s(C, b_0)$$ where $\{h_s(\cdot, \cdot)\}_{s\in\{1...t\}}$ are known and differentiable functions. Therefore $$f(b_u|C, b_0) = \delta(b_u - h_u(C, b_0))$$ where $\delta(\cdot)$ is the dirac delta. And thus $$f(b_u) = \int_{C, b_0}\delta(b_u - h_u(C, b_0))f(C, b_0)dCdb_0 = \int_{K(b_u)} f(C(t), b_0(t))dt$$ where $K(b_u)$ is a subset (a path) in $\mathbb{R}^2$ such that $$K(b_u) = \{(C, b_0) \in \mathbb{R}^2: b_u - h_u(C, b_0) = 0\}$$ Finally, we have that the update rule is $$f(C, b_0|b_u) = f(C, b_0)\frac{\delta(b_u - h_u(C, b_0))}{\int_{K(b_u)} f(C(t), b_0(t))dt}$$ Intuitively, this says that, for any given $b_u$, the density for a point $(C, b_0)$ will be zero if, starting from that point and following the dynamic we don't arrive at $b_u$ at time $u$. And when we do arrive to that point, then the updated density of the point $(C, b_0)$, given $b_u$ will depend on the ratio $$\frac{f(C, b_0)}{\int_{K(b_u)} f(C(t), b_0(t))dt}$$ which corresponds to the density of this point $(C, b_0)$ relative to the total mass of the set $K(b_u)$. I find this very intuitive. 
 Besides the interpretations you mention, you can think of the normalizing constant as the value of the prior predictive distribution at the observed x. If the prior predictive is discrete then this is a probability mass, and if the prior predictive is continuous it is a probability density. The prior predictive is in the continuous case is $$ p(x) = \int_\Theta p(\theta)p(x|\theta) $$ Which is a distribution that assigns probability mass/density to the outcomes in the sample space. Then when x is observed it is fixed at the observed x and fits in the denominator of Bayes' theorem. However, note that with continuous distributions there is no mathematical constraint on the density value assigned to a set with measure zero (i.e., zero probability), and since any specific point on a continuous distribution indeed has measure zero then technically the value of the density on the prior predictive at exactly x can be set arbitrarily. But that aside, I think this way of visualizing the normalizing constant is fairly intuitive. You can read more here . (Let me know if you don't have access) This too, which is a bit more modern. 
 Relating to the Surv function in R. Could you please explain this to someone without an extensive statistics background? I need to know this for my research. 
 I am trying to analyze data from 15 different populations, each of which has a different sample size (ranging from 4-200+), to be able to compare data from the same question across populations with very different sample sizes. For example, I want to compare population A's (n=4) 100% response with population B's (n=200) 75% response to the same question and with 11 other populations. The data set is nearly 1000 entries. I am using SPSS. Are z-scores the best way to standardize? Would calculating z-scores and then looking at the mean z-score for each population allow me to compare responses to the same question across the various populations? What do you suggest? 
 The estimation method is described in section 2.2 (from page 62) of Mixed-Effects Models in S and S-PLUS by Pinheiro and Bates ( link ). The function uses a least-squared method to obtain the values. If another estimation method is used, it can obtain MLEs, e.g. without the 100 intercepts in the example. The 100 intercept values in the example are byproducts of the specific estimation method. 
 Let's say I have a site on which users buy and sell products from each other, and I wish to see which products in a particular category (defined by me) are purchased most frequently. A) What is the minimum data set that will allow me to say with reasonable confidence that Product A will find a buyer whereas product B will not? B)How do I go about selecting an algorithm to make such a prediction? Is there a "most efficient algorithm" that can make the best predictions with the least data? 
 estimate b value is -2.871 and the exponentiated b value is 0.084.The dependent variable is income. It is in ordinal level of measurement. how do i interpreted this value in ordinal regression. 
 Someone posted a similar question here but i couldn't get my job done see i want to define my user_metric and use it in KNN I have a signature problem it seems but i don't understand it. thanks def mydist2 (x,y):ValueError Traceback (most recent call last) in () ----&gt; 1 neigh.fit(traindata,train_labels) 2 neigh.score(testdata,test_labels) C:\Users\Fagui\Anaconda2\lib\site-packages\sklearn\neighbors\base.pyc in fit(self, X, y) 801 self._y = self._y.ravel() 802 --&gt; 803 return self._fit(X) 804 805 in _fit(self, X) 256 self._tree = BallTree(X, self.leaf_size, 257 metric=self.effective_metric_, --&gt; 258 **self.effective_metric_params_) 259 elif self._fit_method == 'kd_tree': 260 self._tree = KDTree(X, self.leaf_size, (sklearn\neighbors\ball_tree.c:8381)() (sklearn\neighbors\dist_metrics.c:4032)() (sklearn\neighbors\dist_metrics.c:10628)() as a bonus question, i'd like to pass gamma as an argument thanks very much 
 I am considering the following conceptual question from Introduction to Statistical Learning , chapter 3, number 4. I collect a set of data ($n$ = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$. (a) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = \beta_0 + \beta_1X + \epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. (b) Answer (a) using test rather than training RSS. The community around the text has answered this in terms of model flexibility. The cubic polynomial makes a tighter fit against the training data and has a smaller training RSS. The overfit of the training data causes a higher test RSS. My question is about the cubic fit on the data with an underlying linear relationship. Wouldn't a cubic regression identify the lack of importance of $X^2$ and $X^3$ as predictor variables? I prepared some sample data to prove this for myself: linear fit on linear data cubic fit on linear data Am I missing something in my thinking? 
 You just need to choose some sensible options in . Here I used constrained nls, forcing the coefficients to have opposite signs. I ran this after your code above: The fitted curve is the light dashed line right up the middle of the purple curve 
 I have panel data that I need to calculate a unit-root test on. The data is unbalanced and has gaps. Does anyone know of a library that handles unbalanced panel-data with gaps? I have tried the command but it does not handle unbalanced panel data as has been documented here . 
 In at one point it uses $P(X_0, \cdot)$ notation, but I don't understand what does it mean. I also don't understand how variation distance function ($d$) can accept $(X_0, \cdot)$ when it should take only distributions. 
 In El Ghaoui et al. (2010), "Safe feature elimination in sparse learning" and following works, screening rules are derived for Lasso (as well as other L1-penalized problems): $ \min_w \|y-X w\|^2 + \lambda \|w\|_1$. The SAFE rule in particular guarantees $w_i = 0$ if $|X_i ^T y| &lt; \lambda - \|X_i\|_2 \|y\|_2 \frac{\lambda_{max}-\lambda}{\lambda_{max}}$, where $\lambda_{max} = \|X^T y\|_\infty$. Suppose instead I have different regularization penalties for different coefficients: $ \min_w \|y-X w\|^2 + \|\lambda \circ w\|_1$. Is the corresponding SAFE rule now $|X_i ^T y| &lt; \lambda_i - \|X_i\|_2 \|y\|_2 \frac{\lambda_{max}-\lambda_i}{\lambda_{max}}$? 
 I am running FE and RE regressions and my Prof. suggested that I run unit-root tests. I have been trying to do that with R but am not able to find a library that will handle unbalanced panels with gaps. My question: can the unit-root test be performed on a subset of the larger model, i.e. can I use a panel with as few as 3 columns: date, group, variable_of_interest? Essentially I am trying to remove the gaps in my data by downsizing to the essential variables, but I am not sure how this will effect the results of the unit-root test. 
 I have used regression to explain a dependent variable with a number of independent variables. I expected independent variables which have higher correlations with the dependent variable to have higher beta weights in the regression model but this is not the case for some variables. That is, there are some independent variables with very small correlations with the dependent variable but with large and statistically significant betas. And some independent variables with greater correlations with the dependent variable but smaller betas. Can someone explain why this has happened? Thanks Anthony 
 I want to perform cluster analysis on the data of a website. The data is mainly visitor history(97000 rows) and has following variables: a)User Device Category b) Traffic Marketing Channel c) Traffic Source d) Marketing Campaign e) Website Landing Page f) User Session Duration g) User Goal Completion Apart from session duration and goal completion rest of the data is categorical. In particular traffic source,campaign,landing page etc have 100's of levels each. Also I believe the goal completion is sparse as most of the time it is 0. Would it be a good idea to: bin the session duration to categories. eg. 1000 seconds to 3000 sec.is one category and so on if a level of categorical variable is not too frequent then just write "others" combine certain levels if they have similar characteristic. Could someone give me tips on how to handle the categorical data, which package in R can handle mixed data type and what would be a good way to interpret the results. I am fairly new to data analysis and need all the help I can get. Thanks a lot in advance! 
 Imagine we have already built our linear regression model, with a certain dataset. Which order of tests would you follow to be sure that whatever conclusions you may want to extract are correct? For example: In the beginning we may want to test for non-linearities not present in our model, but present in reality (model misspecification). For this objective, we could use RESET. What others can we use and why? Now we could want to test for heteroskedasticity(Breusch-Pagan and White's tests Endogeneity (Durbin-Wu-Hausman test) ... Which 'game plan' do you use and why? 
 I ran a linear regression example in R and as a result got the following summary: As you can see, we get the t-values. In this case, how can i change my type of test from t to z? Furthermore, both t and z tests assume normality. So what if i want to run the Wilcox Test? 
 For z-score, you can manually enter the formula based on your current results. If is the vector with raw data then usually is a vector with standardized scores. You can enter it manually this way: 
 You can train on this data (just keep the missing dimensions on zero, or try to put in the mean instead of 0.0), only it depends completely on the data if correct predictions can be made. The only way to find out is by training the neural network and evaluating it. 
 I wanna perform parameter estimation of an underlying AR(p) process given some data. Let's say it's stable. For example an AR(2) process is stable when the conditions $a_2 - a_1 &lt; 1,$ $a_2 + a_1 &lt; 1,$ and $-1 &lt; a_2$ are all satisfied by the AR coefficients $a_1, a_2$. Since this is a small triangle in the $a_1$-$a_2$ space which contains the origin (see e.g. the middle of this blog post for picture), I was thinking whether putting a Gaussian prior is valid. (Actually, the mean is (0, -$\frac13$), which is only close to zero.) Since I kinda think this is reasonable for order 2, and there are other (more complicated) formulas for the coefficients of a higher order processes, I wonder if this is true in general. 
 Currently, I am working with with Kendall tau distance between two rankings. I get a value of , (statistical significance of ) and a of . What does this Kendall score tell us about? Here under are my results and I would appreciate if anyone could enlighten me with these values of and - if they are worthy of adding to a research evaluation. Or is only the and are enough to tell the story? 
 Is it reasonable to use Cohen's d guidelines in the interpretation of a Cox logit d'? 
 I have a time series model that suffers from endogeneity. In other contexts it would be reasonable to use instrumental variables. However, I have not seen this done before with time series. Can I ask if it is valid to use an IV in this context and if there are any examples in the economics literature? 
 Your argument is correct. Since the true relationship is linear, the square and cubic terms in the cubic fit are not significant. And that is confirmed by the large p-values (0.215 and 0.461). As mentioned by @mark999, it seems you are using a very large sample size because the standard errors are so small. If you follow the original question and use $n=100$, the numbers would be different but the conclusion will stay the same. 
 First I want to say I'm completely new to the machine learning paradigm and have only discussed it in theory. I have been trying to put it into practice but I'm confused on how to derive the dataset in a way that would allow me to reverse engineer the following problem: Let's say we have a dataset with a few traits(attributes for a player) and we are trying to reverse engineer the formula for deciding if a player scores a goal: Let's say the attributes are the following. All numbers are from 0-100. Let's say the real formula we are trying to reverse engineer is the following: And let's say if the number comes out to be greater than 85 the player scores the goal. Let's say our data set essentially has a bunch of players kick attempts at the goal, and has a true or false for score like the following: Is there a way to train a neural network that essentially predict outcomes based on a player's attributes? Is this the correct use of a neural network? or is there a better tool suited to figure this out? 
 I needed to do this for a course I'm preparing, so I created this demonstration website: A demonstration of Bayes' theorem as "selecting a subset" in the binomial case (make sure to hide the toolbars, bottom right). Basically, if you show the joint distribution -- which is just $p(y\mid\theta)p(\theta)$ -- you can see the "subsets" of the joint distribution that you need to select, which are those $\theta$ values that correspond to $Y=y$ (whatever you observed). The source code for that page can be found here: Rmarkdown source for page . (I used $\theta$ for the binomial probability instead of $p$ because $p(p)$ looks confusing...) 
 I am working with a Gaussian two factor model: $$ X_i = \beta_iZ_1+\gamma_iZ_2+\varepsilon_i, \space i = 1,2,...,n $$ where $Z_j\sim N(0,1), \space Z_1 \perp Z_2 $ and $\varepsilon_i\space iid\space N(0,1), Z_j \perp \varepsilon_i \space \forall \space i,j$. Obviously this model is not globally identified, one could just simply multiply all loadings by -1. However, it's seems to me that even if I employ non-negativity constraints on some loadings to aviod this source of underidentification the models still seems underidentified. It seems locally identified at least, or am I mistaken? Could anybody point out some conditions/restrictions to ensure local and global identification? 
 I realize that this inquiry may feel relatively novice-like for some among the plethora of stat wizes present here, but I was hoping for a little guidance on how to best go about conducting a basic forecasting calculation to trend a trajectory slightly beyond my available/known data. My historical data on which the forecast would be extrapolated from is approx. 2 years prior, and I was hoping to be able to forecast the continuing trend for just 6 more months. While I realize this could be handled using built-in features of GUI tools like Tableau, I was hoping to learn closer to the foundation level. Any help or advice on how to go about this (methodology, calculations, Z-values, etc.) would be greatly helpful and appreciated. Thanks kindly in advance. 
 I've datas as follows: for represent a customer. There is two products, the first product is represented by its few versions and the product 2 is represented by versions . A customer will buy the two types of products. He can also buy one or few versions of the same product. For example for the first product, he can buy version 1 and 3, and the second version A and B. As you see in my datas, I ponderate the sum in order to have for sum(product1)=1 and sum(product2)=1. I want to study the combinaison between choices between the two products () and () in the other side. The typr of question that I'm trying to investigate, for example : Are poeple that take version 1 and 3 are more likely to take the version D of the other product ? How to do this statistically ? because there is combinaison in the same product that complicate the analyses ? Thanks you a lot ! EDIT 2 : I've used table before extracted information seperately and ponderate them : EDIT 3 :my original datas was, do you think that I should work with instead and do you have an idea which type of analyses suits this type of datas ? 
 This has been bothering me for a while. Both $X$ and $Y$ are material properties. They can be described using a linear regression model built in the log-transformed space, i.e., $\log Y=a \log X+b$. For each material, I measure the property $X$ for $m$ times; while $Y$ for $n$ times. Hence, in order to build the regression model, I need to use the means of $X$ and $Y$. Should I use geometric mean or arithmetic mean here? If geometric mean is to be used, should standard deviation be calculated using the log-transformed variables too? Moreover, I guess the variance of both $X$ and $Y$ should be taken into account when calculating the confidence/prediction bands of the regression model, right? How to do it? It would be highly appreciated if a reference (book/paper) could be given for this question. 
 Given a dataset of binary vectors I want to find how the different variables are related to each other. That is, how does all the different fields depend on each other, which tends to appear together and which tend to never appear together? My first intuition is that this is a problem of finding the structure for a bayes net but I really dont know, maybe it is a problem of clustering, trying to find groups of permutations that often appear together? How would you machine learning people reason about this problem and what algorithms should I look into? Update: Just found out that this is called an occurrence matrix and that it is possible to get the co-occurrence matrix which at least gives me something. 
 I wonder if there is a way to specify custom cost function in sklearn/python? (atm I use sklearn SVC) My real problem has 7 different classes, but to make it more clear lets assume that I want to specify different cost for misclassification for a problem with 3 different classes and I am mainly interested that my model will properly distinguish between class 1 and class 3. if observation has class 1 and model predicts class 1, penalty is 0 (correct classification) if observation has class 1 and model predicts class 2, penalty is 1 if point has class 1 and model predicts class 3, penalty is 2 if point has class 2 and model predicts class 2, penalty is 0 (correct classification) if point has class 2 and model predicts class 3, penalty is 1 if point has class 2 and model predicts class 1, penalty is 1 if point has class 3 and model predicts class 3, penalty is 0 (correct classification) if point has class 3 and model predicts class 2, penalty is 1 if point has class 3 and model predicts class 1, penalty is 2 So the penalty matrix would look as follows: I assume that the 'class_weight' parameter in sklearn does something similar but accepts a dictionary rather than a matrix. Passing class_weight = {1:2,1:1,1:2} would just increase the weight for misclassifying class 1 and class 3, I ,however, want my model get a larger penalty specifically when it chooses class 1 and true class is class 3 and vice versa. Is it possible to do something like this in sklearn SVC? May be some other python libraries/learning algorithms allow for unequal misclassification cost? 
 One possibility is that your independent variables have different scales. This would affect weights in the regression model, but not correlations with the dependent variable. For example, if you were to multiply one of the independent variables by two, its weight in the model would be halved. But, its correlation with the dependent variable would remain the same because correlation is scale invariant. Another possibility is multicollinearity , which means some of the independent varaibles are correlated with each other. In this case, the weights of the correlated variables can behave unexpectedly. But, correlations with the dependent variable won't change. For example, imagine an extreme case where two of the independent variables are perfect duplicates of each other. In this case, there's no unique choice of weights that best predicts the dependent variable. One of the duplicate variables might have its weight set to 0, while the other receives all the weight. But, there are infinitely many ways to weight the two duplicate variables that will produce the same output. 
 The easiest way to set priors on stationary AR(p) processes is to go via the partial correlations, which don't have to satisfy difficult constraints. We did some version of this in this paper ( https://arxiv.org/pdf/1403.4630v2.pdf --- NB: not the newest version on arXiv. It was cut for space) and it worked pretty well. Using Gaussians truncated to ensure stationarity is an old trick (I think it was Chib who suggested it) but for the cost of a Jacobian transformation it doesn't seem really worth the trouble... Hope this helps! Dan 
 Consider a series $Y_t$ generated as an $ARMA(1,1)$ process $$ Y_t=\phi Y_{t-1}+\epsilon_t+\theta\epsilon_{t-1} $$ Suppose our interest centers on estimating $\phi$. We have an endogeneity issue here, as the error term $\epsilon_t+\theta\epsilon_{t-1}$ is correlated with the regressor $Y_{t-1}$, so OLS of $Y_{t}$ on $Y_{t-1}$ would not consistently estimate $\phi$: $$ \hat{\phi}_{OLS}=\frac{\sum_tY_{t-1}Y_{t}}{\sum_tY_{t-1}^2}=\frac{\frac{1}{T}\sum_tY_{t-1}Y_{t}}{\frac{1}{T}\sum_tY_{t-1}^2}\to_p\frac{\gamma_1}{\gamma_0}, $$ where the convergence in probability follows from standard arguments about plims of $\frac{1}{T}\sum_tY_{t-j}Y_{t-l}$ and the continuous mapping theorem. Now, it is known that $\gamma_0=\sigma^2\frac{1+\theta^2+2\phi\theta}{1-\phi^2}$ and $\gamma_1=\sigma^2\frac{(\phi+\theta)(1+\phi\theta)}{1-\phi^2}$. Hence, \begin{eqnarray*} \hat{\phi}&amp;\to_p&amp;\frac{\sigma^2\frac{(\phi+\theta)(1+\phi\theta)}{1-\phi^2}}{\sigma^2\frac{1+\theta^2+2\phi\theta}{1-\phi^2}}\\ &amp;=&amp;\frac{(\phi+\theta)(1+\phi\theta)}{1+\theta^2+2\phi\theta}\neq\phi, \end{eqnarray*} unless the process is an $AR(1)$, i.e. unless $\theta=0$. Instrumental variables estimation of $\phi$ using $Y_{t-2}$ as an instrument for $Y_{t-1}$, in turn, is consistent for $\phi$: the IV estimator is $$ \hat{\phi}_{IV}=\frac{\sum_tY_{t-2}Y_{t}}{\sum_tY_{t-2}Y_{t-1}}=\frac{\frac{1}{T}\sum_tY_{t-2}Y_{t}}{\frac{1}{T}\sum_tY_{t-2}Y_{t-1}}\to_p\frac{\gamma_2}{\gamma_1} $$ We furthermore know that the autocovariance function of an $ARMA(1,1)$ is such that $\gamma_2=\phi\gamma_1$. Hence, $$\hat{\phi}_{IV}\to_p\phi$$ This works because the error term in this IV model, $\epsilon_t+\theta\epsilon_{t-1}$, is uncorrelated with the instrument, which itself is correlated with the regressor $Y_{t-1}$ due to the autoregressive structure of the process. While this simple example (and I think simple examples are useful) shows how to use instruments in time series analysis, it is somewhat artificial in that if one knew that the process is $ARMA(1,1)$ one could estimate such a process directly. And it is somewhat fragile in that if the process were $ARMA(1,2)$, $Y_{t-2}$ would no longer be a valid instrument, as it would now be correlated with the new error $\epsilon_t+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}$. 
 I don't think cluster analysis is the proper tool here. By any means, the different categories simply are different categories. There is no reason to mix them up! What you want to do is aggregation and summary statistics, as available in the standard toolkits such as G__gle Analytics and Piwik. There is a reason why G__gle Analytics does not "cluster" the visits (and it's not that G__gle doesn't know how to cluster) but provides complex toold duch as funnel analysis instead . 
 I have a table like the following: What's the best approach to group users in RStudio by similarity based on their food type eaten? 
 dplyr can easily do it: Now you can conduct grouped operations on your dataset, for example: 
 I'm trying to fit a 4 parameter boltzmann sigmoid and get an error: "Error in nls(y ~ a0 + (a1 - a0)/(1 + exp((a2 - x)/a3)), start = list(a0 = max(y), : singular gradient" I have figured out that the code runs if the a3 parameter is set to .9 or greater, or -.9 or less. Does anyone have the reason this is? I want to provide a starting parameter for a3 as the slope according to the description on this website: . That is why I have the linear fit coefficient a3.s, but the result is &lt; .9 and I get the error. Is there a way to estimate a3.s prior to use as starting parameter for nls? I am simply using a linear fit of the midpoint of the sigmoid +/- 10 x units - is that the correct interpretation of the a3 parameter? Here is my code: Here is the data: Related to this question: the formula on the linked to website has a slight variation in the equation, where the the a2 parameter is used in the form "x-a2", while the equation I provided, and got from my data acquisition software's curve fitting function is the one I provided in the code with "a2-x". Which form of the Boltzmann is correct? Does the difference matter? 
 When performing a grid search for exploring optimal hyperparameters, what are the typical values, or ranges, that are commonly used for alpha, epsilon, gamma, lambda, C, etc? I'm not focused on a particular algorithm at the moment. And I realize each problem is going to be different. But just wondering if anybody has a resource they can share that provides typical values or ranges that would be good starting points for hyperparameter tuning. 
 I want to compute the correlation between a sample of errors that I have and a normal distribution, and I am a bit lost about the method to use. I also wanted to ask if there is any method that can highlight a relation between a numerical and a categorical variable (something similar to correlation for numerical variables). Thank you very much! 
 Let $a$, $b$, and $c$ denote the estimated means of group A, B, and C respectively. Let: $$V = \left[\begin{array}{ccc} v_{aa} &amp; v_{ab}&amp;v_{ac} \\v_{ba} &amp; v_{bb}&amp;v_{bc} \\v_{ca} &amp; v_{cb}&amp;v_{cc} \end{array} \right] $$ be the estimated covariance matrix of your estimates $[a, b, c]'$. The standard error for the estimate $b-a$ would be: $$SE_{b-a} = \sqrt{v_{bb} - 2 v_{ab} + v_{aa}}$$ The t-stat would be: $$ \frac{b - a}{\sqrt{v_{bb} - 2 v_{ab} + v_{aa}}}$$ T-stat $&lt;$ 2 roughly corresponds to significance at the 5 percent level. $ \frac{c - b}{\sqrt{v_{cc} - 2 v_{bc} + v_{bb}}} &lt; 2 \quad$ and $ \frac{b - a}{\sqrt{v_{bb} - 2 v_{ab} + v_{aa}}} &lt; 2 \quad$ does not imply that $ \frac{c - a}{\sqrt{v_{cc} - 2 v_{ac} + v_{aa}}} &lt; 2 \quad$ Hence it is possible that: $c-b$ is not significant at the 5 percent level, $b-a$ is not significant at the 5 percent level, but that $c-a$ is significant at the 5 percent level. 
 This is going to be very dependent on the algorithm, but I can answer for . Glmnet is fit through a cyclical coordinate descent, we apply an update rule to each coordinate in turn and iterate until convergence. The update rule is equation 5 in this paper . As the authors note, this update rule implies that, if the some parameter $\beta_j$ is zero at some iteration, then it stays zero after the iteration if and only if $$ \left| \langle x_j, y \rangle \right| &lt; N \lambda \alpha $$ where $N$ is the number of training data points. If we let $\tilde \lambda$ denote the minimum lambda for which all parameters are estimated as zero (the set of all such lambdas is clearly an unbounded interval including infinity), the above relation tells us that $$ N \alpha \tilde \lambda = \max_j \left| \langle x_j, y \rangle \right| $$ that is, all parameters are estimated as zero when $$ \lambda &gt; \frac{1}{N \alpha} \max_j \left| \langle x_j, y \rangle \right| $$ This gives us bounds on our grid search When $ \lambda = 0$ we get the un-regularized model. When $ \lambda &gt; \tilde \lambda $ all parameters are zero. To fill in the middle, breaks up this interval into multiplicatively even subintervals, i.e., subintervals whose endpoints are equally spaced on the logarithmic scale. My assumption is that this was done given some empirical evidence, but I'm not really sure. 
 I am developing a system where the management of the supermarket can make decisions on past sales data. There I mainly want to focus on how to predict sales of an item changes based on a discount given to another item (e.g Substitute goods). for that management can decide how much sales of other items may vary based on a discount given to a particular item. Could you please suggest me how the data set should be to approach to this requirement? I.e., what are the tables and columns of the database? 
 If the data can "be described using a linear regression model built in log-transformed space" then (unless I'm missing something problem specific...) you should run a linear regression in log transformed space! That would effectively be using the mean of $\log Y$, $Cov(\log X, \log Y)$, $Var(\log X)$ etc... to calculate your estimates $a$ and $b$ Something to keep in mind too: $$\exp\left( \frac{1}{n} \sum_i \log x_i \right) = \left( \prod x_i \right) ^{\frac{1}{n}}$$ That is, the exponential of the arithmetic mean in logs is equal to the geometric mean in levels. 
 The most convenient data layout for analysis will likely depend on the model you're using. Presumably you haven't already chosen a model, so for the meantime, collect the data into whatever format is convenient for collection, keeping in mind principles of good database design such as fourth normal form . I'm imagining something like 
 I know that Pearson's $r$ correlation coefficient can be classified as follows (Wikipedia entry) : small size if $r \in [0.1, 0.3)$, medium size if $r \in [0.3, 0.5)$ large size if $r \in [0.5, 1]$ In this article the author proposes to calculate the rank-biserial correlation coefficient as $r = W / S$ in Wilcoxon signed-rank test ($S$ is the sum of ranks). My question is are there standard values to classify this effect size as small, medium etc, like those for Pearson's $r$? 
 I would like to know how to choose a link function in generalized linear models in R. Roughly, I have learned that or should be used if dependent variable (y) is binary or count data. How about others here ? Especially, in which situation of dependent variable(y), can I apply or ? 
 We have a dataset with the level of export of a certain good towards Italy for almost all countries of the world for 15 years. We want to infere the effect of the distance of such countries from italy on the exportation levels. Since we have many time points for each country, I first though of using a mixed effect model with the countries as random intercept. But because my fixed effect of interest is the distance, and of course this values is unique for each countries, this would not make sense. So, how should I include the impact of time on the relationship distance-exportation? Would it make sense to use time as the random intercept, even if it is expected that there is some degree of autocorrelation between the time points? Many thanks 
 When we read a book, understanding the notations plays a very important role of understanding the contents. Unfortunately, different communities have different notation conventions for the formulation on the model and the optimization problem. Could any one summarize some formulation notations here and provide possible reasons? I will give an example here: In linear algebra literature, the classic book is Strang's introduction to linear algebra . The most used notation in the book is $$ \mathbf A \mathbf x=\mathbf b$$ Where $\mathbf A$ is a coefficient matrix , $\mathbf x$ is the variables to be solved and $\mathbf b$ is a vector on right side of the equation . The reason the book choose this notation is the main goal of linear algebra is solving a linear system and figure out what is vector $\mathbf x$. Given such formulation the OLS optimization problem is $$ \min ||\mathbf A \mathbf x-\mathbf b||_2^2$$ Interestingly, in statistics or machine learning literate people use different notation to represent the same thing: where the classic equation is (well this is not exactly in the book Elements of Statistical Learning , I made some modification in order to directly compare to $\mathbf A \mathbf x=\mathbf b$) $$\mathbf X \boldsymbol \beta=\mathbf y$$ Where $\mathbf X$ is the data matrix , $\boldsymbol \beta$ is the coefficients or weights to be learned learning , $\mathbf y$ is the response. The reason people use this is because people in statistics or machine learning community is data driven , so data and response are the most interesting thing to them, where they use $\mathbf X$ and $\mathbf y$ to represent. Now we can see all the possible confusion can be there: $\mathbf A$ in first equation is as same as $\mathbf X$ in second equation. And in second equation $\mathbf X$ is not something we want to solve ... In addition, I mentioned $\mathbf X \boldsymbol \beta=\mathbf y$ is not the exactly what people widely used in machine learning, people uses a half vectorized version that summarize over all the data points. Such as $$ \min \sum_i L(y_i,f(\mathbf x_i)) $$ I think the reason for this is that it is good when talking about the stochastic gradient descent and other different loss functions. Could anyone give more summaries on the optimization formulation notations cross different literature? I hope smart answers to this question can be used as a good reference for people reading books cross different literature. EDIT 1: please do not be limited by my example $ \mathbf A \mathbf x=\mathbf b$ and $\mathbf X \boldsymbol \beta=\mathbf y$. I was tying to ask more conventions with explanation cross fields. Edit 2: I agree the original question is too broad. Therefore I revised the description and narrowed it down to the optimization formulation. 
 To give a very general answer to your very general question, I usually start out, at least, by choosing ten or so values that seem to cover the range of the parameter. If the range is [ a , b ] for some real a and b , the values could be evenly spaced between a and b . If the range is (0, ∞), the values could be 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5. If the range is (∞, ∞), the values could be -1e5, -1e3, -1e-2, 1e-2, 1e3, 1e5. 
 What you are trying to estimate in economics is referred to as the cross-price elasticity of demand , how demand for a good $Y$ changes in response to a change in the price $p_x$ of good $X$. Several points to keep in mind: To estimate a demand curve, you need to use shifts in the supply curve . For example, if we were trying to estimate how demand for apple juice changes as a function of the orange juice price we could use: Purposeful experiments: (eg. we randomly raise price of orange juice and see what happens to purchase of apple juice.) Natural experiments: (eg. hit to Florida orange crop reduced orange juice supply You cannot estimate a demand curve just looking at movements in price and quantity in the abstract because you don't know whether the changes are driven by changes in supply or demand! Changes in supply allow you to estimate a demand curve and changes in demand allow you to estimate a supply curve. Example of horribly wrong inference: Imagine you're trying to estimate demand for snow shovels as a function of the price of mechanized snow blowers. And imagine in the data, you see sales of snow shovels goes DOWN when the price of snow blowers is LOWER !?!? How could they be complements? Shouldn't they be substitutes?! The answer is that the price of snow equipment is lower in the summer (when demand is low) and higher in the winter (when demand is high). You can't estimate a demand curve using demand curve shifts! 
 I have few thousands letter's images which are all colored black on a white background. I wanted to extract their features so that it was possible to cluster them effectively. My attempt was to divide an image on 25 parts, calculate black pixels to white pixels ratio in each part (if there were no white ones I assumed the ratio is twice the black pixels) and thus convert an image to a vector of 25 values. I also added a 26th attribute which denoted if an image represents a letter which has a dot/something else outside its 'body'. Then I used a k-centroids algorithm to cluster the data, but the results were highly unsatisfactory. Is there any simple, yet effective way to do what I'm trying to do? (Images are of different size, mainly with varying width (from 5 to 30 pixels) and rather stable height (29 to 31 pixels). As I said, they are all black and white.) 
 If you include distance as a continuous variable, which seems more logical to me, then you can include country as a random intercept to take account of the fact that over time the values for country are correlated. Allowing a random coefficient for time allows for the slopes to vary randomly between countries. Whether that is sensible depends on subject matter knowledge but I would have thought that was what you wanted to do. 
 Perhaps a related question is, "What are words used in different languages, and what are the connections between these words?" Notation is in some sense like language: Some words have region specific meanings; some words are broadly understood. Like powerful nations spread their language, successful fields and influential researchers spread their notation. Language evolves over time: language has a mix of historical origins and modern influence. Your specific question... I would disagree with your contention that the two follow "completely different notation." Both $X\boldsymbol{\beta} = \boldsymbol{y}$ and $A\mathbf{x} = \mathbf{b}$ use capital letters to denote matrices. They're not that different. Machine learning is highly related to statistics, a large and mature field. Using $X$ to represent the data matrix is almost certainly the most readable, most standard convention to follow. While $A\mathbf{x} = \mathbf{b}$ is standard for solving linear systems, that's not how people doing statistics write the normal equations. You'll find your audience more confused if you try to do that. When in Rome... In some sense, the heart of your revised question is, "What are the historical origins of statistics using the letter $x$ to represent data and the letter $\beta$ to represent the unknown variable to solve for?" This is a question for the statistical historians! Briefly searching, I see the influential British statistician and Cambridge academic Udny Yule used $x$ to represent data in his Introduction to the Theory of Statistics (1911). He wrote a regression equation as $x_1 = a + bx_2$, with the least squares objective as minimizing $\sum\left( x_1 - a - bx_2\right)^2$, and with solution $b_{12} = \frac{\sum x_1x_2}{\sum x_2^2}$. It at least goes back to then... The even more influential R.A. Fisher used $y$ for the dependent variable and $x$ for the independent variable in his 1925 book Statistical Methods for Research Workers . (Hat tip to @Nick Cox for providing link with info.) Good notation is like good language. Avoid field specific jargon whenever possible. Write in the math equivalent of high BBC English, language that is understandable to most anyone that speaks English. One should write, whenever possible, using notation that is clear and that is broadly understood. 
 A good guide is to look at the relationship between the mean and the variance. If the variance does not vary with the mean - Gaussian. If it varies proportional to the mean - Poisson. As mean squared - gamma. As mean cubed - inverse Gaussian. 
 The original random variable $X_{t+1}$ is normally distributed. Call its distribution $P_{X_{t+1}}$ Define a function $$g(v) = v \cdot 1_{\{v &gt; z_t\}}$$ where $1_{\{\cdot\}}$ is the indicator function. This can also be written as: $$g(v) = \left \{ \begin{array}{cl} v &amp; v &gt; z_t \\ 0 &amp; \text{Otherwise} \end{array} \right .$$ Another way to phrase your question is: what is the expected value of $g(X_{t+1})$? We can write this as: $$E[g(X_t)] = \int_{-\infty}^{\infty} g(v) P_{X_{t+1}}(v) dv$$ We know that $g(v) = 0$ for $v \le z_t$, and $g(v) = v$ for $v &gt; z_t$. So, we can split the integral across two intervals: $$E[g(X_t)] = \int_{-\infty}^{z_t} 0 \cdot P_{X_{t+1}}(v) dv + \int_{z_t}^{\infty} v \cdot P_{X_{t+1}}(v) dv$$ The first term is clearly zero, so we're left with: $$E[g(X_t)] = \int_{z_t}^{\infty} v \cdot P_{X_{t+1}}(v) dv$$ $X_{t+1}$ is normally distributed, so we can substitute $N(\mu, \sigma^2)$ in for $P_{X_{t+1}}$ $$E[g(X_t)] = \int_{z_t}^{\infty} \frac{v}{\sigma \sqrt{2 \pi}} \exp \left [ {-\frac{(v-\mu)^2}{2 \sigma^2}} \right ] dv$$ Evaluating the integral gives the final answer: $$ E[g(X_t)] = \frac{\mu}{2} \left [ 1 - \text{erf} \left ( \frac{z_t - \mu}{\sigma \sqrt{2}} \right ) \right ] + { \frac{\sigma}{\sqrt{2 \pi}} \exp \left [ -\frac{(z_t - \mu)^2}{2 \sigma^2} \right ] } $$ where $\text{erf}(\cdot)$ is the error function You can check that this is correct by simulation. Draw many samples from $N(\mu, \sigma^2)$, set values less than $z_t$ to zero, then take the sample mean. Edit (as suggested by user12): In the case where $X_{t+1}$ has mean zero, plug $\mu = 0$ into the last equation above, to obtain: $$ E[g(X_t)] = \frac{\sigma}{\sqrt{2 \pi}} \exp \left [ -\frac{z_t^2}{2 \sigma^2} \right ] $$ 
 I assume you're trying to reverse-engineer a human-programmed formula, like in a video game, rather than trying to model a stochastic relationship in nature. If you know the hidden formula is something like (since the example you gave is of that form; can be rewritten as ), and all you don't know is the values of through and the value of , then a neural network is overkill. All you need is a multi-parameter function minimization routine like R's or Python's . Minimize the negative sum of the logarithms of the likelihood of each observation. The likelihood of each observation is just the probability of the observed outcome (1 or 0) given the model parameters being considered by the minimization routine. 
 It looks like this "Kendall score" is only an intermediate step in the computation of τ (the Kendall correlation) and not of any substantive interest itself. There's no need to report it. Focus on the τ. 
 "Survival object" is not a statistical term. What is meant here in the R documentation is "object" in the programming sense of the word . The package uses this kind of object for things such as the response variable in . 
 All due respect to Kodiologist, I wouldn't structure your data matrix the way he's laid it out. Too much useful information is being lost with a weekly summary or isn't accounted for. Daily information would be much more sensitive and helpful for estimating elasticities and cross-elasticities, particularly for panel data modeling which is one of the best ways to elicit information of this type. That said, you haven't told us some important things about your data. One can infer that it is at the "item" level. Is that synonymous with a UPC (sku) or are items aggregates of multiple UPCs? Do you have features or factors that describe those items, e.g., category, size, quantity, etc. Is it information from Nielsen or IRI and at the market level? Or is it actual transaction level data from the store's sales registers? How many stores? You would want your data matrix to reflect relevant structural factors related to cross-sections such as stores and markets. How far back does the information go? To Matthew Gunn's point, you would also want to include factors related to seasonality -- by year, quarter, month, week and day, as available and appropriate. Next, any discount occurs in a "landscape" of marketing factors such as price, advertising, promotion, displays, etc., which are going on simultaneously for every product and brand across the stores, all of which can impact household purchase decisions. Given that, widening your theoretical frame to include product complements (negative cross-elasticities) as well as substitutes (positive cross-elasticities) would be a big strategic win. Related to that are the concepts of competitive frames or consideration sets where a set of items cluster together as being possible substitutes, etc. This effect would be captured with dummy variables to indicate membership in a product cluster. So, your matrix of panel data would have qualitative factors (columns, features or variables) that capture the relevant structural factors, e.g., a column for store id, a column for the date of the sales, columns for the seasonality factors (as appropriate), a column for the items, columns for the relevant descriptive features, and so on. Dates, in particular, are useful for trend analysis. It would be a lot of columns, potentially, and some of it would be redundant. The matrix should also contain time-sensitive information related to pricing and other marketing activities by store, product and date, each with a separate column, as appropriate. The unit of observation (rows) of the matrix would be the store, date, item combination of this information at whatever level of granularity you have available. You will have to make a determination if the data is to be rolled up or aggregated to some higher level to facilitate processing and modeling. Just bear in mind that the more you summarize the information up, the more sensitivity you lose. If you go too high, you can erase the impact of the things you are trying to understand. There are lots of ways to model data like this -- deriving the relevant interaction terms to estimate the elasticities, etc., is the key thing to understand. These interactions will potentially be complex. One of the best introductions to thinking about elasticities and cross-elasticities from a marketing point of view is Lee Cooper's book Market Share Analysis which is available free on his UCLA website. He discusses in considerable detail the various methods for their estimation using interaction terms. Cooper's recommended approach is to use OLS regression in a panel data format. His book also contains really nitty-gritty examples of what the data matrix could look like and how to structure it for modeling. Economists will tell you to pick up Wooldridge's book Econometric Analysis of Cross Section and Panel Data . It is indeed an excellent resource but it is focused on econometric analysis based on panel data models with balanced, fixed N (sample size) and t (time periods). Your data is marketing data and will be neither balanced nor fixed. Wooldridge's prescriptions, however well intended and appropriate for economics, will only confuse you. 
 in: https://en.wikipedia.org/wiki/Normalization_(statistics) it's written that rescaling(min_max_scaling) and standarization are types of Normalization what i see in alot of stackoverflow answers is that they don't say min_max_scaling"rescaling", they say normalization 
 In Distributed Representations of Sentences and Documents , authors introduced PV-DBOW, which uses a paragraph id predicts a small window of context, however, there are many windows of contexts in the same paragraph, why does parameters learned from this model make sense ? 
 could someone please point me to literature about the significance levels of differences in the out-of-sample performance of models? not just the metric (e.g., OOS MSE, OOS MAE, or R^2), but in particular also whether one model predicts "better" than another with what statistical significance levels. this should be particularly well suited to comparing very complex models [e.g., genetic learning computer models] to very simple models. there are some obvious choices, but there is no use to reinventing the wheel. pointers appreciated. 
 EDIT: Removed the old question since it was not phrased properly. Assume there are 10 sacks and each sack can carry 4 pounds of weight. Bricks come in 1, 2, 3 or 4 pounds each. The problem is to fit all available bricks into the LEAST number of sacks. For example if there are 8, 1 pound bricks then it could be filled in either 2 sacks or 8 sacks. But 2 is better since its the LEAST possible. Is there a way to measure this where filling in 2 sacks is better than 4 sacks, which is better than 8 sacks and so on.... NOTE: The bag number into which it is filled does not matter. So, the 2 sacks used could be sack number 1,2 or 1,10. It does not matter as long as the least number of sacks are used. 
 Normally one would use a fit object and plug it into the relevant function to calculate these things. In my situation I'm using a model with pooled coefficients, specifically the average coefficient of several glmnet models(using different imputation sets) so I don't actually have a fit object. It was easy enough to calculate the predictions through model.matrix, but I'm having trouble calculating likelihood based measures and c-statistics in R. 
 Sort the data so that one variable is in order (call it $A$) then take each value of $B$ except the rightmost and count how many values of $B$ to the right of it are larger (call it $p$) and smaller (call it $q$). The add up all the $p$ and call it $P$ and similarly to form $Q$. Then $S = P - Q$. Whether that has any meaning in your scientific question is up to you. It is a way of defining the way in which Kendall decided to measure correlation. $S$ also plays a fundamental role in many other so-called non-parametric tests. 
 Instead of using hand-tuned features, I'd suggest using some automated form of dimensionality reduction, which will automatically extract features from your data. There are many ways to do this, ranging from the simple, linear case (e.g. PCA) to complex, nonlinear 'manifold learning' algorithms (e.g. t-SNE). Neural networks (particularly variants like autoencoders) could also work well. All of the methods I just mentioned are unsupervised, which means that they don't use labels (in your case, you'd give the images as input, but you wouldn't specify what letters they represent). Unsupervised methods work by attempting to find 'structure' in the data; what this means varies from algorithm to algorithm. There's no guarantee that the images will form clusters around the letters they represent; whether this happens or not depends on the data and how structure is defined. You'll probably find this blog post interesting. They run various dimensionality reduction methods on the MNIST dataset, which is a set of grayscale images of hand-drawn digits. If you explicitly want the extracted features to cluster (or at least be separable) in terms of the letters they represent, then it might be better to use a supervised algorithm (i.e. to train a classifier). In your case, you'd give the training algorithm a set of images and a set of labels indicating the letters they represent. The training algorithm will attempt to learn a function (the classifier) that predicts the represented letter from the image. Some classifiers will give you a set of extracted features, and others won't. For example, a k nearest neighbors classifier won't, so it won't be useful for your purposes (assuming that what you want to do is examine the features). A simple, linear, supervised method is linear discriminant analysis (LDA). It will give you a set of features that are linear combinations of your input pixels. It attempts to find the weights of these linear combinations such that, when the data are projected into the feature space, it's possible to linearly separate the data points in each class (i.e. to find hyperplanes that separate the images representing each letter). Note that it may not be possible to linearly separate the classes in this way, and LDA will try to do the best it can. A more complicated, non-linear class of methods is supervised neural networks. In this case, the network would take an image as input, and output the predicted letter the image represents (or a probability distribution over possible letters). The network would be trained to correctly classify images of letters. You can think of the units (i.e. 'neurons') in each layer of the network as representing features of the input image. Therefore, if you want to use the network for feature extraction, simply take the vector of unit activations for some layer of the network. Dimensionality reduction and supervised learning are enormous fields, but hopefully these few examples can help getting started looking for more methods. 
 I am analysing pre-analyical errors in diagnostic labs. There are 15 types of different errors in the lab (such as hemolysed samples, clotted samples, insufficient volume, etc). Each of these errors is presented as percentages (error type 1 number/total numbers of samples). I want to compare these errors to each other. Is this possible? For example hemolysed samples is the most common type of errors (28%). How do you compare this percentage to the least common error 1%? 
 I want to create a system for managers to make decisions based by analyzing the history of sales. I am new to data mining and could you suggest me what kind of analysis I can do and what kind of predictions I can facilitate for the user. Much appreciate if you could provide a sample source code in ms SQL/ C# 
 As a simple example, say you want to predict the house price. You have 5 features. You build 5 models, each trained with one feature. (price, sqft) (price, num_bedrooms) (price, lot_size) (price, num_bathrooms) (price, crime_rate) For a test instance, you can just average the price predictions. That's one way to combine the models. Any other ways to combine models (perhaps a weighted average)? If we average the results, how does will the results compare to just a similar model using all five features? Or how does building a bunch of simple models and combining them compare to building one complex model? One thought is that perhaps this approach can be used when missing data is common. For example, just average the result of three models when sqft and bedrooms are missing for a test instance. The other idea is to use the price outputs as features of a more complex model. 
 I've the following problem which I want to analyze: Sales of a company depends on quantity of product A and B plus advertisement. All these 3 independent variables are continuous variables. Now, There are 3 cities in which products are been sold. So, I want to check the fixed effect interaction of product A and B with city. I was using the following formula for the specification of the model. In both the cases I was getting the interaction result for prodA. My question is what's the difference between the 2 specifications? 
 I do not know much about regression, and, in order to practise, I need to code this different regressions in R or Matlab, for the next function: $$y=\alpha+\beta_1 x_1+\beta_2 x_2+\epsilon$$ where $x_1, x_2$ are uniform variables and $\epsilon$ is normally distributed with mean zero. OLS: $\displaystyle \min \sum_i\left(y_i - x_i^T \beta\right)^2 = ||y-X\beta||_2^2$ Least-absolute regression: $\displaystyle \min ||y-X\beta||_1 = \sum_i |y_i-x_i^T\beta|$ Chebyshev regression: $ \min ||y-X\beta||_{\inf} \equiv \min \max |y_i-x_i^T\beta|$ Ridge regression: $\min ||y-X\beta||_2^2 + \rho ||\beta||_2^2$. You can estimate $\rho$ by cross-validation. Lasso regression: $||y-X\beta||_2^2 + \lambda ||\beta||_1$, You can estimate $\lambda$ by cross-validation. Forward regression: you can estimate the final model by the BIC. Backward regression: you can estimate the final model by the BIC. Thansk a lot. Any help will be very useful for me. 
 I am doing Cross-platform normalization and network-based data analysis of gene expression data to find out potential biomarker of cancer .I have merged 3 datasets without any treatment ( two of Affymetrix Human Genome U133A Array and one with Affymetrix Human Genome U133A Array plus 2, platform GPL96 AND GPL570) using insilicomerging package. I have 229 cancer samples and 74 normal one with i.e a ratio of about 3:1. I want to do the linear model analysis to find out differentially expressed genes. please someone help me 
 I've been trying to run meta-regressions in Revman for the first time. All of the User Guides refer to the use of Covariates (which seem to appear under Data &amp; Analysis heading)... and can be entered via "Data tables by test" (or data tables by study) which also appear as headings once included studies have been added. I can find neither of these headings, nor a way to add covariate variables (that are continuous, as opposed to subgroups). Any help or advice? 
 I'm having a serious problem in and old exam paper, specifically a question on convergence of random variables. Let $X$ and $Y$ be two i.i.d exponential random variables with parameter $\lambda$, so with density function $f_{X}(x)=\lambda e^{-\lambda x}$ and same for Y. Let $Z_{n}=\frac{nX}{X+nY}$ for $n\geqslant 1$ (with $n\in \mathbb{N}$) be sequence of random variables. The question is to study almost sure convergence and convergence in distribution for $n\rightarrow \infty$. Now, I have the solution to this problem, and would not like to see it before trying it myself. I'm given however a hint in the text: $Z_{n}$ belong almost surely to $(0,n)$. Question: how did he even find out that $Z_{n}\in (0,n)$ a.s.? 
 I have the data contains weekly sale of items in several stores on varying time frames. And I have asked to classify the stores based on their yearly sale patterns. I am quite new in statistics and R programming and I want to do it in R; so I would like to know that what are the things that I have to learn to do this task? Btw there are like 4000 stores so that makes 4000 time series data for each store. My first attempt was finding correlation of each store to each other and create classes if there is a large correlation. However I am not sure that this attempt makes sense or not. Edit: Here there are time series graphs of 3 different stores. So how can I compare their patterns and classify the related ones? * I do not have 10 rep so cant upload 3rd one. 
 Where can I find a map or distribution data got each country and even provinces within each country for body height and weight by age and how these very across the world? Thanks. (Not sure if this question is more suited for this site or for the biology site. If the latter than please migrate my post). 
 Survival analysis requires keeping track both of the follow-up time and whether the last follow-up time represented an "event" or not. (In the latter case it's called a "censored" observation.) The simplest survival object produced by in R, for example, can be thought of as a matrix, with one row for each case, the first column representing the last follow-up time and the second indicating whether there was an event at that time (typically 0 for censored observations, 1 for events). For other uses (e.g., interval- censored data or counting-process analysis), the object may have 3 columns. Programs in R that do survival analysis typically expect an object produced by , although sometimes you have to provide separate time and censoring/event variables. Be sure to read the documentation for whichever statistical analysis program you are using, as the coding for "events" can differ from the above. R, for example, also allows 1/2 coding for censored/event, and MATLAB, as I recall, has 0 for event and 1 for censored as its default. 
 I was trying to do a discriminant analysis in SPSS and I have to interpret the output. But when I come to the table of Structure Matrix, I find difficulty in defining the variables I should use in the analysis based on the coefficients of correlation of the variables with the functions. In some articles I read that these values should be greater than 0.3. Is this correct? But what if this coefficients are negative, should I use those variables? 
 Here's the question I have to solve: Let X be a random variable with pmf as given: Simulate this distribution using runif function in R. Is it possible at all? I coded it with sample function: But is it possible to use runif for a non-uniform distribution? 
 The original stacked denoising autoencoder paper contains some comparisons to deep belief networks (i.e. stacked RBMs) Vincent et al. (2010) . Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. However, the comparisons are about classification performance, using the models in generative mode (to synthesize examples), and conceptual points (how the methods work). As far as I recall, they don't directly address your questions about computational cost, etc. 
 Hyperparameters are often searched on a logarithmic scale. This particularly holds for regularization/penalty terms, and cases where the proper scale is unknown. Other hyperparameters can be searched on a linear scale if there's a priori reason to do this. In some sense, you can think of the search range as a 'hyperprior'. When searching for values of multiple hyperparameters, random search may be better than grid search. The reason is that performance can be more sensitive to some hyperparameters than others, and the set of relevant hyperparameters isn't known ahead of time (it can vary with the dataset). In this case, grid search would waste iterations on adjusting hyperparameters that don't make much difference, while keeping the others fixed. Random search would adjust all hyperparameters simultaneously. Adjustments to hyperparameters that don't matter won't change anything, and adjustments to those that do will have a chance at improving performance. See this paper: Bergstra and Bengio (2012) . Random Search for Hyper-Parameter Optimization 
 I have two groups (one placebo, one drug) in which I measured a few variables. I performed factorial analysis separately for the two groups and then I want to compare the factor loadings between the groups; I tried to perform pearson correlation but a few variables DO NOT load on both two factors in the two groups; eg. variable 1 loads in factor 1 in the placebo and not the drug group and variable 2 loads in factor 2 in the drug but not the placebo groups. Any suggestions? For the rest of the variables that load onto both factors in both groups I found a pearson correlation coefficient &gt; 0.8 
 I know how to subset a data frame by rows using a logical test, but can you actually subset a data frame by columns using a logical test? So far, I've just created subsets by naming the columns I want to keep. For example, if I have col1=c(1,2,3) col2=c(2,4,6) col3=c(3,6,9) x=data.frame(col1,col2,col3) I would create a subset of the first two columns like this: x2=x[,1:2] Or a subset of columns 1 and 3 like this: x2=x[c(1,3)] But my professor is asking me to use a logical test to exclude certain columns, and these don't seem like logical tests. Any ideas? 
 You can absolutely combine simple models into a more complex model. This class of techniques is called ensemble methods , and can be very successful. As to whether a specific form of this idea will help on a specific problem, you'll just have to try it and see. If the models are linear, and combined linearly, then it won't help. Some techniques to look up are bagging and boosting . These use different combinations/weightings of the data points to generate the different models, so they're slightly different than your question about using subsets of features. But, looking at the way the different models are combined and the principles behind it can give you some insight. Random forests use a combination of decision trees, where each tree is trained using a different subset of data points and features. Stacking uses a final 'meta model' to combine the predictions of lower-level models. 
 I am working on a problem that has two dimensions, A and B. Within the two dimensions, there are 6,140 unique points. Each point has a cost associated with it. I have a total cost that I cannot exceed. I want to optimally sample N of the points such that: I maximize the variance in A, I maximize the variance in B, I do not exceed my total cost limit Basically, I want to get the most discrete values of A and the most discrete values of B so that my sample can come as close as possible to representing all of the points in the set. Are there any algorithms that would lead me to this solution? Edit: The number of points N can vary. Ideally, you could have a lot of low cost points or a few number of high cost points. The figure below shows the set of points from which to sample. 
 So I have a classification task which predicts a given input as one of 5 classes. Regular error metrics like precision and recall are easy to compute for each class but is it possible to factor the error by considering distance between classes? For example if the order or ranking of the classes was 1 , 2, 3, 4, 5 it would be better if a sample whose true class was 1 to be predicted as 2 instead of 5. 
 For example is a logical operator and indicates logical negation (NOT). Combined with the equal sign you can use it to say "not equal". So you can exclude the third column by saying that you want as an output all columns which are not named 'col3', in this way: . 
 I have this homework question that I can't seem to solve a part of it. The Question is: Suppose your right knee is sore 15% of the time, and your left knee is sore 10% of the time. What is the largest possible percentage of time that at least one of your knees is sore? What is the smallest possible percentage of time that at least one of your knees is sore? Here's what I have so far: Let $A$ be the event that the right knee is sore and $P(A) = 0.15$. Let $B$ be the event that the left knee is sore and $P(B) = 0.10$. $\max{P(A \text{ or } B)} = P(A)+P(B)-P(A \text{ and }B)$ The maximum value is obtained when $P(A \text{ and }B)$ is 0, and the maximum value is $0.15+0.1 = 0.25$ $\min{P(A \text{ or }B)} = P(A) + P(B) - P(A \text{ and }B)$. Now I don't know how to find the max value of $P(A \text{ and }B)$ to calculate the minimum of $P(A \text{ or }B)$. 
 First of all, $P(A \text{ or } B)=P(A)+P(B)-P(A \text{ and }B)$ is always true. Since $P(A)=0.15$ and $P(B)=0.10$, the maximum value of $P(A\text{ or }B)$ is obtained if $P(A \text{ and }B)$ is at its minimum possible value which is $0$. $P(A\text{ and }B)=0$ means there is no overlap between $A$ and $B$ (the left knee and the right knee will never be sore at the same time). Similarly, the minimum possible value of $P(A\text{ or }B)$ is obtained if $P(A \text{ and }B)$ is at its maximum possible value. Since $P(A \text{ and }B)$ is always smaller than or equal to either $P(A)$ or $P(B)$, its maximum is $\min\{P(A), P(B)\}=\min\{0.15, 0.1\}=0.1$. In this case, $(A\text{ and }B)=B$ which means when your left knee is sore, you right know must also be sore. In summary, the maximum value of $P(A\text{ or }B)$ is $0.25$ and the minimum value is $0.15$. 
 Let's say I have a strictly-stationary stochastic process with known PSD (power spectral density). The process has been running, and I have all the data from time $t=-\infty$ to $t=0$. I want to predict the value at time $t=\tau&gt;0$. Is there a way to quantify how accurate my prediction can be, as a function of $\tau$? [I know how to calculate the decay of the autocorrelation function. But I'm not sure that's the right answer, because I have more information than just one known data point.] 
 I need some advice or help me how to get live stock quotes in excel. 
 The problem that this applies to is difficult to explain (for me), which may account for a lot of why I cannot solve it. The example below should highlight the process for which I want to come up with a function or conditional expression: X people sit at a table with N seats (X = N). This is iteration 0 or position 1. Everyone gets up and changes seats. This is iteration 1 and creates position 2. A period of time goes by and then, again, everyone changes seats, BUT they cannot replace seats with the same person as last time (e.g., if I went and sat where Tim was sitting for the first iteration, I CANNOT go sit where Tim was just sitting again; I can only follow Tim 1x) AND they cannot return to a seat in which they have already sat. This is iteration 2 and creates position 3. This continues until all X people have sat in all N seats without ever sitting in the same seat twice or replacing the same person twice. I have been able to solve for particular N/X's, but then fail to have a formula that generalizes to others. Here is an example of a 4-person solution: Here is the formula from Google Sheets: Identified each person w/ number that corresponds to original position. Added subsequent iteration number to that person's number (which corresponds to their original position) If the sum of the person's ID and the subsequent iteration &gt; # of people (4), subtract the sum by 4. This formula solved the problem for 4 people: no one is in a position more than once no one replaced the same person more than once everyone sat in every position. The formula (shown in the second image), however, failed to work for X = 5 and X = 6. THE GOAL: Create a formula/expression/algorithm/script that works regardless of how many people start off at the table. In other words, if I can note the number of people and the positions they are starting at, the answer can systematically transition those people through this process and satisfy the above the conditions. 
 Welcome to CV. Yahoo! Finance will download stock information into a csv file down to the daily level. High frequency data (intraday) is harder to find -- at least for free. CRSP might have it... Of course, there are lots of pay-for-service vendors of high frequency data. You might ask this question on Quora.com. 
 I am studying parameters generated by the method of Ordinary Least Squares, in particular, a parameter's associated standard error. Wikipedia suggests calculating $$s^2=\frac{S\left(\hat\beta\right)}{n-p}=\frac{\rvert\rvert y-X\hat\beta\rvert\rvert^2}{n-p}$$ describing this as the standard error of the regression (SER), or standard error of the equation (SEE). This, however, is not sufficient to calculate the standard error for a given parameter. I found this , which suggests For each parameter, the standard error can be obtained by: $$s_{\hat \beta _j}=s \sqrt{C_{jj}}$$ where $C_{jj}$ is the j$^{th}$ diagonal element of $(X'X)^{-1}$ (note that $(X'WX)^{-1}$ is used for weight fitting). Note, that using this technique yields precisely the same standard errors as those summarized by the method in R. Where does this $C$ come from? I have been reading about this for days, and this is the first reference I have seen to this matrix. 
 In , like , is only the interaction term. And is the same as . includes variables individually as well as their interaction term in the regression model. Notice that all three expressions , and yield same results. So, your two models are equivalent. 
 I have a dataset which has 26 patient records, over an interval of almost 3 years. Each patient has been encountered with hospital visits over the 3 year interval several times. I wish to perform survival analysis for these records using R, where the time to event is the time to encounter. I am however clueless, about the data format for the R tool. Also, does the R tool accept the data in text format? Please explain as to how should the data be formatted in order to input it to R. 
 Not sure if my terminology is correct. But I am trying to find a function to describe the probability density produced when two Erlang Processes are combined. Example: At time 0s a random number generator is interrogated every second. If a value less than .2 occurs then n=n+1, once n=5 then begin the second process (rand()&lt;.1 , n=10). Is there an equation to model the time it takes both processes to terminate? 
 Any undergraduate statistics or econometrics text treating linear regression contains something reminiscent of the following: In matrix notation we have data $\left (\mathbf y, \mathbf X\right)$ and we consider the model $$\mathbf y = \mathbf X\beta + \mathbf u$$ where for the moment we only assume that the regressor matrix contains a series of ones, so that we can safely assume that the "error term" $\mathbf u$ has zero mean. We do not as yet make any statistical/probabilistic assumptions. Calculating the unknown betas by Ordinary Least Squares is a mathematical approximation method that needs no statistical assumptions. We obtain $$\hat \beta = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf y$$ This is the (orthogonal) Linear Projection coefficient vector, and, as a mathematical approximation story, it stops here. Now we want to talk about the "standard error" of the estimates. But that is a statistical concept, and so we must assume something random and probabilistic. Assume that the regressors are all deterministic, but $\mathbf u$ is a random variable. Due to the regressor matrix containing a series of ones, we then have $E(\mathbf u) = \mathbf 0$, where $E$ denotes the expected value. We have $$\hat \beta = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf y = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\left(\mathbf X\beta + \mathbf u\right) = \beta +\left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf u$$ $$\implies \hat \beta -\beta = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf u$$ Since $\beta$ is a constant, we have that $\text{Var}(\hat \beta -\beta) = \text{Var}(\hat \beta)$, where the Variance is the square of the standard deviation. The multivariate version of the variance, is $$\text{Var}(\hat \beta -\beta) = E\Big[(\hat \beta -\beta)(\hat \beta -\beta)'\Big] - E\Big[(\hat \beta -\beta)\Big]E\Big[(\hat \beta -\beta)\Big]'$$ where the prime denotes the transpose. Substituting, $$\text{Var}(\hat \beta) = E\Big[\left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf u\mathbf u'\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1} \Big] - E\Big[\left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf u\Big]E\Big[\left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf u\Big]'$$ Since we have assumed that $\mathbf X$ is deterministic, the expected value applies only to $\mathbf u$ so we have $$\text{Var}(\hat \beta) = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'E(\mathbf u\mathbf u')\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1} - \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'E(\mathbf u)E(\mathbf u)'\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1} $$ Since $E(\mathbf u) =\mathbf 0$ we are left with $$\text{Var}(\hat \beta) = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'E(\mathbf u\mathbf u')\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1} $$ Now comes another benchmark statistical assumption: the $\mathbf u$ is "homoskedastic" which means $$\text{Var}(\mathbf u) = E(\mathbf u\mathbf u') = \sigma^2I$$ where $\sigma^2 &gt;0$ is the common variance of each element of the error vector, and $I$ is the identity matrix. Substituting we get $$\text{Var}(\hat \beta) = \left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\sigma^2I\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1} =\sigma^2\left(\mathbf X' \mathbf X\right) ^{-1} \mathbf X'\mathbf X\left(\mathbf X' \mathbf X\right) ^{-1}$$ $$\text{Var}(\hat \beta) = \sigma^2\left(\mathbf X' \mathbf X\right) ^{-1}$$ The $\sigma^2$ is estimated by $s^2$ as in the OP question, and the diagonal elements of $\left(\mathbf X' \mathbf X\right) ^{-1}$ each is the variance of the corresponding element of the estimated beta vector -taking the square root leads to the standard error of each element. The off-diagonal elements of the matrix, multiplied by $\sigma^2$ give the covariances between the elements of the beta vector. 
 This question is between statistics and the academic part, specifically in the field of education. The thing is that for a couple of years we have been using a methodology for teaching a course for the undergraduate level; the number of students who passed the course was approximately 90% in classrooms composed of 25 to 30 students. A problem arise when a new lecturer arrived and he proposed an alternative methodology for teaching the course, and I and other colleague hypothesize that with this new methodology the number of people who will fail the course will be pretty high. We tested the new methodology in a group of 30 students and the results were that most of the classroom failed the course; the p-value obtained was 0.018. The question here is how to present these results on a report? One criticism that we got is that a sample of 30 students is too small. Any help 
 In Chapter 5 (Neural Networks) of Bishop Pattern Recognition and Machine Learning he mentions several times that the derivative of the error function with respect to the activation for a particular output unit takes the form $$ \frac{\delta E}{\delta a_k} = y_k - t_k $$ where $E$ is the error function, $a_k$ is the activation, $y_k$ is the output, and $t_k$ is the target value. This is equation 5.18 in the book. Could someone explain how this was derived? Thank you! 
 I am using a GRU neural network in dependancy parsing. When analysing the results of my networks I found out that networks who had one hidden layer composed with 100 GRU learned much more commun data (not rare) than the large ones. The 300 GRU in one hidden layer learned the uncommun data in a better way than the other (100 GRU) but performed worse with the commun data. Any explanation? 
 Two ideas: Use resampling stats (rather than Gaussian) to determine what probability of the results you got based on prior year data. Would be especially good if you have multiple years of the old data. Compare the academic results outside of this class for the students in this class vs prior classes. As always, however, nothing beats: greater sample sizes for valid, reliable conclusions. Because: Law of Large Numbers (not "averages") and queuing theory. 
 I have a set of 3 data that represent the breakdown voltage of a sample. 140 190 140 I want to find a 90% confidence interval the breakdown voltage. First, thing I did was to find the mean and standard deviation which are 156.6 and 28.8. Next I want to calculate the confidence interval. To calculate the confidence interval, I don't know if I have to use Normal distribution or t distribution? And Also does the lower and upper boundary for the confidence interval are symmetry on the mean. Any help will be appreciated. Thanks 
 There isn't a reference, because it's too minor to be publishable. I've put the computations here . It's just algebra: multiplying out the middle term of the sandwich estimator and collecting terms so that the sums involving $\beta$ are outside the sums over rows of the data, instead of inside. 
 Note that $(Y/X)$ is on $(0,\infty)$ so $\frac{1}{n}+Y/X$ is on $(?,\infty)$, and therefore $\frac{1}{\frac{1}{n}+Y/X}$ must be on $(?,?)$. Now multiply numerator and denominator so as to get rid of the fractions in the denominator. 
 F-measure has an intuitive meaning. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances). With high precision but low recall, you classifier is extremely accurate, but it misses a significant number of instances that are difficult to classify. This is not very useful. Take a look at this histogram. Ignore its original purpose. Towards the right, you get high precision, but low recall. If I only select instances with a score above 0.9, my classified instances will be extremely precise, however I will have missed a significant number of instances. Experiments indicate that the sweet spot here is around 0.76, where the F-measure is 0.87. 
 I have several algorithms which solve a binary classification (with response 0 or 1) problem by assigning to each observation a probability of the target value being equal to 1. All the algorithms try to minimize the log loss function given by the expression $$ -\frac{1}{N}\sum_{i=1}^N {(y_i\log(p_i) + (1 - y_i)\log(1 - p_i))} $$ where $N$ is the number of observations, $y_i$ is the actual target value and $p_i$ is the probability of 1 predicted by the algorithm. Here is some R code with sample data: The sample code gives the log loss for each algorithm: Now I want to combine this algorithms so I can minimize the log loss even further. Is there any R package which can do this for me? I will appreciate references to any algorithms, articles, books or research papers which solve this kind of problem. Note that as a final result I want to have the predicted probabilities of each class and note plain 0,1 responses. 
 I am working on web logs and have a time-stamp variable in the format dd-mm-yyyy hh-mm-ss. I have earlier worked on date variable and found that best way to extract feature from date is to create dummy variables based on day-of-week month-of-the-year etc. Not sure how to deal with hourly data, and for that matter time-stamp data. Any idea how to extract feature or usable information from time-stamp for predicting sales? 
 I might add that in recent IRT work I am undertaking, when all specifications are made the same, I have found IRT results (2PLM) from the ltm package to map on to the same analyses conducted in the Mplus statistical package extremely well, including model solution indices as well as parameter estimates. 
 I have a question (reproduced below) from an exam. It seems to be presumed that the greater the (product moment) correlation coefficient, the "more appropriate" and the "better" the model. Is such a presumption valid? In an experiment the following information was gathered about air pressure $P$, measured in inches of mercury, at different heights above sea-level $h$, measured in feet. (i) Find the product moment correlation coefficient between (a) $h$ and $P$, (b) $\ln h$ and $P$, (c) $\sqrt{h}$ and $P$. ( Answers: (a) -0.9807, (b) -0.9748, (c) -0.9986. ) (ii) Using the most appropriate case from part (i), find the equation which best models air pressure at different heights. ( The answer is that we're supposed to use (c). ) 
 Imagine there is a city with a network of roads. There are specific number of entry/ exit points in and out of the city as well as interior roads that connect points within the city. There are a number of cars that enter the city each day at some time of the day and those cars leave the city the same day. As with any city, there is traffic congestion on some of the roads. Because of which cars tend to slow down and there are speed variations. Each car typically takes the same route in an out of the city, but could take alternate routes in some situations. Each car has a tracking unit on board, which can only track speed at any given point of time, it cannot track position (no GPS). The tracking units continuously send their unique identifiers (say license plate number of the car) along with current speed to a central traffic monitoring system. Unfortunately, the traffic monitoring system does not have a map of their existing network of roads. The question for the experts here is this - Given that all you have is a continuous time series of the speed of each car over a period of time (say several months), is there a way in which it is possible to reconstruct the road network (not the actual geometry, just a flow network) and the likely routes of the individual cars. I am not a mathematics/ statistics/ data science expert, so any hints as to the statistical/ mathematical approaches that can be used would be greatly appreciated. 
 I am interested in detecting multivariate outliers in a low dimensional data set ($n&lt;p$). Various high-breakdown robust methods for multivariate settings such as Stahel-Donoho and Minimum Covariance Determinant estimation is proposed in the literature. Can you give an advice and make a comparison than which estimator should be used and why? I have just found out that, due to computationally intensive calculation of Stahel-Donoho, the MCD estimation is preferable. How can I realize the intensity of calculation? Is there other reason for the superiority of MCD? Thank you! 
 I am naive in statistics. I have the data as something like this. Dataset 1: Dataset 2: Theoretically there is a certain degree of relation between data set 1 and 2. I am getting these values practically. But I do not know clearly how can I measure the correlation for the above data set. (which measures to use). Any help to guide me through this computation /which distance measure to use (how to use) , anything and everything will be so much appreciated. Along with the above, any references to get a good knowledge to solve these kind of problems also useful. Thank you 
 If your data are 17-dimensional probability histograms (i.e. every row sums up to 1) then you can try the following: Format your data as CSV, 2000 rows, 17 columns (+ a textual label column if you have one) Load the data into ELKI Choose a hierarchy extraction algorithm from the clustering.hierarchical.extraction (or so) package. ''Update'': for example which is a nice automatic way for cutting the tree without having to choose the height or the number of clusters . Choose Anderberg hierarchical clustering: because it is faster than . Try complete linkage first and maybe Choose a distance function from the probability package, say chi squared distance or (these worked well for me on such data) Then look at the dendrogram, if this exhibits nice clusters. There probably are some outliers, too. I'd focus on the dendrogram, and only treat the automatic extraction (step 3) as a guide. You have several parameters to vary to get a good result. But at the end, you really need to look at the data again, if it is any good for your problem. Here is an example dendrogram, clusters extracted with above simplified extraction and minimum size 50 objects. The data are color histograms, Chi^2 distance, group average linkage. From that plot, I would try again with a larger minimum cluster size. Let's try sqrt jensen shannon, complete linkage, and min size 100: The yellow cluster to the right should probably be cut into two (probably one is less than 100 objects). Some clusters have an outlier that should be dropped. But maybe not too bad for a start. Too bad you can't "edit" the extracted clusters in the plot. I'm not aware of an easy way to identify e.g. an outlier here easily. 
 up vote 0 down vote favorite hi I would like to make a 2D contour plot given 3 numpy arrays of same size X Y Z with (X,Y) the coordinates and Z the value at (X,Y) now, the function plt.contour seems to work only with a grid, and values at the intersection of the grid, and takes as input a rectangular grid only with 2D matrices as input. see if the input for that function is (XX,YY,ZZ)then XX is a 2D rectangular array where every line is the same and every column is constant, while YY is a 2D rectangular array where every column is the same, and every line is constant Now what function should i use, or what procedure should i follow, if i want to make the contour plot when the input values are NOT values taken on a grid ? thanks 
 I was calling my phone company the other day about some issue. At some point I was told that I'd be put on hold for a few minutes. 5, 10, 15, 20 minutes went by and I was contemplating whether to hang-up. People often do this with queues, they wait for something and if it doesn't arrive soon enough they give up. In terms of Bayesian priors something odd must be going on, the longer I wait the longer I expect to wait. You could argue that the longer you wait the less sense it makes to quit, since they must get to you eventually. On the other hand the longer you wait the slower you assume the process is. The question is, when should I hang-up? 
 In Wooldridge's Introductory Econometrics it is stated that random sampling is a Gauss-Markov assumption. As such it is a necessary condition for the unbiasedness of OLS estimators. While this can make sense intuitively, I'd really appreciate if somebody would refer me to a mathematical proof. 
 I would like to build a Markov chain model as the figure below: But I want to use an IF statement: If you are in state "Override" and the unit is ON then, P_22 = P_22a and P_21 = P_21a. Or if you are in state "Override" and the unit is OFF then, P_22 = P_22b and P_21 = P_21b. So the probabilities for P_22 and P_21 will change according to an IF statement. I wonder if I can do like this, and still call it a Markov Chain or if this type of Markov chain has a specific name. 
 Im currently analyzing a three way interaction in R and my data looks like this: My anova results looks like this: Response: value My question is then, what is the best way to "explore" or analyse this three way interaction and present it? 
 I am learning log-rank test for performing survival analysis. I read article " Statistics review 12: Survival analysis ", section "Comparing survival curves of two groups using the log rank test". However, I could not understand when we can say that the test statistic is significant. I appreciate your help. 
 How would we calculate the information loss of reducing dimensions using PCA ? Would it be the amount of variance loss if we skip certain eigenvectors after the PCA ? 
 My R script run with Knitr generates a statistic summary of electrode pairs accompanied with plots. The problem is that knitr does not render plots in correct sections. In section for pair A knitr do not wait for plot and start to evaluate code for output summary for pair B. So in section B knitr places plot from section A among printed output summary for pair B. My function analizePair calculates some statistics and print them out. I call a ezPlot function inside analizePair function. I also tried to put Sys.sleep(2) after analizePair, but it do not solve the problem. How to force knitr/R to wait until plot is generated and put before starting next section (next iteration of for loop) ? 
 I have a question regarding the interpretation of an interaction term in the following model: $$ Y=β_0+β_1X+β_2Z+β_3XZ $$ $X$ and $Z$ are continuous and $X$ is negatively related to $Y$, whereas $Z$ is positively related to $Y$. Is it true that a negative interaction ($β_3$=negative) suggest that the relationship between $X$ and $Y$ becomes less negative as the value of $Z$ increases? And a positive interaction ($β_3$=positive) suggests that the relationship between $X$ and $Y$ becomes more negative as the value of $Z$ increases? 
 If $X∼Bin(n,p)$, what's the distribution of $Y=log(X)$? Thank you. 
 If we have a dataset like x=(3,4,2,1,4,...,5), we have classic methods (method of moments, maximum likelihood method, etc) to fit a distribution. However, in certain real life cases, we can have uncertain about our dataset: if they are mesurements, there can be incertainty due to diverse instruments, if they are survey resutls, people can be very sure about one choice, and give an estimation, between x and y. sometimes, the data comes from a retreatment: when estimating a fortune of a person who lived in 1900, we have to take into account the inflation. etc. So, let's then consider that along with the initial dataset, we can also give the standard deviation of estimation of each dataset: sdx=(1,0.4,0.3,1,0.7,...,0.8). How can we take this information when fitting a distribution? There are some well established methods (derivated from method of moments, or maximum likelihood method, etc.), any R packages ? When doing a regression, I think that this problem can also be encountered. It seems that weighted regression could partillay deal with this. 
 Eq. 5.11 defines the error as: $$ E(w) = \frac{1}{2} \sum_{n=1}^N ||y(x_n, w) - t_n||^2 $$ Just above eq. 5.18, it is assumed that $y_k = a_k$, since regression uses linear output function (identity) at the output. Thus: $$ E = \frac{1}{2} \sum_{n=1}^N ||a_n - t_n||^2 $$ Now we derive w.r.t. $a_n$: $$ \frac{\delta E}{\delta a_n} = \frac{1}{2} 2(a_n - t_n) = a_n - t_n $$ 
 Why 1 and 5? Because they feel right. I'm sure there are studies on the emotional value and cognitive salience of specific numbers, but we can understand the choice of 1 and 5 without having to resort to research. The people that created today's statistics were born, raised and live in a decimal world. Of course there are non-decimal counting systems, and counting to twelve using the phalanges is possible and has been done, but it is not obvious in the same way as using the fingers is (which are therefore called "digits", like the numbers). And while you (and Fisher) may know about non-decimal counting systems, the decimal system is and has been the predominant counting system your (and Fisher's world) in the past hundred years. But why are the numbers five and one special? Because both are the most naturally salient divisions of the basic ten: one finger, one hand (or: a half). You don't even have to go so far as to conceptualize fractions to get from ten to one and five. The one is simply there, just as your finger is simply there. And halving something is an operation much simpler than dividing it into any other proportion. Cutting anything into two parts requires no thinking, while dividing by three or four is already pretty complicated. Most currenct currency systems have coins and banknotes with values such as 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000. Some currency systems do not have 2, 20 and 200, but almost all have those beginning in 1 and 5. At the same time, most currency systems do not have a coin or banknote that begins in 3, 4, 6, 7, 8 or 9. Interesting, isn't it? But why is that so? Because you always need either ten of the 1s or two of the 5s (or five of the 2s) to arrive at the next bigger order. Calculating with money is very simple: times ten, or double. Just two kinds of operations. Every coin that you have is either half or a tenth of the next order coin. Those numbers multiply and add up easily and well. So the 1 and 5 have been deeply ingrained, from their earliest childhood on, into Fisher and whoever else chose the significance levels as the most straightforward, most simple, most basic divisions of 10. Any other number needs an argument for it, while these numbers are simply there. In the absence of an objective way to calculate the appropriate significance level for every individual data set, the one and five just feel right. 
 How do I determine survival function of x given hazard rate of x is h(x) = hazard function; hazard function is greater than 0 
 I have a biometric data set of size $n \sim 150,000$, with $p=21$ features $x_1,\dots,x_p$ representing information about people $-$ some categorical and some numerical. The original data set had only $9$ features, although a regional feature associated to each data point allowed me to add $12$ more features but which prediction power should be weak. The data is not of very good quality. My target variable is a binary class label $y \in \{0,1\}$. The data is highly unbalanced, as I have $95.5\%$ label $1$ samples and $4.5\%$ label $0$ samples. I am constrained to using a Support Vector Machine (SVM) classifier for this task, that I can nonetheless tune it with other algorithms (e.g. SGD) and kernels. Given the unbalance, I have consulted the following paper , where in page 13 they recommend defining two $C$ values such that: $$ \frac{C_0}{C_1}=\frac{n_1}{n_0} $$ where $n_c$ is the number of label $c$ samples and $C_c$ the $C$ value associated to that class, $c \in \{0,1\}$. This method allows to control for the unbalance $-$ assuming that the number of misclassified examples from each class is proportional to the number of examples in each class. The SVM objective function becomes: $$ \frac{1}{2}\|\mathbf{w}\|^2 + C_0\sum_{i \in I_0}{\xi_i^{\,(0)}} + C_1\sum_{i \in I_1}{\xi_i^{\,(1)}} $$ where $I_c = \{i: y_i = c\}, c \in \{0,1\}$. However, I am a bit uncomfortable using this method: I get $C_0 \sim 22 \times C_1$ and there isn't really a justification for overweighting class $0$ with respect to class $1$ $-$ as I have explained, it is more important to predict class $1$ than $0$. On top of that, if I evaluate the performance of my classifier with respect to accuracy, I get a worse test result $-$ $\sim 63\%$ $-$ than with a dummy majority classifier with test accuracy $\sim 95.5\%$ which, in this context, is not an unreasonable classifier. Given that it is more important to predict class $1$ than class $0$, how is it possible to tackle this problem in a reasonable way? Is it realistic to set $C=C_0=C_1$ and nonetheless manage to get an accuracy higher than $95.5\%$? 
 Studying AR models, I found that there are two properties that these models can have stationarity and causality. For what concerns stationarity , I have studied that this condition is satisfied if the equation $\phi(B) = 0$ has all roots outside the unit circle, i.e. they are in modulus greater than one. Instead, for what concerns causality , I am having some troubles: I mean, the conditions for causality seem to me the same of stationarity (at least for what concerns simple $AR(1)$, $AR(2)$). Moreover, I am not sure of having understood what does causality actually mean. Increasing my doubts, I have also found that some people talk about an invertibility condition for $AR$ models, but I have understood that it was only concerning $MA$ models and that it is a kind of counterpart for stationarity, given the fact that $MA$ are always stationary. Could you help me making a bit of order in my mind? 
 I have a theory $f$ (actually a set of coupled non-linear differential equations) that, from a vector of $n$ initial conditions $\vec x$, is able to predict $m$ values $f(\vec x) = \vec y$. I can measure $\vec y$ in nature with some covariance $\mathbf \Sigma$. I want to find the initial conditions $\vec x$ that most likely explain $\vec y$ according to my theory $f$. So I will obtain an estimate $\hat x$ using some sort of $\chi^2$ minimization such as $$\hat x = \underset{\vec x}{\arg\min} \; \sum {\frac{\left[ f(\vec x) - \vec y \right]^2}{\mathbf \Sigma^2}}.$$ I know from my theory that two measurable variables $y_i$ and $y_j$ co-vary. I can simulate $f(\vec x)$ for many different choices of $\vec x$ and find that their covariance is strong. However, they are measured independently of each other, so their measured covariance is 0. How can I propagate the simulated covariance into the objective function? 
 Consider the null $\theta=\theta_0$ and examine the constrained optimization problem of maximizing the log-likelihood subject to the constraint imposed by the null: $$ \phi=\log L(\theta)-\lambda(\theta-\theta_0) $$ The first order conditions are $$ \frac{\partial\log L(\theta)}{\partial\theta}=\lambda $$ and $$ \theta=\theta_0 $$ Substituting yields $$ \frac{\partial\log L(\theta_0)}{\partial\theta}=\lambda $$ The l.h.s. of this expression is, of course, the score, whereas the r.h.s. is the Lagrange multiplier. Either version of the test statistic would next normalize by some estimator of the variance of the score/$\lambda$ under the null so as to get a standard normal test statistic when the null is true. 
 Reading about the true meaning of 95% confidence ellipse, I tend to come across 2 explanations : The ellipse that contains 95% of the data Not the above, but the ellipse that explains the variance of the data. I am not sure I understand correctly but they seem to mean that if a new data point coming in, there is a 95% chance that the new variance will stay in the ellipse. Can you shed some light ? Thanks. 
 Introduction Suppose I observe 30 subjects attempt a given task in 3 separate occasions and I give them a score. To analyse each subject's performance over time, I can use a multilevel / mixed effect / hierarchical model in which intercepts and slopes are allowed to vary by subject as follows (unfortunately the terminology and the notation vary across disciplines, so bear with me here): $$ Score_{ij} = \beta_{0j} + \beta_{1j} \times Time_{ij} + e_{ij} $$ Where: $$ \beta_{0j} = \beta_{0} + u_{0j} $$ $$ \beta_{1j} = \beta_{1} + u_{1j} $$ Here, j = {1, 2 ... 30} indexes subjects and i = {1, 2, 3} indexes the observation. In some disciplines, observations are considered to be nested within subjects, so subjects are the level-2 index and observations the level-1 index (hence "multilevel" or "hierarchical" models; but again, in some literature one finds that higher numbers indicate more aggregate levels whereas in other literature it is the opposite). Time is a categorical variable that is zeroed at the first observation to model linear change between subjects and within subjects over time (note the two indices) and thus it takes the values Time = {0, 1, 2}. Now, while $\beta_0$ and $\beta_1$ give the mean intercept and slope that are the same across all subjects (some call them "fixed" effects but the terminology is disputed), $u_{0j}$ and $u_{1j}$ are parameters that vary by subjects and are assigned probability distributions (which is why some call them "random" effects). Indeed: $$ u_j \sim N(0,\Omega_u) $$ $$ \Omega_u = \begin{bmatrix}\sigma^2_{u0} &amp; \\ \sigma_{u01} &amp; \sigma^2_{u1}\end{bmatrix} $$ And $e_{ij}$ is the residual error, that is, how far the observed values vary around each subject's prediction, which is $\hat{y}_j = \hat{\beta} + \hat{u}_j$. $$ e_{ij} \sim N(0, \sigma^2) $$ One can say that $u$ are the level-2 random effects and $e$ the level-1 residual. Now that the definitions are out of the way, here comes the question. Question How do I calculate the standard error and confidence intervals around a subject's fitted value at each time point? Is my attempt below correct? My take Let us consider the first subject, so that j = 1 (I will not include the subscript in the section below for clarity). At time 0 (i.e., Time = 0, i = 1), the fitted value for this individual is: $$ \hat{y}_{i=1} = \hat{\beta}_0 + \hat{u}_0 + 0 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0 $$ This is the fitted intercept for subject 1. Clearly the residual error is not part of the equation because it is $e_{ij} = y_{ij} - \hat{y}_{ij}$. Because $Y$ is a sum of random variables, the variance on this subject's intercept should be: $$ var(\hat{y}_{i=1}) = var(\hat{\beta}_0) + var(\hat{u}_0) + 2 \times cov(\hat{\beta}_0, \hat{u}_0) $$ However, this page tells me that, and I quote, the model assumes the fixed and random effects to be orthogonal so $cov(\hat{\beta}_0, \hat{u}_0) = 0$ and the variance reduces to $var(\hat{y}_{i=1}) = var(\hat{\beta}_0) + var(\hat{u}_0)$. At time 1 ( Time = 1, i = 2), the fitted score for subject 1 is: $$ \hat{y}_{i=2} = \hat{\beta}_0 + \hat{u}_0 + 1 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0 + \hat{\beta}_1 + \hat{u}_1 $$ Again a sum of random variables with $\beta_1$ and $u_1$ being multiplied by a factor $Time = x = 1$, so the variance is: $$ var(\hat{y}_{i=2}) = var(\hat{\beta}_0) + var(\hat{u}_0) + var(\hat{\beta}_1) + var(\hat{u}_1) + 2 \times (cov(\hat{\beta}_0, \hat{\beta}_1) + cov(\hat{u}_0, \hat{u}_1)) $$ I think that the formula is justified because $\beta$s and $u$s are orthogonal across each other, but not between each other: the global intercept and slope are correlated, and so are the subject's intercept ans slope (and indeed packages like in R will return these correlations, we'll get to this later). Finally, at time 2 ( Time = 2, i = 3), we have: $$ \hat{y}_{i=3} = \hat{\beta}_0 + \hat{u}_0 + 2 \times (\hat{\beta}_1 + \hat{u}_1) = \hat{\beta}_0 + \hat{u}_0 + 2\hat{\beta}_1 + 2\hat{u}_1 $$ And: $$ var(\hat{y}_{i=3}) = var(\hat{\beta}_0) + var(\hat{u}_0) + 2^2var(\hat{\beta}_1) + 2^2var(\hat{u}_1) + 2 \times 2cov(\hat{\beta}_0, \hat{\beta}_1) + 2 \times 2cov(\hat{u}_0, \hat{u}_1) $$ Since $\beta_1$ and $u_1$ have now a factor $Time = x = 2$ in front of them, their variance is multiplied by $x^2 = 2^2$. Likewise, we have to take two times the sum of the covariances. The covariance between $\beta_0$ and $\beta_1$ is $x_{\beta_0}x_{\beta_1}cov(\hat{\beta}_0, \hat{\beta}_1)$, that is, $1\cdot{2}\cdot{cov(\hat{\beta}_0, \hat{\beta}_1)}$. The covariance between $u_0$ and $u_1$ is $x_{u_0}x_{u_1}cov(\hat{u}_0, \hat{u}_1)$, that is, $1\cdot{2}\cdot{cov(\hat{u}_0, \hat{u}_1)}$. Since we have to take twice their sum, this expands as in the equation above, $2 \times 2cov(\hat{\beta}_0, \hat{\beta}_1) + 2 \times 2cov(\hat{u}_0, \hat{u}_1)$. Right? Finally, if I want to calculate the 95% confidence interval around any of the fitted values (i.e., any of the $y$s at time 0, 1, or 2) for my subject I calculate: $$ CI_{\hat{y}_i} = \pm 1.96 \times \sqrt{var(\hat{y}_i)} $$ Is this correct? I am asking because the CIs I calculate using these formulas are rather different from those simulated by and (yes, those are simulation, but I have run many of them and results are consistenly different), so I am wondering whether it is my theory that is flawed. If the theory is correct, I have a reproducible example ready to add that compares the results using my approach and these two packages and we'll figure out why outcomes differ so much. Thank you, k. 
 Let's say I have a data set where half of the data points are labelled as positive and half are labelled as negative. My task is to create a classifier which recognises when a sample from the dataset is positive. The most useless classifier I could come up with would be to flip a fair coin whenever I get a sample to decide if the sample is positive or negative. One way to quantify the performance of that classifier is the f1 score which is in expectation 0.5 for a large dataset since recall is expected to be 0.5 and precision is expected to be 0.5. Additionally the f1 score should in principle be concentrated. That is intuitively the baseline f1 score I would compare everything to. Now instead of that useless classifier I could create another classifier which in my opinion is just as useless where I simply say that all the samples I receive are positive. In that case my precision is still 0.5 but the recall is 1. This leads to an f1 score of 2/3 so if I use the f1 score as a measure to decide which classifier is better I should pick this one instead of the random one. This is perhaps more of a philosophical pondering but in my opinion a way to select a classifier should not distinguish between those two classifiers (why should I prefer the second option?). Therefore I wanted to ask about alternatives that do pass such a test. I'd prefer a single number which is a function of true positive rate, false positive rate, true negative rate and false negative rate. 
 Actually, neither explanation is correct. A confidence ellipse has to do with unobserved population parameters , like the true population mean of your bivariate distribution. A 95% confidence ellipse for this mean is really an algorithm with the following property: if you were to replicate your sampling from the underlying distribution many times and each time calculate a confidence ellipse, then 95% of the ellipses so constructed would contain the underlying mean. (Note that each sample would of course yield a different ellipse.) Thus, a confidence ellipse will usually not contain 95% of the observations. In fact, as the number of observations increases, the mean will usually be better and better estimated, leading to smaller and smaller confidence ellipses, which in turn contain a smaller and smaller proportion of the actual data. (Unfortunately, some people calculate the smallest ellipse that contains 95% of their data, reminiscent of a quantile, which by itself is quite OK... but then go on to call this "quantile ellipse" a "confidence ellipse", which, as you see, leads to confusion.) The variance of the underlying population relates to the confidence ellipse. High variance will mean that the data are all over the place, so the mean is not well estimated, so the confidence ellipse will be larger than if the variance were smaller. Of course, we can calculate confidence ellipses also for any other population parameter we may wish to estimate. Or we could look at other confidence regions than ellipses, especially if we don't know the estimated parameter to be (asymptotically) normally distributed. The one-dimensional analogue of the confidence ellipse is the confidence-interval , and browsing through previous questions in this tag is helpful. Our current top-voted question in this tag is particularly nice: Why does a 95% CI not imply a 95% chance of containing the mean? Most of the discussion there holds just as well for higher dimensional analogues of the one-dimensional confidence interval. 
 I have a question where I have some (n) inputs and one output. I have about 15000 training examples of such data. My problem is as follows: In the input, some (m) variables can be selected by me, but the rest (n-m) are out of my control. Now at any given time, I will be given the values for these (n-m) variables. My job is to give the values for the rest m variables, such that I receive highest possible value for my output. How do I approach such a problem? Are there any specific genetic algorithms designed for such a problem? Or any mathematical modelling techniques specific to such a problem? 
 I am conducting a meta-analysis of cognitive-behavioural interventions on self-esteem and I could use some assistance navigating through the statistical procedures. I have extracted pre-post scores from single group designs and experiment/control scores from RCTs and calculated g for self-esteem rating scales and depression rating scales. Based on the nature of the studies, there are four themes that I would like to explore and I would like to ensure that I use the correct statistical tools and set the correct hypotheses. The first task is to see if there is a difference between single-day workshops and multi-session therapy over several weeks. The hypothesis is that the former will yield smaller effect sizes than the latter. Second, we understand that single-group designs tend to have larger effect sizes than RCTs and we would like to see if that is the case. Third, we would like to test to see if the three different CBT-based interventions that are used are significantly different. We believe that they will not be. Lastly, we would like to know if the level of self-esteem prior to treatment is a predictor of outcome. It is unclear as to the direction that this might take, though. It could be argued that there is a ceiling effect, which limits the impact of an intervention on people with high self-esteem, or that low self-esteem is enduring enough to resist treatment compared to those with higher self-reported self-esteem. Would it be as simple as creating dichotomous variables for each of the above and then using a meta-regression analysis of the four variables using R or would some of them be better suited to a Z-test between subgroups? I assume that as we would see in primary studies, multiple Z-tests would lead to a rise in the overall alpha. I am mindful that the third variable (intervention) is an a priori acknowledgment of the null hypothesis and the fourth one might be too vague for inclusion, so I would appreciate any guidance on how to proceed correctly. 
 We are looking at tournament performance of chess players over time and have a question about the random effects modeling for this. Specifically, every chess player belongs to at least one club, but some of them can belong to two clubs. Players play in multiple tourneys per year. We would like to integrate this into the model, but it is not entirely clear to us how we would specify it in for example lme4 in R. Edit: Data example just to illustrate the problem Considering for simplicity just to model the random effect intercepts and without a nesting structure: A model with would exclude all players that are not part of a second club. Is there any way to include a second club random effect in this model when only a smaller subset of all players possess this random effect (in addition to the other club), without losing all the ones who only belong to one club? Thank you! 
 Degrees of freedom = 27 (35 - 8) Residual sum of squares = 668.8 (the residual deviance of size:side) MS = 668.8/27 Then just set up the ANOVA table, for example with size:type; Factor size:type SS 804.2 DF 2 MS 402.1 F 402.1/(668.8/27) And continue for each interaction Then just do these for each interaction 
 While experimenting with Spark library MlLib, I questioned myself if I understood well the mechanism of PCA algorithm, because output of MlLib algorithm was not what I expected to get. so for given data of 20 features and 10 observations: I applied pca and then transform my initial data : I expected to have the rdd only with 5 features, but the values of those features should be the same, as in the initial rdd. As far as I understand pca, it keeps only the features/vectors that have the most important variance score. But instead of this I've got the rdd of 5 features, but values of those features were modified in the way I don't understand. Before: [1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0] After : [1.14461218805706,0.8986586176492057,-0.580979228415323,0.09346270935831252,0.238925870721517] Before: [1.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0] After : [1.1606629979632594,0.6110896934565042,-0.3717757311830274,-0.22954508416826663,0.18370214647002148] Before: [0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0,0.0] After : [1.6723486323814756,0.41082473274832476,-0.7526031076025419,-0.26291312583276166,0.11992366811728228] Could you explain me why those features are modified? and when my understand of pca is wrong? Does the algorithm make something else with the data, and not only selection of features? 
 PCA is not a feature selection method, but it can be used for that purpose. PCA is simply a feature transformation, where it first chooses linear subspaces that are orthogonal to each other (eigenvectors), then projects your original data on these subspaces and returns you the projected vectors. That's why you see different values. As for feature selection, what is usually done is to utilize the fact that the eigenvectors are ordered in decreasing variance, which is quantized by the eigenvalues. So when you select the first (which is or ) components, you still hold the axes that explain a lot (90% or 99%) of the variance. Therefore, instead of your original dimensions, you can continue your task with a smaller set of dimensions (combinations of input features) without much information loss. 
 Consider the following true data generating process (DGP): $$y = x_1\beta_1 + \ldots + x_p \beta_p + \varepsilon = x_{true}' \beta + \varepsilon$$ where $x_i \in {R}^n$, $y \in {R}^n$ and ${E}(\varepsilon) = 0$, $Var(\varepsilon) = \sigma^2$. The corresponding OLS estimator is denoted by $\hat{\beta}_{true}$. Underfitting: the model is estimated with less variables than needed. I.e. $q&lt;p$ variables are used: $$y = x_1\beta_1 + \ldots + x_q \beta_q + \varepsilon = x_{under}' \beta + \varepsilon$$ My aim is to prove that in this case, the OLS prediction is biased but with smaller variance. \begin{eqnarray*} {E}(x'_{under}\hat{\beta}_{under}) \neq x_{true}'\beta \quad &amp; &amp; Var(x_{under}'\hat{\beta}_{under}) \leq Var(x_{true}'\hat{\beta}_{true}) \end{eqnarray*} $ $\ Progress: I have been doing progress in the expected value. If the true model is the next:$$Y=X_{True}\beta_{True}+\epsilon$$, and in the underfitting model we use less variables, we have that: $$Y=X_{under}\beta_{under}+\epsilon$$. We can observe that: $$\hat{\beta}_{True} = (X_{True}^T X_{True})^{-1}X_{True}^T Y$$ $$\hat{\beta}_{under} = (X_{under}^T X_{under})^{-1}X_{under}^T Y$$ so $$E(\hat{\beta}_{True})=\beta_{True}$$ and $$E(\hat{\beta}_{under})=\beta_{under}$$. So they are different, ¿Is this correct? 
 Generally, you can do that. Things to consider: One input always means just one numeric input value. So with four input variables, you can only use one sample of your dataset (for example the last month) to predict the next month. Your attributes contain categorical attributes. Consider encoding them in a One-Hot vector . In a 4-3-1 network, you only have one numeric output value. That is fine, if you want to output the total sales. If you want to predict multiple attributes or a categorical one, you should have multiple output neurons as well and use one-hot encoding for the categorical attributes. It might make sense to consider multiple months for the prediction (let's say the last 5 months). You can let the network use the data of multiple months by either increasing the number of input values and feeding the data directly or by using a Recurrent Neural Network , preferably a LSTM network. However, I don't know, if R has a library for LSTM networks. 
 How can I prove the following assertion? If $x = A^{-1} \theta$, then $\nabla _ x f = A^T \nabla_\theta f$. I have found this in a section on natural gradient descent in Pieter Abbeel's lecture notes. 
 I thought that (shaded) error regions are used when the data on the x axis is continuous rather than discrete. However in the plot below, data points had discrete values on the x axis, nevertheless an error region was used instead of multiple error bars. This doesnt seem to have been done for esthetic reasons, as the inflexion points in the error region don't correspond to the inflexion points in the lines (the x positions of the data points) 
 I have a homogeneous Markov chain with transition matrix I want to compute $P(Y_1 = 1| Y_2=2)$ where $Y_t, t=1,2$ is the observation at time $t$ and $Y_0=3$. I tried with Bayes' rule, so $$P(Y_1 = 1| Y_2=2)= \frac{P(Y_2=2|Y_1=1)*P(Y_1=1)}{P(Y_2=2)}.$$ But from the transition matrix I can get only $P(Y_2=2|Y_1=1)=0.3$. How do we compute marginal probabilities $P(Y_2=2)$ and $P(Y_1=1)$ from a transition matrix? 
 The solution is to use the function with to remove the empty levels of the interaction term. Here's how it looks in context: 
 You would typically also be given a distribution $\pi$ over initial states. You could then read $P(Y_1 = x)$ directly off of $\pi$, and compute $P(Y_2 = x) = \sum _z P(Y_1 = z, Y_2 = x)$. 
 Assume that $$ E(X+Y)=E(X-Y)=0 $$ $$ V(X+Y)=3 $$ $$ V(X-Y)=1 $$ Show that $E|X+Y|\leq\sqrt3$. If in addition, it is given that $(X,Y)$ is bivariate normal, calculate $E|X+Y|^3$. For the 1st part, I used the inequality that $$ E|X+Y|\leq \sqrt{E(X+Y)^2}=\sqrt3. $$ But I don't know how to approach the 2nd part. One thing that I understood is, $$ X+Y\sim N(0,\sigma_1^{2}+\sigma_2^{2}+2\sigma_1\sigma_2\rho), $$ where $$ V(X)=\sigma_1^{2} $$ $$ V(Y)=\sigma_2^{2}. $$ Here we can see that $Var(X)+Var(Y)=2$. How can we find them separately from the given data? Also $E|X+Y|^3$? 
 Suppose I have a data set containing every tax return lodged with a particular tax authority. Suppose I would like to allow people to study trends and patterns but not see any individual's tax return information. I can produce a lot of aggregate statistics which don't necessarily identify any individual or disclose any information about their tax affairs. A simple one might be the average taxable income for various age groups. A more complicated one might be the number of individuals in each age group where the claim for interest deductions is above a certain threshold. My understanding is that, when aggregates are released publicly by government authorities, they are first checked by statisticians who de-identify the data. The most detailed information I can find online is about censuses, but presumably the same techniques apply to tax statistics. I have heard it called 'statistical disclosure control'. Techniques include blanking out cells with only a few individuals in them, applying random adjustments to numbers, and 'swapping' records between groups. Statisticians are expensive and the requirement for human vetting precludes online querying. Is it possible to do this automatically, that is, run a program that de-identifies any arbitrary aggregate data? Or do you need to examine each table and decide what techniques to use based on the characteristics of that data? What I have in mind is that I would like to (but I don't know if it is feasible) be able to ask any pretty much question that boils down to an SQL query containig a GROUP BY clause and be able to get an answer without breaching any individual's privacy. 
 I've recently started to read about sparsity estimation. Let's recall that the sparsity function is defined as $s(\tau)=f(Q(\tau))$, where $f$ is the population density and $Q$ is its quantile function. A particularly popular estimador was obtained by Siddiqui in 1960 [1] and studied in depth by Bloch and Gastwirth in 1968 [2]. This estimator allows the simple form: $$S_{mn}=\frac{n}{2m}(X_{[n\tau]+m:n}-X_{[n\tau]-m+1:n}),$$ where $X_{i:n}$ is the i-th order statistic of a sample $\{X_i\}$ of $f$. On the other hand, a natural and a priori naive approach to sparsity estimation would be to make a kernel density estimation of $f$, $\hat{f}$, evaluate it at the sample quantile $X_{[n\tau]+1:n}$ and then invert it. This is: $$s(\tau)\approx\frac{1}{\hat{f}(X_{[n\tau]+1:n})}.$$ Now, I would certainly expect Siddiqui's estimator to perform much better than this one. However, I have carried simulation studies to compare them and they have shown the opposite. So my question is: why has Siddiqui's idea received so many attention in the literature?. [1] [2] https://projecteuclid.org/download/pdf_1/euclid.aoms/1177698342 
 I have some problems with IV estimation. I read in several works of experiments that using the 2SLS with just one instrumental variable but more than one $X$ variables. When I try to run a 2SLS regression the software ask me to put a number of instrumental variables that is almost equal to the $X$ variables. This stuff is new for me, so my question is very simple. How can I achieve a 2SLS using only 1 instrumental variables and more than 1 $X$ variables? 
 I have a subjectivly healthy population of approx 1200 individuals with three measurements on the continous scale, we can call them y1, y2 and y3. All of these are strongly related to age in a non-linear fashion and I would like to create age-dependent uni- and multivariate 95% reference bands. The variables y1, y2 and y3 are not normally distributed and, at least one, are almost impossible to tranform to normality (the problem really is that the residuals becomes non-normal or heteroskedastic no matter what I try). I'm guessing the reason for this are the presence of outliers. I can't really exclude the outliers from the material as I have no evidence of data entry error or disease. Robust methodology seems like the path to take. Would the following strategy be "statistically" sound: Perform robust regression (mmregress in stata with appropriate fp terms) on untransformed data (three regressions with y1, y2 and y3 as dependent variable and age as independent variable). Find outliers by plotting robust mahalanobi's distance of residuals (mcd in stata) against expected chi2-distribution and visually identify ouliers (mark them, but keep them). Transform y1,y2,y3 to approx multivariate normal distribution (mboxcox in stata). Perform robust regression with transformed y1,y2,y3 with appropriate fp terms of age. Calculate residuals for the three regressions (observed minus predicted value). Check assumption: are the residuals univariatly normal distributed for all ages? Are they multivariatly normally distributed? (if not so for the full populations maybe it is sufficient if the assumptions hold for the outlier-free sample) Check assumption: are the variances and covariances constant (lowess of squared residuals and residual products) with regard to age? (if not so for the full populations maybe it is sufficient of the assumptions hold for the outlier-free sample) Use mcd to obtain the robust variance-covariance matrix of the residuals. Take sqrt of diagonal terms of the variance-covariance-matrix and use as a "robust" SD for the univariate reference intervals (predicted +/- 1.96*SD) and then transform back to original scale. Calculate the 95% multivariate reference region by using (y-py)'C-1(y-py)=7.815 where y and py are 3 element column vectors with observed and predicted values respectivly, C-1 the inverted "robust" variance covariance matrix and 7.815 from the chi2 distribution for a 95% reference regions and 3 df. Transform back to original scale. Main hesitations: Is it ok to relax the assumptions by just checking for them in the outlier free subsample? Is it ok to use the robustly estimated variance-covariance matrix for calculation of univariate SD and reference intervals? Any thoughts? Objections? Suggestions? 
 In classification, if I take 2 features and color them according to label, I obtain a plot like this , which gives intuition about the effectiveness of my features. How can I do a similar plot for regression? My aim is to gain (and give, in a paper or presentation) intuition about different features I use as input to a kernel ridge regression. The only way I can think of is to take one dimension (i.e. one feature/input) and place it into a plot where x = feature, y = label. But I'm not sure if it will make sense. Maybe an ordering in x or y will make it nicer. But still not sure if this is a good idea enough to do. So I'm open to any advice :) Thanks in advance, 
 I compared two models in R using anova() function and this is the output I got: Model 1: y ~ x2 + x7 + x8 Model 2: y ~ x8 Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 24 68.281 2 26 148.872 -2 -80.591 14.163 8.667e-05 *** Signif. codes: 0 ‘ ’ 0.001 ‘ ’ 0.01 ‘ ’ 0.05 ‘.’ 0.1 ‘ ’ 1 How should I interpret the value for Sum of Squares for model 2? I know sum of squares is always positive so I am confused about this one. 
 I have a dataframe in R, which has in the rows some observations and in the columns various features--both quantitative and qualitative--. I want to reorder somehow the columns of my data frame, it order to start with 5 specific numeric variables in the start, which belong in a specific group. My notion for this, is that afterwards i would like to use a PCA variant methodology, which needs the groups of my continuous variables to be ordered together in a group sequentially. In my case, the columns of my data frame are ordered like this: colnames(dat) [1] "SUV" "CCT5" "FD" "k3" [5] "MTHFD1" "HPGDS" "PAPSS2" "INF" [9] "SLC26A2" "k4" "NPM1" "TIMP1" [13] "NR3C2" "CA2" "UGDH" "MEF2C" [17] "POP1" "ENO1" "DKC1" "EDNRB" [21] "VRK1" "CD44" "GSTP1" "AARS" [25] "PDE2A" "MAOA" "CCT8" "CA4" [29] "PPA1" "RAE1" "PGM1" "PPP1CB" [33] "ACADM" "NME1" "PPARG" "PRDX4" [37] "PPARGC1A" "CTPS1" "CD36" "IARS" [41] "CCT4" "CA1" "SULT1A2" "ACADS" [45] "Meta_factor" "Disease" "Location_Type" "Tumor_Stage" [49] "Lymph_Node" Thus, i would the columns of my data frame to start with the following variables: FD, SUV, K3, INF,k4 and then the following variables. Is this possible from a function or package in R ? 
 Which is preferred for use in journal publication: Root-Mean-Square Error or Root Mean Square d error? Root-mean-square sounds a little casual, like "ice tea" instead of "iced." On the other hand, Cross-Validated's own tag uses just "square." If there is a difference in usage across field, environmental science would be most relevant. 
 Thinking about this question, I came across Bartholemew et al (2011), which lists the following assumptions of the linear factor model, assuming $p$ observed variables: iii) $e_{1}, e_{2}, ..., e_{p}$ are uncorrelated with each other v) the $f$s are uncorrelated with the $e$s They write on p181 that Assumptions (iii) and (v) imply that the correlations among the $x$s are wholly explained by the factors. However, as far as I can tell they don't elaborate on this. Why are these conditions sufficient for the factors to wholly explain the observed variables? Are those conditions necessary for the factors to wholly explain the observed variables? Bartholomew, D. J., Steele, F., Galbraith, J., &amp; Moustaki, I. (2008). Analysis of multivariate social science data. CRC press. 
 Here are some textbook references: Page 546 of Ott and Longnecker says "recall that MS stands for mean square" Page 211 of DeGroot and Schervish (3rd edition) defines the mean square d error Page 30 of Draper and Smith (3rd edition) discusses the mean square. Page 158 of Draper and Smith, discusses MSE, which they call the residual mean square $s^2$. Page 202 of elements of statistics and Probability by Lippman defines mean square d error. The two that mention the term RMSE directly, add the "d" at the end, but the others do not for the inner term mean square. 
 So this is a confusion matrix in case any readers haven't seen one: $$ \begin{array}{l c c} &amp; Predict + &amp; Predict-\\ Actual + &amp; a &amp; b \\ Actual - &amp; c &amp; d \\ \end{array} $$ And this is the formula for calculating $F_1$ from a confusion matrix: $$F_1 = \frac{2a}{2a+b+c}$$ So if half the items are positive in reality and all are predicted positive: $$\begin{array}{l c c} &amp; Predict + &amp; Predict-\\ Actual + &amp; 50 &amp; 0 \\ Actual - &amp; 50 &amp; 0 \\ \end{array}$$ Then $F_1$ in this case is equal to $2/3$ as you stated in your question: $$F_1 = \frac{2(50)}{2(50)+50+0} = 0.67$$ But if half the items are positive in reality and all the items are randomly predicted, then there are many different ways for this to occur and the resulting $F_1$ score will range between $0.00$ and $1.00$. Let's imagine we have just two items, then there are four possible results for the random classifier: $$\begin{array}{l c c c} Item &amp; Reality &amp; Predict_1 &amp; Predict_2 &amp; Predict_3 &amp; Predict_4 \\ \hline 1 &amp; + &amp; + &amp; + &amp; - &amp; -\\ 2 &amp; - &amp; + &amp; - &amp; + &amp; -\\ \hline F_1 &amp; &amp; 0.67 &amp; 1.00 &amp; 0.00 &amp; 0.00\\ \end{array}$$ So all that was just to say that the value of $F_1$ for the random classifier is actually quite variable, unlike the always-positive classifier. The alternative I'd suggest is the $S$ score first proposed by Bennett, Alpert, &amp; Goldstein (1954). It assumes a 50% probability of classifying any given item into its correct class "by chance" alone and discounts this from the final score. It also uses all cells of the confusion matrix (unlike $F_1$ which ignores $d$ or the number of "true negatives"). $$p_o = \frac{a+d}{a+b+c+d}$$ $$S = \frac{p_o - 0.5}{1-0.5}=2p_o-1$$ In the first example above, where $F_1=0.67$, the $S$ score would be equal to $0.00$ and would capture the idea that the predictions are doing no better than would be expected by chance. $$p_o = \frac{50+0}{50+0+50+0}=0.50$$ $$S = 2(0.50)-1 = 0.00$$ In the second example above, where $0.00 \le F_1 \le 1.00$ with a mean of $0.42$, the $S$ score would actually range between $-1.00$ and $1.00$ with a mean of $0.00$. Thus, both classifiers would be deemed equal by the $S$ score: $$\begin{array}{l c c c} Item &amp; Reality &amp; Predict_1 &amp; Predict_2 &amp; Predict_3 &amp; Predict_4 \\ \hline 1 &amp; + &amp; + &amp; + &amp; - &amp; -\\ 2 &amp; - &amp; + &amp; - &amp; + &amp; -\\ \hline F_1 &amp; &amp; 0.67 &amp; 1.00 &amp; 0.00 &amp; 0.00\\ S &amp; &amp; 0.00 &amp; 1.00 &amp; -1.00 &amp; 0.00 \\ \end{array}$$ You can find more information about classification reliability at my website , including a history of the S score and functions for calculating it in MATLAB and even generalizing it to multiple classifiers, multiple categories, and non-nominal categories. 
 Your first guess is correct. The most straightforward and often the best way to depict the relationship in the sample between two variables is to make a scatterplot. Other types of plots can still be useful, especially if it isn't the case that both variables are continuous. For example, if one variable is a count and the other is a discrete ordered variable, a dot plot can work well. If one variable is continuous and the other has a few discrete values, box plots can work well. And so on. 
 I am trying to simulate event data based on the results of paper. The paper published the hazard ratios (using a Cox hazard function) of unemployment of different groups. To get the hazard function I took the (natural) logs of these ratios ($\beta$), then used the coefficients to generate a hazard for each observation (using data similar to the authors data). For each observation I now have a hazard: How can I use these hazards to simulate the time of an event (i.e. leaving unemployment)? I have been able to find out that $h_i(t)=\frac{f(t)}{S(t)}$ where $f(t)$ is the probability density function and $S(t)$ is the survival function. Do I still have to simulate a probability density function of an event occurring at (t) and a survival function in order? EDIT : How I generated a hazard for each observation $$h_i(t) = h_0(t)exp(x_i\beta)$$ I used the hazard ratios from the paper to get $\beta$'s (i.e. $ln(HR)=\beta$). I plugged data into the linear equation to get an $x_i\beta$ value for each observation. calculated $exp(x_i\beta)$. i.e. 1.Hazard ratios from publication: 2.Get $\beta$'s: $$\Rightarrow {C2}_i \beta_{C2} + {C3}_i \beta_{C3} = - 0.02 {C2}_i + 0.23 {C3}_i$$ where ${C2}_i$ and ${C3}_i$ are dummy variables: $\mathbf{1}_{{C2}_i}(\text{if } i \text{ from country 2})$ and $\mathbf{1}_{{C3}_i}(\text{if }i \text{ from country 3})$. 3.Calculate hazard $$\Rightarrow h_i = exp(- 0.02 {C2}_i + 0.23 {C3}_i + \epsilon_i)$$ where $\epsilon_i \sim N(0,1)$ is a randomly generated error. Note 1: I used an arbitrary baseline hazard ($h_0(t) =0.03$) for the moment but need to update this at some point. Note 2: The data I used contained the same variables used by the authors (but not the same dataset). I am using R, but did these computations manually without a survival package. 
 I have a heavily positively skewed continuous outcome variable y. Therefore I log transformed it to achieve normality. My interest is to assess the association between a few explanatory variables (one binary variable (Yes/No), one ordinal variable (0,1,2,3,4), two count variables) on y. I used generalised linear regression and treated the binary and the ordinal variables as categorical variables IN Proc GLM in SAS. The two count variables have many zeroes. The R squared is extremely small (~20%). The residual doesn't indicate severe departure from normality. Multicollinearity was assessed by using tolerance, variance inflation index and condition index. All this measures are around 1 so collinearity is not a problem here as far as I understand. However, there is an indication of heteroscedasticity from the residual vs predicted plot. As the predicted value increases, the variability of the residuals gets smaller. Is being homoscedastic very important in my case? If so, how can it be resolved? I tried following the steps in this document , but the heteroscedasticity still presents. 
 I need to simulate the posterior distribution of intraclass correlation coefficient $\pi(\rho|y)$ where $y$ is the data set and $\rho=\frac{\sigma_a^2}{\sigma_a^2+\sigma_e^2}$ with $\sigma^2_a\sim IG(\beta,1)$ , $\sigma_e^2\sim IG(\alpha,1)$, $\rho\sim Beta(\alpha,\beta)$ where IG stands for inverse Gamma distribution. Taking the random effect model $$Y_{ij}=\mu+\alpha_i+e_{ij};i=1,\dots,n;j=1,\dots,m$$ where $\mu\sim N(0,1e+06)$,$\alpha_i\sim N(0,\sigma_a^2)$ and $e_{ij}\sim N(0,\sigma_e^2)$, the text say that I can simulate this posterior with the following information $$\begin{cases} [\theta_i|\mu,\sigma_a^2]\sim N(\mu,\sigma_a^2);\theta_i=\mu+\alpha_i\ i=1,\dots,n\\ [e_{ij}|\sigma_e^2]\sim N(0,\sigma_e^2)\\ [y_{ij}|\theta_i,\sigma_e^2]\sim N(\theta_i,\sigma_e^2)\\ [\theta|y,\mu,\sigma_a^2,\sigma_e^2]\sim N_n\Big(\frac{m\sigma_a^2}{m\sigma_a^2+\sigma_e^2}\overline{y}+\frac{\sigma_e^2}{m\sigma_a^2+\sigma_e^2}\mu\textbf{1}_n,\frac{\sigma_a^2\sigma_e^2}{m\sigma_a^2+\sigma_e^2}\mathbb{I}_n\Big)\\ [\mu|y,\theta,\sigma_a^2,\sigma_e^2]=[\mu|\sigma_a^2,\theta]\sim N\Big(\frac{n(1e+06)}{n(1e+06)+\sigma_a^2}\overline{\theta},\frac{n(1e+06)}{n(1e+06)+\sigma_a^2}\frac{\sigma_a^2}{n}\Big)\\ [\sigma_e^2|y,\mu,\theta,\sigma_a^2]=[\sigma_e^2|y,\theta]\sim IG\Big(\alpha+\frac{nm}{2},1+\frac{\sum_{j=1}^m\sum_{i=1}^n(y_{ij}-\theta_i)^2}{2}\Big)\\ [\sigma_a^2|y,\mu,\theta,\sigma_e^2]=[\sigma_a^2|\mu,\theta]\sim IG\Big(\beta+\frac{n}{2},1+\frac{\sum_{i=1}^n(\theta_i-\mu)^2}{2}\Big)\\ \overline{\theta}=\frac{1}{n}\sum_{i=1}^n \theta_i , y_i=\frac{1}{m}\sum_{i=1}^m y_{ij} \end{cases}$$ Obs: The reference I used as base is here sampling-Based Approaches to Calculating Marginal Densitie I know that if I have $[\sigma_e^2|y,\mu,\theta,\sigma_a^2]$ and $[\sigma_a^2|y,\mu,\theta,\sigma_e^2]$ I can get the distribution of $\rho$. What I want to know is How they get the distributions of $[\sigma_e^2|y,\mu,\theta,\sigma_a^2]$ and $[\sigma_a^2|y,\mu,\theta,\sigma_e^2]$ using the conditional distributions? 
 I don't think this is possible. The issue is that all kinds of domain- and context-specific tricks can be used to identify people in a dataset. You can automatically protect against certain classes of identifiability exploits, such as small cells in contingency tables, but unless you have a smart statistician actually try to identify people and then patch up any such vulnerabilities he's found, you have no real protection against another smart statistician who wants to identify people. The analogy to computer security is clear. Programs can be written or supervised during execution to automatically resist certain well-known exploits, such as buffer overflows, but ultimately, you need a smart human security expert if you want protection against smart human adversaries, because security holes are infinite in variety. 
 This is the model I'm working with, some variables are int, some are num and some are factors. How can I calculate de relative importance of the predictor variables? I want to be able to tell which of the predictors has a bigger impact on the FrecS variable. I've tried the relaimpo package in R, but it wont run if my model is not gaussian. Can I simply state that the highest absolute value estimated coefficient is the most important predictor? Can I rank them using the p-value (lowest p-value means greatest importance)? I've read about Wald z-statistic and Pratt index, but to be honest I'm still quite lost. 
 You need to distinguish between exluded and included instruments, with the latter also known as exogenous regressors. If there is one regressor that you feel is endogenous, you need at least one variable that is not already in your equation that can act as an instrument, i.e. an excluded instrument. If there are more regressors ("$X$ variables") than just the endogenous regressor, the assumption is that these are not endogenous if there is just one endogenous regressor, so that they may "serve as their own instruments", i.e., "included instruments" which may be included in the list of instruments. 
 Let's say that I've two types of labeled data for a classification problem. The setup is the following: Raw data set R a lot of raw data with hundreds of features. Labeled data set A A small amount of data labeled in a very controlled and scientific way. Labeled data set B A large amount of data labeled by a heuristic algorithm (H) that is manually tuned and adjusted by an expert. A human expert has used the set A to create the algorithm H. The algorithm uses some of the features from raw data, but not all. The expert has likely picked features that are known to scientifically affect the phenomenon or that have proven to help classification. The set B has been observed by hundreds of users and they often point out if the classification doesn't match their experience. Question Are there machine-learning methods that are particularly suited to use both set A and set B to create even better classification algorithm than heuristic algorithm H. 
 Multiple imputation is not yet a widespread technique, so there are not many guidelines yet on how to conduct and present reasearch using multiply imputed datasets. One aspect is graphics. How would you plot data from multiply imputed datasets? Say you simply need a bar chart to report the frequency of the categories of a categorical variable. Would you use the average frequencies in the N imputed datasets? 
 Test the performance of model and model. using R Package ‘SMCRM’ Can not get the right probability and do not know why. Do not really understand what this sentence mean and how to use it (treeresponse, use.name,) what things the [seq... ] actually get. "1- unlist(treeresponse(x.crct, cr[ind == 2,]), use.names=F)[seq(1,nrow(cr[ind == 2,])*2,2)]" How to use the result and to draw ROC curve and calculate AUC Here is the code here is the part of the results from print 
 I'm familiar with and and if I understand well the advantage of on these algorithms is we can use different lost functions. Is it correct to say: Using means building trees on the raw residual of the previous tree. Using means replacing residuals by theirs signs, ie - 1 for negative residuals and 1 for positive residuals and build the next tree on these transformed residuals. This reduces the impact of outliers If these statements are correct don't you think it's misleading to use the term to describe something which just use the raw residuals? For instance in OLS regression square means what we expect which is residuals are really squared. 
 I have a logistic regression model. I'm looking for a non-graphical way to find the optimal cut-off where sensitivity is above a threshold(say 0.95) and maximizes sensitivity+specificity. I don't have a fitted model. Only two vectors of observations and predicted probabilities. 
 A linear process $X_t$ is defined to be causal if $X_t=\psi(B)w_t$ where $w_t$ are white noises and $\sum_{j=1}^{\infty}|\psi(j)|&lt;\infty$. $X_t$ is defined to be invertible if we can write $w_t=\pi(B) X_t$ where $\pi(B)=\pi_0 + \pi_1 B+\pi_2 B^2 + \cdots$ and $\sum_{j=0}^{\infty}|\pi(j)|&lt;\infty$. Apparently, an arbitrary $\text{AR}(p)$ model, $$X_t-\phi_1 X_{t-1}-\phi_2 X_{t-2}-\cdots - \phi_p X_{t-p}=w_t$$ automatically satisfies the requirement to be invertible since $\pi(B)=1-\phi_1 B - \cdots - \phi_p B^p$ and $\sum_{j=0}^{\infty}|\pi(j)|=1+\sum_{j=1}^{p}|\phi_j|&lt;\infty$. You can take a look at this note . 
 Is there any sound way to add a confidence measure to the prediction of a k-NN classifier? In my case I have a binary classification problem and the dataset is pretty balanced, with around 50% of the examples belonging to each class. I'm thinking about simply considering the amount of votes for the winning class and use the proportion to the total votes as a confidence measure, but I'd like to know if this has been studied and if there are more appropriate ways. 
 I don't totally agree with the accepted answer: the null hypothesis of the KPSS test is not stationarity, but trend stationarity, which is quite a different concept. To summarize: KPSS test: Null Hypothesis: the process is trend-stationary Alternative Hypothesis: the process has a unit root (this is how the authors of the test defined the alternative in their original 1992 paper) ADF test: Null Hypothesis: the process has a unit-root ("difference stationary") Alternative Hypothesis: the process has no unit root. It can mean either that the process is stationary, or trend stationary, depending on which version of the ADF test is used. If the "deterministic time trend alternative hypothesis" version of the ADF test is used, then both tests are similar, except that ones defines the null hypothesis as the unit-root while the other defines it as the alternative. 
 How can I calculate the statistical significance () of spearman's footrule ? I came across the formula at this wiki page https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient#Determining_significance For , it is calculated using the formula on the following page. Look at the section for " Significance tests " https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient but for Spearman's is not clear for me. Any hints 
 I'm dealing with binary classification problem. I would like to perform it with logistic regression. Each row (observation) in my data set might have up to around 15 very similar rows ('similar' defined as: [n-1] features with exactly same values, one feature with necessarily different value,Target (y) is always same (0 or 1) among these similar observations). Note that some of rows might not have similar observation at all (they just unique). Now, according to logistic regression assumptions I understand that I can't just keep all these similar (dependent) observations in my training set. However, seems like I can't keep only unique observations in my final training set since features values distribution of rows with many similar rows is different from features values distribution of rows without any similar rows. Does difference in features values distribution (training set vs. real life) is a big issue which will lead to negative impact on model accuracy? What would be the proper approach to this kind of problem? 
 I made Johansen test. The max eigenvalue test resulted in 5 cointegrated variables with contegration rank is 1. The trace test resulted in 5 cointegrated variables with contegration rank is 3. I expect an error correction term of the format V1-a2V2-a3V3-a4V4-a5V5. Or equivalently, all being "-" in the above ect1, ect2, ect3 terms. All are not "-", e.g., for the 1st ect1: -4.843e-02 +2.939e-05 +1.634e-04 -7.808e-07 +9.200e-07. There are "+" and "-" coefficients in the error correction terms above. Does this show wrong unsuitable formulation in VECM? The cointegration rank is either 1 (max eigen) or 3 (trace). Both present "+" and "-" coefficients in error correction terms. Any idea? Can I go with the usual ritual vec2var(ca.jo(...), r=1 or r=3) or something must be handled properly? 
 My dependent variable is percentage (0%~100%). My tests variables are two dummy variables. It seems that I cannot use regression directly. I read several materials, but they have different suggestions. Some suggest GLS, because the dependent variable is proportion (percentage between 0%~100%). Some suggest Tobit model, because the dependent variable is truncated (between 1 and 0) and with many zeros. Some suggest Beta regression. How do I choice? Thanks in advance. 
 A google search of " root mean square d error " gives 318,000 results, while a search for " root mean square error " gives 1,660,000 results A google scholar search of " root mean square d error " gives 61,300 results, while a search for " root mean square error " gives 224,000 results I declare that " root mean square error " is preferred. 
 Will we get a different pdf using the two methods? 
 I have this function : using log: Now, alpha itself can be decomposed and considered as a function of two variables w1 w2. We have: Altogether: My question is how to consider this alpha, and how to include it in the y regression. Let's say I have 30 values for w1, w2 and 30 values for x. My opinion (which is probably false): I would like to do a regression with w1 and w2. But a regression of what on what ? then incorpore the results of this regression, in the second one, the ln(y). I am a bit lost on how to approach this problem. Thank you for your help ! EDIT : x, w1, w2, y are all different variables (column). Each of these variables has 30 observations (rows). w1 and w2 do not relate directly to y, but relate to x and hence relate to y. In other words : w1 and w2 are measuring education of a population. Alpha represents the productivity of this population in the field of research. x represents the number of researchers. y represents the number of patents. 
 I think its a rule when using AIC that should the best model be within 2 unit AIC of the second best model, both are considered with equal weight. Does the same rule-of-thumb apply to Deviance information criterion? see Section 4.4.1 in Mark: A gentle introduction 
 I'm dealing with binary classification problem. I would like to perform it with logistic regression. In addition, this is rare events problem (lets assume mean target value is 1%) so I use 1 to 4 down-sampling. After the model has been built I need to adjust the intercept using the well known formula. Here comes a challenge: the users I'm going to apply the output model on (lets call them 'group A') have strongly different rate of positives (lets assume 0.2*1%=0.2%). Plus, the distribution of the features value among these users is different than entire training set distribution. Assume I don't have a possibility to collect my data set only from events of 'group A' users. I see here 2 options but seems like they both are pretty bad: Option 1: adjust the intercept using positives fraction of entire population, 1%. However, this model will probably overestimate (because the fraction of positives among group A is 0.2%). Option 2: adjust the intercept using positives fraction of 'group A' users only: 0.2%. This option looks better but the fact that features values distribution of group A is different from entire data set might also lead to over/under estimation (I'm not sure if it is really an issue, is it?). What is the best way to deal with such a problem? 
 A friend claims that because there is a 50% chance for a coin to land on heads, the fact that the last three coin flips landed on tails means that there is a higher chance for the coin to land on heads with the next flip. Could you explain this problem with this reasoning? 
 The coin has no memory and doesn't know what it did last time. 
 I'm interested in majoring in statsitics in my university as an under-grad. I am wondering whether I need to be a good programmer to do statistics because I really struggled in python and java even though I studied them for 1.5 years. 
 You increase your chances of getting hired in the industry very significantly if you can code well. You will also be able to ask for much higher salary. Overall, if you keep struggling then your options will be very limited not only at the start but throughout your career. Now what is a "good programmer"? You don't have to be as good as professional programmers, but you must be able to code any algorithm you came up with. It may not be the best written code, but it must do what's required in terms of the logic of the algorithm. For a statistician that's the definition of "good" in my opinion. You should be able to implement any statistical algorithm and method that you studied so far, that's your measure of how good you are. UPDATE: To your struggles with Java: that's not a typical stat language, so I wouldn't hold this against you. Python is used in data science a lot, but look at who uses it: mainly people with some kind of a programming background. Some statisticians like Python, of course, but it is (like Java and C++) a system programming language. I'd call it a "proper programming language", i.e. one that even pro programmers use without reservations. What I'm trying to say is that mere fact that you're struggling with these two particular languages is not yet a sign of an issue. However, if you struggle with programming in any language (any that you tried) then you have to think about the future in this profession seriously. 
 If you treat the coin flips as independent, which is the usual assumption, then the previous flips have no effect on the next flip. So, this claim doesn't hold from this perspective. However, a fair coin should have 50% heads if there are an infinite number of flips, so from this perspective, it seems reasonable to think that more heads will occur in the rest of the flips. But these three trials don't affect the rest of the trials, and 3 is too small as the number of trials anyway. 
 I wouldn't have used Bayes' rule to solve this. I would have computed $P(Y_1 = 1| Y_2=2)$ times $P(Y_1=1|Y_0=3)$. Then divide that for $P(Y_2=2|Y_0=3)$ computed using Chapman-Kolmogorov equation, I mean summing for all the possible $i$ intermediate states. Remark: Bayes'rule is computed as $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ 
 Im prforming a Linear regression but i don't know how to verify that the coefficients are linear (Performing with Gretl software) could you guys help me to find a way to verfy this? Thanks in advance! 
 Expanding on AlexR.'s comment, the notation $P(X_0, \cdot)$ denotes the transition kernel of the Markov chain starting at $X_0$. In other works, it denotes the probability distribution of the Markov chain after 1 step. Here '$\cdot$' denotes a stand-in set. So for a set $A$ in the state space, $$P(X_0, A) = Pr(X_1 \in A \mid X_0 = X_0). $$ Similarly, $P^t(X_0, \cdot)$ denotes the probability distribution after $t$ steps for the Markov chain starting at $X_0$, i.e $$P^t(X_0, A) = Pr(X_t \in A \mid X_0 = X_0). $$ If you have understood this notation, you will realize that in the equation for the total variation distance, $d_{TV}(P^t(X_0, \cdot), P^t(Y_0, \cdot) )$ measures the distance between these two probability distributions . So you have two Markov chains starting at $X_0$ and $Y_0$ and each go $t$ steps, and updates the distribution of the Markov chain. This distance metric measures how far are these two Markov chains. 
 It's root mean square because it's the root of the mean of squares, not the square of the root of the mean. According to the Merriam-Webster's : root–mean–square : the square root of the arithmetic mean of the squares of a set of numbers For a similar parameter, the International Electrotechnical Commission (IEC), in its Online Electrotechnical Vocabulary , suggests root-mean-square too. Google Ngram viewer confirms the predominance of root mean square ( also with error ). 
 I think there are several lines of thought here: The perceptron is closely involved in what was later named the Artificial Intelligence Winter . It gives ANN an aura of mystery (they were thought gone and resurrected in the 80s). They are inspired by biological systems. They could give us insights into how we work, move and live ourselves. They help us reflect on our brains, our nervous systems, our reactions and movements (there is a big research area modeling how biological neurons work). We speak here of Cognitive Science . They give amazingly good results in many practical applications. When people learn how they work and how some problems can be posed, ANN actually give better results than other techniques (assuming that there are enough data to support the problem). This apparent easiness of use may lead to think that they do magic... There is people claiming that science and technology is drifting paradigms, from a mechanistic way of viewing the world, to a more connectionist one. We often tend to speak in terms of networks , connections , paths or nodes (where other people would have spoken about structures or mechanisms a while ago). This is an issue on philosophy of science. Neural networks are more than a set of equations. They may have a different meaning for different areas of study as well as practical applications. Actually, they give philosophers and sociologist an area to study about such deep issues such as interdisciplinary scientific communication. 
 You can of course train on any data you want. The thing is that the network will learn what you show it . That is a very important thing to have present: You may want to train a network using measurements or not, using complete or missing data. It doesn't matter, as long as you know what you are doing. I try to give some examples: Say, you have some noisy data. You want a network to give you a function that resembles (predicts) that function. You may profit from filtering the noisy data first (which may include replacing missing values!), and then train the ANN on it. It will not(!) give you the noisy data after training, but it might give you something better: An approximation of the function without noise. Let's say you want to classify things. You have a number of features that you think may separate the classes. If there are some samples where a feature has a missing value, you may try to A) Drop that feature at all (if all missing values miss the same feature), B) Drop those particular samples, C) Replace the value by a value that is typical (care!) to the class it belongs to (in which case the ANN would rely on the other features to assign a class. Depending on the task (you didn't say if you want to classify, approximate a function or something else...) and the amount of data you have, you could think on training only with the complete samples and test with the incomplete ones. This, of course, depends on what exactly is missing. Depending on the nature of the missing variables, there are techniques for dealing with missing data . I have good experiences with interpolation of continuous variables, for instance. Also have in mind that backpropagation does not let you train after training : Once you're done with the training, you do not incorporate more examples to improve the performance. 
 I'm trying to identify outliers using mahalanobis distance acquired by the covariance matrix determined by the MCD-selected observations of a data matrix X (i.e. the vectors contained within the minimum covariance determinant ellipse). When using R's covMcd function (from the robustbase library) and setting alpha = 1, I don't get the same results as just running cov(X). Why would this be the case, if I'm specifying that covMcd find a minimum spanning ellipse that covers all observations in X? Here's an example: I have a feeling that I'm misunderstanding the role of the alpha parameter. If so, can someone please elaborate more clearly the role of alpha? 
 The SIMCA-P software is able to produce plots of covariances with jackknifed confidence intervals for OPLSDA models. Is there any R package implementing the same feature? Or is there other means of geting such plots ? 
 I have a dataset (consumer panel data) reshaped using mlogit.data and it looks like the following: but when I tried to use this data to fit in mlogit, I keep getting errors. I am not sure why I am getting this error. I am new to mlogit, would greatly appreciate any help. 
 I have a dataset for which I have made two types of longitudinal models. Each of the models has a random intercept and random slope. If the first model has a response Y and covariate X (the random slope is also for this covariate), then second one uses a response log(Y) and covariate log(X + 1). The DIC for the first model is positive 4700 and for the second model it is -13038. The models actually give almost the same fit when I check them using posterior predictive checks. I am wondering if it even makes sense to compare these two models using DIC? especially in light of these extremely disparate results of DIC. Does it also makes sense to compare them with Bayes Factor? I would be thankful if you could give me some pointers in this regard. 
 To expand on Kilian's answer: Categorical variables have different implications if meant to be inputs or targets . I your case, they are inputs. Apart from the encoding method, you may want to be careful that the (continuous) output is not heavily influenced by them (i.e. a strong change in a categorical input may lead to a bump in the output). The number of hidden units is a difficult task. For testing, go with the architecture you have (4-3-1), although I'd rather tend to something like 4-9-1... In any case, if you want to optimize, you need to iterate and try different hidden layer's sizes. A recommendation is to start with 1 or 2 nodes and iterate upwards, then graph your results and see if there's a point where more nodes do not help anymore. In many time series forecasting problems you want to include previous information. Consider adding the previous' months sales information in some form. Of course you'd have more inputs nodes then. If there are no recurrent neural networks available, you can incorporate this information in normal MLP as normal inputs. Have in mind that the results change in every run, since (most commonly) the weights are randomly(!) initialized, thus (potentially) leading to different results. You can check this easily: Train, train again and compare the results. This can give you a hint about the goodness of other parameters. 
 I am working with some Artificial Neural Networks (ANN) (feed-forward multilayer perceptrons trained by backpropagation) and would like to have some concepts and ideas clearer, so that I won't make a mess, above all with measures of error. In this question , the number of parameters is pretty straightforward defined. I tend to (let's say) like this approach, since I can easily understand that the connection weights are what we calculate after we finish tuning the model (i.e. training the ANN). Well, after the discussion on this other question , I undederstand that the number of effective degrees of freedom change during the training process, as the network improves it's prediction capability. I fancy that part of my confussion comes from the calculation of the weights being an iterative process, where you refine the result each time. I understand the idea that you penalize the calculation of a statistic each time you estimate it from an estimation : the general result is that you lose one degree of freedom for each parameter estimated prior to estimating (residual) standard deviation It seems to me that if you get 15 weights trained over 100 epochs, you would lose 1500 degrees of freedom to be substracted from the total training samples... 1500 is quite a number... And it wouldn't be such a huge ANN... Ok, so far with the confussion, I can tell you what I would like to do. If I want to calculate the Adjusted R² , I need the number of predictors, k . The R² comes from the target and simulated data, of course (just two series of N numbers, no specific model involved). → Question 0 : Is it correct to say that the parameters are the degrees of freedom, i.e. that the number of degrees of freedom is the number of parameters in an artificial neural network? → Question 1 : Is k (number of predictors) in the formula for Adjusted R² the number of effective degrees of freedom of the trained network or just plainly the number of weights? → Question 2 : How would I find the number of degrees of freedom of a trained neural network, if it were to change at different training epochs? → Question 3 : Is an ANN with smaller weight values (rather than a less number of them) a simpler model? Or: Does changing the value of a parameter change the complexity of a model? What is the role of random weight initialization here? Note : I am aware that these were 4 questions already, but since they are closely related to each other, I thought it better to put them here together. Note 2 : I am (really) trying to have this stuff clear, so I'm very very open to any hints, explanations, suggestions or edits ;) 
 I want to model a dependent variable Y on an independent variable X (and possibly some covariates as well). In my situation, it is safe to assume that the relationship is monotonically increasing. Once the model is built, I will have new observations of X. For each, I want to estimate the value of Y such that the new case has probability p of exceeding the estimate. This is sort of like a reverse of logistic regression. Instead of fixing the cutoff criterion for Y to be considered a success and estimating for each case the probability of success given the observed value of X, I want to fix the probability of success and estimate for each case the cutoff criterion for Y given the observed value of X. I know that, if I have a bivariate normal distribution, the distribution of a prediction is a location-scaled t distribution whose parameters can be derived from the regression model and the observed X value, and so I can use the quantile function of that distribution to find the desired value of Y. However, if there exists a statistical technique designed for the current purpose, and which makes fewer assumptions (or is more robust to violations of assumptions) than linear regression, I'd prefer to use that instead. 
 Suppose that we have $\vec y = W \vec x$, where $\vec x \in \mathbb R^n, \vec y \in \mathbb R^m$ and $W$ is some matrix (that could, e.g., by the top $k$ components from PCA). One reasonable way to define the information that $Y$ has about $X$ would be the mutual information, $I(Y;X) = H(X) - H(X | Y)$. The second term is analogous to what you are thinking of as "variance loss". It generalizes the idea that if $Y$ explains variance in X, the mutual information goes up. If $X$ is Gaussian, i.e., $\vec x \sim \mathcal N(\vec \mu, \Sigma_X)$, then $Y$ is also Gaussian, as is the joint distribution. For a Gaussian distribution, we can exactly write down the formulae for different entropies and we get an expression like this, $I(X;Y) = 1/2 \log \frac{|\Sigma_{X}| |\Sigma_{Y}|}{|\Sigma_{XY}|}$. Whether this lines up exactly with variance loss would depend on how you define it. In the Gaussian case, the mutual information only depends on these covariance matrices, so your intuition is correct and mutual information is directly related to explained variance. An alternate perspective on the "information loss" comes from considering the multivariate mutual information, or total correlation, as is explored here . 
 I came across this question in a book and having trouble understanding it. Suppose $(X,Y)$ are continuous random vector with joint pdf $f(x,y)$ and support $\mathcal{X} \times \mathcal{Y}$. In particular suppose that the marginals $f_X(x)$ and $f_Y(y)$ have support $\mathcal{X}$ and $\mathcal{Y}$ respectively. I need to verify that $k(x,y|x^\prime, y^\prime) = f(x|y^\prime)f(y|x)$ is a density function. I think this problem has to do with applying Fubini theorem but what confuses me are $x^\prime$ and $y^\prime$. There is no mention to what they mean.Should I consider them to be other random variables distinct from $x$ and $y$ but with supports in $\mathcal{X}$ and $\mathcal{Y}$? I am not sure of how to understand this problem. Any help would be appreciated. 
 Let $Y_1,\dots,Y_n$ independent random variables with $Y_i\sim &gt; Poisson(\lambda_i)$. For the likelihood model $$log(\lambda_i)=\sum_{j=0}^p\beta_jx_ij$$ with $x_i=(x_{i0},\dots,x_{ip})$ where $x_{i0}=1$. Find a)Log-likelihood for $\beta$ b)$\frac{\partial{l(\beta)}}{\partial{\beta}}$ c)The Fisher information matrix First $$log(\lambda_i)=\sum_{j=0}^p\beta_jx_ij\Rightarrow \lambda_i=e^{\sum_{j=0}^p\beta_jx_ij}$$ The density of Poisson is $$f(y;\lambda_i)=\frac{e^{-e{\sum_{j=0}^p\beta_jx_ij}}e^{\sum_{i=1}^n\sum_{j=0}^p\beta_jx_ij}}{y_i!}$$ then a) $$L(\beta)=\frac{e^{-\sum_{i=1}^n e^{\sum_{j=0}^p\beta_jx_ij}}e^{\sum_{i=1}^ny_i\sum_{j=0}^p\beta_jx_ij}}{\prod y_i!}$$ $$l(\beta)\propto -\sum_{i=1}^n e^{\sum_{j=0}^p\beta_jx_ij}+\sum_{i=1}^ny_i\sum_{j=0}^p\beta_jx_ij$$ b) $$\frac{\partial l(\beta)}{\partial\beta_a}=-\sum_{i=1}^n e^{\sum_{j=0}^p\beta_jx_ij}x_{ia}+\sum_{i=1}^n y_i x_{ia}$$ c) $$\frac{\partial ^2l(\beta)}{\partial\beta_a\partial\beta_r}=-\sum_{i=1}^n e^{\sum_{j=0}^p\beta_jx_ij}x_{ia}+\sum_{i=1}^n y_i x_{ia} x_{ir}$$ the firsher information matrix is $$I(\beta)=-E\Big(\frac{\partial ^2l(\beta)}{\partial\beta_a\partial\beta_r}\Big)=E\Big(\sum_{i=1}^n e^{\sum_{j=0}^p\beta_jx_ij}x_{ia}+\sum_{i=1}^n y_i x_{ia} x_{ir}\Big)$$ is that correct or I made a mistake? 
 It helps to think carefully about exactly what type of objects $\hat \theta$ and $\hat g$ are. In the top case, $\hat \theta$ would be what I would call an estimator of a parameter. Let's break it down. There is some true value we would like to gain knowledge about $\theta$, it is a number . To estimate the value of this parameter we use $\hat \theta$, which consumes a sample of data, and produces a number which we take to be an estimate of $\theta$. Said differently, $\hat \theta$ is a function which consumes a set of training data, and produces a number $$ \hat \theta: \mathcal{T} \rightarrow \mathbb{R} $$ Often, when only one set of training data is around, people use the symbol $\hat \theta$ to mean the numeric estimate instead of the estimator, but in the grand scheme of things, this is a relatively benign abuse of notation. OK, on to the second thing, what is $\hat g$? In this case, we are doing much the same, but this time we are estimating a function instead of a number. Now we consume a training dataset, and are returned a function from datapoints to real numbers $$ \hat g: \mathcal{T} \rightarrow (\mathcal{X} \rightarrow \mathbb{R}) $$ This is a little mind bending the first time you think about it, but it's worth digesting. Now, if we think of our samples as being distributed in some way, then $\hat \theta$ becomes a random variable, and we can take its expectation and variance and whatever we want, with no problem. But what is the variance of a function valued random variable? It's not really obvious. The way out is to think like a computer programmer, what can functions do ? They can be evaluated. This is where your $x_i$ comes in. In this setup, $x_i$ is just a solitary fixed datapoint. The second equation is saying as long as you have a datapoint $x_i$ fixed, you can think of $\hat g$ as an estimator that returns a function, which you immediately evaluate to get a number . Now we're back in the situation where we consume datasets and get a number in return, so all our statistics of number values random variables comes to bear. I've discussed this in a slightly different way in this answer . Is it correct to think of this as each observation/fitted value having its own variance and bias? Yup. You can see this in confidence intervals around scatterplot smoothers, they tend to be wider near the boundaries of the data, as there the predicted value is more influenced by the neighborly training points. There are some examples in this tutorial on smoothing splines. 
 Using square loss means building trees on the raw residual of the previous tree. It is fair, but given your last statement don't you think it's misleading to use the term square loss to describe something which just use the raw residuals? I fear your missing an important point about gradient boosting. In gradient boosting the trees (or whatever weak learners) are not fit to the loss function (the function being minimized), they are fit to the gradient of the loss function with respect to the prediction (*). That's hard to digest, so here's how it works out in the squared error case. Here the loss function for a single data point is $$ L(y, \hat y) = (y - \hat y)^2 $$ The $\hat y$ is what I'm calling the prediction. If we treat it like a formal variable and differentiate, we get $$ \nabla L (y, \hat y) = -2(y - \hat y) $$ It is this function that the trees are fit to, we plug the appropriate values of $y$ and $\hat y$ into $\nabla L$, and the result is our working response for one round of boosting. In the initial boosting iteration we initialize $\hat y$ to a constant (here the mean of the response), and in subsequents to the prediction of the previous rounds of boosting. Using absolute loss means replacing residuals by theirs signs, ie - 1 for negative residuals and 1 for positive residuals and build the next tree on these transformed residuals. This is mostly correct, but it's good to interpret it in light of the above. For absolute loss we are minimizing the absolute value loss function $$ L(y, \hat y) = \left| y - \hat y \right| $$ so the trees are fit to $$ \nabla L (y, \hat y) = -1 \ \text{if} \ \hat y &lt; 0 $$ $$ \nabla L (y, \hat y) = 1 \ \text{if} \ \hat y \geq 0 $$ Since the magnitude of the updates is controlled by the gradient at each stage of boosting, you can see that in the squared error case, larger gradients (residuals) give larger updates, but in the absolute case, the update scale is fixed. (*) These trees are then used to define essentially a direction to move in when minimizing the loss function, ala gradient descent. Why is it common to mention the loss function and not directly the associated gradient to avoid this confusion when we describe a GBM? Because the gradient part is an implementation detail. It's an important one, but still an internal part of the algorithm that does not need to be exposed as part of an interface. The loss function is what is being minimized , while the gradient is how is is being minimized . The first thing is much more important, it needs to be communicated to everyone involved with a model, even the manager of the non-technical department who is still not convinced that $(x + y)^2 \neq x^2 + y^2$. But, at the end of the day, it's a matter of convention. We tend to name models is accord with the loss function being minimized (absolute error, square error), or an associated distribution (the poisson model minimized the poisson deviance loss function). I'd like to think there are good reasons to do so, but even if you disagree, a good reason to use the traditional names is that people will understand you better that way. 
 I am trying to measure the Net Promoter Score of a segment of customers for an online product. There is an attribute of these customers that I believe might affect their Net Promoter Score. So I want to block for that variable. 30% of the customers exhibit the attribute I am talking about. If 150 customers are a representative sample of the customer segment, should I ensure that 30% of that sample exhibits the attribute? Or should it be 50/50? My gut is 30% but I want to double check. 
 I have a question in multivariate analysis, involving a bit of probability theory im trying to somehow prove that the constant c gives me the reciprocal of pik**2 and prove the expectation of x1 and x2 are equal to zero 
 I have to solve the following problem: Suppose we have a bayesian net in which we have the following variables: R, PA and PR Let: P(R) = 0.1, P(PA) = 0.5, P(PR|R, PA) = 0.6, P(PR|¬R, PA) = 0.4, P(PR|¬R, ¬PA) = 0.1 and P(PR|R, ¬PA) = 0.2 What is the probability of P(¬R, PR, ¬PA)? I started with P(¬R) and P(¬PA), because I can compute them as follows: P(¬R) = 1 - P(R) = 0.9 P(¬PA) = 1 - P(PA) = 0.5 Then I think I can compute P(PR|¬R) and use bayes rule, however: P(PR|¬R, PA) = 0.4 ⇒⇒ P(PR|¬R) * P(PA) = 0.4 \Rightarrow P(PR|¬R) = 0.8 I also have P(PR|¬R, ¬PA) = 0.1 \Rightarrow P(PR|¬R) = 0.2 The same for P(PR|R)... I get different results, so I can't apply bayes rule. This means I am obviously doing something wrong, where is my mistake? How can I solve it? 
 I've just calculated a fixed effects model in Python: I'd like to plot my model using matplotlib and statsmodel outputs However, this fails: Is it possible to plot mixed LM models in Python easily? 
 I'm calculating a very basic WLS statsmodel model where I weight my least squares regression based on the population size of each input data point. My understanding is I should use the reciprocal of the population as my weight input, as each weight should increase to if a value is less important. Is that correct? I've also seen weights applied in the form of the (population size / population mean), or the reciprocal. It's not clear to me which I should be using. My model is created as follows, where I regress the poverty rate of a population area against a candidate vote percentage. My weights are applied to the WSL model. Is this correct? 
 Is there a way to calculate the correlation between two time series that have been adaptively downsampled and thus (may) have different y values? This is easiest to explain with an example, so consider the case of two downsampled time series (n=100-&gt;down_n=5) in the range of time-points 1 to 100: Because the x values are all increasing, there is a definite correlation, however the oddly spaced y values mean that it's not as strong as it initially seems. I know I could interpolate so the y values match (or just add some of the original points back in), but I have downsampled because of memory constraints in other processing so I want to avoid adding points back in. Hopefully this makes sense. I tried to keep it clear without overloading information. Thanks in advance! EDIT - To clarify the data: In the example x is the value and y is the time. In the downsampling, floating point y values can be interpolated. So (10,1) means that at time point 1, the observed value was 10, while (20,30.5) means at time point 30.5, it is likely/interpolated that the value for x would have been 20. 
 I'm trying to use xgboost on python here is my code. xgb.train works but i get on error with xgb.cv although it seems i used it the correct way the following works for me but if i change the line by this one i get the following unexpected error: File "", line 45 bst.res=xgb.cv(plst,dtrain,num_round,nfold = 5,evallist,early_stopping_rounds=5) SyntaxError: non-keyword arg after keyword arg EDIT1 : i tried changing the order of the keyword as well and i get the following error EDIT2 after all, there is no need in cv for a validation set. there is no argument in the signature of xgb.cv (while it is present for xgb.train) so i removed it and changed the line by then i get this error 
 I trained and tested two models on the same dataset in a 10-fold cross validation manner. I want to show that one model is supreme than the other. Therefore, I want to show the better model has a higher test accuracy mean. In best practice, should I use paired t-test, unpaired t-test or other kinds of test? Should I use Welch's test if people are concerned about unequal variances? 
 I'm working on a project that requires some clustering analysis. In performing the analysis, I noticed something that seemed odd to me. I understand that in k-means the total sum of squares (total distance of all observations from the global center) equals the between sum of squares (distance between the centroids) plus the total within sum of squares (sum of the distances of each observation to its centroid). But I also see that total sum of squares is not exactly equal to the total variance of the distribution, which I don't understand. What I've noticed is that the two numbers (total sum of squares from k-means analysis vs. total variance) get closer to each other as the sample sizes get larger. Here's a quick simulation in R that shows what I'm talking about: Any ideas what's going on here? Rounding issues? Something having to do with how the algorithms are implemented? Am I calculating something incorrectly? Or do total sum of squares and total variance actually mean something substantively different? Thanks for any help anyone can provide. 
 I have multiple independent variables and multiple dependent variables, some categorical and some quantitative. I have created a data sheet with dummy columns appropriate to each categorical variable. My team and I have ran various tests with our data, including Multivariate Multiple Regression, and will need to re-do all of them in light of a multicolinearity test that will surely eliminate some variables. This is our last step before writing up a manuscript, so all and any help is deeply appreciated. I have access to SPSS, SAS and R (though no experience with R). Multicolinearity tests are simple enough for multiple regression with SPSS, but I'm lost when it comes to multiple multivariate regression. Any suggestions for how to test for multicolinearity for multivariate multiple regression? Here is a link to some of our data: https://docs.google.com/spreadsheets/d/1nTBcGgBl99Wb5bAjzY2j26W-vkAE-rKRvisbB-CMFGs/edit?usp=sharing Our dependent variables are the columns labeled with names of academic fields, all on the left-most side. Our independent variables are the columns to the right of those, from "Major Fields" to "Average ACT." To the right of that are the columns for the dummy variables. I've color coded the dummy columns to the variable to which they correspond. 
 I believe the issue here is the function in R computes the unbiased estimator of the variance, i.e. $s^2 = \displaystyle \frac{\sum_{i = 1}^n (x_i - \bar x)^2 }{ (n-1)}$ I think what you are getting from k-means will be equivalent to the MLE of $\sigma^2$, i.e. $\hat \sigma^2 = \displaystyle \frac{\sum_{i = 1}^n (x_i - \bar x)^2 }{ n}$ 
 In Discovering Statistics using SPSS 4th Edition by Andy Field, it was recommend to include the interaction term between the independent variable $x$ and its corresponding natural logarithm transform $\ln(x)$ variable to check for violation of the linearity assumption. What is the statistical theory behind this? This a quote from the book: This assumption can be tested by looking at whether the interaction term between the predictor and its log transformation is significant (Hosmer &amp; Lemeshow, 1989). I've also recently found out that this transformation is called Box-Tidwell transformation. 
 There's a terrific visualization of k-means at . You can select the number of centroids and their initial placement, and see how the initial placement impacts the ultimate cluster assignments (that is, how the solution resolves to local minima). 
 I'm an intern for a movie studio and my boss has said to use what I know about R and predictive modelling (which is 2 edX courses) and make some sort of predictive model. The data I have available is pretty extensive (box office of film, film type, countries, cities). I'm trying to explain to him that I need to know what the business want predictive modelling to solve but he doesn't listen. All he says is that you have a month's time to come up with some sort of presentation on your project. What should be my approach? 
 Currently I'm using the ARIMA provided in R, the training series is a seasonal time series, with some values close to zero in each period, and I find that when the training series have a descending trend, then in the result of the forecast, there will be some negative values. But the time series should be positive on every timestamps, is there a way to add constraints in ARIMA so as to prevent forecasting negative values? If adding constraints is not possible, is there a way to transform the predicted results so as to make them all positive and still capture the tendency? 
 The algorithms should be compared on the exact same training/test sets, so a paired test makes sense. The tricky issue with using a single data set to estimate generalization performance is that data has to be re-used across multiple runs, meaning there's overlap in the training sets (and sometimes test sets, depending on the procedure). This can produce erroneous results because it violates the independence assumption of common statistical tests, and can lead to underestimating the variance. For comparing the generalization performance of two algorithms on a single dataset, paired t tests with 10-fold cross validation can give an inflated type 1 error (i.e. you'd incorrectly detect a significant difference more often than you should). See this paper: Dietterich (1998) . Approximate statistical tests for comparing supervised classification learning algorithms Instead, he suggests using the '5x2cv t test' (a paired t test using 5 runs of 2-fold cross validation) or 'McNemar's test' (if computational resources are more limited). Unlike the t test w/ 10-fold cross validation, both of these methods have acceptable type 1 error. But, they have higher type 2 error (meaning a greater probability of failing to detect a true difference). In this paper: Nadeau and Bengio (2003). Inference for the generalization error they propose the 'corrected resampled t test', which adjusts the variance based on overlap between the training/test sets. It has proper type 1 error, and greater statistical power (i.e. lower type 2 error) than the 5x2cv t test and McNemar's test. In this paper: Bouckaert and Frank (2004) . Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms they argue that a test should have not only acceptable type 1 error and low type 2 error, but also high replicability (meaning the outcome of the test shouldn't depend strongly on a particular random partitioning of the data). They find that the 5x2cv t test has low replicability. The corrected resampled t test has higher replicability, and they propose a modification that further increases it. This paper considers the case of comparing multiple algorithms on multiple data sets: Demsar (2006) . Statistical comparisons of classifiers over multiple data sets 
 I generated the following ROC curve using Rapid Miner to compare few binary classifers but I don't know how to interpret the curve of "Random Forest" model It doesn't look like a curve 
 This question relates to my previous query regarding differences in IRT parameter estimates (2PLM model) between the ltm package in R and the Mplus latent variable program which is linked here: 2PLM IRT modeling of rare event behavioral data: Why changing discrimination and difficulty values? Now having obtained similar parameter estimates across platforms and having a sense of reliability to my estimates, I want to have a better grasp of what one considers 'plausible' discrimination and difficulty (threshold) estimate values in 2PLM IRT modeling. As is the case with any data, this will obviously hinge on sample size, response patterns etc. However, because I am dealing with unique maternal behaviors that are very rare-- even in a child protective setting-- many of my binary indicators have extreme cuts, in some cases with only a 1% proportion of occurrence (i.e., score = 1) on a sample size of N = 343. Despite this, all of my models successfully converge and the best loglikelihood is replicated--and in cases where the best LL is not replicated, contending solutions present essentially identical parameter estimates. However, for some dimensions assessed, I get extremely large discrimination and difficulty estimates (e.g., well beyond the -3 to +3 prototypical scale of theta, or the latent trait under consideration) along with extremely large standard errors (in parens). Take, for example, data for the following binary indicators of two different latent traits: Indicator 1: discrimination = 25.71(547.95), difficulty = .60 (1.44) Indicator 2: discrimination = -.11 (0.45), difficulty = -30.58 (130.42) Obviously, these are very large values for the discrimination parameter (indicator 1) and the difficulty parameter (indicator 2) and the SEs are even more extreme. Nevertheless, the models converge and given the inherent nature of the data, I am inclined to treat these values as accurate estimates of the sample under consideration. As to their degree of precision, this will have to await replication efforts but given the laborious nature of coding such data, I imagine it is likely going to be some time before another dataset of this nature and size is assembled. With all that in mind, I am interested in recommendations or 'best practice' guidelines in such situations assuming limitations of the data and precision of the estimates are openly acknowledged. This paper on sparse data bias with maximum likelihood estimators was useful in many ways, but did not quite suit my needs in the current situation 
 I am a CS student and naive in statistics. I have 2 related data sets, to which I want to find correlation. Let me explain the case first: This is a question related to computer architecture. One data set indicate time taken (to read certain "DATA" from memory/cache). -- Say reading is EVENT A Other data set is binary which indicate that particular "DATA" is removed or not from memory.--Say REMOVING is EVENT B If EVENT B happens on certain "DATA X" then EVENT A for that "DATA X" takes more time. If EVENT B do not happens on certain "DATA Y" then EVENT A for that "DATA Y" takes less(normal) time. Cache/Memory Hit and miss is the actual cases, I am looking at. I explained them in abstract way. One can observe when there is 1 in 'access' then corresponding time have to be increased from base delay. They are 2 different data sets as mentioned above. But because of noise in measuring sometimes even though 1 is there, more delay cant be observed. Even though 0 is there more delay observed. Because of this correlation is not 1 (which is to be 1 in ideal case). I want to measure how much they are correlated in presence of such noise. Sample Dataset 1: Dataset 2: I do not know clearly how can I measure the correlation for the above data sets. Any help to guide me through this computation /which distance measure to use (how to use) , anything and everything will be so much appreciated. Along with the above, any references to get a good knowledge to solve these kind of problems also useful. If this question belongs to math.stackexchange or stackoverflow, please migrate it to that. Thank you 
 This is a question about deriving the full conditional(s) from a joint pdf rather than about Gibbs sampling. When you consider the joint distribution of the model $$ f(y,\mu,\alpha,\sigma_a^2,\sigma_e^2)\propto\prod_{i=1}^n\prod_{j=1}^m \sigma_e^{-1}\exp\{-(y_{ij}-\mu-\alpha_i)^2/2\sigma_e^2\}\times\prod_{i=1}^n\sigma_a^{-1}\exp\{-\alpha_i^2/2\sigma_a^2\}\times\pi(\mu)\times\sigma_a^{-2(\beta+1)}\exp\{-1/\sigma_a^2\}\times\sigma_e^{-2(\alpha+1)}\exp\{-1/\sigma_e^2\} $$ the conditional of $\sigma_a^2$, $f(\sigma_a^2|y,\mu,\alpha,\sigma_e^2)$ is proportional to $f(y,\mu,\alpha,\sigma_a^2,\sigma_e^2)$ as a function of $\sigma_a^2$. Hence you can get rid of all multiplicative terms that do no involve $\sigma_a$: $$ f(\sigma_a^2|y,\mu,\alpha,\sigma_e^2)\propto\prod_{i=1}^n\sigma_a^{-1}\exp\{-\alpha_i^2/2\sigma_a^2\}\times\sigma_a^{-2(\beta+1)}\exp\{-1/\sigma_a^2\}$$ which factorises as $$ f(\sigma_a^2|y,\mu,\alpha,\sigma_e^2)\propto\sigma_a^{-n-2\beta-2}\exp\left\{-\sigma_a^{-2}\left[1+\frac{1}{2}\sum_{i=1}^n\alpha_i^2\right]\right\} $$ which you can identify as an $IG(\beta+n/2,1+\frac{1}{2}\sum_{i=1}^n\alpha_i^2)$ density. A similar argument leads to the conditional distribution of $\sigma_e^2$ as being another $IG$ distribution that does not depend on $\sigma_a^2$. Note however that the ratio $$\rho=\dfrac{\sigma_a^2}{\sigma_a^2+\sigma_e^2}$$ does not enjoy a Beta distribution any longer, since the scales of the two $IG$ distributions differ. It is therefore doubly necessary to simulate both $\sigma_a$ and $\sigma_e$ since (a) they are used to produce simulations for the other parameters and vice-versa, and (b) $\rho$ cannot be simulated directly. Remember that, when you use Gibbs sampling, you produce a sample of random variables simulated from the target distribution. In your case, the Gibbs sampler will produce a sample of $(σ^2_a,σ^2_e)$, hence a sample of ρ's. The empirical distribution function based on that sample is a converging approximation to the true posterior distribution of ρ. 
 I can't really find any data on how to go about normalizing the input for the following VGG-16 model I am using https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3 Right now I am inputting a black and white image (copied 3 times to make it compatible). I have the colors normalized between 0 and 1. I am not sure if this is correct for VGG-16. Is the VGG model expecting mean - std normalization? Or does it not really matter? 
 How to calculate rank from multiple parameters ? Parameters are bandwidth ,dealy ,suucess ratio ,these parameters values are used to determine the trustworthiness of a peer while accessing a service from a peer?. My aim is to get the best peer,by using any ranking method...which method can i use 
 Say I have the following graph: From this data I wish to create a directed graph. However, some of the weights are simply spurious noise. For example in the case of A -&gt; B and B-&gt; A there are smallish weights and a rather large weight respectively. One way I could build this graph is by grouping the edges (along with direction), summing those groups and thresholding. A second method would be to count the occurrence of a particular edge direction and threshold that (in which case the B -&gt; A edge is in the minority and will get cut). In the case of undirected graphs there are algorithms such as GLASSO to deal with such graph generation problems. Are there similar things for directed graphs? Note that in the real problem I have 1000's of nodes and each pair will have an associated time stamp so that I can build a matrix to analyse the graph structure. 
 Could anybody help me to understand that why is that for Brownian motion, the variance of the increment $Z(t+s)-Z(t)$ is the time interval $s$? I understand the math, but what is the intuition behind that? 
 I have the histogram in the picture: I would like to compute the mode. I fit first the distribution with a beta distribution and then I compute the mode. Is there a better approach? I would appreciate if you could add MATLAB code. 
 can any one explain that why CRFs are not applicable for unsupervised learning? thanks in advance 
 You can perform a Principal Component Analysis (PCA) based on the correlation matrix of your continuous variables. PCA is a method of dimension reduction and hence it is usually applied to reduce the number of collinear variables, like in your case. Many softwares, such as PC-ORD, can perform it. But, I am not sure that you can do correlation analyses with your categorical or nominal variables at the same time. However, you can get some ideas on how to handle these nominal variables after you have reduced some of collinear continuous variables through PCA. 
 I have a dataset of &gt;10000 gene expression profiles and I would like to test the effect of 3 explanatory variables and their interactions on each gene expression profile using information criterion (let's say AIC) by automatic model selection (e.g. glmutli R package). I am aware of false positive results of repeated tests. Is there any way to account for false discovery rate? I assume that, FDR could be the critical issue here. 
 The short answer is that no, you do not have to. The explanation is in the documentation of the survival package. See . The main reasoning is that an individual is at risk only in disjoint time intervals. Since the Cox partial likelihood is a sum over event time points, at each of those time points an individual may contribute with at most one line from the data. 
 I would like to suggest you to try a GLM (Generalized Linear Model). Here is the link ( ) for some details. 
 Suppose that X1, . . . , Xn form a random sample from the normal distribution with known mean µ = 0 and unknown variance σ^2. Consider testing the hypotheses H0 : σ^2 ≤ 2, H1 : σ^2 &gt; 2. (a) Show that the UMP test of these hypotheses is of the form: Reject H0 when T ≥ c, where T is a statistic and c is a constant. Find T. (b) Determine the value of c when n = 10 and α = 0.05. 
 If you think in terms of a discrete-time approximation of the process, the increment in a time interval $\delta t$ will have mean 0 and some variance. If you sum up the contributions in non-overlapping time intervals they will be independent. By basic properties of variance (specifically, $\text{Var}(X+Y)=\text{Var}(X)+\text{Var}(Y)$ when $X$ and $Y$ are indepdendent) the variance of the total increment in $n$ such time intervals time $s=n\,\delta t$ will be the sum of the increment-variances and so will be proportional to the total length of the time interval $s$. The process is defined so as to make that constant of proportionality $1$. 
 I was wondering why standard errors are (severely) downward biased when you are using the (general) instrumental variable - estimator or the generalized method of moments (gmm) estimator. 
 I have two matrices, X and Y (i.e., X is 15x1000 and Y is 15x2000, 15 representing cell lines, and Y gene expressions, and X is a dataset of spectral data). Both datasets are preprocessed, and normalized from 0 to 1. In order to find if spectral data can reflect a sets of genes (cluster), I want to find the right input for hierarchical clustering and verify the scientific veracity of the clustering. My preliminary approach was to convert the Pearson correlation matrix of X and Y, which I call Z, to a dissimilarity matrix to a vector (as pdist) using the 1-Pearson. In Matlab: or Z is 1000x2000. Another approach would be the 1-absolute value of Pearson correlation matrix. Highly correlated values are close to 0. My 1st question although the above approach sounds good to me, should I use a matrix of Distance correlation rather than a dissimilary matrix before clustering ? I found this post and maybe got confused: Understanding distance correlation computations My 2nd question is which hierarchical clustering method would be most appropriate for this input of data? In fact I am not sure that Ward is the most appropriate, So eventually I will need to compare various methods. (Here, I evaluate the optimum number of cluster using gap statistics). My 3rd question : Then, to evaluate the difference between different hierarchical trees, what are the best tools available in Matlab (e.g. to quantify that some trees are nearly identical, and that some are drastically different, depending on the methods). So far, I found the function silhouette and the cophenetic correlation coefficient. 
 Suppose I have a univariate Gaussian distribution with mean $\mu_X$ and standard deviation $\sigma_X$, and I know the random variable $X$ is least some positive value $y$: $X \geq y$. What is the conditional expectation $\mathbb{E}[X | X \geq y]$ of $X$ given $X \geq y$? Is there a closed-form expression for this? 
 I am using ChainLadder package for statistical analysis of a run-off triangle with I=17 and J=14. When I use BootChainLadder() it appears on R console "Number of origin periods has to be equal or greater then the number of development periods". But I have origin periods greater then development periods! I tried to add other arguments and to change the number of simulation (R argument) but it appears the same error ever. Could someone help me, please? 
 Chow test, multiple break-point test (such as Bia Perron test), Quant-Andrews break point test etc. are different test available in Eviews. Read description, assumptions and interpretation of each test before applying it your data set. 
 You could use Dynamic Time Warping (DTW) to do this: DTW is an algorithm for measuring similarity between two temporal sequences which may vary in speed. The advantage of DTW is that it neither needs to have a steady rise/declination in the time series (as in your example), nor needs the variance in speed be steady over time: DTW can deal with both using its warping. Therefore the timestamps of samples don't play any role any more, just the ordering of samples in the series matters. The downside of DTW is that it has $n^2$ complexity, so you will probably need to downsample very long series first, or use a complexity limitation like the Sakoe Chiba band (see e.g. here or here ). Here's a short DTW example in R using the package: ... adding a Sakoe Chiba window to restrict complexity: 
 I am looking for studies regarding the robustness of a multivariate Gaussian copula. Specifically I am wondering whether estimates of the dependence parameter in a multivariate Gaussian Copula (Sigma) are robust to violations of the underlying assumptions. Many approaches that rely on normal distribution assumptions tend to be robust when those assumptions are violated. I am wondering whether the same extends to the Gaussian copula. Let me give you some more details on the problem I am struggling with: I am using the approach proposed by Hoff 2007 to estimate the dependence structure between a set of demographic variables. The problem is that the sample I am using is biased in the sense that some demographic groups are over- and others under-represented (e.g., too many old people). I am wondering to which extent my estimate of the dependence structure is robust to mild forms of bias. Thanks very much for your help in advance. Michael Hoff, Peter D. (2007), "Extending the Rank Likelihood for Semiparametric Copula Estimation," The annals of applied statistics, 1 (1), 265 - 83 
 A typical convolutional neural network layer consists of: an affine transformation (e.g. convolution) a non-linear activation function (e.g. ReLU) a pooling stage (e.g. max-pooling) The second stage is often referred to as the "detector stage". Is there a reason we call it that? Does it have anything to do with feature detection? 
 I think the question is about interpreting the output of unit root test. First understand the decision rule for any statistical test : If p-value &lt; level of significance (alpha); then null hypothesis is rejected . If p-value &gt; level of significance (alpha); then we fail to reject the null hypothesis. Level of significance (alpha) is chosen by the researcher. Most common value is 0.05 Now understand, Unit Root Test : Null Hypothesis is "Series has a unit root" Alternative Hypothesis is "Series does not have a unit root" Finally Stationarity Vs Unit Root If a series has a unit root, it is non-stationary So, if p-value (for unit root test) &gt; 0.05 (at 5% level of sig); then Then the series has a unit root, hence it is non-stationary. 
 There are six different unit root test available in Eviews: The Augmented Dickey-Fuller (ADF) Test Dickey-Fuller Test with GLS Detrending (DFGLS) The Phillips-Perron (PP) Test The Kwiatkowski, Phillips, Schmidt, and Shin (KPSS) Test Elliot, Rothenberg, and Stock Point Optimal (ERS) Test Ng and Perron (NP) Tests Each test has some assumptions, for example, ADF test should be used is series has auto-correlation but it is homoskedastic. and PP test should be used if series has both issues suto-correlation and presence of heteroskedasticity. It is better to select the suitable test for stationarity, instead of applying all. You can read more about these test at All the best! 
 Suppose that we have a model with an input and an output. The model is exact (no structural uncertainty). However there is an error in the output ( the error is detected from already given exact values of the output). Suppose we have three inputs ( $x$, $y$, and $z$) . Since the ouput is uncertain and the model is exact, then surely the outputs error is due to some error in the inputs. Question :Given the error of the output, determined using several runs of the model $f$, how can we detet the error on each of the inputs? In other words suppose $$f(\tilde{x_i},\tilde{y_i},\tilde{z_i})= \tilde{u_i}$$ where $i=1,...,n$ for $n$ large enough. The known error of the output is denoted by $e_u$, so that for any $i$, we have $\tilde{u_i}= u_i \pm e_u$ where $u_i $ is the exact value of the i-th run of $f$. Now if we write $\tilde{x_i}= x_i \pm e_x$, $\tilde{y_i}= y_i \pm e_y$, and $\tilde{z_i}= z_i \pm e_u$, where $x_i$, $y_i$, and $z_i$ are the unknown exact i-th values of the inputs, and $e_x$, $e_y$, and $e_z$ are the unknown errors of the inputs. Our aim is to find $e_x$, $e_y$, and $e_z$ using $e_u$. Any suggestions ? I started thinking with the inverse propagation of uncertainties, however I think this problem is much simpler than that. So for that I asked some help. Thank you in advance 
 my data is strictly based on binary values..Is it possible to apply ID3 algorithm on binary data... i want to partition my data using tree..i have read kd tree, binary decision diagram, ordered binary decision diagram.. 
 I am trying to build an R tool for forecasting a (hopefully) wide range of time-series. I have settled on using several models, taking the forecasts from each, and deriving a weighed average of them using some weights. My approach for arriving at appropriate weights for the averaging is to evaluate each model several times on parts of the historical data. For example, for monthly series I do the following: I evaluate a one-step forecast for each model (five of them) for each of the last 12 months in the historical data $\{a_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,12\}\}$ with $a'_{j}$ the actual observations. I evaluate six non-overlapping (ex. Oct+Nov+Dec, then Jul+Aug+Sep, etc.) three-step forecasts for each model, taking the mean of the forecasts for each of the five models at a time $\{b_{i,j}\mid i\in \{1,\ldots,5\},j\in\{1,\ldots,6\}\}$ with $b'_{j}$ as the mean of the relevant actuals at each time. Finally, I evaluate four six-month-overlapping (ex. Jan through Dec, Jul through Jun, etc.) 12-step forecasts for each model, taking again the mean for each model, getting the final set $\{c_{i,j}\mid i\in\{1,\ldots,5\},j\in\{1,\ldots,4\}\}$ with $c'_{j}$ the means of the relevant actuals. I put $$A=\left(\begin{array}{c}a_{i,j}\\b_{i,j}\\c_{i,j}\end{array}\right), x=\left(\begin{array}{c}w_1\\\ldots\\w_5\end{array}\right), b=\left(\begin{array}{c}a'_j\\b'_j\\c'_j\end{array}\right)$$ ! and use from the package to optimise $x$ to give the least MAE between the two vectors $Ax$ and $b$. So my question is Is this approach conceptually valid, considering that this evaluates something like the whether the model procedure is approriate for the time series, and not whether a particular model-with-parameters is? EDIT: Question paraphrased significantly to focus on aspects not answered here . 
 Apart from an abuse of notation ($f$ having multiple meanings), there is no true difficulty in defining $$ k(x,y|x^\prime, y^\prime) = f(x|y^\prime)f(y|x) $$ as a density on the pair $(X,Y)$ since The conditional density of $Y$ given $X$ is $f(y|x)$, which integrates to one; The marginal density of $X$ is $f(x|y')$, which also integrates to one. What may sound confusing is the parametrisation in $(x',y')$ but this simply indicates that the distribution does not depend on $x'$. A standard occurrence of this setting is in Gibbs sampling or slice sampling (which is possibly where you found this description): when using two full conditionals for the Markov transition, one simulates [at iteration $t$] $X_{t+1}|X_t,Y_t\sim f(x|y_t)$ $Y_{t+1}|X_t,Y_t,X_{t+1} \sim f(y|x_{t+1})$ or $$(X_{t+1},Y_{t+1})|(X_{t},Y_{t})\sim k(x,y|x_t,y_t)$$ and the move does not directly depend on the value of $X_t$. Note that this is not incompatible with $$(X_{t+1},Y_{t+1})\sim f(x,y)$$ in the stationary regime of the Markov chain, since $f(x,y)$ is the stationary density while $k(x,y|x_t,y_t)$ is the conditional density at iteration $t+1$. 
 In my problem I want to develop a system capable to generate a summary or fusion of different texts that talk about similar topics (e.g. news articles). I have read about deep learning and for now it seems that LSTMS may be a good way of tackling my problem (I read this article that gave me some intuitions). I have also read this paper which tells about an experiment performed with the aim of finding out which architectures are suited for different problems, but I can't seem to find any of them which is similar to mine. First I want to know about which architecture should I use. At high level I think it seems clear that LSTM or GRU may be a good start but in that paper is told that optimal architectures usually are adhoc to the type of problem, so I guess I will have to go deeper and find out which components should I use and how should I connect them (stuff like combining/separating update/forget gates and so on). Given this adhocratic behaviour of LSTMS I probably should not use the minimalistic library Keras because it provides a too high level interface (if I am wrong please tell me because I haven't tested it yet). Anyway, I also have to decide which tools should I use (preferably in Python). For this matter I have watch this playlist that gave me some insighs but I am not sure about it yet so I would appreciate any suggestions. Thanks in advance. 
 To my untrained eye a pattern appears in this candle chart, where down-days (dark purple) tend to occur consecutively. I have a very basic understanding of statistics and R software, but it's been a while and I'd like to find the right search terms to get started with identifying / analyzing / confirming this pattern over longer series. Can you see a statistical phenomenon here or could you point me in the right direction, method or keyword? 
 I want to compare several models built using the codes I have written in R for a mixed-effects model. I already knew that function in car package provides , which is a factor that we can use to compare models in a mixed-effects modelling analysis. However, I realised that function in package seems to do the same thing and I need help as I am confused and the R help did not quite help. How are these functions different? Which one should I use? (I had 100 participants, 50 from each language group, from which each 25 participants in a language group received a different either list A or list B of the items.) [packages and were used to build the models] Following are the codes I used to build the models: This is the function I used for comparing the models in order to find the most optimal model: [as this function shows the AIC for each model] Resulting table: As I have also seen the following function to be used for what seems like the same type of comparison, I used it and the following table was the result I'm guessing the difference between the two is only the being the corrected version in the function (which seems to be more appropriate for small sample sizes, but not exactly sure of it. And am I correct to interpret that, based on the results, my core model (model m1.1.1) is the most optimal model in this case? (any comments on the interpretation of the model would be much appreciated as I am quite new to this type of analysis technique.) 
 I don't fully understand ergodic . And I have trouble in this problem(21.8) from book introduction to information retrieval . Consider a Markov chain with three states A,B and C, and transition probabilities as follows. From state A, the next state is B with probability 1. From B, the next state is either A with probability $P_{A}$,or state C with probability $1-P_{A}$. From C the next state is A with probability 1. For what values of $P_{A} \in [0,1]$ is this Markov chain ergodic? I thought that I can construct the matrix and compute the probability many times. And if the result matrix is stable and no-zero. But I guess this method is not so good. Thanks. 
 I derive that you don't yet know which relations to analyse. You could for example use some clustering to identify relations worth investigating further: Those clusters give you an idea of what some trends worth looking at might be. You could use those to split your data into more homogeneous groups, which you can then analyse/model further: 
 I have completed Nando's undergraduate machine learning class CPSC 340. Now I want to dive deeper into Probabilistic Graphical models. I have seen table of contents of both the books, and Koller's don't cover topics like Latent Variable models, Factor analysis, Reproducing kernel hilbert space, State space models etc. So which book should I go for, Murphy's or Koller's? And from a job perspective, do employers seek people specialized in one field like PGM's or they are more interested in someone with broad skill set like someone who knows bit about everything like latent variable models, PGM's etc? 
 let's suppose; I am fitting some models to a data set: I want to understand the use of anova() to compare different models . My questions are: As discussed model compare by anova test , I need to put more general model later. So is anova(m2, m1) a wrong way of doing comparision? anova() can be used for nested models (models have a shared set of predictor variables and the same outcome variable, but one model has one or more additional predictor variables), can we use anova(m2, m3) or anova(m1, m4)? How to interpret output of anova() using three or more models ? For example; anova(m1, m2, m3) or anova(m1, m2, m3, m4). - 
 I would like to compare the goodness of fits statistics, $R^2$, of different statistical models estimated by the seemingly unrelated regressions model. In the lecture note of Professor Thorton on SUR , it says that the generalized $R^2$ for the system of equations can be formulated as $$R^2 = 1- \frac{|S|}{|A|}$$ where $A = y'y$ with $y = [y_1, y_2,...,y_M]_{T\times M} - [y_1^{Mean}, y_2^{Mean},...,y_M^{Mean}]_{T\times M}$ is the matrix of deviation of dependent variables from their mean. ($M$ is the number of equations and $T$ is the number of observation) $S$ is the residual cross products matrix, i.e. $S=\hat{\epsilon}'\hat{\epsilon}$. I could not find any papers of texbooks discussing this type of goodness-of-fit measurement. Are there any interpretations to compare with McElroy's $R^2$? And I've also read (in somewhere) that another $R^2$ for the SUR is $$R^2 = 1 - \frac{|\Sigma|}{|\Sigma_0|}$$ where $\Sigma$ is the cross-equation contemporaneous variance-covariance matrix of errors, and $\Sigma_0$ is the one when regressor is the only constant. How can we justify that this may reasonable measurement of goodness-of-fit statistics for the SUR? 
 Meta-regression certainly seems the way to proceed here. You have the question of whether to fit a model with all your moderators simultaneously (assuming you have enough data points), to fit one at a time, or some combination of these. I would have thought that the distinction between RCT and non-RCT was one which any model should include as it is (a) well known (b) likely to bias any other moderator effect. Of the other three it is up to you. I assume self-esteem is a study level variable and so any inference there will be an ecological one and so possibly less interesting. So to summarise, I would always include RCT v n-n-RCT in any model plus whatever else I was testing and I would consider a full model if there are enough data points. 
 Let's look at this example: I know that the distances between clusters can be computed using the Lance und Williams Formula, e.g. $$D(A \cup B,C)=\alpha_1 d(A,C)+\alpha_2 d(B,C)+ \beta d(A,B) + \gamma |d(A,C)-d(B,C)|.$$ with $\alpha_1 = \tfrac{|A|}{|A|+|B|}, \alpha_2 = \tfrac{|B|}{|A|+|B|}, \beta = \tfrac{|A||B|}{(|A|+|B|)^2}, \gamma = 0$ for centroid linkage (see also https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#Lance_und_Williams_Formel ). I also know that, in R, the dendrogram and the function computes the distances between two clusters with this formula, e.g. after merging the two closest points (in example above: point 1 and point 2), the distance between the cluster that consists of point 1 and 2 and the second cluster that only consists of point 3 is, according to the Lance und Williams Formula with $\alpha_1 = \alpha_2 = 0.5, \gamma = 0$ and $\beta = 0.25$: . Therefore, the dendrogram shows a merge of those two clusters at "Height" . However, since "in centroid method, the distance between two clusters is the distance between the two cluster centroids", I would have computed this distance differently, namely: compute centroid of point 1 and 2, which is at (0.5, 0) "centroid" of point 3 is point 3 itself (located at (0.5, 0.9), see plot). euclidean distance between the two cluster centroids is therefore , which is not the same distance as computed with the Lance und Williams Formula. So, my questions is: Why do we use the Lance und Williams Formula (e.g. the ) to plot the dendrogram and do not use "the distance between the two cluster centroids", which is ? 
 In repeated-measure ANOVA, Greenhouse-Geisser correction is commonly used to handle sphericity issues (when there are more than three levels in a condition). But does it deal with data that violate the independence assumption (data points gathered from the same individuals in a cell)? 
 At the moment each criterion has two values, one of them is zero, the other varies. To put them all on the same scale it would be simplest to convert them all to have values zero and unity. So multiply the first one by 2, leave the second one, and so on. The last one N needs dividing by 5. Centering by subtracting the mean will still leave them with different ranges. 
 I am doing a Bayesian MCMC fit to some data using a simple model and I want to understand how to handle nuisance parameters. I am looking at this tutorial . The model is a line: $$y = m x + b$$. The log likelihood (Gaussian) is written as: $$\mathrm{ln}\,p(y|x,σ,m,b,f)=-\frac{1}{2}∑_{n}\left(\frac{(y_n−m x_n−b)^2}{s_n^2}+\mathrm{ln}(2\pi s_n^2)\right)$$ where: $$s_n^2 = \sigma_n^2 + f^2 (m x_n + b)^2 $$. Here, $f$ quantifies how much the variance is underestimated. I have a few questions: Is the parameter $f$ here a nuisance parameter and is this the proper way to marginalize over it as shown in the tutorial? Do you just need to fit $f$ as a parameter in your likelihood and prior, but since it is not explicitly included in the model (since it is handled as part of the error on the data), it is termed a nuisance parameter? Does including nuisance parameters affect the errors on the model parameters from the Bayesian MCMC fitting? I assume you don't need to adjust the error bars of the data explicitly because this is handled in the prior and likelihood during MCMC. Do you treat the number of nuisance parameters as additional free parameters when calculating the number of degrees of freedom? If my understanding is correct, I think the answer is YES to all of these. 
 If you do it computes the same p-value as so the order is not crucial. However, in the former case the change in degrees of freedom becomes negative which might look confusing in the output. (As pointed out in the other Q/A you referred to.) No. It is technically possible to enter but the result is not a valid ANOVA. If you want to compare these models, you can either resort to information criteria (AIC, BIC, ...) or use dedicated tests for non-nested hypotheses (e.g., encompassing test, Vuong test, Cox test, J test, etc.). If you carry out this reports two tests which essentially correspond to and . However, the error sum of squares is taken from in both of these comparisons which might lead to small differences. Due to 2. the sequence of models must be nested in each step to return valid ANOVAs. But you can do an encompassing test between and by going through , i.e., . 
 I'm working on dataset that contains a variable from 0 to 1 (0 - 100%). Distribution of the variable differs depending on the context (defined by another variable). Depending on the context the distribution of it may: be concentrated close to 0, e.g. [0, 0.05, 0.05, 0.06, 0.10, 0.15, 0.8] look similar to normal distribution, e.g. mean = 0.5, sd = 0.1, [0.31, 0.4, 0.46, 0.48, 0.5, 0.51, 0.55, 0.59, 0.72] be concentrated close to 1, e.g. [0.2, 0.85, 0.9, 0.94, 0.95, 0.97, 1] Example of such variable may be test results across some tests having different difficulty. When a test is easy then most of students receive high results (90-100%), when it's hard than most results are low (0-10%). When its difficulty is well balanced, then e.g. most of results is 60-80%, but there are also significant numbers of results in ranges 45-60% and 80-95%. I'm looking for a way standardize it, so I'll be able to compare values between contexts. For now I've got an idea to work on percentiles of the aforementioned variable. But maybe there are smarter approaches? 
 Nothing wrong with using interaction term in the model, if theory suggests so. How to include interaction term in the model will depend on the software you are using. In R, you can directly use interaction term using x1:x2 (only the product term) or x1*x2 (interaction as well as the main effects). In other softwares, like Excel, SPSS, STATA, EViews etc., you can always create a new variable for interaction term and use it in you model. 
 I fit a deep learning regression model at first and get some results and then I shift the outcome into binary classification by set a threshold(assume median),and fit a deep learning classification model. I also use cross validation methods to find the best model in regression and classification.(regression by RMSE,classification by cross entropy error). And I am wondering is there any way to evaluate which is better between the best classification and regression model? Thanks 
 I'm trying to run a BRT analysis with survey data. The BRT analysis needs no missing values in variables included in the dataset so I omitted all NA cases in my dataset. This is my sintaxis: library(gbm) library(dismo) Datos2 &lt;- na.omit(Datos2[,c("gust_app", "sexo", "edad", "habitat", "fac1_1")]) head(Datos2) Datos2$gust_app &lt;- Recode(Datos2$gust_app, '"No"=0; "Sí"=1; "."=NA', as.factor.result=TRUE) Datos2$sexo &lt;- Recode(Datos2$sexo, '"Mujer"=2; "Hombre"=1', as.factor.result=TRUE) Datos2$habitat &lt;- Recode(Datos2$habitat, '"Menos de 10.000 hab."=1; "10.000-50.000 hab."=2; "50.001-100.000 hab."=3; "Más de 100.000 hab."=4', as.factor.result=TRUE) Datos2$edad &lt;- Recode(Datos2$edad, '"18-29 años"=1; "30-44 años"=2; "45-59 años"=3; "60 o más años"=4', as.factor.result=TRUE) gbm1 &lt;- gbm.step(data=Datos2, gbm.x = 2:5, gbm.y = 1, family = "bernoulli", tree.complexity = 5, learning.rate = 0.001, bag.fraction = 0.5) When I check if there is any missing values into the dataset variables ["is.na(Datos2)"] There is no TRUE cases. Nonetheless, the analysis is still giving me an error message: "missing value where TRUE/FALSE needed" Would anyone be so kind as to help me with it? Thanks! 
 You don't need Bayes Rule for this. From the information you have given for the Bayes network the joint probability factors like this: $$ P(R, PR, PA) = P(R)P(PA)P(PR|R,PA) $$ so you can calculate the joint probability directly from the conditional probabilities. Like you said you know $P(\neg r)$ and $P(\neg pa)$ and you also know $P(pr| \neg r, \neg pa)$. 
 I'm on my project to predict the amount of demand of products in a store. we have lot of 0 on the amount of sales of products so when I did multiple regression, predictions of the demand had negative number and I got weird residual plot. enter image description here 1.It has big difference between under 500 and over 500 in my plot. I want to use weight to make nice residual plot. but How can I decide weight? And I don't want negative predicted amount of demand. so I used Negative binomial regression and poisson regression to prevent making negative number in prediction. and I got some crazy residual plot. enter image description here 2.Can I keep using this binomial regression for prediction? 3.Could you recommend any other useful regression model? 4.If variance of data is bigger than mean of data, should I have to use quasi-poisson regression? 
 I am doing a Bayesian MCMC fit using emcee in python. I first maximize the log of the likelihood and use the results as initial parameter starting points in my MCMC. I am using a uniform prior and Gaussian likelihood. I sometimes find that maximizing the likelihood gives non-physical values outside the bounds set by my prior, and it causes the MCMC to not sample the parameter space. Is there a way around this besides making the prior boundaries large enough to include these non-physical values? I would still like the MCMC to sample the parameter space, and once the chain stabilizes, I always find it stays away from these unphysical regions. 
 I'm trying to use ARIMA process to predict the behaviour of a time series, the probleme I face is that I can't get the order of each component of ARIMA, the lag is between 0 and 1, same goes for the acf. The time series has no tendency, and I guess no seasonality. My question is : Does this mean that ARIMA orders are all 0s, and if so, is the HoltWinters a good (or the best) solution for modelling this kind of TS ? 
 So i have the following $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + e$ where $x_1$ and $x_2$ are dummies. We have 512 observation, 212 of obs have $x_1 = 1$, 345 of obs have $x_2 = 1$ and 84 of obs have $x_1 = x_2 = 1$.The standard error of the regression is 3,14. Determine the error of the estimate $\hat{\beta_1}$ I have the calculation formulas from " How are the standard errors of coefficients calculated in a regression? " First i compute $(X'X)^{-1}$ then using the formula for variance I get. $Var(\hat{\beta}) =\sigma^2(X'X)^{-1} = 3.14^2 \frac{1}{2(x_1^2+x_2^2)-(x_1+x_2)^2}\begin{pmatrix} x_1^2+x_2^2 &amp; -(x_1+x_2) \\ -(x_1+x_2) &amp; 2 \end{pmatrix}$ However after this i do not know how to get the standard error for $\hat{\beta}$. I also tried looking at $\hat{\beta}=(X'X)^{-1}X'y$. But do now know what to put in y here. Do does anyone know how to calculate the standard error for $\hat{\beta}$ by hand? 
 Something is driving those peaks in the second plot - it's pretty much impossible that all these peaks would have the same height just by chance. These are likely large orders from one particular customer. Find out how to forecast these, and don't worry about fitting any model until after you have accounted for the largest drivers. 
 I am doing a gene expression analysis on blood samples collected from the same 8 goats. My sampling points are before LPS-treatment (0h), and 1h, 2h, 5h and 24h after treatment. I want to compare the mean expression of each post-treatment to the before-treatment values. Due to problems in RNA-extraction, one sample is missing at 1h and one sample is missing at 2h. Is it possible to do a paired t-test with unequal sample size? E.g n=8 at 0h and n=7 at 1h/2h? I'm using log2 transformed data for the statistical analysis. (We are have decided not to correct for multiple comparisons, but we use a fold-change limit instead) 
 Stochastic Binary is not a smooth function, it is like a step function where as Sigmoid is a smooth function. Although at extreme ends, large positive and large negative Sigmoid give a similar behavior as Stochastic but on moderate values it transits slowly. So a small change in the weights and bias will cause small change in the output, whereas this is not the case with Stochastic Binary neurons. Refer to this book by Michael Neilson and I'm sure your confusion will be blown away. He explains it so well. 
 I am carrying out a study into the handedness of racehorses. There are clockwise and anti-clockwise racecourses. I want to test to see if 1) horses are more likely to win if they are left handed on anti-clockwise (left hand) courses than those who are right handed and 2) if they are more likely to win if they are right handed on clockwise (right hand) courses than those who are left handed. Would the one sample z - test for proportions be appropriate. I have looked at 280 races so there are 280 winners in all. Thanks, I look forward to hearing your thoughts. 
 I read this chapter from Michael Neilson book which claims by quoting the following equation, equation no. 5 $$ \delta output \approx \sum_j\frac{\delta output}{\delta w_j}\Delta w_j + \frac{\delta output}{\delta b}\Delta b $$ I didn't get how he came up with this equation. Can you help me understand it? 
 Different people use different notions of what it means to be ergodic. As far as I know the weakest form of ergodicity is requiring that there is exactly one invariant distribution. Let us call this weakly ergodic . For a finite state Markov chain, this is equivalent to requiring that the chain is irreducible, i.e. it is possible to reach any state from any other in a finite number of transitions. In your example, you have transition matrix $$ \begin{pmatrix} 0 &amp; 1 &amp; 0 \\ P_A &amp; 0 &amp; 1 - P_A \\ 1 &amp; 0 &amp; 0 \end{pmatrix} $$ Let $\pi = (\pi_A, \pi_B, \pi_C)$ denote any invariant distribution. The equations for invariance give $$\pi_B = \pi_A \quad \mbox{and} \quad \pi_C = (1-P_A) \pi_A,$$ or after normalization, $$ \pi = \frac{1}{3 - P_A} (1, 1, 1 - P_A).$$ So regardless of the value of $P_A$, there is a unique invariant distribution and the chain is weakly ergodic. For a weakly ergodic chain, time averages of functions converge to averages over the invariant distribution for every initial condition. Often, people have something stronger in mind when talking about ergodicity. Let us say that a Markov chain is strongly ergodic if it is irreducible and aperiodic. In the above Markov chain, if $P_A = 1$, the state $C$ is transient (and so the chain is not irreducible) and if $P_A = 0$, the chain is periodic. So to have strong ergodicity we require $0 &lt; P_A &lt; 1$. For a strongly ergodic chain, the probability distribution of being in a particular state at time $n$ converges to the invariant distribution for large $n$. I recommend Norris, Markov Chains for background reading. 
 As you are looking into the winners, the numbers lend themselves to be displayed in a 4 fields contingency table, which usually calls for a chi-square test for independence or a Fisher test. However, you should consider, whether your data are i. i. d. enough. If there is one horse, that's left-handed and wins a lot of anti-clockWise courses than that's not the same, as if a lot of left-handed horses win a lot of anti-clockwise races. edit: This may be your contingency table 
 The approach you suggest is to fit a model to sampled data from each condition, then use some measure of the models' variability to test whether they're different at a particular set of time points. This seems similar to some established regression techniques, which involve assuming you know the basic functional form, then using that form to do bootstrapping. These methods are the parametric bootstrap and bootstrapping residuals (more about this in a second). So, I think your general idea makes sense; the question is how to implement it. After fitting models to the time series, the question is how to estimate variability at the time points of interest. A bootstrapping approach could work. But, it won't be possible to use the simple bootstrap (i.e. to resample the data points) because the data are correlated in time. That's why Rob Hyndman suggested the parametric bootstrap. In that approach, you'd fit a model to each time series, repeatedly simulate new data from the model, then run statistics on the simulated data. The model in this case wouldn't be a simple curve, but a generative model (i.e. it would have to give a probability distribution from which you could sample new points). Here's a paper using that approach. They use Gaussian process regression to model the time series and do parametric bootstrapping. Their method might work well for your data. You'd use the same model fitting and bootstrap procedure, but the thing you'd test would be the equality of the mean at particular time points. Kirk and Stumpf (2009) . Gaussian process regression bootstrapping: exploring the effects of uncertainty in time course data Another possibility along similar lines is to resample the residuals. The procedure would look like this: Let's say the sampled time series is $\{x_1, ..., x_n\}$ for the first condition and $\{y_1, ..., y_n\}$ for the second condition. Say we're interested in the differences at time points $t_1, t_2, t_3$. Fit a model to each sampled time series (e.g. weighted sum of Gaussian basis functions, as you're currently using). The model should fit the data well and represent your beliefs about its temporal structure. Evaluate the models at each sampled time point. Call these fitted values $\hat{x}_i$ and $\hat{y}_i$ for each time point $i$. Compute the residuals at each time point. $a_i = x_i - \hat{x}_i, b_i = y_i - \hat{y}_i$ Generate a synthetic version of each time series by resampling the residuals: For each condition at each time point, randomly draw a residual (from all time points, with replacemement), and add it to the fitted value (at that time point). For condition 1 at time point $i$, the synthetic time series is $x^*_i = \hat{x}_i + a_j$, where $j$ is a randomly chosen integer from $1$ to $n$. For condition 2 at time point $i$, the synthetic time series is $y^*_i = \hat{y}_i + b_k$, where $k$ is a separate, randomly chosen integer from $1$ to $n$. Fit a new model to each of the synthetic time series. The models should have the same functional form as used to fit the original data. The model fit to $x^*$ is $f_{(i)}(t)$. The model fit to $y^*$ is $g_{(i)}(t)$ The subscript $i$ denotes the current bootstrap sample (i.e. the number of times we've run through the loop). The models are continuous functions of time, so they can be evaluated at any time point $t$. Evaluate the new models (fit to the synthetic data) at the time points of interest, where we want to calculate the differences. That is, calculate $f_{(i)}(t)$ and $g_{(i)}(t)$ for $t \in \{t_1, t_2, t_3\}$. Record these values. Repeat steps 4-6 many times (e.g. 10,000). Each iteration will produce a single bootstrap sample. We now have a set of bootstrapped function values at each of the time points of interest. That is: $f_{(i)}(t)$ and $g_{(i)}(t)$ for $t \in \{t_1, t_2, t_3\}$ and for $i \in \{1, ..., 10000\}$ For each time point of interest $t$, run a statistical test comparing the bootstrapped values $f_{(i)}(t)$ vs. $g_{(i)}(t)$. I.e. test the null hypothesis that there's no difference. Or, even better, calculate a confidence interval on the difference, since it's more informative. Resampling the residuals relies on the assumption that the residuals are identically distributed, so it would be good to check that this is true. This condition could be violated, for example, if the variance changes over time . Possibly of interest: This chapter describes bootstrapping residuals. These notes briefly compare the simple bootstrap, parametric bootstrap, and bootstrapping the residuals. 
 I fitted the GPD to the right tail of nine log return series (I multiplied log returns by -1, so modeling the right tail equals modeling the losses) with a threshold equal to the 95% quantile. Some of them have positive shape parameter $\xi$, implying infinite right endpoint, whereas other have negative values for $\xi$, which implies finite upper bound. Since having negative $\xi$ in this framework would imply finite losses (right endpoint defined as $-\frac{\beta}{\xi}$) and hence is rather unrealistic, I'm estimating $\xi$ over a moving window of 1000 observations. Having 4344 return/losses data, I estimate $\xi$ and $\beta$ from observations 1-to-1000 to observations 3345-to-4344, ending up with 3345 values for both parameters. I get a stairwise pattern. $\xi$ and $\beta$ tend to have the same values for x consecutive observations, then move a bit up or down, and and then again stay constant for other y observations. This makes sense, since I'm modeling 5% of 1000 data, i.e. the 50 most extreme cases. As you can imagine, the 50 most extreme cases tend to be the same for a while, it is unlikely that any new return/loss data is going to affect the upper right tail. So such pattern is justified by the definition itself of the distribution which only models exceedances over the threshold. My question: would you say that it is theoretically/intuitively justified to assume a dynamic model where $E[\xi_{t+1}]=\xi_{t}$ and $E[\beta_{t+1}]=\beta_{t}$, i.e. where the forecast for the next period equals the realization over the last 1000 observations? In the paper posted below, the authors use a specific equation for the parameters: To me this makes less sense for the reason stated below, i.e. $r_{t+1}$ would have to fall within the upper 95% part of the distribution in order to influence the estimation, and this happens on average only 5% of the time. Since deviations from the actual estimate are rather unlikely, I would simply apply a model which every day adjusts the expectation for day t+1 according to the estimate over $[(t-999):t]$. What do you think? Is this approach justifiable? What could I write in my project to justify such approach in the eyes of my supervisor? 
 You can not do a t-test for paired data with unequal sample-sizes but you can omit cases with missing data for the analysis of these times: Do a n=8 comparison for 5h an 24 h and an n=7 comparison for n=1h an n=2h. Have you considered repeated-measurements-ANOVA instead of all those t-tests? With n so small and 4 different tests, alpha Inflation is an issue to be considered. 
 Consider the regression model: $$ y_i=bx_i+e_i,1\leq i\leq n.$$ where $x_i$'s are fixed non-zero real numbers and $e_i$'s are independent random variables with mean zero and equal variance. $(a)$Consider estimator of the form $T=\sum_{i=1}^{n}a_iy_i$(where $a_i$'s are non random real numbers)that are unbiased for $b$.Show that the least square estimator of $b$ has the minimum variance in this class of estimators. Minimizing $\sum e_i^{2}=\sum(y_i-bx_i)^2$ $w.r.t$ $b$,we get the least square estimate of $b$ as, $$\hat b=\frac{\sum y_ix_i}{\sum x_i^{2}}$$ Now,$var(\hat b)=\frac{\sigma^{2}}{\sum x_i^{2}}$,where $\sigma^2=var(y),$I assume. Now,I tried to minimize $Var(T)=\sum a_i^2\sigma^{2}$ subject to $\sum a_i=1$ and I am getting $a_i=\frac{1}{n}$,i.e.,$T=\bar y$ as the $BLUE$ of $b$.I don't know where I am making the mistake.May be,my approach is not right. 
 As opposed to restricting to just one handedness of the course, another possibility is to measure wins and losses where group A is horses of same-handedness of course versus group B being horses with different-handedness of course. But you may want to use mantel-haenszel statistic where you stratify upon the handedness of the course. I highly caution against making any causal claims though: Handedness of horse is likely inherited from parents, as is some degree of speed or racing expertise. On top of that, stud horses are presumably chosen because they have good racing qualities, perhaps handedness being one of them. So this isn't a randomized trial where handedness of horse is randomly assigned to each horse. I'm not sure how it works, but it might be interesting to see the handedness of participating horses by handedness of race track - is there a sizeable difference in the number of lefthanded horses running on clockwise versus counterclockwise tracks? That might indicate whether there could be motivation to submit one's horse to a track with handedness that would give (perceived) advantage. Also, +1 to @bernhard for pointing out that unless each horse appears only once in the dataset then there might be some issues with i.i.d. data 
 I am trying to implement the EM algorithm with Student's t distribution. Any chance there is MATLAB code for this implementation that I can find somewhere 
 Currently trying to use from the base R package, I really do not understand how the formula $ \sqrt (\sum(x_i - y_i)^2) $ works in that case. As an example: The way I understand the formula, I thought it would like like below by computing $ \sqrt((1-2)^2)$ then $ \sqrt((2-3)^2)$ then $ \sqrt((3-4)^2)$ : Where am I wrong ? 
 You're looking for parameter ranking. This has been answered before. Have a look at here: Link 
 I am new at scorecard development and I use SPSS for running the statistics necessary in the process. I in the step of the logistic regression part for 37 variables using stepwise regression. I get in the end only 5 significant variables. But I need at least 8 variables for a scorecard. Do you know any tricks or adjustments that should be done to get more significant variables? Any answer, much appreciated. Lucia 
 He's not saying that the output is a linear function of the weights and biases. This can't be true because the activation function is sigmoidal. He's saying that: if you were to apply a small change $\Delta W$ to the weights and a small change $\Delta b$ to the bias, this would cause a small change $\Delta \text{output}$ to the output. And, $\Delta \text{output}$ would be approximately linear with respect to $\Delta W$ and $\Delta b$. The reason is that, because the activation function is smooth, it's possible to construct a local linear approximation, which is valid in the vicinity of the original $W$ and $b$. Think about a first-order Taylor series approximation ( here's a good illustration) . Imagine a smooth, nonlinear function in 1d. Pick a point on the curve and draw a tangent line at that point (i.e. a line that passes through the point, and whose slope is equal to that of the nonlinear function at that point). If you step away from the point by a small amount, the value of the the line at your new point will be close to the value of the original function at the new point. This means that you can locally approximate the nonlinear function with a straight line. But, if you step away too far, the approximation won't be valid anymore. The same concept applies in higher dimensions (e.g. in the case of the sigmoidal activation function with multiple weight parameters). In this case, the linear approximation is a hyperplane instead of a line. 
 Let's assume you mean $X\sim\mathcal{N}(\mu_X, \sigma_X)$ and that $y$ is a constant chosen independently of observing $X$. Reduce the problem to finding the conditional expectation of $Z = X-y$ conditional on $Z \ge 0$: adding $y$ to that value gives the desired answer. (Whether $y$ is positive is immaterial.) The governing property of conditional probability is the multiplicative relationship $$\Pr(Z\in\mathcal{A}\,|\,Z \ge 0)\Pr(Z \ge 0) = \Pr(Z\in\mathcal{A\cap[0,\infty)})$$ for all measurable sets $\mathcal{A}$. In particular, letting $\mathcal{A}=(z,\infty)$ for some $z\ge 0$, solve for the conditional probability: $$\Pr(Z \gt z\,|\,Z \ge 0) = \frac{\Pr(Z \gt z)}{\Pr(Z \ge 0)}.$$ The left hand side is the conditional survival function while the numerator and denominator on the right are both in terms of the survival function of $Z$ itself. Write $\Phi(z; \mu, \sigma)$ for the Normal distribution function with mean $\mu$ and standard deviation $\sigma$. Its complement $1-\Phi$ is the survival function. Because $Z$ obviously is Normal with mean $\mu_X-y$ and standard deviation $\sigma_X$, the survival function of the positive part of $Z$, $Z^{+}$, is $$S_{Z^{+}}(z) = \frac{1-\Phi(z; \mu_X-y, \sigma_X)}{1 - \Phi(0; \mu_X-y, \sigma_X)}$$ for $z \ge 0$. Its integral gives the conditional expectation. Add back $y$ to give the answer $$y + \frac{1}{1 - \Phi(0; \mu_X-y, \sigma_X)}\int_0^\infty \left(1 - \Phi(z; \mu_X-y, \sigma_X)\right)dz.$$ As the integral of a complementary error function , it has no simpler expression in general. 
 I simply want to visualize data projections onto best principal component in 2D coordinate system by using Matlab. I' m new in Matlab, so give me some good explanations. 
 The equation is simply the derivative of the output of the sigmoid perceptron. I will try to explain it, and then I will point you to some further resources. The equation (5) comes from taking the partial derivatives of equation (4) and ignoring the x inputs. We want to understand how the output changes when the x inputs are constant (when they're constant they don't change the output) - but when we do change the parameters w and b as if they are the inputs. The equation shows how much of the change in the output comes from (is attributable to) each input. When one of the inputs, let's say the bias b, increases by 1, and everything else stays the same, how much would the output change ? This equation, if solved for a specific perceptron, gives us a way to calculate this. If you would like to be able to go the full way, like he did, from (4) to (5), I recommend the Khan academy calculus class, which is really excellent. It delivers the material in small pieces followed by exercises. You could focus on just the derivation sections from here: https://www.khanacademy.org/math/differential-calculus 
 Box and Tidwell (1962) presented a somewhat general approach for estimating transformations of the individual predictors (IVs), and work through the specific case of estimating power transformations of the predictor variables (including that power 0, which - with appropriate scaling - corresponds to taking logs as a limiting case). In that particular case of power transformations, it turns out that there's a connection to regressing on $X_j\log(X_j)$. So if you have nonlinearity of the kind where the true (conditional) relationship between $Y$ and $X_j$ is linear in $X_j^{\alpha_j}$ then it can be used to check for $\alpha_j\neq 1$, or indeed to estimate $\alpha$ values. Specifically, when regressing on $X_j$ and $X_j\log(X_j)$ the coefficient of the second term divided by that of the first is an approximate estimate of $\alpha_j-1$. (This estimate can be iterated to convergence.) If that estimated $\alpha_j$ is close to 1 then there's little indication of a need to transform. Note that since the two terms in the product $X_j\log(X_j)$ are both functions of $X_j$, this is simply a transformed $X_j$ so I wouldn't call that an interaction; it's just a transformed predictor. (Indeed, even if I were somehow tempted to do so, since $\log(X_j)$ is not included as a predictor I still wouldn't tend to describe that second term as an interaction.) Box, G. E. P. and Tidwell, P. W. (1962) Transformation of the independent variables. Technometrics 4, 531-550. 
 I use lasso logistic regression and want to test the influence of a suqared (X1^2) variable on the outcome. Assume, I put all variables in my model and it looks sth. like this: Y (0/1) X1 X1^2 X2 X3 When I now perform a lasso regression, variables X2 and X3 remain in the model. However, variable X1 gets kicked out (zero) and X1^2 remains in the model. Assume it looks sth. like this. 0 0.34 0.67 0.12 Question 1: Did I implement the squared variable correctly (X1 and X1^2)? Question 2: Does it make sense that X1 is out of the model bit X1^2 is in the model. Can one interpret this? Question 3: Should I fix X1 at a certain level, or protect it from shrinkage? 
 I am reading An Introduction to Statistical Learning . In this book, section 4.2, page 130 (text given below) mentions that linear regression would not be useful to predict ordinal variables because the difference between one pair of values of the dependent variable might not be equal to difference between the other pair of values of that dependent variable. If the response variable’s values did take on a natural ordering, such as mild, moderate, and severe, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression. 
 "down-days (dark purple) tend to occur consecutively" can this be paraphrased as "after a dark purple it is more likely that a dark purple will follow than a white? Then just count after every dark purple whether a dark purple or a white follows. You can check, whether your counts are compatible with a 50%-chance using R's funktion binom.test(). If not, please specify your understanding of "occurs consecutively". 
 Some people told me that AIC and residual deviance are good statistics to assess multinomial logistic regression model. But I don't know how to use it in multinomial logistic regression. Anyone here know formula for finding AIC and residual deviance in multinomial logistic regression? 
 Goal: I am interested to learn if there is a way to set the range of an imputation algorithm for Missing Not at Random (NMAR) data, such as Multiple Imputation or Maximum Likelihood Estimation. Background: I am currently analyzing mortality data from CDC WONDER. The CDC suppresses citizens' health data in a region if there are so few instances that the confidentiality of the subjects and the reliability of the results are jeopardized. For example, in the CDC's Multiple Cause of Death data, if the number of deaths in a county is less than 10 over the time period indicated, the CDC will specify the number of deaths as 'Suppressed,' in place of the actual value. Thus, I want to know if there is a way to use an imputation algorithm for NMAR data when the potential outcomes are only [0,9]. I am sure that others have run into similar problems, so any words of advice and links to explanations or R/Python (or similar) tutorials would be appreciated. Thank you all for your help. 
 Linear regression assumes linear relationships between the independent and dependent variables, e.g. $y=\beta_0+\beta_1 x$. The meaning of $\beta_1$ is that per unit change in $x$ your $y$ changes by $\beta_1$. If your $x$ is ordinal, then there is no unit of $x$ per se, hence, the slope $\beta_1$ is meaningless. The unit of measure of $x$ could be meters or yards, in this case $x=1,2,3$ are one meter or yard away from each other. If you have some kind of an ordinal distance measure such as far away , not too far and very close , and you denote them with 0, 1 and 2 then the distance between 0 and 1 is not necessarily equal to the distance between 1 and 2. It makes no sense to use linear regression then 
 I am a newbie to stats and this question is to know which distribution i need to resort to. If i have to come up with a prediction model to find the count of people who claim insurance based on few predictors like age, number of vehicles, premium paid, and insurance amount.(pardon me if my example is too cheeky). In my current data set, i know if variance is equal to mean i should resort to poisson distribution, if the variance is greater than mean i know i have to use negative binomial distribution, my question is assume if the variance is lesser than the mean, in this case which distribution can i apply? 
 I'm not sure if I'm following what you are asking, so I'll do my best to answer the question. It appears that what you are after is selecting weights based on time series cross validation or rolling forecast and using regression coefficients as weights on the hold out data. Check out Diebold's forecasting book and a chapter on forecasting combination. What you may be after is called time varying combining weights. See the below for a section from the above referenced book. If you have more than 3 or 4 methods to combine then you are better off using simple averages as opposed to optimal weights. You could also use trimmed means or Winsorized means to remove outliers in place of simple averages. Several research actually supports using averages, which can be thought of an extreme form of regularization or shrinkage estimators. Check out these two articles and several references in those articles. Combining forecast by Scott Armstrong in Principles of forecasting . Simple robust averages of forecasts: Some empirical results by Jose and Winkler 
 I want to do cluster analysis of all countries in Africa to create groups of countries without specifying the groups or regions. What best method can be used , how do I do it in R studio ( right codes )...I really appreciate your help urgently 
 You might consider a randomization test. It's used for time-series analysis of single subject and small n data sets. It can be done in R relatively simply, and Dugard, File, &amp; Todman have published Excel macros to do it as well. Here are a few references: For R - Dugard et al. (book) - https://books.google.com/books?id=eja_F7boht8C&amp;source=gbs_similarbooks 
 How principal component analysis handled high multicolinearity in data set? 
 One problem with using linear regression to predict ordinal variables is that you can't interpret the regression coefficients. Imagine you wanted to predict someone's yearly income with a set of variables (gender, race, education level, etc.). But instead of having exact yearly incomes, you only have a variable indicating whether their income, such as: y = 1 if yearly income &lt; \$10,000 y = 2 if \$10,000 $\leq$ yearly income $\leq$ \$50,000 y = 3 if \$50,000 $&lt;$ yearly income $\leq$ \$60,000 y = 4 if yearly income &gt; $60,000 y is clearly an ordinal variable because it's ordered but the gap in income between each level is not the same. You can run a regression and you will get an answer. For example, you might get that $\beta$ = 1 for gender, meaning that men earn more than women if men is coded as gender = 1. But how can you quantify the amount of the difference in income? You can't, because a change from 3 to 4 can mean a big difference in income (a person where y = 4 could be making $1,000,000 a year), while a change from 2 to 3 has a maximum amount of income difference. Remember that ordinal variables are not capturing the exact values , but rather are buckets for a continuous variable. 
 I highly recommend Luktepohl's "New Introduction to Multiple Time Series Analysis" or Tsay's "Multivariate Time Series," both of which are specifically dedicated to multivarate time series analysis. Both texts contain much more if you are so inclined to read beyond VARIMA. Tsay even has his own multivariate time series library for R and has exercises in the book that you can practice with. 
 I am working on developing a model to forecast five minute exchange rate return. My return series does not show trend. No auto correlations are significant. How can we develop a model to predict returns in this situation. I found that around the release time of some scheduled news items volatility is very high. I need to incorporate this fact as well in my model. Your suggestions are appreciated? I would like to know any references regarding this problem. Thanks 
 I'm checking the data accumulation in segments with Herfindahl–Hirschman index, the idea is to get segments with low index. Do you think this is a good way to detect this accumulation? and do you see problem if I model with segments with high accumulation?. 
 Could you please help me with my confusion: I have used the robust multiple regression model(both X and y have been normalized by using zscore) and want to get the beta(standard coefficient of the two regressors), however,the results show that the coefficient is bigger than 1. I remembered if I used the zscore to normalize the data, I would get the standard coefficient. Could you please tell me where I was wrong, and if possible, could you please tell me how to get standard coefficient in the robust multiple regression? Thanks! Xinyuan 
 Since the original question mentioned the decaying gradient problem, I'd just like to add that, for intermediate layers (where you don't need to interpret activations as class probabilities or regression outputs), other nonlinearities are often preferred over sigmoidal functions. The most prominent are rectifier functions (as in ReLUs ), which are linear over the positive domain and zero over the negative. One of their advantages is that they're less subject to the decaying gradient problem, because the derivative is constant over the positive domain. ReLUs have become popular to the point that sigmoids probably can't be called the de-facto standard anymore. Glorot et al. (2011) . Deep sparse rectifier neural networks 
 Consider the following "true" multiple regression model (observation index omitted for simplicity): $$ Y = \alpha_{1} + \alpha_{2}X + \alpha_{3}Z + u $$ where $u$ is white noise, normally distributed. Importantly , assume $X$ and $Z$ are correlated. Say I estimate the following "first-step" regression via OLS: $$ Y = \beta_{1} + \beta_{2}X + e $$ Note that due to the correlation assumption, $\beta_{2}$ is biased. From here, I construct the residuals $\hat{e}$, which can be understood as the "component of $Y$ not explained by $X$". Then, I estimate the "second-step" regression via OLS: $$\hat{e} = \gamma_{1} + \gamma_{2}Z + v$$ Will $\gamma_{2}$ be an unbiased/consistent estimator of $\alpha_{3}$? 
 Your minimum variance problem is: $$ \begin{equation} \begin{array}{*2{&gt;{\displaystyle}r}} \mbox{minimize (over $a_i$)} &amp; \mathrm{Var}\left( \sum_i a_i y_i \; \middle| \; \{x_i\}\right) \\ \mbox{subject to} &amp; \mathrm{E}\left[\sum_i a_iy_i \; \middle|\;\{x_i\}\right] = b \end{array} \end{equation}$$ Remember you can always substitute $y_i = b x_i + \epsilon_i$ $$ \begin{equation} \begin{array}{*2{&gt;{\displaystyle}r}} \mbox{minimize (over $a_i$)} &amp; \mathrm{Var}\left( \sum_i a_i (bx_i + \epsilon_i) \; \middle| \; \{x_i\}\right) \\ \mbox{subject to} &amp; \mathrm{E}\left[\sum_i a_i(bx_i + \epsilon_i) \; \middle|\;\{x_i\}\right] = b \end{array} \end{equation}$$ Simplifying, you can show this is an equivalent problem to: $$ \begin{equation} \begin{array}{*2{&gt;{\displaystyle}r}} \mbox{minimize (over $a_i$)} &amp; \sum_i a_i^2 \\ \mbox{subject to} &amp; \sum_i a_i x_i = 1 \end{array} \end{equation}$$ Which will have a nice solution! You'll see that the solution $\mathbf{a}^*$ to the above optimization problem will make your estimator $T$ equal to $\hat{b}$. 
 Lemma : The following are equivalent $$ Y_n\rightarrow Y \quad stably \\ (Y_n,Z)\xrightarrow{d}(Y,Z) $$ for any $\mathcal{F}$-mb. rv. $Z$. Proposition: Assume $Y_n\rightarrow Y$ stably, where $Y$ is $\mathcal{F}$-mb. Then $Y_{n}\xrightarrow{P} Y$. Proof: Since $ Y_n\rightarrow Y \quad stably$ and $Y$ is $\mathcal{F}$-mb, we know by the Lemma, that $(Y_n,Y)\xrightarrow{d}(Y,Y)$. Thus $Y_n -Y\xrightarrow{d} 0$ which means, that $Y_n\xrightarrow{P} Y$. Stable convergence is a property of the random variables itself, not their distribution functions. Thats why you define its limit on an extension. However your Definition connects two concepts, the stable convergence, and the weak $L_{1}$-conergence. 
 I am of two minds with this. In a traditional modeling approach, one would never include a product feature without including their lower level features, and in the case of a quadratic feature that means you'd include the linear feature as well. This preserves the interpretation of the coefficients, which is valuable for inference. LASSO differs from traditional regression in two ways: firstly, it is focused on prediction and secondly (more importantly), a zero value for a coefficient does not reflect a belief that there is no association with the outcome of interest; instead, it merely says that the effect is so small, it is effectively zero. In both cases, we have a rationale for having zero valued coefficients for lower level features. On the other hand, I would be concerned that we have introduced some issues with internal validity. As we know in time series analysis, higher level polynomial terms have a tendency to "balloon" outward when a tendency toward asymptotes is more of a believable mechanism, like in biological homeostasis, growth curves, population dynamics, economics, etc. etc. I don't believe that including lower level features will do magic in this regard, but I have a strong belief it will provide better external validity (i.e. in datasets whose structure may differ markedly from the training and validation datasets). So to answer the direct questions: 1) I can't tell, there is no code 2) Yes and no. No, there is no believable interpretation of the quadratic term without the lower level linear term. To describe why LASSO obtained this, you can say that the gradient of the quadratic surface in this variable was very close to zero at the origin. 3) I would recommend the following two-step approach: identify significant features using LASSO and CV obtained tuning parameter. Then, refit the model using the corresponding regression model including all the significant terms you identified in the original LASSO as well as the lower level terms. 
 I am writing up ANCOVA results and my R output provides t tests for each of my model coefficients of interest, but my sense is that it's more conventional to report F ratios (which I know are the square of the t values). I just want to be sure not to inadvertently alarm reviewers who might think I'm doing something terribly wrong. Note, I'm not interested in reporting omnibus (e.g., 2+ DF F-tests). These are all 1 DF tests. 
 Is there a model of HMM with a probability that the next "item" in the list doesn't fit the criteria for the model and needs to be skipped or do I need to figure out how to remove these myself and feed in only data that applies to the chain (if that's even possible)? All the formulas I've seen state "no skip" within the chain. (Just need references/terms to look up and research) 
 I am doing a Generalized Linear Mixed Model, using SPSS 23. How do I include tests for pairwise comparisons? 
 I have a set of Survey responses in the format shown below. I want to provide a mathematical measure of Variability to show "how different" the population is, but I'm not sure how to do so without doing 16! comparisons. Is there a way to compute any sort of Measure of Variability with Categorical Data like this using either excel or R? 
 A non-positive definite Hesssian matrix is indeed a problem, and you should not trust your results. If you search the web for some variations of "Hessian matrix not positive definite" then you will find many answers. On my most resent encounter with this problem I assembled the following possible fixes that I tried, in order: 1 make sure no large amounts of missing data 2 check scaling of predictor variables; If an order of magnitude or more off, the model might have trouble calculating variances 3 check all variables to make sure none is a constant 4 check if there is a near perfect linear dependency b/t two variables (e.g., height &amp; weight); if so, one may be deleted 5 reduce the number of exclusions (they could combine to leave you with less variation in your data) 6 check if a random intercept captures all the variation 7 make sure have specified a SUBJECT variable on the RANDOM subcommand 8 make sure there is level 2 variation in the outcome 9 make sure there are level 2 predictors 10 run models and check if some covariance estimates are either 0 or have no estimate or no standard errors at all 11 increase the number of MAX ITERATIONS 12 increase the number of (Fisher steps) for SCORING 13 remove INTERCEPT from RANDOM line (may not be needed &amp; may make solution impossible to obtain if it makes no difference) 14 use a simpler covariance structure (with fewer unique parameters; avoids redundant parameters) 15 check if too many levels of the random effect and not enough fixed observations within each random level 16 try using a different missing data technique 17 if hypothesis allows it, run a population-averaged (repeated measures) model (which has no random effects, but accounts for correlations within individuals) 18 work directly with eigenvalues to adjust zeros up to .05 My matrix was positive definite when I got to #12. Scaling the variables to within the same order of magnitude and increasing the max iterations seemed to help the most, but every dataset is going to be different. I have no fix for the error message: Data Structure: One or more subject fields were specified but not actually used in the analysis. 
 Suppose I run a binomial GLM (in R) with response variable [0,1] and 2 predictor variables that are both categorical. Let's call them and where has 3 factor levels (,,) and has 2 factor levels (,). Therefore: Let's say the summary output of the coefficients looks like this: So I understand and are constrained in the intercept and the estimate values presented are log-odds ratios (I am familiar with interpreting them). Let's say now I want to determine the estimate when is true or = 1 and is =1 (true), then I would just calculate it as B0 (intercept) + B1 (coeff of a2) = -0.62049 + -2.24304 Or if is true and then I would calculate it as B0 (intercept) + B3 (coeff of a3) + B4(coeff of b2). = -0.62049 + -1.76965 + -0.79545 But now how do I calculate the Standard errors and/or 95% Confidence intervals for the last two scenarios? Can I just add the standard errors in the same way? Or I would think I leave out the SE's of B0 (the intercept)? So that in my first scenario where a2 is true and b1 is true, then I would just calculate it as SE of B1 (SE of a2) = 0.16111 And in my second scenario where a3 is true and b2 then I would calculate it as SE of B3 (SE of a3) + SE of B4(SE of b2). = 0.14147 + 0.07918. Am I correct in my assumptions of interpreting the model output or am I completely wrong here? 
 Some background I have a data set with around 2000 samples and 16 features. Me and my colleague are trying out different methods for clustering and currently we are converging to the usage of a Gaussian Mixture Model. We established that according to an information criterion, the best number of mixtures is 6. Then we received 200 more samples and ran the clustering again. In this case the number of clusters remains the same, but one of the old clusters got severely split up among the other ones. My colleague is a psychologist, so he had invested some time in interpreting the old clusters and was rather unsatisfied with the change when the new data arrived. I am trying to convince him that we need to investigate the stability of the clusters in order to understand if there are any particular groups in the data that stand out, i.e. is there any true stable underlying structure. So I came across this article , which has been cited around 260 times, so I went to try it out. I can calculate the figure of merit mentioned in the article and this looks like a reasonable way to go forward, but for 5 samples of half the data, it is rather small. I have also reordered the data according to the clustering of the whole data set. This gives me a connectivity matrix like the one below, you can see that some of the groups are rather small. Now for the 5 resamples I get the following connectivity, where connectivity is the average number of times there was a connection in the 5 resamples. The original structure remains, but some of the clusters split/mix up more than others. I intend to run this 1000 times to get a better feel for what is going on. My questions So I have a few questions regarding estimating the stability of clusters in general, and what to do about it. I resample only half of the data, should I be resampling more? Or maybe just create bootstrap samples and remove duplicates? If some of the clusters are splitting up a lot, is the right way to go forward to merge those clusters or just use the most stable ones? I guess this is a subjective question, depending on the end goal, but I would like to here a generic answer if someone has some input in that regard. Can you point me to any case studies or more reliable sources on this matter? 

Autoregressive Conditional Heteroskedasticity (ARCH) models are used for time series in which the residual variance changes over time. The variance of the error term is assumed to follow an autoregressive (AR) process. Compare GARCH.
 TLDR: How can I model the relationship between two count variables that are both dependent on the same third, unobserved, continuous variable. Detail I am interested modelling the effect of an explanatory variable on the value of a zero-inflated count variable. (For those interested the dependent variable is RNA protein interaction counts from an iCLIP experiment, and the independent variable is the abundance of the RNA molecule). I wish to know: Does the independent variable (abundance) explain the counts. I.e. how strong is the relationship between the two variables. Do other categorical independent variables have an effect once abundance has been accounted for. Unfortunately, I don't know the value of abundence, I only have a estimate of it. This estimate is itself a count variable (counts from an RNA-seq experiment). It is generally assumed in the field that these counts follow a negative binomial distribution. Perhaps an example dataset would help. In R: My first thought was to try using a (zero-inflated) negative-binomial regression, but there are several problems with this. Not least is that the results don't look like a very good fit: Note the use of pseudo-counts due to the fact the regression expects a multiplicative relationship between the predictor and the response - I don't know how much of a problem this is, but using a sqrt link function without the pseudo-counts makes little difference. A chisq on the residual deviance, tells us that the model doesn't fit, and this can be seen by looking at a plot of the fitted values vs what is expected: Red is fitted values, green the theoretic predictions, blue a loess fit to the data. Trying to fit a zero-inflated model, using from the give pretty much identical results. Is there a better model I could try an use? Or a completely different approach, not based on glms? 
 I'm using XGBoost package (under Python) on a Kaggle set. I'm running the following code so the params are the same, except that in CV i don't need to print the evallist ? (its not an accepted parameter i think ?) then here is what i get I don't understand why the result in the single run has a much higher logloss (eval-mlogloss 2.689) than in the CV run (test-mlogloss 2.123) while i'm running it on the same set right and same parameters right ? also why are there 20 lines in res ? i specified 5 folders, but is the CV always running 20 times ? (this 20 doesn't seem to be a parameter and i haven't seen this number in any doc ?) thank you 
 I am running the following syntax in IBM's SPSS: GENLINMIXED /DATA_STRUCTURE SUBJECTS=Subject# /FIELDS TARGET=any_error /TARGET_OPTIONS DISTRIBUTION=BINOMIAL LINK=CLOGLOG /FIXED EFFECTS=atsp11b_replacedMV_scaled2 USE_INTERCEPT=TRUE /RANDOM EFFECTS=ID USE_INTERCEPT=TRUE /BUILD_OPTIONS TARGET_CATEGORY_ORDER=ASCENDING INPUTS_CATEGORY_ORDER=ASCENDING MAX_ITERATIONS=1000 CONFIDENCE_LEVEL=95 DF_METHOD=RESIDUAL COVB=MODEL PCONVERGE=0.000001(ABSOLUTE) SCORING=1 SINGULAR=0.000000000001. atsp11a_replacedMV_scaled2= level 2 predictor (an academic score on another test) any_error= level 1 outcome (1 if error; 0 if no error on trial on my test) CLOGLOG=complementary log-log (because there are not that many trials with errors) No error messages or warnings are triggered. However, I get the following in the Model Viewer under "Data Structure": One or more subject fields were specified but not actually used in the analysis. This concerns me because my only subject variable under DATA_STRUCTURE is Subject#, and if that is not being used, then the analysis must not be clustering (the correlation of trials within subjects is not accounted for). Should I be concerned? Does this mean I don't need Subject# (there is no need to cluster)? Is there something else I should try? 
 I am using Excel, and I would like to visualize or in some way examine and display in as simple a way as possible how a three variable formula changes by manipulating each variable. The formula can be: $$ f(x) = \frac{Y ( 100 + ( 5Z) + (8W) )}{100} $$ The best thought I had so far is three different 2d tables for every combination of Y, Z and W. Do you think there is a better way, some combination or better cleaner visual way? 
 I have heard this advice repeatedly however recently when I was looking at my own graph with CIs I had a panic attack because the error bars overlapped, yet my analysis told me the difference between the means was significant. I later learned here that I was making an incorrect assumption that the CIs couldn't overlap. I don't know how idiosyncratic my error was to me, or if there's any reason to suspect those who see my bar graph might draw the same erroneous conclusions. Do you recommend one over the other? Are CIs really less prone to misinterpretation than SEs? 
 How should one go about turning a frequentist result into a Bayesian prior? Consider the following pretty generic scenario: An experiment was conducted in the past and a result on some parameter $\phi$ was measured. The analysis was done with a frequentist methodology. A confidence interval for $\phi$ is given in the results. I'm now conducting some new experiment where I want to measure some other parameters, say both $\theta$ and $\phi$. My experiment is different than the previous study --- it is not performed with the same methodology. I would like to do a Bayesian analysis, and so I will need to place priors on $\theta$ and $\phi$. No previous measurements of $\theta$ have been performed, so I place a uninformative (say its uniform) prior on it. As mentioned, there is a previous result for $\phi$, given as a confidence interval. To use that result in my current analysis, I would need to translate the previous frequentist result into an informative prior for my analysis. One option that is unavailable in this made up scenario is to repeat the previous analysis that led to the $\phi$ measurement in a Bayesian fashion. If I could do this, $\phi$ would have a posterior from the previous experiment that I would then use as my prior, and there would be no issue. How should I translate the frequentist CI into a Bayesian prior distribution for my analysis? Or in other words, how could I translate their frequentest result on $\phi$ into a posterior on $\phi$ that I would then use as a prior in my analysis? Any insights or references that discuss this type of issue are welcome. 
 I assume that the dependence on $n$ is through $\hat \theta = \hat \theta_n$, and that $\theta$ is fixed. Let $\varepsilon &gt; 0$ and $\gamma &gt; 0$. Let $F_n$ be the distribution function of $n \|\hat \theta_n - \theta\|^2$, i.e. $F_n(y) = \mathbb P(n \|\hat \theta_n - \theta\|^2 \leq y)$. The stated assumption on convergence in distribution means that $F_n(y) \rightarrow F(y)$, where $F(y)$ is the limiting distribution. Let $N$ sufficiently large so that for $n \geq N$, $\sup_{y \geq 0} |F_n(y) - F(y)| &lt; \varepsilon/2$ and $F(n \gamma^2) &gt; 1 - \varepsilon/2$. Now for $n \geq N$, $$\mathbb P(\|\hat \theta_n - \theta\| &gt; \gamma) = \mathbb P(n \| \hat \theta_n - \theta\|^2 &gt; n \gamma^2) \rightarrow 1 - F_n(n \gamma^2) \leq 1 - F(n \gamma^2) + \varepsilon/2 &lt; \varepsilon.$$ This shows that $\hat \theta_n \rightarrow \theta$ in probability and in particular for any $c \in \mathbb R^p$, $(\hat \theta_n - \theta) \cdot c \rightarrow 0$ in probability. 
 The stochastic integral is defined as $$u_t = \int_{t-1}^t e^{-\kappa(t-s)}\int_0^s e^{-c(s-r)} \, dW(r) \, ds.$$ where $W(t)$ is a standard Brownian motion, $\kappa$ and $c$ are both positive. I know this integral can be viewed as $$u_t = \int_{t-1}^t e^{-\kappa(t-s)}J_c(s) \, ds,$$ where $J_c(s) = \int_0^s e^{-c(s-r)} \, dW(r)$, is an Ornstein-Uhlenbeck process. But how do we handle this double integral and use Ito's isometry to get the variance of it? Further, does this integral admit a Wold representation, that is, $$u_t = \sum_{j = 0}^\infty F_j \varepsilon_{t-j},$$ where $\varepsilon_t \sim \mathrm{iid}(0, \sigma^2)$. Here is my calculations, which may be wrong, I am not so sure... $$\begin{align} Var(u_t) &amp;= E(u_t^2) = E\left(\int_{t-1}^{t} \int_{0}^{s}e^{-\kappa(t-s)-c(s-r)}dW(r) ds\right)^2 \\ &amp;= E\left(\int_{0}^{t} \int_{r}^{t}e^{-\kappa(t-s)-c(s-r)}ds dW(r) \right)^2 \\ &amp;= \int_{0}^{t} \left(\int_{r}^{t}e^{-\kappa(t-s)-c(s-r)}ds\right)^2 dr \\ &amp;= \int_{0}^{t} e^{-2\kappa t + 2cr} \left(\int_{r}^{t}e^{(\kappa-c)s}ds\right)^2 dr \\ &amp;= \dfrac{1}{(\kappa-c)^2}\int_{0}^{t} e^{-2\kappa t + 2cr} \left(e^{(\kappa-c)t}- e^{(\kappa-c)r} \right)^2 dr \\ &amp;= \dfrac{1}{(\kappa-c)^2}\left(\int_{0}^{t} e^{-2c(t-r)} dr + \int_{0}^{t} e^{-2\kappa(t-r)} dr - 2\int_{0}^{t} e^{-(\kappa+c)(t-r)} dr\right)\\ &amp;= \dfrac{1}{(\kappa-c)^2}\left(\dfrac{1-e^{-2ct}}{2c} + \dfrac{1-e^{-2\kappa t}}{2\kappa} - 2\dfrac{1-e^{-(\kappa+c)t}}{\kappa+c}\right) \end{align}$$ which indicates $u_t$ exhibits heteroskedasiticity. But still, I don't know if it is weakly stationary. 
 This is my first post on Cross-Validated. I read a lot of question related to my problem, but no one was completely satisfying. I have two time series that are sampled at very different time intervals, e.g. one is sampled every 10 seconds, while another every day. I'd like to evaluate the correlation between the two series (perhaps something like cross-correlation). Is it right to align the one series to the other (maybe averaging the value of the one with the finest sampling rate)? Is there any common practise in treating this problem? Is there any Spark-based library that can be used to easily deal with such problems (with Java support, as )? 
 I have a question about combining survey results that involve a measure of a population and a measure of a sample of a population. Here is what I have: 1) In a given state, I surveyed recycling availability (either yes or no) for all major cities such that 50% of the population was accounted for. This is not a sample but rather an accounting of all major cities in a state, and thus there isn't a margin of error. 2) In the same state, I surveyed a sample of the rural population's recycling availability (again either yes or no). This is a sample of all the rural population, and thus there is a margin of error associated with this survey. For example, say there are two big cities in a state, and 50% of the population lives in either one city or the other. Now let's say each of those cities has recycling services, so 100% of the city population of that state has access to recycling. There is no margin of error, because I sampled the entire population of the city. In this same state there are 500 rural communities, and I randomly sampled 100 rural communities and found that 70% have recycling and 30% do not. Now since this is only a sample of the whole population, there is a margin of error for this survey. Let's say the state has 10 million residents. That would mean 5 million of them have access to recycling because they live in one of the larger cities. Another 3.5 million with some margin of error would also have access to recycling. Then 85% of the state's population has access to recycling, but with what margin of error? How would I combine these results to accurately show the recycling availability in this state with a margin of error? Thank you. 
 The problem: I have two measurement techniques, X and Y, measuring the same variable, crack depths in a metallic structure. Based on previous studies, I know the i.i.d. random error associated with both methods: $$ \epsilon_X \sim N(0, \sigma_{\epsilon_X}) \\ \epsilon_Y \sim N(0, \sigma_{\epsilon_Y}) $$ Assuming method Y is accurate but imprecise (i.e., no systematic bias) and method X is neither accurate nor precise (i.e., with a linear bias), the calibration line is currently specified as $$ Y=\beta_0+\beta_1 X_a+\epsilon_Y \\ X_a=X_o+\epsilon_X $$ I'm using a Bayesian errors-in-variables (EIV) framework to estimate the regression line and the associated prediction interval, similar to the model described in this post . Question 1: Since this is an errors-in-variables regression, given a measurement $x_i$, is it correct to use the regression line directly to estimate the true value and associated error via the Bayesian posterior density? Does this EIV approach make inverse regression unnecessary? The following Python code shows how the 80% highest density prediction interval is being calculated from the MCMC traces, assuming $Y$ is t-distributed: Question 2: Is there a way to estimate the true depth if both methods suffer from an unknown linear bias? The only information I currently have is the random normal errors described above and a scatterplot of the data, which shows a linear relationship between $Y$ and $X$ and that the correlation is $0&lt;\rho&lt;1$. There is an obvious relative bias, but without assuming one of the measurements is accurate, I can't think of a way of estimating the true bias. 
 I have recorded data from a neuron from different trials. Each trial we show a stimulus to a rat and then we measure the neural activity. I want to measure the PSTH which is peristimulus time histogram. However since each trial has a different duration, I can't just compute the PSTH where it needs to have a fixed trial duration. If I want to make my question more general it would be stated as follows: Suppose I have a point process with a defined underlying rate function (which may or may not be homogenous). Now I have different observations from this process, i,e., different trials with different durations. Say, one time I have data from this process for 2 seconds and another trial for 5 seconds and so on. How can I compute the underlying the rate function assuming that the underlying rate function doesn't change from one trial to another trial? 
 It's been some time since you posted this question, so I hope my response is still relevant. I'm an analyst at a large retailer who specializes in promotion modeling, and we do much work in this area using regression techniques to estimate the effects of promotions among various other elements driving sales performance. I'd like to answer your question, but I need more details regarding your model. How is the data in your table organized? Does it include historic data for each product, or just a snap shot of the SKU selection at a given time. Based on what information you've presented, here are my thoughts: It may be that the variable is identifying that products on discount tend to possess lower sales (think expensive brands with lower sales volumes compared to big volume inexpensive products, if the expensive brands are on sale but typically have smaller volumes, the coefficient will capture this). If you model a particular SKU's sales history and price (incl. discounts) you may see a different story (one that captures the response to the promotion). Other than that, has a p-value greater than 0.05 and should be removed from the model (if you go by the p-value criterion), as you suggested. An important thing to think about here is how your model is interpreting the data. If no variable has been added to account for the differences in product sales (i.e. market share) of each brand/SKU, and your model is based on snapshot data (no history), then it is likely your model is just identifying that products on promotion have lower sales compared to all other products, it does not necessarily mean that the promotion is causing sales to be lower. One option here is to include a variable that takes into account the natural difference in sales between SKUs (maybe market share %)? You'll have to explore options like these and see how it affects your model output. You always want to make sure that if you're adding variables, they aren't correlated with the other variables in your model. Don't give up and good luck! 
 I understand the definition of Cohort Fertility Rate as the average number of live births that a woman born in a particular year has had by the end of her reproductive life. But is there a formula to calculate it? I tried googling but in vain. I have seen formula for many other fertility indicators like CBR, IMR etc. All of them have some formula for direct computation but I couldn't find any such idea for CFR. If there isn't any formula then please direct me to the mechanism of its calculation. 
 If I had a vector of weights for each observation And I had a function that did not accept weights as an argument: Can I multiply each variable by the weights to get the desired output? There is a version of kmeans that accepts weights: But there are aspects of regular output that I still want, including . And just for learning purposes, Is it possible to just multiply all observations by the weight vector to get the equivalent output? Did I successfully "add" the weights by multiplying each variable by the weights or am I missing some aspect of weighted kmeans theory? In other words, is structurally equivalent to ? 
 For the following data, I can do a nested ANOVA followed by post hoc test for as follows using the function. The subgroup is nested within the group . p value for H = Tech and Error = Rat pvalue for H = Rat and Error = Within Tukey HSD for Group How to do proceed with Tukey HSD for subgroup within similarly with the package? I have tried the follwing withoud success 
 XGBoost shows the performance in every iteration (in your example, 100 iterations will have 100 lines in the training.), i.e., it shows the performance during the training process but not showing you the final results. You can turn off the verbose mode to have a more clear view. In addition, keep in mind Anything can happen in testing set Training loss will decrease with more iterations 
 To keep all the inputs to a network on the same scale, they are usually normalized so that they end up being represented as number of standard deviations from the mean. Is this something that needs to be done for the result of trained word embeddings, e.g. vectors from GloVe or word2vec? If so, should I use the mean/variance of only the embeddings, or all the samples in my data set? 
 This is probably a basic question, so apologies. I am trying to determine what is the most accurate reconstruction method out of 11 options, for which I have results for 15 specimens. I've looked into one-way repeated measures ANOVA but I'm not sure that it is appropriate. Any help? Thanks in advance! 
 If the forecasting scheme provides point forecast, you can use either Mean Absolute Error (MAE) or Mean Square Error: $$\text{MAE}_h=\frac{1}{n}\sum_{i=1}^{n} |e_{h}|$$ $$\text{MSE}_h=\frac{1}{n}\sum_{i=1}^{n} (e_{h})^{2}$$ in which $e_{h}$ is the $h$ period ahead forecast error. i.e. the difference between the realized value (Y_h) and the point forecast (F_h): $$e_h=Y_h-F_h$$ Note that such measure are size dependent and you might want to use Mean Absolute Percentage Error: $$\text{MAPE}_h=\frac{1}{n}\sum_{i=1}^{n} |e_{h}/Y_{h}|$$ However, if the forecast scheme generates a probabilistic forecast, in other words, a distribution for all possible outcomes, you have other choices: probabilistic score rules. In the following Q is the forecast with q(.) as its probability mass or density function A Linear score rule (improper and therefore not recommended): $$S(Q,Y_h)=q(Y_h)$$ A logarithmic score rule: $$S(Q,Y_h)=-log(q(Y_h))$$ A Quadratic score rule: $$S(Q,Y_h)=-2q(Y_h)+\int_{\mathbb{R}}q(z)^2dz$$ Need more information? I recommend the following sources: Gneiting, T. and Katzfuss, M. (2014). Probabilistic forecasting. Annual Review of Statistics and Its Application, 3:347–373. Makridakis, S. G., Wheelwright, S. C., and Hyndman, R. J. (1998). Forecasting: Methods and Applications. John Wiley &amp; Sons, 3rd edition. 
 I have two very large unpaired control / variant groups, and a dependent variable that is the success/failure of a binomial, with a relatively small effect size between the treatment groups. If I just do a simple unpaired t-test between the two treatment groups' mean success rates, there is a clear significant difference between them. What I really care about here is the relative difference between groups, i.e. (variant rate - control rate)/control rate. (I fully realize we're already getting into dicey territory when dividing one random variable by another.) However, I also want to test if subgroups across both treatments - e.g. splitting my control/variant samples by age - are also experiencing the same relative difference as the overall groups, or if they're significantly different. What's the right test for the null hypothesis here? Things I've considered: A Lack-of-fit F-test might be able to pick out if there's an outlier age group somewhere in there that's not behaving like the other age groups, but that won't measure relative differences. If one age group is underperforming in both treatments, that would show up as a significant lack of fit, even if the relative difference is the same across treatments. I could do a difference-in-difference analysis here, but using the overall change as a baseline doesn't sound very kosher, since that's ultimately somewhat dependent on the specific age group I'm testing. Moreover, if I want to test lots of different age groups, that quickly becomes a multiple comparisons problem without something like a Bonferroni or F-test to keep the false positives down. 
 Could some one help me understand what is the main difference between votes and probability in the type argument of predict.randomforest? My understanding is that if the individual trees are grown to maximum depth or full homogeneity, then vote share would be equal to probability. I may be wrong 
 I have this question which I can't figure out, maybe because the answer I got is not correct (trying to match the answer). A six-sided die is thrown twice, but not a normal die. If X is the result of the die, then: $$ P(X=1)=P(X=4)=0.2 $$ $$ P(X=2)=P(X=3)=P(X=6)=0.1$$ $$ P(X=5)=0.3$$ I am interested in the probability of getting an even number twice. The answer I got is . I can't figure out why... the explanation is: $$ 0.04+2*0.01 $$ But it makes no sense to me. 
 Is there a site or mailing list where they post postdocs for data science, more so in Europe? There's definitely a mailing list for machine learning: https://groups.google.com/forum/#!categories/ml-news/postdoctoral-position I wonder if there is a similar one for data science or data analysis postdoc positions? I've searched on LinkedIn, monster, glassdoor already. 
 Possible? Yes, as you've shown. Valid? Depends on what you mean. It's an estimate, and estimates can be biased. Consider the case where half the respondents give you exact measurements (e.g. 22 hours) and then half give you a binned estimate (e.g. 21-30 hours). If you calculate the average binned estimate as you showed above n1 x midpoint of category 1 + n2 x midpoint category 2 ..... divide by total n then you could add that number with the mean exact measurement, divide by 2, and get an estimate of the average working hours. Or maybe you want to give more weight to the mean exact measurement, and so you could do a weighted average of the two means to estimate the average working hours. A third estimator could look like this: Bin the exact measurements into the three categories, and then find the deviation of the empirical average within a bin from the midpoint of that bin. (e.g. with exact hours observed as 22, 24, and 23, the average within the bin is 23, which deviates from 25.5 by 2.5). Then, you may choose to use the empirical average within the bin (instead of the midpoint of the bin) in order to calculate the average work hours from the observations that had measurement in categories/bins: n1 x empirical average (from observations with exact measurements) within bin 1 + n2 x empirical average within bin 1 ..... divide by total n Another estimator could take a parametric assumption and/or Bayesian framework to estimate the average from the obervations with binned measurements. There are plenty of estimators. Theory of statistics can show that some may "work better" than others. If you're a frequentist you'll probably want one with 95% asymptotic coverage. Those estimators would probably be the "most valid". As another answer points out, your proposed method is likely to be biased, and so maybe not as "valid" as you would like. Reporting the percent of observations in each bin, however, is a very good way of explaining your data. if you feel strongly above giving an estimate of the overall mean, you could do so, but be sure to be clear that you used a midpoint-calculation statistic like your proposed method, and perhaps state that your estimate is not very precise. 
 $$P(Even) = P(2)+P(4)+P(6)$$ $$~~~~~~= 0.1+0.2+0.1$$ $$ = 0.4~~~~~~~~~~~~~$$ P(Consecutive Even) equals the probability that role 1 and 2 are even. Since they are independent throws then you can multiply the 2 probabilities. 0.4*0.4 = 0.16 
 After training a doc2vec network can you only compare word vectors with each other and doc vectors with each other? Or does it also make sense to compare word vectors with doc vectors? Well, of course assuming that the dimensionality of the doc vector is the same as for the word vector (as for instance in a neural network that sums all word and doc vectors instead of concatenating them for classification). For instance, if I have a high cosine similarity between a document vector and a particular word vector, does this imply that the document is somehow semantically similar to the word and vice versa? Thanks! 
 please help me with my question!! I have been asked this question by my teacher and NO NONE knows 
 I'm working with a large SPSS file and every quarter I import new raw data. I then transform the data in various ways, such as recoding into new categories. Is there a simple way to restrict such transformations to only the new data/cases? I know there are workarounds I can use (e.g., doing the transformations in Excel or in a unique SPSS file and then merging), but I'm curious if there's a more straightforward approach. Thanks! 
 I calculated the dummy coding for study interaction between carbohydrate dietary intake with a specific gen polymorphism. The reference group is low intake and protected genotype. the outcome of logistic model was as follow. Is it possible that EXP for both risk factor (high intake and risk genotype) was less than each risk factor separately? how I can interpret it? 
 MultinomialLogisticLossLayer states, that Each prediction vector $ \hat{p}_n $ should sum to 1 as in a probability distribution: $ \forall n \sum\limits_{k=1}^K \hat{p}_{nk} = 1 $. Is that really the case? It seems to me that the only requirement is that in order to calculate the multinomial logistic loss $ E = \frac{-1}{N} \sum\limits_{n=1}^N \log(\hat{p}_{n,l_n}) $, the variables $\hat{p}_{n,l_n}$ have to be in $[0,1]$. In my specific case I'd like to do hierarchical classification with a network predicting probabilities for all nodes of the taxonomic tree at once. Training data is labeled with the leaves of the taxonomy, so in order to train the inner nodes, I want to use the adjacency matrix as the infogain matrix in a InfogainLossLayer (which is a generalisation of the MultinomialLogisticLossLayer ). Because I train all taxonomic nodes at once (inner nodes and leafs), their probability does not sum up to 1, and instead of a softmax (which makes $\sum\limits_{k=1}^K p_k = 1$) I have to use a sigmoid (which only ensures that $i_k \in [0,1]$). 
 I would like to measure the divergence (or, more appropriately, symmetric difference) between two distributions $P$ and $D$. In general, you could consider using a measure like Jensen-Shannon divergence (JSD). However, consider the case in which we are attempting to judge our predictions of the distribution of scores for an olympic diver. If the scores are out of 5, and the true distribution is $[0, 0, 0, 1.0, 0]$, then a guess of $[0, 0, 0, 0, 1.0]$ is much better than a guess of $[1.0, 0, 0, 0, 0]$. However, to a measure like JSD or Kullback–Leibler, these guesses are equally bad, since the distribution is assumed to be over categorical outcomes as affirmed here . Are there any accepted ways of comparing these distributions given the assumption that the distribution is over items that can be considered to have an ordering? Simply smoothing the predictions would have this effect to some degree, but that would require adding more parameters (i.e., smoothing strength) and I'm not sure how valid it would be. 
 By unimodal, I think the OP plainly means that there is just one interior mode (i.e. excluding corner solutions). The question is thus really asking ... $$\text{why is it that brand name distributions do NOT have more than one interior mode?}$$ i.e. why do most brand name distributions look something like this: ... plus or minus some skewness or some discontinuities? When the question is posed thus, the Beta distribution would not be a valid counter example. It appears the OP's conjecture has some validity: most common brand name distributions do not allow for more than one interior mode. There may be theoretical reasons for this. For example, any distribution that is a member of the Pearson family (which includes the Beta) will necessarily be (interior) unimodal, as a consequence of the parent differential eqn that defines the entire family. And the Pearson family nests most of the best-known brand names. Nevertheless, here are some brand name counter examples ... Counter example One brand-name counter-example is the $\text{Sinc}^2$ distribution with pdf: $$f(x)=\frac{\sin ^2(x)}{\pi x^2}$$ defined on the real line. Here is a plot of the $\text{Sinc}^2$ pdf: We could also perhaps add the family of cardiod and distributions related to this class ... with pdf plots such as: The family of reflected brand name distributions would also perhaps be possible brand name contenders (though, these might be thought of as a 'cheat solution' ... but they are still brand names) such as the Reflected Weibull shown here: 
 This is a rather general question. If the question is vague and hard to answer in a few lines, I'd be happy if someone just point me to some readings. Thanks in Advance. I am working on a multi-class classification problem with a large datasets (&gt;3000 variables) and I am looking to reduce the dimensionality. My first step is to using "filter" methods to evaluate the relevance of a single feature and the target variable to eliminate some very low score features. I am planning on just using Mutual information as my relevance score, but as I read about it I realize there are more methods such as Fisher score, Gini index or simply correlation coefficients. Q: Is there a better relevance measure for the given classifier I'd use? Eg. If I decide to use SVM, or decision tree as my classifier, would one of the relevance measure be more optimal than the others? EDIT: I understand there are better methods (eg, some wrapper methods) in practice, but I am more interested in the theoretical analysis of how these filter methods mesh with the classifiers. 
 An urn has 5 black balls and 4 white balls in it. We randomly sample a ball, and return it to the urn (sampling with replacement), until we get 2 balls with the same color. What is the probability that the first ball was white, if we know that the last one was white ? I tried building a tree, and realized that the experiment can have 2 or 3 stages, not more. Then I tried to fit conditional probability, but I got a fairly long fraction, which made no sense to me. Could you please help me to solve this problem ? Thanks ! 
 As I understand you need to do this automatically. You could do so by using the threshold that minimizes $FPR^{K_P} + FNR^{K_N}$ (using all possibilities values below a certain $FPR$ of your choice). For example $FPR^2 + FNR^2$ would equally (quadratically) penalize $FP$ and $FN$ error, while e.g. $FPR^3 + FNR^2$ would penalize $FP$ more drastically than $FN$. The larger $K_N$ and $K_P$, the closer $FPR$ and $FNR$ will stay together - while different $K_P$ and $K_N$ will account for your needs of more strongly minimizing $FPR$ or $FNR$. Still, $K_N$ and $K_P$ need to be chosen beforehand. What exact values are suitable for your problem will depend on its details and your needs towards $FPR$ and $FNR$. 
 Problem description question is at the end: I am testing to see what factors are responsible for vegetation change over a large landscape over a period of 100 years. The response variable is an ordinal measure of vegetation change describing how different the vegetation is in terms of its successional state. The response can be 0 (no change), 1 (a little bit of change), 2 (moderate change) or 3 (Maximum change). Predictors: I have a model with a random factor (Blockshed: watersheds to control for spatial autocorrelation), an ordered factor (tsf: time since fire)and two continuous variables (Z.elev: standardized elevation; Z.solar : standardized solar insolation). I am using the CLMM procedure in R:Ordinal package. Because there are three possible levels of tsf (short, medium, very long), the model tests both linear (L) and quadratic (Q) terms for the variable (n-1 models, if the TSF had 4 levels, it would also test Cubic) . These two models are indicated in the output by TSF.L and TSF.Q. As you will see both are significant, so I would assume I would choose the model with the lowest p-value, or if they are identical (as is the case here), choose the simplest (L) model. This is the code to run the model: The following is the output from the model: Question: what is CLMM doing in the Linear vs Quadratic models? The response variable is an ordered factor with values of 0, 1, 2 I have read the CLMM package description, and it does not specify what is going on under the hood. Created using this code: 
 You just do something like See for further details. 
 A question in my survey asks about the age of respondents. Respondents are asked to choose one of these four ranges: 18-29 30-49 50-64 65 and over How can I calculate the average age of respondents? 
 You're stuck because you are relying on algebra. Draw a picture. Every application of Bayes' Theorem works like this. Here is the setup. I chose to show a million people because we will get whole numbers in the end. Of these, some are diseased. I put them into a special group as shown. We are going to move them around--without losing track of any of them--as they get tested. Move the people into areas according to their test results: positive or negative. Some were diseased, some were not. I have drawn four arrows to chart their movements. We know some of the numbers. For instance, $0.0002$ of those million people, or $200$ of them, are diseased. The remaining $999,800$ are not. We also know that one percent of the diseased people will have a false negative and two percent of the disease-free people will have a false positive. Because these are pretty rare outcomes, I have made their arrows thin. Therefore the remaining $0.99$ of the $200$ diseased people will test positive and the remaining $0.98$ of the $999,800$ disease-free people will test negative. (This is what it means not to lose track of anybody.) These facts are shown with fat arrows to indicate a lot of people pass their way. It remains only to total those in each test group: Observe that most of the people in the positive test group came from the disease-free population, because so many people do not have the disease. Being able to see this is the point of the exercise. Unless you are good at math, the algebra only obscures this important realization. You may read off the answer: Out of every million people who are tested, expect that $20,194$ will test positive. The chance of a positive test therefore is $$P(T)=\frac{20194}{1000000}=0.020194=2.0194\%.$$ 
 I leave the details for you to fill in, and do not provide proofs, because this is self-study. Define $Y_i = X_i^4$ and find the density for the $Y_i$ Compute $\mathbb E Y_i$ Use the result in 2. to construct an estimator based on $S = \sum_iY_i$ and appeal to a theorem about unbiased estimators that are functions of complete, sufficient statistics. 
 I am running mixed-design ANOVA with one within-subjects repeated measures factor (2 measures), one between-subjects factor (3 categories) and one covariate. Results are following: 1) Significant main effect of between-subject factor 2) Significant effect of covariate 3) Insignificant effect of repeated measures 4) Insignificant effect of interaction between repeated measures X covariate 5) Insignificant effect of interaction between repeated measures x between subject factor In the following step I would like to do some post-hoc testing. I know that the global F-test (for the homogeneity across all groups) does not need to be significant for post hoc testing – and here is my problem. The global F-test for the repeated measures effect (exaclty n=2 means are compared) is insignificant (p=0.77) but when I run post-hoc Scheffe’s test the difference between means is (highly! p&lt;.00001) significantly. How is it possible that these two results contradict each other? The other puzzling result is for the interaction between repeated measures and between subject factor. Since the Scheffe’s test is coherent with F-test, I wouldn’t expect any significant post hoc comparison. In fact, I have 2. Again, how this is possible? Many thanks! 
 Say I here are two operations that start simultaneously Operation x has a mean of 5 days and a standard deviation of 3 days Operation y has a mean of 6 days and a standard deviation of 2 days I am looking for a way to figure out how long does it take for both operations to finish? Thanks 
 I'm trying to assess the performance of an offer on a website. I divided recent buyers into two groups and gave one group a discount. Here are the revenues before discount &amp; for 8 weeks after discount: I'm thinking about conducting a t-test to assess statistical significance of the offer on the test group. Should I? In order to be able to compare revenue levels, I divided the revenues in the test group by 9. Does this make any sense? Would I need other data to assess statistical significance of this offer? 
 I have an issue regarding the ANOVA analysis. Let me explain what I am looking for…You have 3 factors (A,B,C). Now in CCD or BBD you get ANOVA for Term Seq.SS Adj. SS Adj.MS F and P ————————————————– A B C AB AC BC A^2 B^2 C^2 Here, I got the solution to calculate Seq. SS of Factor A,B,C. Cross Interaction of Factors – AB,AC and BC. But unable to get the value of Quadratic Terms i.e. A^2 or B^2 or C^2. Same way, I could generate the exact value in Adj. SS for Factor A,B,C. Cross Interaction of Factors (AB,AC and BC). But unable to generate the exact value of Quadratic Terms i.e. (A^2 = AA, B^2 and C^2). So how can i calculate these values ? 
 I performed a lasso logistic regression on two modles. One model contains only the control variables. The other model contains controls+linguistic measures. When I search for the optimum lambda using cross validation, I get the following results in terms of mean deviance error. Controls only: 1.215301 | 4 degrees of freedom (df) Full model: 1.198776 | 5 df Could you help me interpret this result? My full model shows a lower mean error, but the difference seems not too impressive. Moreover, my full model has five variables in the model that are non-zero, which is one more than in the controls only model. Is there any advice on how to compare those results? I read that if two models show similar performance (cv errors), one should stick to the sparser model. Earlier I posted a pretty similar question, but still am not too confident (link: Interpret and compare lasso models ). At least now I improved my model to be at least as good as the controls only model. 
 The earth mover distance could do the trick. Informally, it considers distributions as a piles of dirt. The dirt can be moved, with an associated cost given by the amount of dirt moved times the distance it's moved. The distance from distribution A to distribution B is the minimum possible cost of moving dirt to transform A into B. 
 I want to place a Multiple Regression model into a production system and use the Prediction Interval as a threshold for anomalies. I've seen how I can calculate the Prediction Interval two ways: $$ \hat{y} \pm 1.96 \hat{\sigma} \sqrt{1 + \mathbf{X}^* (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}^*)'}. $$ Referenced here and $$ \hat{y}_h \pm t_{(\alpha/2, n-p)} \times \sqrt{MSE + [\textrm{se}(\hat{y}_{h})]^2} $$ Referenced here With the first it seems like the prediction interval value changes based on new observations ($\mathbf{X}^*$) and the second appears to be a fixed prediction interval based off of one initial calculation after creating the regression model. I would personally like to use a fixed prediction interval for the case I'm considering but I'm not certain if I'm thinking about this all wrong. Are these two prediction interval calculations different from each other? 
 I have built a convolutional neural network in TensorFlow to predict an object's location. It is a multi-color object, hence it looks different depending on it's orientation, camera angle and so on. The CNN is able to detect the object in clean images that are (very) similar to the training images: little clutter, few colors in the background...thus mostly homogeneous backgrounds (like single color walls, floors, heaters, ...). However, every time when one of these clean backgrounds change, like another floor color for example, the results get messed up. On strongly varying backgrounds (like multi-color walls, people in the background, ...) the CNN misses to locate the trained object very often. To lower misclassifications and make the CNN more robust against varying backgrounds, I thought it may be a good idea to train inhibitory samples. Therefore I have recorded a dataset of images where the training object is not present at all. The dataset's size (number of samples) is comparable to the "positive feedback dataset" which covers the trained object. My idea was to alternately train one image with the training object (+ its labels) and one image of the inhibitory dataset, where the labels are all zero . Unfortunately this procedure was not very successful up to now: most attempts showed bad or even unusable results. I am softmaxing the network's output and use cross-entropy to calculate the training error (thus training the network fully supervised). My question is : how could I train inhibitory feedback in order to make the network more robust to cluttered backgrounds and "force" a zero-like output for inputs that do not show the trained object? Is it a bad idea to 50:50 train images out of a positive and then a negative dataset? Could I alter my approach to get better results? 
 Using the glmer() function in the LME4-library in R I computed logistic models, of the form: Y ~ cat1 * cont1 + (1|Subject) where, obviously, Y is the binomial outcome variable (0 or 1), cat1 is a categorial variable (0,1,2) and cont1 is a continuous variable). Then, using confint(model, method = "boot") I computed confidence interval on the variables. Now I would like to plot a graph of the chance P(Y==1), I want to plot P against cont1 for every cat1. So you'd say: X = B(0) + B(cont1) * cont1 + B(cat1:1) * (cat1==1) + .... + etc And then: P(Y==1) = 1/(1-exp(-X)) Which does exactly what I expect. But now, I want to incorporate the bootstrapped confidence intervals (so not std. error * 1.96!!) in the graph. I have the numbers, I do not know how to interpret them, what would be the formula for e.g. the 97.5 % line and the 2.5 % line? Thanks in advance! 
 The unpredictability in your problem is very much related to the entropy rate generated by the state transitions. I suggest you have a look at the topic "computational mechanics", which addresses information-theoretic questions in stochastic processes (see e.g. ) Think of it in the following way: a random sequence of length $L$ has entropy $H(L)$, thus the entropy rate is $h=\lim_{L\rightarrow \infty} H(L)/L$. In this case, your walker generates $h$ bits of information per step. If you want to be a bit more fine-grained, compute also the excess entropy, which is $\lim_{L\rightarrow \infty} H(L)-h L$ which captures the amount of information you need to acquire in order to synchronize with the process. Your example of a predictable walker gives $h=0$ and $E=1$, because as soon as you have one bit (home or work) you are fully synchronized with him, and he becomes fully predictable. Sorry that I don't write the code. I'm still learning... 
 I have several comments. I'm not sure whether they qualify for an answer but they might help you get a few steps further. For exploratory displays you can simply use the hurdle idea and generate plots of (a) the binarized response (=0 vs. &gt;0) against the regressors and (b) the truncated subset positive (log-)count response against the regressors. For your and the regressor this yields: These show that for the binary zero hurdle a quadratic would probably fit much better. And for the positive counts no particular relationship pops out. In short, the dolphins have been sighted at medium depths but given that there were sightings the depth does not seem to lead to more or less frequent sightings. (Of course, this is just a marginal plot but the formal models seem to support this.) You can also try to add fitted regression lines into these plots, e.g., by fixing all regressors at their means (or other typical values) and then varying only the current regressor of interest. But I'm not sure whether this is what you're after. As for the two data sets and two models, I'm still not convinced how you got to your solution. For you the and models seem more or less equivalent with the latter being slightly better. Note also that there is quasi-separation in the data with respect to the variable - hence the large coefficient and the even larger standard error. (For the 14 observations with there are no observations with .) Finally, the two parts of the models may have different regressors. It seems worth exploring this because in your current version many variables are still non-significant. Thus, something like: yields an even better AIC. I didn't check a cross-validation, AUC, etc., though. 
 I'm running a predictive model using the logistic model in SAS and, currently, I'm trying to perform some diagnostics about the collinearity issue in the estimated model. To do that, I followed step-by-step what the SAS support suggested (look at here ). Shortly, I run the following code to compute the diagonal weight matrix : in order to use that in a linear regression model with weights $w$ and to be be able detecting the presence of collinearity in the explanatory variables, as follows: The output is around 5 in the last condition index value for the variable. According to what SAS suggests in the Support Blog, where one detects collinearity between the intercept and an explanatory variable because of an index condition equal to 315, the model should be not affected by collinearity. Can you confirm that? Does it exist some cutoff points in such kind of analysis? 
 I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this: fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....) There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors? 
 This is an old problem without an efficient solution. As you know, confidence intervals and prediction intervals are very different things. Bootstrapped variance estimates for parameters will not give you robust prediction intervals. A theoretically correct approach would require you to iteratively bootstrap the data by hand, fit mixed models and obtain predictions, then you must calculate bias, and using a theoretically correct approach to obtain bootstrapped prediction interval estimates. This would probably take days to do. Furthermore, the question of whom exactly you're predicting effects in needs to be addressed. Predictions can be obtained at any heirarchical level of correlation. Mixed models capture individual level effects. So you might be giving predictions for a particular individual (in which case the random effect is a fixed, non-zero value), or it could be marginalized to a population averaged result. If you consult the default method, you'll see the option which requires you to tell R among exactly whom you're prediction. Also this is an illuminating read: https://cran.r-project.org/web/packages/merTools/README.html 
 I'm currently studying from Hinton's neural network course and he just introduced the cost function used with the softmax output function: \begin{align} C &amp;= -\sum_{j}t_j\log y_j \\[5pt] y_j &amp;= \frac{e^{z_j}}{\sum_{k \in group}e^{z_k}} \end{align} In the slides, he says that "C has a very big gradient when the target value is 1 and the output is almost zero". This makes sense as we will want to minimize $C$ since the target and output are not the same. However, what if the target value is 0? Here, wouldn't $C$ always be zero so it doesn't matter what weights you have in $y_j$? 
 Imagine a web site, let's call it NullException, which has 1,000,000 registered users, and averages 50,000 unique visitors per day. I am trying to model the visitation, so that for tomorrow, the model would simulate whether each user (from 1-1,000,000) will visit or not. If visitation was totally random and uniform, I could just pick a random number $0&lt;x&lt;1$ for each user, and any $x &lt; 0.05$ would be a visit. But of course visitation is not random. A huge group of users (at least half) haven't visited in a while and their likelihood of visiting tomorrow is below 1%. Meanwhile others are highly frequent and their likelihood of visiting tomorrow are &gt; 0.8. In fact, in a given year, most users will visit 3 or fewer days, while a small group, perhaps 50-100 will visit 200+, even 300+ days. Everyone else is somewhere in the middle... once a month, twice a week, etc. My question is, how to build a probability density function that represents this pattern? I tried different takes on gamma, beta, and chi-squared functions, but they are either too "generous" (i.e. not enough very infrequent guests) or too "restrictive" (virtually zero highly-frequent guests). It seems the handful of very-high-frequency guests are statistically non-existent, as every function I've tried essentially zeroes out these users (i.e. calculates fewer than 1 in a million users) (Note: it is important to model this on a user-by-user basis... not to shortcut it and just do an estimate of 50,000 users +/- some $\sigma$.) 
 I'm designing an ecological experiment to test how 2 treatments (one manipulated, and one non-manipulated control) effect fish density at at their respective locations. The experiment will be done in a series of small, isolated ponds using fish traps at each treatment location. Half the ponds will have 2 traps in unmanipulated areas, and the other half will have 1 trap in a manipulated area and one in an unmanipulated area. I'd like to do a block design to remove the effects of variability among ponds (size, depth, initial fish density, etc.). The presence of the manipulated treatment will affect the fish density at the non-manipulated treatment. I'd like to determine if there is a statistical difference between treatment groups both within and among ponds. Is it possible to use a block design with the stated interference? Here's a visual representation, 1 trap will be located at each star: 
 On this site it has been confirmed multiple times that, contrary to what is often heard, hypothesis tests don't have any issues with large sample sizes. As a matter of fact, the probability of Type 1 errors when the null hypothesis is true, doesn't depend on the sample size (see for example here ). However, people are often taught that to perform some inference procedures (ANOVA, inference for linear regression, etc.), they need to first check the validity of the underlying assumptions (for example, errors are normally distributed, etc.) using an hypothesis tests (for example, a normality test on the residuals of the linear regression). The unlucky disciple tests for normality on a 10^7 sample, finds that the test rejects the null, and she/he falls into despair. I think this is the point which generates confusion. How would you assess the validity of the assumptions behind the inference procedure, without an hypothesis test? If this question is too general, let's just consider the two cases I cited (ANOVA and inference for the coefficients of a linear regression model). I've been advised to make Q-Q plots. They are great, but in some cases their interpretation can be a bit subjective...I'd rather look for a tool that let me estimate by how much to inflate the C.I.s for the $\hat{\beta}_i$ if residuals "don't look normal"...bootstrap, maybe? Also, I have another doubt: if at large sample sizes we say that an hypothesis test is not the right tool to check the validity of the assumptions underlying a certain inference procedure, but we also say that sample size doesn't affect the reliability of NHST, then this would mean that hypothesis tests are never (no matter what the size of the sample be) appropriate tools to verify inference assumptions...is that correct? 
 You could get fancier, but here's a simple, standard way to do it. Say you have a set of spike times for each trial and you want to calculate a binned PSTH. All times are measured relative to the stimulus at time 0. Set up a number of equally-sized, non-overlapping time bins covering the interval from the minimum to the maximum time point in any trial. Let $t_i$ be the center time of bin $i$ and $\Delta t$ be the bin width (both measured in seconds). Let $s_i$ be the total number of spikes that fall into bin $i$ (summing over all trials). Let $o_i$ be the number of trials that are long enough to include $t_i$. The average number of spikes in bin $i$ (averaged across trials) is then $s_i / o_i$. Dividing by $o_i$ instead of the number of trials lets you deal with trials of different lengths. The firing rate in bin $i$ is $f_i = s_i / o_i / \Delta t$ (spikes per second = Hz). 
 Align spike times to the stimulus to the stimulus. Define a time window relative to the stimulus. Select a subset of trials where that time window is well defined, e.g. it doesn't include end-of-trial, stimulus2, trial wasn't aborted, etc. In each of the trials selected in 3, count how many spikes are inside the time bin and divide by the width of the bin. This is a single trial firing rate. Compute the average of single trial firing rates. Another method is to convolve your spike train with a Gaussian kernel. This is how most "smooth" PSTHs are computed. The spike train can either come from a single trial - giving you a single trial firing rate - or you can combine spike times across trials to compute an average firing rate. Of course you'll still need to 1. align and 2. remove trials with confounding events in the window of interest. 
 It is often stated that the null hypothesis should be written as an equality (see for example here ), like for example $\mu_A-\mu_B=0$, while the alternative uses an inequality (for example, $\mu_B\gt\mu_A$). I think that this kind of null hypothesis is called "simple". My question is, why should we always prefer simple null hypotheses? Suppose for example that I have an existing machine learning code A, and I develop a new code B, based on a different paradigm (maybe A is a neural network, while B is a random forest). I test A and B on some data. I can define an accuracy metric for the codes, thus for each test I can say whether A or B was more accurate. It seems extremely unlikely to expect that A and B could have the same average accuracy on the population from which the data are drawn. After all, they're completely different algorithms. Thus, a NH such as $H_0:p_A=p_B$ seems quite "unnatural" to me. Rather, since I want to know if new code is more accurate than the old one, I would try to falsify the NH that B is no more accurate than A, i.e., $H_0: p_B\le p_A$. Thus, the alternative would be $H_a:p_B\gt p_A$. Why is this kind of null not appropriate for testing? I guess the issue is that with the "compound" null (is this the correct term?) I cannot easily find a distribution of a test statistics, and thus I may not be able to perform NHST. For example, in the two sample t-test, the null that the mean of the two samples is the same allows me to derive easily the distribution of the test statistics (mean is zero, and standard deviation is derived from sample standard deviation and number of samples). If instead I assume that one mean is larger than the other, then I don't know the parameters of the test statistics distribution. Is this correct? 
 The bias-variance decomposition of MSE is $MSE = (\mathbb{E}(\hat{\theta})-\theta)^2 + Var[\hat{\theta}]$. $\mathbb{E}(\hat{\theta})$ can be explained as the average from the randomness in the training data. In a real-world scenario, we are trying to build models based on a particular dataset, in this sense, what does it mean by "the randomness in the training data"? Is it relevant to sampling from the data? Edit : Suppose given a dataset D, it is easy to get an estimate of the parameter $\hat\theta$. However, $\mathbb{E}(\hat{\theta})$ implies there are multiple estimations of $\hat\theta$. My question is, why there are multiple estimations for the same dataset? 
 There is nothing wrong with your proposed test. It is possible to derive the sampling distribution of the null with a compound null. What we do, in essence, is use the sampling distribution of the simple null, and if the truth were that $p_B\ll p_A$, then the rate of type I errors would be less than your stated alpha. What you are getting at is called a non-inferiority test. Here is a non-gated and practitioner-friendly paper: Walker, E., &amp; Nowacki, A. S. (2011). Understanding Equivalence and Noninferiority Testing . Journal of General Internal Medicine, 26(2), 192–196. It may also help to read some of the existing threads on CV that are related to this topic. I have provided somewhat related answers: Why do statisticians say a non-significant result means “you can't reject the null” as opposed to accepting the null hypothesis? Can a paired t-test test if the difference between two means is less than a specific value? You can also read the threads categorized under the equivalence tag. On an unrelated note, you should not use the t-test to compare the accuracy of two classifiers. Since these are correct or incorrect, you would use some method appropriate for binary data, such as the z-test for two proportions. However, since the classifiers will almost certainly be assessed using the same data, McNemar's test should be used (see my answer here: Compare classification performance of two heuristics ). 
 I'm trying to forecast the future distribution of a particular interest rate based on its quarterly percentage changes. My assumptions are that: The observations are independent The distribution holds across time (stationarity of the quarterly percentage changes) When I run Shapiro / K-S tests of normality on my historical data, I find very strong evidence in favor of rejecting the null hypothesis that both types of change my data could have been generated from a normal distribution, so I want to forecast based on the empirical distribution. My questions are: Is there any way to determine whether or not using the empirical distribution gives a better estimate than using a normal distribution? I'm using $\textsf{R}$'s command to generate potential paths for MC simulation -- is this the "right" way to sample from the empirical distribution? Are there issues I'm failing to consider properly since the empirical distribution is discrete? Many thanks. 
 I am trying to understand how can I use tensor to predict events. Let's suppose that I have users Ui * and I have predictors Pj over time. What are the tools and concepts that I need to know in order to understand using tensors to infer and predict values based on given information over time. Any paper/books that you think would help understand the underlaying concepts would be welcomed. 
 For a regression between two variables, as the number of data we have goes to infinity, will the p-value eventually settle on 0 or 1? 
 Meanwhile, I learned that the log used in glmnet is the natural logarithm. It's simple as that and pretty basic. I was a little confused at the time I asked this question, because I know the natural logarithm as ln and not log. Should have tried out a little more before asking. Anyway, maybe this pretty basic question might help someone else. 
 Just make sure you resample from your data with replacement, and you give every data point the same chance of being chosen. Then you should be good. Here's a quick R example: You probably want to wrap that in a function and generate a bunch of them. Here's the math behind the idea. As long as your historical data set is large enough, you should be drawing almost exactly from the "true" distribution of percentage changes. Denote a random draw as $X^*$. If you're assuming all of these percentage changes are iid then $$ P(X^* \le x) = \frac{1}{n}\sum_{i=1}^n 1(X_i \le x) \to P(X_1 \le x)$$ as $n \to \infty$ by the law of large numbers. I wouldn't worry about the discreteness. Rather, I would worry about the fact that you're not treating the time series data as a time series. All of the work above assumes that one time's change is independent from the others. I am not a domain expert here, so I can't say how much trouble you're going to run into. 
 I am trying to find the MLE of the following functions but I'm getting stuck. I know the method and steps to follow but Pi notation is confusing for me. 1) f(x) = øx^(ø-1), 0 &lt; x &lt; 1 and 0 &lt; ∞ . Let X1, X2, ... Xn be a random sample. What is the MLE of ø? So I can get it into pi notation as multiple from i=1 to n, øxi^(ø-1). I figured I can pull out a ø^n, but I'm stuck after that. Is there some type of trick here? 2) f(x) = (1/ø)x^((1-ø)/ø), 0 &lt; x &lt; 1 and 0 &lt; ∞ . Let X1, X2, ... Xn be a random sample. What is the MLE of ø? I'm not even sure how to approach this one. How can I get this into a sensible form? 
 I was reading MatConvNet's tutorial for (convolutional) deep learning and it said: "...the receptive field size for the layer. This is the size (in pixels) of the local image region that affects a particular element in a feature map." which makes sense with the traditional definition of a receptive field. Its usually thought as the number of pixels that affect a particular node in the feature map. However, when I went and do the exercise they have the following table: where we can see that the convolution layer (layer 3) has a rf size (receptive field) of size 5. I was wondering, how did they get that number for the receptive field? I thought that the receptive field just referred to the size of the image size of the input to compute a feature map, i.e. the same size as the filter size of that convolution layer (Thought, I am aware the concept can extend to lower layers as explained on chapter 9 of Begnio, Goodfellow, Courville BGC deep learning book). Regardless, even aware of the extension definition, I am still unsure how to the number 5 was obtained on layer 3. Any ideas? 
 The equation you wrote for $C$ looks like the cross entropy loss for a single data point in a multiclass classification problem. In that case, $y_j$ would be the classifier's estimated probability that the class is $j$ given the input. $t_j$ would be an 'indicator variable' that takes the value $1$ if the true class of the data point is $j$ and $0$ otherwise. $t_j$ must be $1$ for exactly one value of $j$ (because the data point has a single, definite class). Since the sum is taken over all $j$ (i.e. all classes), there would be no problems with everything being zero. 
 There is AICc , which is AIC with a correction for sample size: where ''n'' denotes the sample size and ''k'' denotes the number of parameters. I think AICc can be used to compare models based on different sample sizes. 
 What kind of confidence interval does return? Is it normal approximation? I went on GitHub, but could not look it up in the code. How can I look it up in the code? 
 I have time series rate data for 7 states and have been asked to weight this, based on population size. For 5/7 states, the number of participants has been recruited based on population size, so you would think that no weighting would have to occur here. However, the other 2 states are vastly over-represented and reports as follows: - One reports year round - The other reports for a proportion of the year, and minimal data (under-represented data) is collected for the remainder of the year. What is the best way to weight this data? 
 I have a very large data set (18 million observations) that I would like to transpose by subsetting based on one variable and creating 900 new variables out of those sub/ets. Example code and desired output format below: Example data: Code: Desired output format: Because of the size of my data set, I'm wondering if I have use a do loop to find first and last observations of each var and then iteratively subset. Does anyone know a sql or proc to achieve the desired output format? 
 Let $X_0, X_1, ... , X_n$ be independent random variables, each with mean $\mu$ and variance $\sigma^2$. Let $Y_i = X_i − X_{i−1}, i = 1, 2, ... , n.$ What is the expected value of $\bar{Y}$ What is the variance of $\bar{Y}$ EDIT This is just a practice problem to help me study for an upcoming test. I don't understand the question really. So basically each value of Y in the sample is derived from consecutive values of X right? How does this help me figure out the mean or the variance? 
 These aren't separate models for the linear and quadratic term for ; the results you show are for one single model containing both terms. What the results are suggesting is that you need to have the quadratic term rather than just represent/approximate as a linear function only. 
 I analysed ICC (2, 1) for interrater reliability between 2 testers on the same subjects, my results confused me: The SPSS output ANOVA turned out to be significant, but I got high value. I thought you always get low ICC values if ANOVA is significant? 
 I am finishing my undergraduate thesis and I was asked this problem of negative constant in my examination. So I am trying to find out how to explain what to say in regard to the negative constant. In my model, I have 15 predictors and one response. And in binary logistic, the response is either 1 or 0. So the examiner insisted that the response should only be either 1 or 0, not negative. So why is the constant negative if all the predictors excluded? What should I say here? What is the problem? Much appreciated on the help. 
 If you transform the predicted values of to the original scale with the inverse of , , then you can compare the RMSE normally, afterall you are interested in predicting values in one scale scpecifically, be it the original or the transformed one. 
 Resampling isn't the same as tuning. It gives you an estimated performance metric. When you tune over a hyperparameter subspace you actually perform your resampling strategy over each evaluation point of that subspace. So, in , when you resample a linear regression without tuning you get an estimate of the performance metric of your choice, and the regression coefficients are estimated over the training data of each resampling instance. 
 My colleagues at work use an interesting metric to compare two predictive models that I have never seen before. It can be used for both classification and regression. But we consider classification at the moment. The algorithm is the following. Given vectors of $preds_1$(probabilities that event=1 from first model), $preds_2$(for second model) and $true$(true values of either 1 or 0): Calculate $Ratio = \frac{preds_1} {preds_2}$. I guess this could also be repaced with difference between predictions. Given distribution of $Ratio$, create $N$ bins of equal size. Say if $N=10$, we create 9 cut points $Ratio_1$&lt;$Ratio_2$&lt; ...&lt; $Ratio_9$, such that number of events in each bin between any of the cut values is $N/10$. In each bin, calculate $Actual$ / $Expected$ for both models, where $Actual$ is sum of true values for that bin, $Expected$ is the sum of either $preds_1$ or $preds_2$. And then, just look at this A/E plot as a function of bin number. The model that is flatter and closer to 1.0 across all bins, is a better model. It kind of makes sense. We do see some models being significantly flatter even though metrics like AUCs are very close. I have not seen anything like this in literature. Does someone use this metric? Can there be any potential pitfalls? 
 It's not a confidence interval. The object is a subclass of the class (i.e., its "parent" class). The parent class defines the method here: Specifically, this method returns an interval that gives equal probability on either side of the median . Unfortunately, they do erroneously call this a confidence interval, even though its just a plain old interval of a probability distribution. For example, a 90% interval would give the values that are at the 5th and 95th percentiles, since both of these are 45% above or below the median. You'll notice that both this description on the one for the poisson both say that it is an interval that captures a percent of the distribution. Hence, this is really not a confidence interval, but a probability interval (it contains percent of the total probability). 
 Here's a general hint for MLEs: we almost never directly optimize the product (i.e., the actual sample likelihood) $L$: $$L(\theta;x)=\prod f(\theta;x_i)$$ Instead, we take the natural log of the likelihood (called the log-likelihood) (sometimes shown as $\mathcal{L}$, and then proceed to try to maximize that: $$\ln L(\theta;x) := \mathcal{L}(\theta;x) = \sum \ln(f(\theta;x_i))$$ 
 A few hints to get you in the right direction: Expectation is a linear operation: $E[X+Y]=E[X]+E[Y]$ Try writing out the numerator of $\bar{Y}$...notice anything interesting happening? For independent variables, the variance operation is also linear. 
 I think you are misunderstanding the methodology. You trained your algorithm on aggregated data from the whole match. That is, your algorithm learned how to predict the end result of a match given all the statistics of the match, or in other words, a posteriori . It wasn't trained to predict the end result of a match before that match actually happened. So, no, you can't pass an empty data.frame to it, that wouldn't make sense. What makes sense is to train an algorithm on data know a priori to predict the end result (you'll probably come to the realization there's not much predictive power in that), or alternatively trying to predict results given past events, like a time series. This problem can also be coerced to a survival analysis. 
 As pointed out by the other poster, you cannot treat time series data as a simple random sample due to the correlations between adjacent observations in time. A nice nonparametric approach to generating sample paths is the block boostrap and here Note that the first link also points you to the handy package in R. 
 hi, the above question is one of the past exam question of the course which i am studying. The question i want to ask is: how can i answer part a)? I calculated the U=27 and mean = 15{(5*6)/2} , so i know that i can use normal approximation to do it. While the question need me to use the exact distribution to handle the problem. I know the no. of possible outcome is 11C6=462. If n is small , i can simply list out all the possibilities . but now i have 462 cases, it is obviously not to do in this way. So what should i do? 
 Typically in mediation analyses we see diagrams like the following, in which is the causal variable , is the outcome variable , and is the mediating variable . Meanwhile path is called the total effect , path is called the direct effect , and path is called the indirect effect . The equation is often mentioned. When does it exactly hold? 
 David Kenny writes on his website that: The equation of exactly holds when a) multiple regression (or structural equation modeling without latent variables) is used, b) the same cases are used in all the analyses, c) and the same covariates are in all the equations. However, the two are only approximately equal for multilevel models, logistic analysis and structural equation modeling with latent variables. I'll welcome, and will accept, an answer that improves on this explanation, e.g. by discussing why these conditions are necessary for to exactly equal , and why they're only approximately equal for multilevel models, logistic analysis and structural equation modeling with latent variables. 
 For a problem that I have, I have to create a function in R that chooses the tuning parameters by K-fold cross validation. I have no idea how to even start this, so any assistance would be much appreciated. 
 I would like some insight and some typical "rules of thumb" I can use to decide when to transform continuous predictors into factors and vice versa for classification and regression trees. Should I just experiment with different combinations and use the test error rate as a judgment criterion? If so, wouldn't this get impractical as the number of predictors increases? 
 I have survey responses that are between 1 and 10. The questions are related to a category. For example, out of 20 questions, 5 are related to category A, 6 are for category B, and 9 are for category C. How can I analyze this data to determine which category is the most important driver of the overall quality score. Regression seems like a good initial run but I would think that the categories would be lost in the analysis. FWIW, I am using R for this analysis. 
 I'm trying to reproduce a generalized linear model in R. The original SPSS model has distribution Gamma, link Log, with a continuous dependent variable and all independent variables categorical. In R this should be: But the coefficient estimates here don't match the ones from the SPSS output (B values) and the significant variables aren't all the same either. The SPSS model also gives tests of model effects for the original categorical variables (rather than just the dummy coded levels) using the Wald Chi-squared test. How do I obtain these in R? How are 'deviance', 'scaled deviance' and so on in SPSS comparable to R's null deviance and residual deviance? Appreciate any help at all. 
 Check out the following paper which directly addresses your query and ways to potentially adjust for the issues that the sparsity is likely to cause (i.e., very large ORs etc.). 
 Can you tell me please, which package must I use to test the hypothesis of unconditional coverage for VaR with the Kupiec’s test in R ? Is it rugarch? And how to do this test? (for ARMA-GARCH model) 
 The tree-classification algorithm can handle both discrete and continuous. There is no such thing "rule of thumb" when to transform and when not to transform, because it all depends on how you view your data. If you believe your data should be categorical (for example, income-groups), you should use a factor variable. However, if precision is required (eg: income per month), then you'll need to do it continuously. Ask yourself, if you were a user, which way to view the data would make more sense? For example, when you complete tax-returns, you'd have to give your exact income. However, you'd only need to tell your friend a rough-estimate (eg: 2000-3000 per month). 
 I and a group of fellows are currently self-teaching some statistical methods using online resources. We mainly use R to tinker with algorithms such as K-means clustering and linear regression. Personally, I come from a computer science background, thus I manly plot and compute over data originating from runs of computer software. I found some interesting but small datasets about humans i.e. a dataset that provides samples on body mass index, obesity, blood pressure, sex and age. These data I found in particular more interesting since the use of linear regression makes immediate sense and can be interpreted even by a beginner like my. Does anyone know about more general fitness data? Such as samples with sex, age, weight, minutes of activity per week, etc. Either I am really bad on googling lately or these data are somewhere deeply hidden on the web. Nevertheless, as a computer scientist I thought it would be easy to just collect the data myself. Here is an attempt to do exactly that: https://steinbauer.org/2016/06/how-fit-are-you/ 
 I need some advice for choosing the correct model. I have 3 binary variables: Risk (0 = the person doesn't take financial risks, 1 = the person takes financial risks), gender (0 = female, 1 = male), and type of person (type 0 vs. type 1). The types are well defined, but for me, it's a binary variable for comparison. The questions of interest are: 1. Do type 0 males take more risks than females (of any type)? 2. Does type 0 males takes more risks than type 1 males? 3. Same like question 2, just for females. Which model should I use? I thought maybe logistic regression, with interaction and perhaps contrasts, but I am not sure. Could you please advice me on which model is most appropriate? I will be using SPSS, so if contrasts are required, I would appreciate a tip on how to do it there. Thank you in advance ! 
 I have a problem regarding pooling data from replicate experiments. I have done an experiment where I collected multiple tissue samples from 5 mice, then the samples were allocated to treatments and analysed after 5 days. I repeated this experiment 4 times. The data was pooled and analysed by General Linear Model Univariate with the treatment as a factor and replicate as a random factor. Now my paper has gone out for review, and one reviewer has commented: "Pooling, while valid, with replicates as you have done, is valid if the replicates were not different from each other - where do you address differences between replicates in your data?" My questions are: Is the reviewer correct? Is there anything wrong with my analysis? Should I report the effect of the replicate on the model (eg P value), or the interaction with between the replicate (random factor) and the treatment (factor)? There is a similar previous question here, but the answer didn't address the questions I have: Pooling data from replicate experiments Also this question, but I didn't understand the answer! independent replicate experiments yielding contrasting results 
 I am trying to understand the output of the bs() function from the splines library in R For a single vector of numbers, I get back several vectors of values, one for each knot. That makes sense. However, I'd like to understand how to combine the basis functions to plot the smooth curve representing the original data. I have a vector of values: x [1] 10.89 15.63 16.02 16.59 17.80 22.15 25.38 26.03 27.05 25.50 29.00 27.36 26.49 25.68 50.25 50.29 30.20 26.78 23.69 27.50 [21] 26.01 22.73 21.02 17.65 Call the bs function in the splines package: The output gives a nice matrix of values, with 6 columns, as expected, but the range of values is nowhere near my input. How to I translate this back to the smooth curve? Thanks! 
 I have general question Is it common to predict one variable and to get the realtionship, for example the relation between (X)cost and (Y)leads, Then to take the predicted leads and put it on differnet equation of Leads(X) to (Y)new customer? Based on cost to predict using the number of leads the new customer? Thanks 
 Firstly, you will need to normalize the data to average spending per person (also stated in your question). You can consider to use two samples t-test (or Welch t-test) for comparing between your control and treatment group. To do that, you should normalize your data. Consider this simplified example: You transform it to net-effect relative to the beginning of your experiment. Eg: If you now run t-test on your groups, you will be testing the net-effect of discount vs non-discount relative to the beginning of the experiment. Under the null hypothesis, your discount is worthless thus the net-effect for your second group should be close to your first group. If you can reject your null hypothesis, you can prove statistically that your discount has some non-random effects. 
 Let say we want to do regression for simple using standart deep neural network. I remember that there are reseraches that tells that NN with one hiden layer can apoximate any function, but I have tried and without normalization NN was unable approximate even this simple multiplication. Only log-normalization of data helped But that looks like a cheat. Can NN do this without log-normalization? The unswer is obviously(as for me) - yes, so the question is more what should be type/configuration/layout of such NN? 
 Am relatively new to data linkage in general and the R RecordLinkage package in particular. I have data like below: as you can see I have two data frames, (11 rows) and (5 rows). I have inserted a row in both which should in theory definitely be a link, the user James Earl Jones. However I have 2 concerns. The line results in output where the last column always shows as NA, even though I am sure of at least one row being identical in both datasets. What does this mean? This is related to another SO question which is yet to be answered. The lines give a result as following: (a) Why does it show 0 matches and 0 non-matches, when there is definitely at least on match (James Earl Jones) (b) Is the argument in the function optional? and if so, what happens when you leave it out vs putting it in? (c) Can one use this package even in the absence of a "Gold Standard" to perform record linkage, and not record linkage evaluation? Kind regards, Tumaini 
 I have an iteration of my algorithm and it gives me a column vector as a result (1899x1 double). I want to form a new matrix composed of these column vectors together sequentially (for example, the 2nd column vector from the iteration will be the 2nd column in the newly formed matrix), like this example: After merging it should be and saved it in *.mat file. Can anybody help? Since I have 422 files, the vector size should be 422x1899double.I will be thankful if anyone suggest me how to write a loop which can do so. Thanks before. 
 In the online book of Nielsen the cross entropy cost function is given as below: $$ C = -\frac{1}{n} \sum_x [y \ln a+(1−y)\ln(1−a)] $$ When $a$ is equal to 1 the last ln becomes $ln(0)$. And that is undefined (Well, if you take the limit from right side, it converges to negative infinity). Then the cost will be undefined or infinity. Isn't this a problem? How is this dealt with? Do we just assume $a$ will never be exactly 1? 
 I have a question related to data comparing. First of all, my dataset is composed by cities of the world. In this cities, we have a maximum of 24 tags that indicate what these cities are best for. These tags have a descrete weight made by users. For example: The tag number of 24 is not fixed. It is the maximum number but maybe a city have just 1 tag. The value parameter is the weight that has been set by the users (they increment by 1) The total_tag_weight parameter is the total weight of the tags of the city. The norm_value is the value of the tag normalized across all cities made with The zscore_interdest parameter is a standard z-score made with It is not shown in the JSON but I also have the total weight of the same tag across all the cities, as well as its max value, min value, mean and standard deviation. All the score parameters that I have made just indicates me that the big cities, with a lot of "tags +1" by people are best in everything, but I don't think this is very real. So maybe I need some normalization/standardization. My question is, is it the z-score a good measure to compare across all cities with a given tag? For example: Which is the best city for "seeing museums"? If not, Which could be the best solution to compare a given tag between all cities? When this problem is solved, I mean, I have a score and I am able to know which is the best city for "seeing museums", which is also the best solution to know how much a city in percentage is for a given tag given that the tag number is not fixed? For example: How much of "seeing museums" is a given city? Thank you very much to everybody. 
 I have 4 variable VAR model and I need to test causal relations (probably by Granger causality) and dynamic interactions. How do I test for the dynamic interactions? My knowledge is very limited 
 I've been using to fit the residual component of some data. I've written an algorithm to automatically select the ARIMA model. Results are not quite as good as I had hoped, so I am looking for suggestions on how could I improve things. Please find below a description of what I've tried thus far. I am dividing the data into a training and a forecasting/testing set. Up to this point, I've based my choice of the ARIMA model only on the training set. I allow the ARIMA parameters p and q to run from 0 to 7. This choice of range is arbitrary. The d parameter is allowed to be either 0 or 1. This function tries each of them and storages the results: Next, I look for the model that miminises RSS (total squared residual) using: I had initially tried to use as much data as possible for training. To my surprise, despite using up to x6 more data, the fitting quality worsens. I made a scan of the training set length, plotting for each the total squared residual and the total squared residual per unit of training set. This is shown in 1 . Here, clearly, the fit quickly deteriorates the more data I use, up to some point, where the ARIMA-fit-residual stabilizes. Also here, the best ARIMA model is consistently the highest-order available MA model. 0-0-7. Summarising, my optimisation strategy yields an ARIMA fit that: (Fixed) Consistently (stubbornly) selects the MA model with of the highest available order Does not improve with the size of the dataset Do any of you know whether this is somehow expected? Has any of you tried something different? Update The rejection of all the AR models was due to an error on the first function. These models return with less entries than the original series and, hence, the RSS returns nan. These new function does the trick: Plus this extra one: The results are much better, albeit the overfitting. Here 2 is the updated plot showing the residuals and model choices. 
 I have collected data from 50 banks in order to analyse the effectiveness of regulators policies. the data is of ordinal nature with liker scale response. I would like to use my data in spss for analysing it but now i am confused with the test to be used to solve it and calculate the effectiveness rate. please suggest me the statistical solution 
 I'm familiar with these terms 1 - MRE Mean Relative Error 2 - MMRE Mean Magnitude of Relative Error I need to compute MdMRE which is Median Mean Relative Error. I searched on the net but didn't find its formula. Any help related to this formula will be appreciated. 
 If my interpretation of "until we get $2$ balls with the same color" is correct then there are $6$ possibilities: WW WBW WBB BB BWB BWW The corresponding probabilities are easy to find. E.g. $P(BWW)=\frac59\frac49\frac49$. To be found is: $$\frac{P(WW)+P(WBW)}{P(WW)+P(WBW)+P(BWW)}$$ 
 the following paper P. Vincent, H. Larochelle Y. Bengio and P.A. Manzagol, Extracting and Composing Robust Features with Denoising Autoencoders, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML‘08), pages 1096 - 1103, ACM, 2008. contains a variety of images on the MNIST dataset that show how well features are recognized when different levels of noise are added. Especially important are the last picture where he shows that the more noise is added the better the network learns dependencies between variables. With low noise levels features do not stand out. The link to the paper is the following: 
 Is it possible to compare two (or more) models of decision tree obtained with ctree on different data? In particular I would like a statistical test that compares two decision trees (builded with the same predictors' names) in R cran. 
 I suspect your examiner was trying to trick you and/or test your knowledge of logistic regression. A negative intercept is relatively easy to interpreter as very low proportion of occurrences of the event of interest in the original sample in the absence of further influence from variables $X_1 \dots X_{15}$. In general, with logistic regression you analyse the association of a binary outcome with a set of predictors. In particular you are modelling the $\log(\text{odds})$ of that particular outcome. The odds themselves are simply: $\text{odds} = \frac{\text{Pr(of Occurring)}}{\text{Pr(of Not Occurring)}} = \frac{\text{Pr(of Occurring)}}{1 - \text{Pr(of Occurring)}}$ where $\text{Pr}$ refers to proportions (or loosely speaking probability). From this later equality it follows that: $\text{Pr(of Occurring)} = \frac{\text{odds}}{1 + \text{odds}} $. Now, particular to your case, a negative constant ($\beta_0$) simply means that the baseline proportion of your sample is quite low: $\exp(-4.587) / (1+ \exp(-4.587)) \approx 0.01008.$ This not catastrophic; maybe you have not centred your variable $X_i$ for example, this commonly leads to this phenomenon, but in case you need to be able to explain why your baseline is so low. If you truly have a very low occurrence of events in the original sample you may want/need to consider rare-event logistic regression (see King &amp; Zeng's 2001 paper on Logistic regression in rare events data for a first taste). As a quick step-through though to find the change in terms of the proportions that are modelled you need to: Get the $\log(\text{odds})$ estimate. Exponentiate it to get the $\text{odds}$. Get the new proportions as: $\text{Pr}_{\text{new}} = \frac{\text{odds}}{1 + \text{odds}} $. As a final comment: the statistical significance of parameters included the model you present seems relatively low so giving a solid reason as to why you included them is crucial. I am against $p$-value hunting -which is a bad thing- but a 15-variable model with not a single very strongly ($p \leq1e^{-3}$) statistical significant variable seems a bit awkward at first glance. The user @gung has given a very good answer on the matter too here . 
 I have a data sample with 600 units, this data sample is representative. From these 600 units I compute economic sentiment indicator on monthly base in period 2006-2010. From 2011 I want to change (extend) my data sample, now I want 700 units and I want to preserve former 600 and only add 100 to my former data sample. From these 700 units I compute again economic sentiment indicator for period 2011-2016. Now I have two time series, each of them is based on a slightly different data sample but both have 600 common units. And my question is whether I can link these two time series into one time series 2006-2016. It means if the economic sentiment indicator is still comparable regardless the different data samples (but both representative and chosen from the same population). If those two time series are not comparable, I would like to know if there are some methods or transformation bridges to make them comparable without some difficult converting or imputations. 
 I have a linear model in R, equation is below lmmodel31 &lt;- lm(log(ExpensePerMember) ~log(IncomePerMember) + BranchCluster + log(TotalMember) +RentCluster + FoodCluster+IncomePerMember*TotalMember,data = ModelData_V9) Output has some non linearity at tails, thus I introduced some changes in model and increased the power of: log(IncomePerMember) to log(IncomePerMember)^2 but this has no impact on model. No change in AIC and R square value. Can someone suggest what mistake I am doing here. 
 I'm currently looking into survival analysis regression models and can't quite wrap my head around the following: Say I want to model time to occurrence of a cardiovascular event and use age (as opposed to follow-up time) as the underlying time scale. It seems clear to me that it doesn't make sense to include age as a covariate in the model, but what about (2-way) interaction terms between age and another risk factor, say smoking or blood pressure? Would it make sense to include such interaction terms as covariates (also in light of the usual advice to always include both of an interaction's constituent main effects, which in this case wouldn't seem to make much sense for the age main effect)? Also, assuming it would be possible to include such interaction terms, should I treat age as a time-varying covariate (i.e., re-calculate the hazard in different years of the follow-up based on different values of age) or should I use the baseline age value only? Thanks! 
 After computation of one model many times with GLMM (the DV has 3 values / I compute the model for the 3 values and for 26 different cohorts separately) the results at first were no surprise: In the null model with only one random intercept the VPC (Variance/(Variance+pi²/3) is identical with the conditional R²glmm(c) (Nakagawa &amp; Schielzeth 2013, implemented in MuMin-package). For 2 values of the DV the VPC and R²glmm(c) are exactly the same. But for the 3rd value with the smallest share (around 3 % per cohort on average, N(total)~6.000) it's only true for some cohorts (often with a share &gt; 7% and larger VPC) But here in most cohorts, the VPC is anywhere between 0 and .15 (just in one cohort smaller than .01), but R²glmm(c) is persistent 0 (in fact &lt; 0.001). It seems that it's impossible for R²glmm(c) to get smaller values. Does anyone know why? 
 You are overfitting . If you have the choice between an MA($q$) and an MA($q+1$) model, the larger model with more degrees of freedom will almost always fit the data better and yield smaller residual sums of squares. (I would have expected the same to happen for the AR orders, but that this does not happen may be due to the fact that you are modeling residuals .) ARIMA models are typically selected based on information criteria, like aic , AICc, or bic , after deciding on whether to difference or not based on a statistical test. The documentation for the function in the package for R may give you some inspiration as to what to look at. Edit : Cagdas Ozgenc correctly notes that increasing the MA order will not necessarily always reduce the residual sums of squares, because the conditional sum of squares estimation is not convex. To illustrate this effect, I simulated 10,000 white noise time series of 100 realization each, fitted MA($q$) models for $q=0, \dots, 7$ and noted the RSS. Below are boxplots of $$\Delta(q) := \text{RSS}_{\text{MA}(q)}-\text{RSS}_{\text{MA}(q-1)}$$ against $q$. Out of the $10,000\times 7=70,000$ possible differences, $69,851 = 99.8%$ were negative, i.e., a larger model yielded smaller RSS - although there were zero moving average dynamics in the simulated series. R code: 
 As an exercise in understanding, I am wanting to partition the variance in what is effectively a repeated-measures ANOVA (I think) with two measurements - 'pre' and 'post' treatment. I want to equate this with the variances (between and within) that an intra-class correlation is based on. I have written a simulation and the sums of squares (and mean squares) equal that given by an ANOVA. But that's not the same as the between and within variances for an ICC (obviously I am not correctly understanding how this is formulated). I know that: rho(ICC) = sigma^2 (between)/sigma^2(total) where sigma^2(total) = sigma^2(between) + sigma^2(within) As I have specified the bivariate correlation above, we can therefore work out: sigma^2 (between) = rho * sigma^2(total) = sb2 in the simulation How do I manually calculate SSB so that it equals sb2? Thanks. 
 I have questions regarding to Fractional response model building and prediction performance. Fractional response model (frm) estimates parameters by using qausi-likelihood estimator, so robust SE values can be obtained. With these, we can perform hypothesis testing whether a predictor is significant or not. Are there any statistical numbers that I should focus on when building frm e.g., MSE, etc.? When we use the frm to predict, how can we measure its accuracy? And what are the statistical numbers which can tell we should redevelop the model? 
 You can first generate the success/failure variable ($X$), and then generate the age ($Y$) with a different distribution depending on the value of $X$. That will give you correlation. To quantify the correlation, the simplest way is to shift $Y$ according to the value of $X$. The amount by which you shift will be a measure of the correlation. 
 I have a k nearest neighbors (kNN) distance matrix for $n$ samples $S_n$, which is a sparse, almost-symmetric matrix with the property $x_{ij} ∈ \{0, x_{ji}\}~∀~i, j ∈ \{1,...,n\}$. A matrix entry $x_{ij} &gt; 0$ if the sample $S_j ∈ \text{knn}(S_i)$. I want to convert it into a symmetric matrix by replacing every $x_{ij} = 0$ with a corresponding $x_{ji} ≠ 0$ if available. Given a distance matrix created by a k nearest neighbors function, is there a better/more efficient way to do this “fold” of the matrix? 
 With help by the discussions here I successfully trained various models for classification. As an example say I trained a stochastic gradient boosted model (gbm) and an extreme gradient boosted tree (xgboost). They are trained using cross-validation on a training set and then tested on a test set measuring AUC (I get values aroung 0.87 Now I would like to combine those models to get an even better one. I tried to average the predicted probabilities and yes, AUC slightly improved on the test set. But if I stack the models in the following sense: calculate the predicted probabilities $p_{\text{gbm}}$ and $p_{\text{xgb}}$ on the training set and use these as predictors. train some model (linear, tree) in the sense $\text{class} \sim p_{\text{gbm}}+p_{\text{xgb}}$ Models of this kind have AUC of 0.9 on the training set and 0.8 on the test set (less than the individual models). Isn't using something more sophisticated than average or linear weighting just overfitting the training set? The information about the data does not get more. It is just hidden in the stage-one predictions. I would appreciate any comment! 
 This will be pretty easy, you'll see. 
 I have a setup that can measure the distance between two beacons . The first beacon is aware of its 2D location and is moving around while measuring the distance to the second beacon. I've setup a system of equations that I solve using non-linear optimization. I optimize for $\bar{x}$ and $\bar{y}$, i.e. the position of the second beacon, the error in the distance measurement ($r$): $$ \mathrm{error} = e_i = \sqrt{(x_i-\bar{x})^2 + (y_i-\bar{y})^2} - r_i $$ The algorithm optimizes the squared error in the distance. $$ \mathrm{cost} = \sum_{i=1}^N e_i^2 $$ In this manner I have found the location of the second beacon. The averaged residual gives me some indication of the certainty with which I know the location of the second beacon. The certainty of my estimation is still unclear however and I would like to estimate this. How can I go about determining the covariance matrix, $\Sigma$? 
 I am working over the statistical validation of data. Till now I have computed MMRE, PRED and MdMRE. But I need alternatives to these because MRE is sensitive to data with large MRE's. 
 You may want to look at the mae , mape or the mase . (The MASE was originally defined for time series data, but it can be used for any predictive use case once you have defined a benchmark model.) I'd strongly encourage you to read the full wikis for each tag. The different error measures measure different things, and a prediction optimized for (say) MAE may yield systematically biased predictions. Alternatively, you could work with loss-functions and evaluate your predictions using these. This is probably your best approach, but of course it requires thinking about the loss function you want to use. 
 The dot in sample.formula doesn't separate sample from formula , other than visually. It is just a variable name. R variables names can consist of alphanumerics and dot (.) and underscore (_) with one exception. Here is the actual rule: " A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. Names such as ".2way" are not valid, and neither are the reserved words. " The second case (i.e., the case of is_spam~. ) is different and is explained above. 
 Got a ctree with four labels, but the categories are long text therefore just the first is shown. is there any way or parameter for manipultating (rotate) text, edge label names and this way force them to be all shown in plot? My real data is much bigger but this sample is ok to test it if you do not use zoom tool. Regards 
 Bendix Carstensen has a nice presentation in which he shows how this can be accomplished by splitting individual follow up time into small pieces and use a Poisson model for the rates thereby using the time scale (in your case age) as a covariate rather than part of the outcome. If you use , the package contains the tools you need. Another option is to extend the Cox model to include time-dependent coefficients. See this vignette for the package in . In that vignette it is also explained why it is not appropriate to include a covariate * age interaction directly in the Cox model. EDIT: Removed the bit about baseline age. That would of course not make sense with age as the time scale. Typed a bit too fast... 
 I tried to use the Kolmogorov-Smirnov test to test whether a sample is exponentially distributed. With the try and error method I tried a couple of rates. This is a small simple example of what I do: Here is the result R gives me: The p-value is very low whereas the test should accept the null-hypothesis. I do not understand why it does not work. 
 I'm working with this really cool multiple measurement dataset. Because I want to measure an effect over age, not measurements, I recoded every measurement into age categories and calculated weighted means per age category (assembling weighted means per age category with means from every measurement). Unfortunately, some subjects contribute to the mean of one age category twice (let's say: subject 1 is 40y/o at measurement 1 and 44 at measurement 2; it wil fall in category 40-44yrs both times). Is there a way to: a) identify the subjects that have this twice in one category problem b) adjust for it? (In R or SPSS) Thanks a lot! 
 I have coded some NN using neuralnet and caret packages. But these neural networks are of 2-Dimension (I dont know even this term exists) But I was curious whether "3D neural network" exists. While searching I came across this website and found this neural network. I have a good understanding of feed forward and back propagation concepts in neural network. If a 3d neural network exist how does it work?Is there any package in R which can be used to create models(and predictions) for this type of neural network? If not can you suggest any other platform? 
 I have 20 items measured with 7 point ordinal scale by roughly 200 of respondents such that every item's relative importance was assessed by every respondent. Item example: What is the importance of personal integrity for project succes ? Possible answers: absolutely no importance (1) - minimum importance (2) - rather minor importance (3) - moderately important (4) - quite important (5) - very important (6) - critically important (7) There is no assumption about equidistance of the scale. Perception of distances vary by respondent, and is very individual. Let's say the scale is truly ordinal. I need to identify: The differences of measured importance for every item to identify items valued lower and higher than others. The subset of items that are highly valued by more than 80% of respondents. To find the answer to problem 1 I'm considering two aproaches: pairwise sign test (related samples) with signifficancy adjustment. I prefer to use False Discovery Rate (BY) adjustment instead of Family-wise Error Rate (such as Holm or Bonferroni adjustment). Friedman's ANOVA (not sure if it doesn't assume equidistance of the scale) To answer problem 2 I intend to : Select all items scored higher by neutral position by at least 80% or respondents (this is an arbitrary decision to the some extent). Let's agree for the moment it is 4 items. Compare each of the selected 4 items with all others (4x16 comparisons) using a sign test with signifficancy adjustment. Make my final set by adding every items that were not significantly lower valued by respondents. As a result, my final set will contain 4 items selected at the beginning and all items that were not significantly different from them. Do you think these approaches are correct? 
 I am trying to get my head around the right choice of experimental design and analysis for a collaborative software I have created and would like to evaluate. The software allows groups to conduct a specific group problem solving method and it consists of three phases: IDEATION, CLUSTERING, DISCUSSION (simplified). The output of each phase is the input of the next phase. The outputs are very structured artefacts/templates. I have 2 factors with 2 levels each: A: Process Model (1: control / 2: advanced - involving substeps) B: Task Type (1: creativity / 2: strategy development) I would like to measure 4 dependent variables (some for the overall process, others only for particular phases): (1) OVERALL: Subjective rating of satisfaction with the overall process (2) OVERALL: Usability score (3) IDEATION: Number of elements in IDEATION template + diversity (4) CLUSTERING: Number of disagreements I want to run two repetitions in a counter-balanced experiment to control for the order of the process model (A) the task (B) for each participant. Participants will work in groups of 3. Group 1 | A1B1 | A2B2 Group 2 | A1B2 | A2B1 Group 3 | A2B1 | A1B2 Group 4 | A2B2 | A1B1 Now I am wondering what is the correct way to conduct the statistical analysis: I am confused about which analysis method is the right choice for this multifactorial and multivariate design. Am I supposed to use a multivariate GLM or repeated-measure MANOVA or can I investigate the dependend factors independently (at least the overall dependent variables 1 and 2)? Is it better to simplify the design? If so, do you have suggestions how? Thank you for any help! Best, John 
 Ok. The answer came to me itself. The output $a $ is obtained by a sigmoid function, e.g. logistic function. These functions become 1 only at infinity. In practice they never become 1. 
 SGD is a common strategy for training deep neural networks with millions of examples in synchronous parallel mode and has been shown to converge ( see Dekel et.al. ) at $\mathcal O(1\sqrt{KM}) $ where $K$ is the total number of updates by the master and $M$ is the number of examples which need to be processed before each update. This means that compared to a serial (ie, a single processor) SGD, synchronous parallel SGD will have a speed up linear in the number of machines when the dataset size $\mathbf{\to} \infty$. For asynchronous parallel SGD , what is the speedup? 
 I am currently working on building a web analytics data repository which gets its data from another data source. I want to set up a Data Quality check which ensures the right data ingestion. Currently I am thinking of using the forecasting of measures to validate the quality of the data. Comparing the results from both the data sources is not feasible. Am I going in the right direction? Are there any other statistical methods through which I can do this kind of validation ? 
 Methods overview Short reference about some linkage methods of hierarchical agglomerative cluster analysis (HAC). Basic version of HAC algorithm is one generic; it amounts to updating, at each step, by the fomula known as Lance-Williams formula, the proximities between the emergent (merged of two) cluster and all the other clusters (including singleton objects) existing so far. There exist implementations not using Lance-Williams formula. But using it is convenient: it lets one code various linkage methods by the same template. The recurrence formula includes several parameters (alpha, beta, gamma). Depending on the linkage method, the parameters are set differently and so the unwrapped formula obtains a specific view. Many texts on HAC show the formula, its method-specific views and explain the methods. I would recommend articles by Janos Podani as very thorough. The room and need for the different methods arise from the fact that a proximity (distance or similarity) between two clusters or between a cluster and a singleton object could be formulated in many various ways. HAC merges at each step two most close clusters or points, but how to compute the aforesaid proximity in the face that the input proximity matrix was defined between singleton objects only, is the problem to formulate. So, the methods differ in respect to how they define proximity between any two clusters at every step. "Colligation coefficient" (output in agglomeration schedule/history and forming the "Y" axis on a dendrogram) is just the proximity between the two clusters merged at a given step. Method of single linkage or nearest neighbour . Proximity between two clusters is the proximity between their two closest objects. This value is one of values of the input matrix. The conceptual metaphor of this built of cluster, its archetype, is spectrum or chain . Method of complete linkage or farthest neighbour . Proximity between two clusters is the proximity between their two most distant objects. This value is one of values of the input matrix. The metaphor of this built of cluster is circle (in the sense, by hobby or plot). Method of between-group average linkage (UPGMA). Proximity between two clusters is the arithmetic mean of all the proximities between the objects of one, on one side, and the objects of the other, on the other side. The metaphor of this built of cluster is generic, just united class ; and the method is frequently set the default one in hierarhical clustering packages. Simple average , or method of equilibrious between-group average linkage (WPGMA) is the modified previous. Proximity between two clusters is the arithmetic mean of all the proximities between the objects of one, on one side, and the objects of the other, on the other side; while the subclusters of which each of these two clusters were merged recently have equalized influence on that proximity – even if the subclusters differed in the number of objects. Method of within-group average linkage (MNDIS). Proximity between two clusters is the arithmetic mean of all the proximities in their joint cluster. This method is an alternative to UPGMA. It usually will lose to it in terms of cluster density, but sometimes will uncover cluster shapes which UPGMA will not. Centroid method (UPGMC). Proximity between two clusters is the proximity between their geometric centroids: squared euclidean distance between those. The metaphor of this built of cluster is proximity of platforms (politics); Median , or equilibrious centroid method (WPGMC) is the modified previous. Proximity between two clusters is the proximity between their geometric centroids (squared euclidean distance between those); while the centroids are defined so that the subclusters of which each of these two clusters were merged recently have equalized influence on its centroid – even if the subclusters differed in the number of objects. Ward’s method, or minimal increase of sum-of-squares (MISSQ), sometimes incorrectly called "minimum variance" method. Proximity between two clusters is the magnitude by which the summed square in their joint cluster will be greater than the combined summed square in these two clusters: $SS_{12}-(SS_1+SS_2)$. (Between two singleton objects this quantity = squared euclidean distance / $2$.) The metaphor of this built of cluster is type . Some among less well-known methods (see Podany J. New combinatorial clustering methods // Vegetatio, 1989, 81: 61-77.) [also implemented by me as a SPSS macro found on my web-page]: Method of minimal sum-of-squares (MNSSQ). Proximity between two clusters is the summed square in their joint cluster: $SS_{12}$. (Between two singleton objects this quantity = squared euclidean distance / $2$.) Method of minimal increase of variance (MIVAR). Proximity between two clusters is the magnitude by which the mean square in their joint cluster will be greater than the weightedly (by the number of objects) averaged mean square in these two clusters: $MS_{12}-(n_1MS_1+n_2MS_2)/(n_1+n_2) = [SS_{12}-(SS_1+SS_2)]/(n_1+n_2)$. (Between two singleton objects this quantity = squared euclidean distance / $4$.) Method of minimal variance (MNVAR). Proximity between two clusters is the mean square in their joint cluster: $MS_{12} = SS_{12}/(n_1+n_2)$. (Between two singleton objects this quantity = squared euclidean distance / $4$.). First 5 methods permit any proximity measures (any similarities or distances). Last 6 methods require distances; and fully correct will be to use only squared euclidean distances with them, because these methods compute centroids in euclidean space. Therefore distances should be euclidean for the sake of geometric correctness. At worst case, you might input other metric distances at admitting more heuristic, less rigorous analysis. Now about "squared". Computation of centroids and deviations from them are most convenient mathematically/programmically to perform on squared distances, that's why HAC packages usually require to input and are tuned to process the squared ones. However, there exist implementations - fully equivalent yet a bit slower - based on nonsquared distances input and requiring those; see for example "Ward-2" implementation for Ward's method. You should consult with the documentation of you clustering program to know which - squared or not - distances it expects in order to do it right. Methods MNDIS, MNSSQ, and MNVAR require on steps, in addition to just update the Lance-Williams formula, to store a within-cluster statistic (which depends on the method). Methods which are most frequently used in studies where clusters are expected to be solid more or less round clouds, - are methods of average linkage, complete linkage method, and Ward's method. Ward's method is the closest, by it properties and efficiency, to K-means clustering; they share the same objective function - minimization of the pooled within-cluster SS "in the end". Of course, K-means (being iterative and if provided with decent initial centroids) is usually a better minimizer of it than Ward. However, Ward seems to me a bit more accurate than K-means in uncovering clusters of uneven sizes (variances) or clusters thrown about space very irregularly. MIVAR method is weird to me, I can't imagine when it could be recommended, it doesn't produce dense enough clusters. Methods centroid, median, minimal increase of variance – may give sometimes the so-called reversals : a phenomenon when the two clusters being merged at some step appear closer to each other than pairs of clusters merged earlier. That is because these methods do not belong to so the called ultrametric. This situation is inconvenient but is theoretically OK. Methods of single linkage and centroid belong to so called space contracting , or “chaining”. That means - roughly speaking - that they tend to attach objects one by one to clusters, and so they demonstrate relatively smooth growth of curve “% of clustered objects”. On the contrary, methods of complete linkage, Ward’s, sum-of-squares, increase of variance, and variance commonly get considerable share of objects clustered even on early steps, and then proceed merging yet those – therefore their curve “% of clustered objects” is steep from the first steps. These methods are called space dilating . Other methods fall in-between. Flexible versions . By adding the additional parameter into the Lance-Willians formula it is possible to make a method become specifically self-tuning on its steps. The parameter brings in correction for the being computed between-cluster proximity, which depends on the size (amount of de-compactness) of the clusters. The meaning of the parameter is that it makes the method of agglomeration more space dilating or space contracting than the standard method is doomed to be. Most well-known implementation of the flexibility so far is to average linkage methods UPGMA and WPGMA (Belbin, L. et al. A Comparison of Two Approaches to Beta-Flexible Clustering // Multivariate Behavioral Research, 1992, 27, 417–433.). Dendrogram. On a dendrogram "Y" axis, typically displayed is the proximity between the merging clusters - as defined by methods above. Therefore, for example, in centroid method the squared distance is typically gauged (ultimately, it depends on the package and it options) - some researches are not aware of that. Also, by tradition, with methods based on increment of nondensity, such as Ward’s, usually shown on the dendrogram is cumulative value - it is sooner for convenience reasons than theoretical ones. Thus, (in many packages) the plotted coefficient in Ward’s method represents the overall, across all clusters, within-cluster sum-of-squares observed at the moment of a given step. To choose the "right" method There is no single criterion. Some guidelines how to go about selecting a method of cluster analysis (including a linkage method in HAC as a particular case) are outlined in this answer and the whole thread therein. 
 Is there a way to easily check the "geeglm" model like its residuals or something else? I have performend an analysis (geeglm, geepack, R) with an ar autocorrelation structure and i would like to validate the correct use of the model by checking it but I cannot find anywhere how to perform any validation . Thanx a lot 
 I've ran a three-level meta-analysis using the package in R, and I'm now trying to generate the Profile Likelihood Plots for my two variance components. I've already created an object, , using the function, and following the instructions outlined here , this is what my code looks like: This generates totally blank pdf documents and the following error message: Specifying the ylim values myself (after some trial and error), using the error message disappears, and I get plots that looks like this: That is, no data except for the dotted, vertical and horizontal lines, where the vertical line corresponds to the estimate of the variance components. What's happening here? How can I go forward? Could there be something wrong with my analysis/data, or am I doing something wrong? I don't know how to proceed here. 
 Update for all those who have the same problem with CDC data: The CDC WONDER Suppression Toolkit fixes the suppression issue, and employs an algorithm described here . However, the site is not compatible with Mac or Linux systems. 
 I want to know which test should I use on the following data: I counted the number of events per person over a 4 year period (2007-2011). I created different groups which are: I want to check, whether there is a association between the number of occurences in the specified time period and the length of the period where no event occurs (regarding the period 2011 - 2014). The data is not censored. Till now I used the log-rank test for this analysis but I got the hint, that this test is maybe not the best choice for the data and that I should use a Poisson regression or negative binomial test. It is important to say, that it is assumed, that the number of events influence the occurrence of another event. Additionally, I found on wikipedia, that the log-rank test is for two groups but R did not give me any error or warning when analysing my data (with 3 groups). Here is the Kaplan-Meier plot for the data: I know that the poisson regression has some assumptions but I could not find out exactly how I can proof them. Which test would you suggest? Let me know, if you need further informations. I hope you can help me. 
 I read a lots of discussions and articles and I am a bit confused on how to use SVM in the right way with cross-validation. If we consider 50 samples and 10 features describing them. First I split my dataset into two parts : the training set (70%) and the "validation" set (30%). Then, I have to select the best combination of hyperparameters (c, gamma) for my SVM RBF. So I use cross-validation on the trainnig set (5-fold cross-validation) and I use a performance metrics (AUC for example) to select the best couple. Finally, I use the best hyperparameters on the "validation" set and I mesure the performance metrics. My questions are : is the ratio 70/30 for splitting the dataset appropriate ? is it usefull to use cross validation on the "validation" set ? is it better to make a loop on this procedure in order to have randomly differents compositions of the training and validation sets ? if 3 is better, how many loops and which statistics on the performance metrics ? do we agree that use cross-validation on the full dataset is the worst thing to do ? Thanks a lot! 
 Let's try below code in R and think contrasts methods. In default, R uses in treating unordered categorical variables. (It contrasts each level with the baseline level (alphabetical first); gives you details.) In this example, baseline level is species setosa , so coefficients: and are setosa 's intercept and slope and their comes from the difference between setosa 's intercept (or slope) and zero. is versicolor 's intercept, is its slope. Their comes from the difference between versicolor and setosa . 
 If we have a normal random distribution of points in N-space, we often use $\sqrt{det(COV)}$ to represent the volume, where COV is the NxN covariance matrix. In 2D Cartesian coordinates ($x$,$y$), $volume_{2D} \approx \sqrt{det(COV)} = \sqrt{var(x) var(y) - cov(x,y)^2}$. The problem I am having is that in my data, there is a non-linear spatial correlation between $x$ and $y$, namely $x^2 + y^2 = r^2$, where $r$ is randomly distributed in a narrow interval. In other words, our distribution looks like a ring and is better represented using cylindrical coordinates, $r$ and $\theta$ where $r_{min} \le r \le r_{min} + \Delta r$. How can I statistically represent a similar volume metric in these coordinates? Can someone provide a reference to a book or article? Edit: So my question is apparently really unclear. I have simulated some data in R to better demonstrate what I am talking about. Each distribution has 1000 "samples" (or whatever you want to call them). The $volume_{2D}$ measure detailed above correctly describes the following distributions (generated from x and y both drawn from a random normal distribution with mean 0 and various sd's) as having "small volume" and "large volume" However, it gets the following distributions (drawing r and theta from uniform distributions from $r_{min}$ to $r_{max}$ and $0$ to $2\pi$, respectively) backwards. They should be "small volume" and "large volume". What I'd like is a measure that would correctly compare all of these distributions. Again, the best measure would be able to correctly rank even between the Gaussian and ring distributions, if that is possible. I hope this is an easy question that I just don't know where to look to answer... 
 In most descriptions of Forest Plots the x-axis is an Odds Ratio or a Difference of means etc. But I'm now reading a paper where the x-axis is labeled "Proportion" with the scale going from 0 to 0.8. How does one interpret this? The scale is linear, and not log. OR scales are typically log? The authors report they've used MedCalc for the analysis &amp; the plot looks like something a canned, out-of-the-box routine in the software produced. https://www.medcalc.org/ Edit: They call the plots "Forest plots of proportions of disease response rates". My guess is that their x-axis quantity is something like the proportion of subjects that showed "improvement" or response to the intervention. But I'd love any comments. 
 I am trying to answer my own question after doing few initial experiments. I tried the SMOTE technique to generate new synthetic samples. And the results are encouraging. It generates synthetic data which has almost similar characteristics of the sample data. The code is from by Karsten Jeschkies which is as below The got the following results with a small dataset of 4999 samples having 2 features. Sample or the small data description Histogram is as follows ￼ Scatter plot to see the joint distribution is as follows: After using SMOTE technique to generate twice the number of samples, I get the following Histogram is as follows Scatter plot to see the joint distribution is as follows: 
 I am trying the following: Now according to the python documentation: Facs[0,:] should match exactly v[:,0]*np.sqrt(w[0]) But it doesn't. What am I doing wrong? Edit: Fixed the code to call Data. Documentation saying that v is normalized eigen vectors is here Blog showing that pca.components_ is loadings is here . However, the link does not verify the blog's statement. So, blog is wrong probably. Hence to get the loadings I need to do: Am I correct in saying that ? 
 First of all, I would like to apologize if this is an duplicate of an earlier question. I find it hard to believe that I am the first person to ever have this sort of problem, but I am so far out of my comfort zone that I do not really know what to search for (I have tried of course). Me and my partner are a doing bachelor thesis, where a part of it have been creating an artefact in the domain of information retrieval and natural language processing . We have run a series of experiments with this artefact with several components or flags activated. In total, 32 experiments were conducted since five different flags were set on of off in all possible permutations ($5^2 = 32$). Originally, we would have thought that we should only test if any result was statistically significant compared to the setting were all flags were disabled. Our supervisor told us however that we should compare all results to each other, to see which of the flags would generate a statistically significant difference to each other. Since this would make 496 comparisons ($31^2/2$), this is far more than what could be done by hand. Our supervisor recommended using R or pystat for simplifying the task. We have however no previous experience of either of these, all tough getting into R didn't seem to complicated, but we still don't know how to get the requested data. Our current results from the experiments are in the following form (not all experiments); True Positives False Positives False Negatives True Negatives No Flags 174 879 2541 13224 E 303 2051 2412 12052 EF 301 1955 2414 12148 F 219 1211 2496 12892 S 224 1332 2491 12771 SE 304 2112 2411 11991 We want to see if certain flags yields an improvements in F1-score over others, in other words, if true positives increases and false positives and false negatives decreases. When calculating out F-Score, we use an unweighted F1-score were precision and recall are equally weighted I have come as far as exporting the results into a CSV file and loading the file in a R script, but I do not know how to process it from there. How would we go about this? It can be worth mentioning that we will display the results as their F1-scores, but as far as we understand we cannot compute statistical significance directly on a F-score compared to an other F-score. 
 The answer I found is very complicated I do not know if there is a direct method (?). The expression above for the likelihood*prior can be "easily"(or at least in a more simple way) simplified using vector z=$(\beta, y)$: $p(w|X,y)=\exp{z^{T}Rz}$ Then once you got there you use woodbury inversion formula to get the covariance of z. Then you use Theorem 4 part b to extract the covariance and mean of conditional $\beta|y$. The process can be found in Bishop's Pattern and Recognition for ML Book on chapter 2 section 2.3.3.. I have no idea why my question was downvoted. 
 A '3d network' might commonly be described as a network with 2d layers. It's not fundamentally different from any other network because the principles of activation are the same. The activation of each unit is a linear combination of its inputs, passed through a (typically nonlinear) activation function. In one sense, the dimensionality is just a property of the drawing. You could draw the units anywhere you wanted (using however many dimensions you wanted) and the function would be the same. Instead, it's the connectivity that matters. There are a couple reasons to 'organize' networks layers into well defined shapes. One is for convenience in thinking about things (e.g. in the case of processing 2d inputs like images). This is particularly the case when connectivity is constrained. For example, in a convolutional network that processes images, each unit receives connections from a local 'patch' of units in the previous layer. Thinking about the layers as 2d makes intuitive sense here because it lets us talk about things like 'local patches'. But, as before, you could completely scramble the 'locations' and the function would be the same as long as connectivity is the same. In convolutional networks (e.g. for image processing), there's an additional benefit to representing the layers in 2d. Because of the way these networks constrain the weights/connectivity, representing the layers in 2d makes it possible to use the 2d convolution operation when computing the activations of all units in a layer. Although it doesn't change the fundamental function of the network, it's a very computationally efficient way of implementing things. This is just one example, and there could be other cases where representing units on a grid with some dimensionality makes it possible to play computational tricks that speed things up (e.g. in the blog post you linked). Actually, the network in the blog post is a different beast because it's a spiking network that tries to emulate biological neurons slightly more closely than a standard artificial neural net. But, that's a whole different issue. 
 You are used to looking at meta-analyses of comparative primary studies but this is a meta-analysis of non-comparative studies so the authors are meta-analysing the proportion of some quantity. Note that the proportions have confidence intervals which are not symmetrical on the scale shown. Whether the software (or the authors) transformed the proportions is not something we can tell from what you give us but it would be usual to do so. There are many possible transformations: log, logit, arcsine square root, Freeman-Tukey, ... 
 Your code doesn't do what you think it does because: you put some exogenous variable instead of in your call to . you are multiplying by which is not necessary. is equal to doesn't return eigenvectors in any particular order, but the scikit method does. That means that the elements of will be the same as the elements of , but the arrangement of the matrices will be different. PCA decompositions of these types are invariant to sign-flips on all the eigenvalues. This code (which btw is reformatted to conform to python standards by lowercasing variable names and using spaces around commas and operators) does what I think you want. import numpy as np from sklearn.decomposition.pca import PCA The output is: So for 10 random tests of 2-dimensional data, the and matrices were precisely identical two times. However, every time, no matter what the random data used was, the elements of were the same as the elements of (except for possible sign changes). 
 I am trying to build a credit scoring model and have discovered and interesting approach for feature selection. I am looping through all features and removing them one by one (using variable importance criterion) from model until test set has approximately the same AUC as the train set. What supprised me was when I selected a model that satisfied AUC criteria, most of the variables had bad significance: The ROC curve on train and test sets are approximately the same: I am inclined to use this model because it does not overfit and performs the same on test set. My questions is - what are your thoughts, would you use this model if the goal is to use it in real-time? EDIT: Here is the variable removing process. I choose the model for which train AUC and test AUC were one the same level 
 I am doing a research project on the topic of replicability in psychology. To do this, I did a set of 10 meta-analysis on different studies. My problem is the following : some of those studies were between subject but most were within subject. To calculate the effect sizes in those studies, I believe I need to know the correlation between the two measures. However, most studies do not report any correlation, and since I do not have access to the original datasets (some studies are more than 15 years old), it is not possible for me to retrieve them. When looking for a solution on the Internet, I have been finding conflicting answers : - should I just ignore the correlation, as this article seems to suggest it ? ( ) - should I use a random correlation ? - Or try to find a median correlation in the field ? (If so, how ?) - Should I do something else ? I am hoping that I am not the first one to be in such a predicament and that some one more well-versed in statistics than me (a Philosophy major) might be able to provide some insight on this issue. In any case, I am very grateful to any one reading and commenting on this post ! 
 I have some biomass data (total count -discrete- of individuals and total dry weight -continuous-) that I would like to model with environmental parameters recorded at different spatial scales (temperature -continuous-). My biomass data is zero inflated mainly due to scale inaccuracy and as a results I have many sampling points that present a heigh number of individuals but no biomass reading. I do however have some true zeros and I don't to ignore them in the analysis. The sampling effort between blocks presents some discrepancies (lost samples or logistical problems) and I have been reading that CPG was more robust than a delta approach in this case. CPG is completely new to me and I am still struggling a bit to understand all the math behind although I do get the general picture and philosophy (I think...). To put it into context, all the sampled blocks are part of a fragmentation experiment in the same landscape and before running a CPG I was gonna test for spatial autocorrelation between sampling units for each block. Is this the right approach to deal with this data set? I want to be able to see if there is a trend between biomass and temperature data at different scales but I also want to take into account all my biomass readings that have a zero output but still have a count of individual recorded. Cheers, N. 
 Does somebody know a good book which outlines the time series analysis in Stata, that is, the various commands explained. I am aware of the Stata manuals; however, they are not that user friendly for me. I am searching for a book similar to "Microeconometrics using Stata" by Cameron and Trivedi which is related to time series analysis. 
 I am planning a 2-arms trial that tests a response rate in patients to a drug. The response rate indicative of the drug working would be 10%, while 25% for the control group. 266 subjects are needed to test the hypothesis at the end of the study (2-sided test with 0.05 alpha, 90% power). However, if I would like to conduct an interim analysis for futility after 133 patients are accrued, how do I choose the best method for calculating the futility threshold? 
 You have a number of options. If you can find information about the likely correlation then you can impute a value of $r$ from them. As a sensitivity analysis you can try a range of values and see how they affect the answer. If the authors have published the means and standard deviations from the post condition you can just treat the study as a between groups design and ignore the pre- readings. This loses precision in general but there may be o viable alternative. 
 Let us consider a variable $Y$ on which we want to apply a multivariable linear regression on the variables $X_1$ and $X_2$. $X_1$ and $X_2$ are collinear by construction, with $X_1=ab$ and $X_2=bc$, in which $a$,$b$, and $c$ are measurable quantities for which we acquired observational data. The objective is to evaluate the correlation between $Y$ and $X_1$ while controlling for $X_2$, and vice-versa. Is there a a more adequate way of testing for correlation on one variable while simultaneously controlling for another variable? Is it legit to perform such an analysis, even in the presence of collinearity? Note that $X_1$ and $X_2$, while related, are different quantities that are expected to be correlated with $Y$ for different reasons (akin to density and surface area, and its relation with volume, for instance). 
 I have a linear mixed effect model in which the dependent variable is a log-transformed frequency of livestock predation , and there are three predictor variables, one of which is also transformed: 1) quantitative, proportion of time spent outside park, LOGIT transformed 2) untransformed, categorical, 2 levels: before and after 3) categorical, 3 levels: early dry, late dry and wet The model is the following: I want to know how to interpret the effect of this 3-way interaction on the frequency of livestock predation. I don't know what are the things I should be paying attention to for achieving a correct interpretation of this effect. I have seen descriptions on how to interpret models where both outcome and predictor are transformed, as here Interpretation of log transformed predictor but this is for log transformation and no interactions. The summary of my model is the following: This is a plot of the frequency of livestock predation in the before and after time period. It is in the late dry season when predation is higher in the after period, contrary to the effects observed in the other seasons. To try to explore what this was telling me, I coded prop_out as a dummy variable (OUT= animals that spend more time outside park (frequency outside &gt;0.5), IN= animals that spend more time inside park (frequency outside &lt;0.5) and plotted this. I realized that what this interaction is showing is that for animals that spend more time outside the park, there is higher predation in the after period in the late dry season. How can I interpret this formally, and report it numerically, when the proportion is quantitative and Logit transformed? How would the odd ratios work here? Many thanks, 
 Can a loop be used to simulate 1000 logistic growth curves in R? I have a data set of about 1500 length-at-age data. I need to: bin data in 3 year bins draw 20 random samples per bin estimate parameters for 1000 realizations of the logistic growth curve. 
 Ok, if each tree $i$ has an age-independent hazard $\lambda_i$, then its lifetime $T_i$ follows an exponential distribution such that the probability that the tree is dead after a time interval of length $t_i$ becomes \begin{equation} p_i = P(T_i \le t_i) = 1-\exp(-\lambda_i t_i). \end{equation} If we in turn assume that pollution has an additive effect on the log of the hazard, that is, $\log\lambda_i=\beta_0 + \beta_1 x_i$, we get the overall model \begin{equation} p_i = 1-\exp(-\exp(\beta_0 + \beta_1 x_i + \log t_i)), \end{equation} or \begin{equation} \log(-\log(1-p_i) = \beta_0 + \beta_1 x_i + \log t_i \end{equation} which is a glm with cloglog link-function and $\log t_i$ as offset. This is fitted in R by doing This model is similar to the one you're trying to reproduce in that survival probabilities $e^{-\lambda_i t_i}=(e^{-\lambda_i})^{t_i}$ decrease with powers of annual survival probabilities $e^{-\lambda_i}$ but the exact effect of pollution is slightly different from what what follows from the logit link assumption. 
 This is my first post on this site. I'm a linguistics graduate student who is struggling to grasp the basics of statistics. I've run a questionnaire in which participants had to rate sentences from 1 (totally unacceptable) to 7 (fully acceptable). I had two different factors with two levels each (a 2x2 design). Following previous papers whose authors used the same design, I have log-transformed the ratings and then I have calculated z-scores by subject: After that, I've considered ratings above and below 2.5 standard deviations from the mean as outliers and I've removed them (also following previous studies). I report here the histogram for the cleaned data: And these are the histograms per condition: As you can see, the data is far from normal. My question is the following: does this matter if I want to conduct a linear-mixed effects model? If it does, how can I normalize the data? Thank you very much! 
 I used sigmoid and hyperbolic tangent activation functions successfully as the activation function of the output layer.If using softmax is necessary, can someone please help me with how the backpropogation implementation will change ?any help is appreciated. 
 If you use something like a generalized linear mixed model, then the response variables don't have to be gaussians. This fact is the key differentiator from GLMM and LMM. 
 I have been given a problem and need to find the best statistical method that would help. The problem is as follows: A business has approx. 100,000 active customers who spend between £500 per annum and £200,000 per annum. The smaller ones transact irregularly, once per year, the larger ones maybe once per week. They are distributed across 15 different market sectors. The business has started to decline, and whilst we have implemented a process for making offers to customers once they have closed their account, it is thought this would be much more effective if we could predict which customers are likely to close, and make offers to keep them before they close. How would you go about determining which customers to make offers to? 
 In my understanding using CV on test data (separately from CV for the train data!) gives no real advantage, as you are already working with a single, fully trained model at this point. So you don't need to evaluate and choose a best model from multiple models or similar any more. Having said this: I can imagine that calculating e.g. some performance spread from different partitions in your test data might make sense - but without the mechanisms of repeats/resampling. For example, in activity recognition, one could measure how well activities were recognized across people in the test data - which boils down to evaluating different test data partitions separately. But you cannot resample your data the same way as with training (for model evaluation and selection). BTW: repeating the CV process is actually done frequently: it's usually called repeated cross validation and is implemented with most ML toolsets already (see e.g. here , which also addresses the amount of partitions and repeats to use). For the amount of data that should go into the train/test partition have a look at this question . 
 Someone told me before that all of them are the same in machine learning algorithms. Is that right or not? Can an algorithm be scalable but can't be parallelized? Can it be parallelized but can't be used for online learning? 
 Error rates control is similar to quality control in production. A robot in a production line has a rule for deciding that a part is defective which guarantees not to exceed a specified rate of defective parts that go through undetected. Similarly, an agency that makes decisions for drug approval based on "honest" P-values has a way to keep the rate of false rejections at a controlled level, by definition via the frequentist long-run construction of tests. Here, "honest" means absence of uncontrolled biases, hidden selections, etc. However, neither the robot, nor the agency have a personal stake in any particular drug or a part that goes through the assembly conveyor. In science, on the other hand, we, as individual investigators care most about the particular hypothesis we study, rather than about the proportion of spurious claims in our favorite journal we submit to. Neither the P-value magnitude nor the bounds of a confidence interval (CI) refer directly to our question about the credibility of what we report. When we construct the CI bounds, we should be saying that the only meaning of the two numbers is that if other scientists do the same kind of CI computation in their studies, the 95% or whatever coverage will be maintained over various studies as a whole. In this light, I find it ironic that P-values are being "banned" by journals, considering that in the thick of replicability crisis they are of more value to journal editors than to researchers submitting their papers, as a practical way of keeping the rate of spurious findings reported by a journal at bay, in the long run. P-values are good at filtering, or as IJ Good wrote, they are good for protecting statistician's rear end, but not so much the rear end of the client. P.S. I'm a huge fan of Benjamini and Hochberg's idea of taking the unconditional expectation across studies with multiple tests. Under the global "null", the "frequentist" FDR is still controlled - studies with one or more rejections pop up in a journal at a controlled rate, although, in this case, any study where some rejections have been actually made has the proportion of false rejections that is equal to one. 
 i have a data with a list of 26 variables and the preference in grade points(1-10 scale) given by 50 respondents to each variable. What kind of statistical analysis can help me to find out the best 10 variables preferred by the 50 respondents among the list of 26 variables. 
 $$ \left\{ \begin{array}{ll} \dot{N_1}=r_1N_1\left(1 - \frac{N_1}{K+b_{12}N_2}\right)\\ \dot{N_2}=r_2N_2\left(1 - \frac{N_2}{K+b_{21}N_1}\right) \end{array} \right. $$ I would like to ask you, how could I interpret this model? How about symbiosis process? I think it is could be something like symbiosis process, because as we could see we have two competing populations. the biggest problem i have with understanding dividing by $N_1$ and $N_2$. There is only one assumption: all parametres are nonnegative. 
 I can think of example in which p-values are useful, in Experimental High Energy Physics. See Fig. 1 This plot is taken from this paper: Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC In this Fig, the p-value is shown versus the mass of an hypothetical particle. The null hypothesis denotes the compatibility of the observation with a continuous background. The large ($5 \sigma$) deviation at m$_\mathrm{H} \approx 125$ GeV was the first evidence and discovery of a new particle. This earned François Englert, Peter Higgs the Nobel Prize in Physics in 2013. 
 Let's say I do a multiple regression, using robust (Stata option). It is a robust standard error regression. I want to analyse and discuss residuals. Residuals versus fitted values Is it sufficient to simply observe a random and homogeneous distribution of the residuals around 0 ? Kernel density estimate of the residuals (Are they following a normal distribution?). As I didn't use an OLS regression, I don't care if the residuals are not normal. True ? Is there anything I forgot ? Thank you ! 
 In doing a CFA in . I had to use the covariance matrix as an input because I was getting some errors with the original data (e.g., negative variances). I would normally have predicted factor scores using the function (lavPredict functions the same), but now that I'm using the covariance matrix it's not possible to do this directly. Is there a way to use the information from the CFA to calculate factor scores in the same way as Lavaan does it (I believe the function uses the method of regression to calculate factor scores)? This is sample code to produce factor scores with raw data as input (using this method I get an error in one of my variances): This is the code to produce factor scores with covariance matrix as input (no error messages here, but can't produce factor scores as there is not data to link them to): How to proceed from here to produce factor scores using the results from Lavaan's CFA analysis? 
 i would like to perform a marketing segmentation on mixed categorical and ordinal datas. Do you have any suggestion? I don't want to perform a full hierarchical clustering (with grower distance) because of the slowness of the algo. I was thinking about K-mode with weights on ordinal datas but i can't modelize it with a real proof of optimality. Any ideas? 
 I'm carrying out a study on reported deaths of vultures (Gyps fulvus) in the Apennine mountains, Italy. I've performed a chi-square test on a 2x2 contingency table relating gender (male, female) to age (adult, subadult). Data are as follows: adult males= 8; adult females= 13; subadult males=13; subadult females= 4. The chi-square is 5.5981 p=0.018, so the two variables (gender, age) are not independent. Is there a method to calculate which observed value/s is/are significantly different from expected value/s? I.e. is the rejection of H0 depending on which particular cell value/s? 
 I am new on this web-site and coming from the field of economics (although interested in High Dimensional Statistics), I am reading Statistics for High Dimensional Data of Bühlmann and Van De Geer. I struggle to get an intuition of what is the compatibility constant. As far as I understood it is a link between the L1 norm and L2 norm of a vector. Although when they compute the upper prediction bound for LASSO I do not understand. In the noisless case they go from $$ \parallel X\hat{\beta}-X\beta^0 \parallel^2_n + 2 \lambda \parallel \hat{\beta_{S_0^c}} \parallel_1 \leq 2\lambda \parallel \hat{\beta_{S_0}}-\beta^0 \parallel_1 $$ to $$ \text{left hand side} \leq \frac{2\lambda \parallel X\hat{\beta}-X\beta^0 \parallel_n}{\phi(S;L)} $$ Using the definition: $$ \phi(S;L)= min ( \parallel X\hat{\beta}-X\beta^0 \parallel_n : \parallel\beta_S\parallel=1 ; \parallel\beta_{S_c}\parallel \leq L) $$ With S the active set, Sc its complementary set, S0 the true active set . It sounds obvious to them that by definition those two lines follow naturally, but it is not obvious to me. Does anyone has an intuition or explaination to provide me ? Sorry if I sound a bit unclear as it is not my domain and I feel a bit lost. Don't hesistate to ask me to be clearer :) Thanks for reading and good evening (which makes no real meaning if not on Greenwich meridian) ! 
 I am fitting a GARCH(1,1) model to the data and want to look at the innovation distribution. I first generate the data and fit it to GARCH(1,1) model with t innovation. Suppose the fitted model is called then I can get the error terms by which is calculated as $(z_1,...z_n) = (\frac{X_1}{\hat{\sigma}_1},...,\frac{X_n}{\hat{\sigma}_n})$ However, if I only extract the parameters $\omega,\alpha$, and $\beta$ estimated by the package and calculate the error terms manually as $\sigma_t^2 = \omega + \alpha X_{t-1}^2 + \beta \sigma_{t-1}^2$ $z_t = X_t / \sigma_t$ The code is: I got very different results between these two approaches by comparing the Q-Q plot of and . seems to be normally distributed following the data generating process, while has a heavy tail following the model specification. I was wondering why they are so different? I arbitrarily chose 1 for for first two terms of $\hat{\sigma}$, so does the difference come from the initial values? And more generally, how does package fit the GARCH model and choose initial values? My understanding is that we need to find parameters using either QMLE or MLE and then find error terms iteratively using my second approach. But I am not sure how is the initial value chosen. Thanks! 
 $X \sim \mathrm{GG}\left(p,d,\theta_{1},\mu\right)$ where $p$ is power, $d$ is shape, $\theta_1$ is scale and $\mu$ is location parameter. Also Consider $Y \sim \mathrm{GG}\left(p,d,\theta_{2},\mu\right)$ where $p$ is power, $d$ is shape, $\theta_2$ is scale and $\mu$ is location parameter. Where, $f(x;θ_1,d,p,\mu)=\left(\frac{p}{\theta_1^d}\right)(x−\mu)^{d−1}e^{−[(x−μ)/θ_1]^p}/Γ(d/p)\;\;\; ,x&gt;\mu\;\mbox{ and } p,d,\theta_1&gt;0$ What is the distribution of $\;Z=\frac{X}{Y}$? All the papers I read so far only have the case without location parameter. Please help. Thanks 
 I would consider a Hidden Markov Model for the underlying state of the customer. Learn the parameters of the model based on the history of your customers, and then try to see if there is any feature in the hidden variables that is a predictor for the customer closing the account. Then finding the probability that your customer is in each one of the hidden states, given his/her past history of purchases, is a simple application of the forward algorithm. Easier said than done, but I find it feasible. 
 I'm working on alien reptiles on Mediterranean Islands. The idea is to assess which variables are influencing/favouring reptiles introductions, which reptiles are being introduced and which island are more vulnerable to this phenomena. In order to answer these questions, I have a database with several variables related to species (i.e. body length, allocthonous status), island (i.e. area, elevation) and human factors (i.e. population size). To relate all these variables I decided to use from the package, since it allows more than one random factor and because it can take spatial correlation into account. However, I tried to apply notation on it (as suggested in its tutorial) but it's not working. Here it is the code line: (Note on variables: , and are binary (0/1). is the name of each island.) The error I get: According to , clustered variables (species is within genus, genus within families), should be wrote as . However, apparently don't recognize that. Do you know the proper way? Another thing, when I run the code with independent random factors (although I think it doesn't make sense), I get another error: The matrix I use is a neighborhood type. I used this code to generate it: 
 When searching for p-value, I realized that the p-value for Pearson correlation can be calculated by simply knowing the correlation coeffiecient and the sample size, as illustrated in this website P-value calculator . I'm wondering whether you can do the same thing using Spearman correlation, and why? 
 I am currently conducting a longitudinal study of the growth of student test scores over time, and many of the districts in my study have switched to a new state assessment. The new assessment and the old one are obviously on two completely different scales. My question is, what is the best way to account for this new assessment, so that I can still correctly measure student test score growth over time? I was planning to simply normalize student test scores by transforming them into z-scores, but I don't believe I will have the state-level data required to perform this transformation. Are there any other studies that I could look at that have had to contend with this problem? 
 Suppose we replace the loss function of the logistic regression (which is normally log-likelihood) with the MSE. That is, still have log odds ratio be a linear function of the parameters, but minimize the sum of squared differences between the estimated probability and the outcome (coded as 0 / 1): $\log \frac p{1-p} = \beta_0 + \beta_1x_1 + ... +\beta_nx_n$ and minimize $\sum(y_i - p_i)^2$ instead of $\sum [y_i \log p_i + (1-y_i) \log (1-p_i)]$. Of course, I understand why log likelihood makes sense under some assumptions. But in machine learning, where assumptions are usually not made, what is the intuitive reason the MSE is completely unreasonable? (Or are there situations where MSE might make sense?). 
 I was learning MatConvNet and I was trying to create a super extremely simple 1D example. I wanted to implement the following NN: $$ f(x; \theta) = \sum^{K}_{i=1} c_i ( x v_i + b_i )_+ = \sum^{K}_{i=1} c_i max(0, x v_i + b_i )$$ I tried writing it explicitly using the library (instead of wrappers) but it didn't work: but I get the following error: when I check the dimensions of it does indeed have 3 units of activations I'd like to process. Thus, why can't I process them? I also tried to do it using wrapper CNN's with no success: I got a similar error: why is this happening? 
 I have a dataset, where individuals ( no repetition of individuals in the data) face different choice sets. I wanted to model the individuals choice using the mlogit function in R. All the examples on mlogit show that the individuals face the same alternatives (or same number of alternatives). But, as in my question each individual face different alternatives (and different number of alternatives), can I still use mlogit. How can I make use of the 'chid.var' option of mlogit for this? Thanks for your advice. 
 With standard errors being so wide, I'm not sure I would consider this model as 'converged' (the fact that different software agree is really just a numerical test, not a test of whether the model is actually unique or even useful). The problem with such large SEs is that parameter estimates are essentially interchangeable, in reality you have no idea where the true population values are (in your example, you can't even be sure the discrimination is positive!). The question should come down to how much do these items actually contribute to your test information to improve measurement precision. Discrimination parameters that are so large are essentially degenerate because they represent a perfect inflection at a given $\theta$ location but nothing more, while extremely difficult/easy items only provide information about individuals who have extreme $\theta$ values (which is generally very rare). They also tend to influence the quality of other parameters in the model, and therefore should be removed or given a strong prior parameter distribution to lessen their influence and keep them closer to a priori more reasonable values. 
 I am trying to do collaborative filtering for implicit feedback datasets by following the seminal paper: The section on ranking says: I have a matrix of 50K X 9K with each cell having no. of times a user has seen a video. I also have a binary matrix of the same (watched or not; 1 or 0) If I divide the dataset into 80:20 in training and test, and run the Recommender algorithm on 80% training, how do I evaluate the above mentioned ranking algo on the test in Python. I guess I am having a bit of trouble in understanding the algo's implementation. Any help? 
 I have been given a problem and need to find the best statistical method that would help. The problem is as follows: A business has approx. 100,000 active customers who spend between £500 per annum and £200,000 per annum. The smaller ones transact irregularly, once per year, the larger ones maybe once per week. They are distributed across 15 different market sectors. The business has started to decline, and whilst we have implemented a process for making offers to customers once they have closed their account, it is thought this would be much more effective if we could predict which customers are likely to close, and make offers to keep them before they close. How would you go about determining which customers to make offers to? 
 I had the same problem. Given a set where each item has a probability and whose items' probabilities sum up to one, I wanted to draw a series of items efficiently, i.e. without sorting anything and without repeatedly iterating over the set. The following function draws the lowest of $N$ uniformly distributed random numbers within the interval $[a,1)$. Let $r$ be a random number from $[0,1)$. \begin{equation} \text{next}(N, a) = 1 - (1 - a) \cdot \sqrt[N]{r} \end{equation} You can use this function to draw an ascending series $(a_i)$ of $N$ uniformly distributed random numbers in [0,1). Here is an example with $N = 10$: $a_1 = \text{next}(10, 0)$ $a_2 = \text{next}(9, a_1)$ $a_3 = \text{next}(8, a_2)$ $\dots$ $a_{10} = \text{next}(1, a_9)$ Now while drawing that ascending series, parallel iterate over your set. For each drawn ascending random number, add the corresponding set item to your result. If you wonder about the $\text{next}$ function: It is the inverse of the probability that one of $N$ uniformly distributed random numbers lies within the interval $[a, x)$ with $x \leq 1$. 
 The short answer is that likelihood theory exists to guide us towards optimum solutions, and maximizing something other than the likelihood, penalized likelihood, or Bayesian posterior density results in suboptimal estimators. Secondly, minimizing sum of squared errors leads to unbiased estimates of true probabilities. Here you do not desire unbiased estimates, because to have that estimates can be negative or greater than one. To properly constrain estimates requires one to get slightly biased estimates (towards the middle) in general, on the probability (not the logit) scale. Don't believe that machine learning methods do not make assumptions. This issue has little to do with machine learning. Note that an individual proportion is an unbiased estimate of the true probability, hence a binary logistic model with only an intercept provides an unbiased estimate. A binary logistic model with a single predictor that has $k$ mutually exclusive categories will provide $k$ unbiased estimates of probabilities. I think that a model that capitalizes on additivity assumptions and allows the user to request estimates outside the data range (e.g., a single predictor that is continuous) will have a small bias on the probability scale so as to respect the $[0,1]$ constraint. 
 I wrote a book review of Sean Becketti's Introduction to Time Series Using Stata that you may find useful. I think this is as close to C&amp;T for TS as you're going to get. 
 Maybe a logistic mixed model where you predict whether a customer will close an account or not (that's the response or y variable) using what you know about the customers, like how much they spend, how many transactions they do and anything else you may know. You could include market sector as a random effect because it's a categorical variable with many classes that may account for baseline variation in whether a customer will close an account. If there are a lot of variables you could use to predict whether people will close accounts, you could also explore machine learning techniques too. Your goal seems to be prediction, so I would try a few techniques and see where you can get the best accuracy, without concerning yourself with ease of interpreting the model. 
 I have data values for events per second (EPS) present in log files pertaining to various devices. The idea is that these values should help us observe a trend and create thresholds for specific times during the day, and specific weekdays. If the values observed in the past hour exceeds the threshold, an alert is generated. What we have tried: We have tried using a trimmed mean method to average out the EPS values for specific subsets of time and day. However, the data is more chaotic than we thought and the thresholds are falling short such that we have a lot of false positives. So we are now looking at machine learning algorithms to see if they offer better performance. The idea is to fit a model to the past data and attempt to predict future values. Alerts are generated on noticing deviations from the predicted values coming out from the machine learning algorithm. We have looked at 'SVM', 'MLP', 'Neural Networks' etc., however we do not know which approach would work best since we do not have significant data science knowledge. Any recommendations are appreciated. Our data looks like this: Is there a tool (like Weka or libsvm) that would allow us to input this data, train the model, and make future predictions? 
 I would argue that it does not make sense, at least not in a Cox model. The reason is that with the main effect of age $t$, the hazard function would be \begin{align} h_i(t) &amp;= h_0(t) \exp(\beta x_i + \gamma x_i t + t) \\ &amp;=h_0(t) \log(t) \exp(\beta x_i + \gamma x_i t) \\ &amp;= h_0'(t) \exp(\beta x_i + \gamma x_i t) \end{align} Since $h_0$ is non-parametric in the Cox model, the "main effect" of age is confounded with the baseline effect of age. Now if age is your time scale, then age at baseline is 0 for everybody. So that's a pointless covariate. If your time scale is time since randomisation or something like that, then the age at baseline is different for the individuals in the data set and it makes sense to include it, as time constant (hence age at baseline ). Now say that your time scale is age since randomisation and you include age as a time-dependent covariate. You have $age_i(t)$ age at time $t$. The hazard you estimate is then \begin{align} h_i(t) &amp;= h_0(t) \exp(\beta x_i + \gamma age_i(t)) \\ &amp;= h_0(t) \exp(\beta x_i + \gamma(t + age_i(0)) \\ &amp;= h_0(t) \exp(\gamma t) \exp(\beta x_i + \gamma age_i(0)) \end{align} As you can see, the hazard ratio of two individuals will depend on age only through the age at baseline, since the first term with $\exp(\gamma t)$ cancels out. It is very useful to write down the hazard and think about what you want to estimate when include time-varying aspects outside the baseline hazard. That is, if you want to model the hazard in a proportional hazards model 
 I have few values of some data, how can I matematicly check which of those are not significant in comparision with the rest? Example values: Question is: How matematicly / statisticly prove that last 10,3452 is insignificant due to too big difference from the rest? (it can be some mistake for example). I don't know if I have to use t-test or something else, tried to find some on google, but anything suitable found. Thank you for answers. 
 I am a lung cancer advocate and volunteer event planner with a nonprofit group that raises funds for cancer research and patient access. We hold 5K walk/run events and I have been asked to develop an incremental set of registration and donation goals. What type of graph/function (i.e. exponential, power, log) would best be suited for our need? Your assistance would be greatly appreciated. Regards, Bob 
 everyone. I am studying the multinomial logit method and the thing is I can't understand equation 3.5 . Why the integral uses the CDF and not just the PDF? 
 do you know any package which includes a Chow-Breakpoint Test in R for Vectorautoregressive Models? I already looked into vars, tseries, strucchange (If I understand the documentation correct, then there is just a test for the univariate case), but I don't find anything useful. 
 Predictions from are not the same as predictions from link . You have reproduced the and are comparing it to the object. 
 [![enter image description here][2]][2]I have data which measures the time when a rat licks a sugar water. When I computer the inter lick interval (ILI), sometimes I see that ILI is less than 40 miliseconds which I don't think a rat can lick this fast, and I am assuming this is coming from a noise from the sensor. I want use fft in matlab to compute the frequency of my licking signal. What all I have is the time when the rat has licked sugar water. Can somebody please tell me how I can compute the frequency of my signal. Here is my code: and the results are shown too if X is the licking time is this code to compute the fft? 
 I'm asking this question on the statistics forum because I'm wondering a bit about drawing appropriate conclusions supported by statistics from skewed data (if it belongs on another area of stack exchange, I'll be happy to move it there). I'm working with a neuron model and attempting to understand spike timing and reliability across trials. The outline of the model is that I have a neuron whose voltage fluctuations from two sources: input from other neurons which basically increase or decrease the voltage of my neuron of interest as Poisson spikes (i.e. 80% of neurons increase my neuron's voltage and 20% decrease it, but they decrease the voltage by 4x the amount of the ones that increase in order to maintain some balance), and intrinsic noise in the neuron. The goal is to understand how these sources of noise affect the mean time til the first spike (i.e. the voltage of my neuron reaches a certain value) and the trial-to-trial variability of the first spiking times. A paper I'm looking at defines the mean latency (ML) as follows: $$ML = \left&lt;t \right&gt; = \frac{1}{N}\sum_{i=1}^n t_i$$ where $t_i$ is the first spike time of the $i^{th}$ realization. The "jitter" is defined as follows: $$J = \sqrt{\left&lt;t^2 \right&gt;- \left&lt;t \right&gt;^2}$$ I have my program set up so that I can create a histogram of first spike times, and then I can tell it to give me the mean spike time and the standard deviation of the values. However, the distribution of spike times is heavily skewed so I'm a bit wary of the conclusions I can draw from this (i.e. what parameters have effects on the first spike timing). I classified the simulations into categories: high noise vs low noise and high frequency vs low frequency (frequency refers to the rate at which the neuron receives input from other neurons) The numbers are as follows: High Noise/Low Freq =&gt; mean = 22.176, std = 21.127, median = 15.2 High Noise/High Freq =&gt; mean = 21.053, std = 20.9007, median = 15.15 Low Noise/Low Freq =&gt; mean = 71.4946, std = 78.2021, median = 47.53 Low Noise/High Freq =&gt; mean = 67.809, std = 68.659, median = 46.85 I also ran the simulations without any background neurons (so just the neuron of interest + intrinsic noise) and received the following: High Noise =&gt; mean = 21.59, std = 20.536, median = 15.4 Low Noise =&gt; mean = 72.471, std = 76.597, median = 45.875 I would like to know what conclusions I can reasonably draw about the effects of intrinsic noise and the effects of other neurons providing voltage input into my neuron of interest (and that I'm not inferring anything I shouldn't be). For example, it seems like the higher the intrinsic noise, the smaller the time it takes on average for a neuron to spike. It also seems like the intrinsic noise lowers the trial-to-trial variability (i.e. the jitter), but the skewness of the data makes me wonder how accurate that is since the coefficient of variation is close to $1$ for practically all cases. For example, if the mean in the first case at 20 and the std was 20 whereas in the latter case if the mean were 10,000,000,000 and the std was 80, it has a higher std but clearly the latter is a lot more stable from trial-to-trial. So I'm unsure what I can say about noise affecting jitter, and also what affect intrinsic noise has on the shape of the tail of the distribution. As for the frequency of neural input into my neuron of interest, it looks like the neuron decrease a little bit the time it takes to fire when it receives more and more input, but the time that it drops is quite small relative to changes in the intrinsic noise level. Thus, the data seems to suggest like intrinsic noise is actually an important feature for inducing quick and reliable spike timing, and that input from other neurons can also induce quicker and more reliable spike timing but not by as much as intrinsic noise levels do. This seems quite counterintuitive biologically because I would expect a neuron to fire as a result of spikes received from other neurons. While intrinsic noise can cause neurons to fire spontaneously, I'm surprised to see it doing so in such a well-behaved manner (i.e. low jitter), although as I stated before, I'm not sure if I can reasonably say that. Thus my question is: What conclusions are reasonable to make about the effects of intrinsic noise and firing frequencies on the trial-to-trial variability of first neural spike times based upon the data simulated? (I'm trying to focus more on what simulations are necessary to run to fill in missing pieces of the data rather than run so many simulations that I have a ton of data but only some of it is useful -- e.g. for medium noise, I'd rather focus on a few different frequency levels rather than try it for 30 different ones if the data is going to behave similarly). 
 lm(formula = Y~ cX + I1+I2+ cX:I1 + cX:I2, data =cracker) Y: the number of cases of the product sold during the promotional period X: the sales of the product in the preceding period, denoted by X. cX: the centered value of X. I1 = 1, if treatment =1, 0 if treatment =2; -1 if treatment = 3 I2 = 0 if treatment = 1; 1 if treatment =2; -1 if treatment = 3 Regression analysis - I got nonsignificant interaction of cX:I1 and cX*I2. Does it mean that the regression lines of Y in terms of X for the treatments have equal slopes? 
 I have a data set which includes 5 binary variables per row of data. I was planning on creating a logistic regression to use 4 of the variables to predict the 5th and measure the significance (if any) of each variable. Before I do that, I thought I'd look at more simple relationships between the 4 predictors. What technique can I use to do this? A correlation matrix wouldn't be suitable as they are binary variables. 
 If you have multiple raters provide continuous ratings for all items, then it would indeed be sensible to calculate and use the mean of all raters for each item in substantive analyses. There are even methods for estimating the reliability of this mean series (e.g., average score intraclass correlations; McGraw &amp; Wong, 1996). One warning I would offer, however, is that count data (which you seem to describe) is not always distributed normally. In the case of a highly skewed distribution, alternative methods for reliability will be necessary (e.g., a weighted chance-adjusted agreement index). If you have multiple raters provide categorical ratings for all items, the mean of these ratings will no longer be meaningful in most cases. However, you could still use a heuristic or voting procedure to aggregate ratings (e.g., use the modal category with some a priori tie-breaking procedure). Determining the reliability of the resulting aggregate will be tricky to estimate, but you could mirror the logic used for continuous ratings and compare the aggregate to all individual ratings and average the results. If, however, you do not have multiple ratings for all items, then it would not make sense to average the results in just the subset for which you do. In this case, you should just pick one rater to use for each item (either randomly or using some explicit rationale). In my work on nonverbal behavior, I have used all three of these approaches for different projects. When using expert raters on a very time-consuming task (Girard et al., 2014), we opted to use categorical ratings from individual raters for each item and use the most senior rater for those few items that were rated by multiple raters. When using non-expert raters on that same time-consuming task (McDuff, Girard, &amp; el Kaliouby, in press), we opted to use a voting procedure to aggregate the categorical ratings of multiple raters for each item. And finally, when using raters on a quicker and highly inferential task (Ross, Girard, et al., 2016), we opted to average the continuous ratings of six different raters for each item. References Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., Hammal, Z., &amp; Rosenwald, D. P. (2014). Nonverbal social withdrawal in depression: Evidence from manual and automatic analyses. Image and Vision Computing, 32(10), 641–647. McDuff, D., Girard, J. M., &amp; El Kaliouby, R. (in press). Large-scale observational evidence of cross-cultural differences in facial behavior. Journal of Nonverbal Behavior. McGraw, K. O., &amp; Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30–46. Ross, J. M., Girard, J. M., Wright, A. G. C., Beeney, J. E., Scott, L. N., Hallquist, M. N., … Pilkonis, P. A. (2016). Momentary patterns of covariation between specific affects and interpersonal behavior: Linking relationship science and personality assessment. Psychological Assessment. 
 Is there any function in Python similar to Squared Multiple Correlation (SMC) in R? What if I want to implement SMC in Python? Is the only way to do it just rewriting SMC into Python line by line? Thanks in advance. 
 I am running a RD fuzzy regression but I am facing an Error in 1:K : NA/NaN argument and Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 0 (non-NA) cases. Can someone help me? 
 I have a large data set with over a million products. The NLP results look like this: A random row (reshaped) looks like this: The dataframe (image) contains information derived from the tokenization of a single product (" Dipperwell Faucet with Drain Stainless Steel Bowl Inner Overflow Cup"). Is it possible to predict the word "faucet" given the NLP output. The ML algorithms I am familiar with usually are in a format Prediction~ Predictor 1+ Predictor 2.. etc.. In this case , the predictors are columns and the predicted value is a cell 
 How does h2o.predict calculate the probabilities of different classes ? In the randomForest package of R, the probabilities are calculated based on the number of votes. For example probability of being class B = number of votes for class B / total number of trees. I can't seem to understand how h2o's RF algorithm provides so much granular probabilities even with few trees 
 As per the comment by @Roland, there is no requirement for the response variable itself to be normally distributed in a linear mixed model (LMM). It is the distribution of the response, conditional on the random effects, that is assumed to be normally distributed. This means that the residuals should be normally distributed. Therefore, you can proceed with fitting an LMM and then check the residuals to see if they are normally distributed. Treating likert item responses as continuous data is a contentious topic - for example see here: Parametric tests and Likert Scales (Ordinal data) - Two different views This simulation study plays down the concerns. Clearly, with fewer levels in the likert scale there is going to be more of a problem. This presentation from one of the authors of the package for R seems to suggest that 10 or more levels is OK. So with a 7 point scale, there is a good chance that the residuals will not be normally distributed, in which case you can look at fitting a generalised linear mixed model for ordinal data - two such packages which fit these models in R are and 
 Suppose I have $n$ individuals which are all either male or female and for each of which we can determine some quantity $X$. I want to decide whether there's a statistically significant difference in $X$ between males and females. Bootstrapping is one possibility to arrive at a $p$-value. When I read about bootstrapping, people usually recommend picking $n$ individuals with replacement at random, such that on average each bootstrapping sample contains $0.632n$ unique individuals. At the firm I work for, people perform bootstrapping by picking $n/2$ individuals $without$ replacement. What are the advantages and disadvantages of these two possibilities? Which of the two gives me a better estimator of the "true" $p$-value? 
 Does anyone know if there is a generalization of convolutional network that works with 4D input tensors rather than 3D. Generally, as I understand it, convolutional network filters accept a certain span of x,y coordinates per filter and the entire depth of that span as well. I have an additional dimension that would need to be required within the filters for the input to make any sense. Is it possible to use a traditional convolutional network for this or is there an abstraction that works for n-dimensional data? 
 I am working through the exercises of a book (Bayesian Reasoning and Machine Learning) for machine learning but I got stuck (I do not understand the question). The following three variable distributions admit the factorization. $p(a,b,c) = p(a \vert b) p(b \vert c) p(c)$ All three variables are binary. The question is how many parameters are required to specify distributions of this form. It might be simple but I am confused by the word "parameter". In fact we have three different probabilites for success and their complements for non success. And of course we have 9 possible combinations: $p(a=tr,b=tr,c=tr) = p(a=tr \vert b=tr) p(b=tr \vert c=tr) p(c=tr)\\ p(a=tr,b=tr,c=fa) = p(a=tr \vert b=tr) p(b=tr \vert c=fa) p(c=fa)\\ p(a=tr,b=fa,c=tr) = p(a=tr \vert b=fa) p(b=fa \vert c=tr) p(c=tr)\\ p(a=tr,b=fa,c=fa) = p(a=tr \vert b=fa) p(b=fa \vert c=fa) p(c=fa)\\ p(a=fa,b=tr,c=tr) = p(a=fa \vert b=tr) p(b=tr \vert c=tr) p(c=tr)\\ p(a=fa,b=tr,c=fa) = p(a=fa \vert b=tr) p(b=tr \vert c=fa) p(c=fa)\\ p(a=fa,b=fa,c=tr) = p(a=fa \vert b=fa) p(b=fa \vert c=tr) p(c=tr)\\ p(a=fa,b=fa,c=fa) = p(a=fa \vert b=fa) p(b=fa \vert c=fa) p(c=fa)$ Can someone give me a hint so that I am able to understand what is meant with "parameters"? Thank you! 
 If my Weka confusion matrix looks like this: True positives for class B = 25. False positives for class B: 1 + 4 + 3 = 8. True negatives for class B: 9 + 15 + 15 + 50 = 89. False negatives for class B: 4 + 7 = 11. The true positive rate for class B: 25 / 36 = 0.694. Yet Weka tells me that the false positive rate for class B is 0.070. How? I cannot quite resolve this. My formula to calculate the false positive rate is FP / (FP + TN), i.e. 8 / 8 + 89 = 0.082. 
 To understand this proof, think very carefully about the difference between min and argmin. If the proof involved minimising using min then there would be no $\delta_0$ outside (or a minus sign). Argmin works somewhat differently. The first equality in the proof states that $\hat\delta=\delta'$ (say) where $\delta'\in\Delta$ and $E_0 (L(0,\delta'(X)))=\min_{\delta\in\Delta} E_0 (L(0,\delta (X)))$. (This follows from the definition of argmin.) Because $\delta'\in\Delta$, we can write it as $\delta'=\delta_0 -v'$ where $v'$ is the function associated with the minimiser $\delta'$. We can then write $v'=\arg\min_{v} E_0 (L(0,\delta (X)))$ where we write $\delta$ as $\delta_0 -v$ and remember that minimising over $\delta$ is equivalent to minimising over $v$ and then writing $\delta=\delta_0 -v$. This is why the second equality in the proof is true. 
 I am analyzing this MISeq microbial community data with Phyloseq and vegan R-packages from coral hosts and I would much appreciate some help and ideas as this is my first analysis of this kind. We have 10 samples of paired bleached and unbleached coral colonies (10 bleached corals and 10 unbleached corals for each spp) from 3 different spp., in 3 different sites, and those corals were tagged and re-sampled from the climax of the bleaching event and afterwards during the bleaching recovery in total in 4 time-points. So we have the variables: Species: 3 coral spp. Status: 2 levels: 10 bleached and 10 unbleached corals per spp. TimePoints: 4 time-points when these same corals were re-sampled Reef_Location: 3 sites I have 2 big questions, around the largest question that would be: "what is the best way to analyze these data": I was planning on running a PERMANOVA on with vegan package, but I am not sure whether to consider some of the variables nested or not. I have tried several combinations, after creating a distance phyloseq object with "Bray" distance called 'physeq.bray.dist', such as: I suspect there should be other ways to run more effectively this PERMANOVA, if whether to consider the sites as nested, and I am not sure as how to treat the repeated sampling on the same corals. Any suggestions?... Should I run other tests as well? Further we have 3 data sets, from bacteria, fungi and microalgae (so 16S, ITS1, TTS2). I am starting analyzing all separately, but would like to compare and see interactions among the 3 microbial compartments. What analyses should I be running for this?... 
 I'm looking for some help on how to approach this problem. Say I have two or more groups of people. Each group has characteristics and attributes. For example, say we have the following two groups: Group 1 (100 members) 10% own a car 45% have a college degree 80% female Group 2 (150 members) 30% own a car 50% have a college degree 35% female I'm trying to figure what test takes these three attributes and ranks them according to how differentiating they are. In this case, from an intuitive point of view, I would rank the attributes like this, starting from most differentiating at the top. female (since 80% is far greater than 35%) owns a car (since 30% is slightly greater than 10%) has college degree (since 45% is not very different from 50%) I've looked into chi-squared test of significance but that test involves comparing observations to expectations. Would I just assume each group has 50/50 proportion for each attribute and run the test against that? I've considered Cramer's V as a measure of association but would like your take on how to best approach this. 
 I have started down the path of using cox PH models to try to understand which variables are driving time-to-migration (event) in my study system. My system has two groups (A,B) of animals that migrate at different times (surv.diff). So, now I want to find out why one group moves earlier than the other. I have asked a couple people whether a CoxPH model will get me to where I need to go, and have had mixed responses. My data- I have daily data of all covariates and the response. Since we are only interested in the reasons behind the difference in timing, all individuals experience an event. However, sample size differs greatly between the two groups (73 migrate early, 8 migrate late). A little about the variables- when putting together a global model of my predictors (mostly time-covarying, so I'm using counting process-style format), I find that the PH assumption is broken for a few variables. For example, temperature has little effect in early fall, but as things progress into late fall, the effect increases. I've been told that in this circumstance, it is common practice to add an interaction with time for those variables, although Therneau's vignette ( https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf ) and this post ( Textbook approach to modeling non-proportional hazards in the Cox model ) advocates for something slightly different. However, I'm still not sure of the correctness of the overall direction I'm taking, so perhaps somebody can help me understand if I am indeed heading in the right direction or not. Thank you! 
 Let $x$ be a random variable denoting the result of a coin flip where $x = 1$ signifies heads and $x = 0$ signifies tails. $x$ will follow the Bernoulli distribution , which has one parameter, $p$, denoting the probability of $x=1$ (i.e. probability of landing heads). Now let's imagine there are two types of coins ($c=1$ or $c=0$) which may have different probabilities of landing heads. Let $b$ be a random variable denoting the result of a coin flip. How many parameters would it take to characterize the distribution of $b$ conditional on the type of coin $c$? 
 A simple way to approach this will be to cluster samples based on their means and then check the distribution of the variances across the members of each cluster. It will be dependent on whether you want to cluster using only the averages or use the entire distribution. These are two different questions. For the second part, you could use something like the bhattacharya coefficient or the Kullback-Leiber distance as the (dis)similarity measure. Bear in mind that the two approaches will give you different clusters.I think you can also specify different assumptions for the underlying variances in the mclust package. 
 True negatives for B requires both: predicted class $\neq$ B (i.e. result is negative) actual class $\neq$ B (i.e. negative result is true) Sum up every entry in neither column B nor row B and you get the number of true negatives: $$9 + 5 + 15 + 11 + 15 + 1 + 50 = 106$$ Then: $$ \frac{8 }{8 + 106} = 0.0702$$ 
 I'm new to theano and found LSTM tutorial's embedding word so confusing as lack of print function. in build_model function we have: I want just example, if our input x (one sentence with 2 word) is [1, 12], then each of this integers show a word in this sentence, in function above each word converted to one vector, right? but in which shape? if 1 converted to [0.1, 0.2, 0.3] and 12 converted to [0.4, 0.5, 0.6] then are these two vectors stand by each other like: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and fed into each LSTM block in hidden layer? The other question is the size (length) of converted vector decided by which variable? (i think it's decided by dim_proj but not sure). Thanks in advance for any help you can provide. 
 I have a data which has two underlying behavior. First there is a periodicity in it. It looks like a sine curve. Secondly the data points have constant growth in it. So, if I have 100 data points without any growth it will look like a sine curve. But due to growth rate in it. there is an increase in magnitude going from point 1 to point 100. I am not sure what is the right term to search for in google. Is there a method for this kind of data analysis? 
 I am doing some hypothesis testing on the results of an electrophysiology experiment. I have electrical recordings of the rate of fire from two different populations of neurons (let's call them neurons type A &amp; B - this my between subjects factor). Complicating things, I have recordings for each neuron both before and after the application of a drug (conditions PRE &amp; POST - my first within subjects factor) and during injections of different levels of current (levels 1-13 - my second within subjects factor). I am interested in determining whether the rate of fire of the two types of neurons are differently altered by the drug, regardless of the current level, i.e. I want to test against the null hypothesis that there is no interaction between the type of neuron (A or B) and the presence of the drug (PRE vs. POST) across current levels. After exploring the data, it was clear that the assumptions of an ANOVA are not being met. (Sphericity is definitely violated.) After looking around a bit more, I decided that the best non-parametric option for testing this type of mixed within/between subjects design was a permutation test. In particular I stumbled across the ez package for R, which seems to provide a very nice, intuitive interface for testing exactly what I want to test. Here is my code for example: Okay, great, seems to work. I get back a P-value for the various interaction terms, including the one I'm interested in. My question is this: how should I actually report this in a publication? I've got a P-value, and that's it - no statistic, no effect size, etc. Should I just report the P-value and the number of permutations? Or should I calculate some stat or effect size as well? I'm afraid that in my discipline (neurophysiology) use of these tests is infrequent enough that I am not aware of any conventions with respect to this. I've looked at other posts on this site, but they don't seem to address this seemingly (?) simple issue. Any help would be appreciated. 
 EDIT (in response to Stephan's comments): I was able to read a paper by Bessembinder &amp; Seguin (1992) on futures trading and stock price volatility, wherein they used the ARIMA model to decompose the error into two separate components (the seasonality component, as well as the random component). Is this possible to do on Stata? Specifically, how do I get the estimated values of the different components of the error, and then store them? Thank you very much in advance. 
 Let's say you repeat an experiment on two different days. On day 1 you get the following data: Day 1 Day 2 The basic idea is if that you do the experiment once, there isn't enough of a difference between the control and experimental groups to be statistically significant (p &lt; 0.05), but if you repeat the experiment you are confident that there is a difference. Because of this experimental normalization issue, just doing a simple t-test wouldn't show a difference between the groups because the baseline varies widely (in this example, the values on day 2 are "off" by a factor of 10). Therefore, how would you show that the control samples are statistically significant lower values than the experimental samples? Do I use a Mann-Whitney U test? 
 What are the practical ways to visualize multiclass multivariate data? For example data with 10-class, 649-dimension. 
 I want to obtain posterior distribution for parameters of a Dirichlet distribution $x = (p_1,p_2,p_3) \sim Dir(p_1,p_2,p_3; a_1,a_2,a_3)$ with uniform $P(a_1,a_2,a_3)$ and observed data $X=\{x_1,x_2,...,x_n\}$. What I have is: $$Pr(a_1,a_2,a_3 | X) \propto P(X|a_1,a_2,a_3)= \Pi_{i}^nP(x_i|a_1,a_2,a_3)$$ $$=\Bigg[\frac{\Gamma(a_1 + a_2 + a_3)}{\Gamma(a_1)\Gamma(a_2)\Gamma(a_3)}\Bigg]^n (\Pi_i^nx_{i1})^{a_1}(\Pi_i^nx_{i2})^{a_2}(\Pi_i^nx_{i3})^{a_3} (*)$$ How can we sample $(a_1,a_2,a_3)$ from this exotic distribution? Is a more intuitive distribution for $$Pr(a_1,a_2,a_3 | X) \propto \Bigg[\frac{\Gamma(a_1 + a_2 + a_3)}{\Gamma(a_1)\Gamma(a_2)\Gamma(a_3)}\Bigg](\frac{\sum_i x_{i1}}{\sum_i x_{i1}+\sum_i x_{i2}+\sum_i x_{i3}})^{a_1}(\frac{\sum_i x_{i2}}{\sum_i x_{i1}+\sum_i x_{i2}+\sum_i x_{i3}})^{a_2}(\frac{\sum_i x_{i3}}{\sum_i x_{i1}+\sum_i x_{i2}+\sum_i x_{i3}})^{a_3}$$ ? Is there a mathematical way to derive this? Empirically, it seems to give a good posterior for the distribution of $(a_1,a_2,a_3)$, but I cannot see how to derive it. Any help is appreciated. Note that the data $X$ is generated by the Dirichlet distribution, there is no multinomial distribution here. $X$ is a collection of tupples of ratios (the elements of each data points in $X$ is less than 1). 
 How should one analyze time series data that were collected during two seasons: season 1 and then at season 2, where during each season a measurement sample for several continuous variables were collected weekly for a period of several weeks, i.e. the data would look like this: Season 1 week1 week2 . . . week8 Season 2 week1 week2 . . . week6 The number of weeks isn't the same between two seasons. My question is how can I compare the differences between the two seasons on each of the variables? Thank you in advance for any comments/suggestions! 
 I have only the following summary information from which I have to approximate the integral $\int_xxf(x)dx$. As the last interval is an open interval, I think I cannot find the midpoint of that interval and use $\sum_xxf(x)$ to approximate the integral. So, what is the usual practice in this case? 
 I have about 20 failure records for a component of a single machine. They are from the beginning of installation to up to date(5 yrs). How can I use this data to do a 2 parameter Weibull analysis and find shape and scale parameters for the component? Can I make this as equal to failure records from 20 Machines? Is shape, Scale values change according to number of records, ex: instead if I use past 15 failures (not 20)? Help appreciated very much. 
 I have three waves of data, and I am trying to estimate group-based trajectories of binge drinking across the three waves. The question asked (at all three waves was): “Over/During the past 12 months, on how many days did you drink five or more drinks in a row?” Response categories were: 0=none; 1=one or two days; 2=once a month or less (three to 12 times); 3=two or three days a month; 4=one or two days a week; 5=three to five days a week; and 6=every day or almost every day. I am using the traj plugin in Stata, and it is limited to normally-distributed continuous variables, dichotomous variables, and zero-inflated variables. Technically, this is an ordinal variable with seven categories. So, it seems like I have two choices: (1) create a dichotomous variable at each wave; or (2) treat this as a count variable and use poisson regression. The latter approach yields much more detailed and seemingly accurate findings, but here is my question: Does poisson regression assume that the distance between the categories (counts) is equal? Also, does anyone see any problems with treating this as a count variable? The distributin of the data takes the form of many count variables I have seen (the distribution at one of the waves is shown below). Thanks. 
 I am trying to build a chatbot(using java). The things that i have done until now are: I have a dataset I have changed those sentences into words I have removed the frequent words like a,an,the etc. So the things that i have to do is: Train those datasets As i researched, i have to use Deep Learning for this. Can anybody here guide me through the training concepts and all that i need to do to train a chatbot. Codes or concept or both are appreciated. 
 There is a well-answered question here . But unfortunately, I don't even understand how the first equation in the answer is derived. Could someone help explain that? $$\text{Beta:} \quad \beta_{x_1} = \frac{r_{yx_1} - r_{yx_2}r_{x_1x_2} }{1-r_{x_1x_2}^2}$$ What is $r$ here? 
 I'm been trying out the anova function in rms. I fit some restricted cubic spline on my continuous IVs in my mode and when I use the anova function on my lrm object I get a list of wald tests. I'm curious as to how the total nonlinear test is obtained? Example taken from the package itself: What is the statistics for the total nonlinear term here? 
 Log-linear analysis is a well established, though in some fields largely forgotten, technique for investigating the relationship between multiple categorical variables. 
 If your data is a time series, you may want to look into triple exponential smoothing, also known as Holt-Winters' method. This can accommodate additive seasonality (where the seasonal amplitude does not grow with the upwards trend over time) and multiplicative seasonality. Here is the difference: This section in Hyndman's &amp; Athanasopoulos' free online forecasting textbook explains Holt-Winters. Here is the entire taxonomy of exponential smoothing methods, based on Gardner (2006, International Journal of Forecasting ) . To actually model such a series, extract trend, seasonal and error components and forecast, I recommend the function in the package for R . 
 The $p$-value is necessarily a function of the sample size. If you artificially half your sample, you will get $p$-values that are too large. If you have a small dataset, then the bias can be substantial. If your dataset is very large, then the difference may be small. If you feel uncomfortable about sampling with replacement, then you shouldn't, but if for organisational reasons there is nothing you can do about it, then you can consider the jackknife. This can be seen as an approximation of the bootstrap, but you don't have to tell them that... 
 From what you explain, you have multiple-seasonalities : intra-daily and intra-weekly. We have had previous questions on this. Searching here for "multiple seasonalities" will yield a couple of posts that may be enlightening. I'd recommend using the function in the package for R , which can deal with multiple seasonalities. I'd use the data up to but not including the data point you are interested in to train a model, then calculate a high quantile forecast, say 95%, then check whether the actual observation falls above that quantile. Let's run an example. We first simulate four weeks of hourly data with both intra-daily and intra-weekly seasonality and a slight upward trend and store this as a object (which stands for "multiple seasonal time series" - see ): Next, we fit a TBATS model. This step can take a while if you have more data, or it can even hang your R - if so, I'd recommend taking only the last few weeks of data. You can inspect the components of your TBATS model using : Now we calculate a mean and a quantile forecast using . Note that if we set , we get a 90% prediction interval bounded by a 5% and a 95% quantile forecast, which is what we want - since we are only interested in larger than expected outcomes, we will simply disregard the lower bound. Finally, we can plot the forecast, including the prediction intervals: As we see, the thresholds will be quite different during different times of day (and different days of the week). 
 It's nice to see academics freely distribute their works. Here is trove of free ML / Stats books in PDF: Machine Learning Elements of Statistical Learning Hastie, Tibshirani, Friedman All of Statistics Larry Wasserman Machine Learning and Bayesian Reasoning David Barber Gaussian Processes for Machine Learning Rasmussen and Williams Information Theory, Inference, and Learning Algorithms David MacKay Introduction to Machine Learning Smola and Vishwanathan A Probabilistic Theory of Pattern Recognition Devroye, Gyorfi, Lugosi Introduction to Information Retrieval Manning, Rhagavan, Shutze Forecasting: principles and practice Hyndman, Athanasopoulos (Online Book) Probability / Stats Introduction to statistical thought Lavine Basic Probability Theory Robert Ash Introduction to probability Grinstead and Snell Principle of Uncertainty Kadane Linear Algebra / Optimization Linear Algebra, Theory, and Applications Kuttler Linear Algebra Done Wrong Treil Applied Numerical Computing Vandenberghe Applied Numerical Linear Algebra James Demmel Convex Optimization Boyd and Vandenberghe Genetic Algorithm A Field Guide to Genetic Programming Poli, Langdon, McPhee Evolved To Win Sipper Essentials of Metaheuristics Luke 
 One standard way of modeling and forecasting time series with multiple seasonalities is the TBATS model (De Livera, Hyndman &amp; Snyder, 2011, JASA ) . It is implemented in the function in the package for R . 
Time series may exhibit multiple seasonalities, e.g., retail sales have intra-weekly and yearly seasonality, and electricity load (and price) has intra-daily, intra-weekly and yearly seasonality.
 Let's suppose I have simulated data (such as the example 1 from the Josh Day site), and I use the glmnet fit on this data. I want to compare the Beta obtained by the fit to the actual Beta, to check how the approximation did. How can I do that? On another note, is there any way to find out what's the bias of the Beta obtained? 
 When designing a randomized controlled trial, should a cluster randomized trial with multi-level treatment be conducted at one study site? For example if it is an evaluation of educational materials, should I randomly select one university then have the different clusters? 
 When calculating a deep neural network such as convolutional neural network, how do we count the layers. Do we only count the number of convolution and fully connected layers? or all of the layers including pooling, local contrast normalization, batch-normalization, scaler, etc layers as well? 
 My question for you guys is; I have a control group that are eating normal food and an experimental group that are only eating a western high fat diet. I wanted to look at how much the experimental group gained in % compared to the control group. However, as the control group will be 0, how can I go about and calculate the statistical significance of weight change between the 2 groups? Thanks 
 Can someone provide an example of 1 (or more) iteration(s) of Gibbs sampling for LDA using real values? I have been searching for a while and I can't seem to find any good examples. Thank you. 
 You could probably do a one-sample t-test, testing, whether zero is a possible candidate for the true mean of the experimental group or a Wilcoxon signed rank test. In R, this could look like this: Wilcoxon has the advantage, that you don't need to argue about normality in your data or big-enough numbers to drop normality assumptions etc. 
 This is a very interesting question I'm also searching for. Unfortunately, I couldn't find an answer yet. That's why I cannot help you with explaining the difference between Raw, AIC and BIC. However, I can help you with your initial question, which model you should choose. The AIC- and BIC-corrected tests are based checking model 1 &gt; model 2 while the raw tests model 2 &gt; model 1. So in your case, the Poisson regression should fit perfectly. 
 Use a simple statistical test for difference in means Difference in means . Unless I did not understand your question correctly this should work. If on the other hand you want to measure % difference in post-pre changes $$(\delta _{Treat}-\delta_{Control})/\delta_{Control}$$ where $$ \delta_{Treat}=x_{Post, Treat}-x_{Pre, Treat}$$that's more a problem with division by 0 than a statistical problem. The test of difference in means can still be validly applied to the $\delta$ quantities. 
 How to find unknown parameter $$ p(x)=\frac{c(θ)}{x^{θ}} $$ $$ θ &gt; 0 $$ And $$ X \in (0;{\infty}) $$ 
 I am implementing a recursive KMeans on large population set 1 Million Vectors, each vector has dimension of 1024. Each cluster K_i at level t gives birth to 2 clusters K_2i, K_2i+1 at level t+1 Is it possible that a centroid C_i for cluster K_i at a level t becomes a centroid again at a level t+1 for one of his children. Can we observe the same thing if we were using a recursive k Medoid. 
 I want to find Markov blanket for a dataset in R or python. The cause- effect relationship for the dataset has not been given. Is there way in R or python such that I can input my dataset and get as output, the Markov blanket for it. The original problem is based on causality. P.S - I have two training datasets, one with all the variables and one with only the target values for the variables. 
 I am looking for methods to cluster very small datasets. Almost all methods I have seen talk about how well they work on very large datasets. By small I am talking 5 elements, 20elements, maybe 50 elements. Particularly focused on 20 elements. Are there some standard methods I am not seeing? 20 elements is just about small enough that it would be viable to brute force it. It seems like it is certain to be able to use some method based on mixed integer programming. To give specifics about my particular problem I have, what I will call "models", and each set of models is about 20 elements. The models are what I want to cluster. I have about 3000 sets of models to cluster, each with about 20 elements. Each model is made up of two things: An ID (which links to other useful information), and a probability function. That probability function takes in some data and tells me how likely, according to this model, that data is. When using the collection of models, I assess the data with each of the models, and then chose the model that gives the highest probability as one that best fits this particular price of data. I initially start with a lot of models which are more or less random in their quality, but which are improved by a separate system to get better and better at modeling particular types of data (the type of data which they currently model best). Often the two (or more) models may become good at modelling the same data. So I want to use clustering to throw out duplicated. So I evaluated over a dataset all the models, and then use the results to determine my distence function between the models. I am currently investigating measures including Correlation between the sets of probabilities output for same point, and also the "Cost to replace", that is how much the total probability of all the data sets with this model being the best would go down if one of the other models was used instead. If when model $i$ is best, I could instead use model $j$ and not loose much probability, then $i$ and $j$ must be generally pretty similar (I have to make this symmetric by adding the transpose). I do not have (or rather do not want to use) an a priori data about the likely number of clusters? But given the maximum number of clusers is one per element, with K-* type clustering it really doesn't take too long to evaluate all values of K. I've been playing around a lot with affinity propagation and k-meniods. Just starting to play with hierarchical clustering now. 
 In the featureContribution function (in the package ), what should we interpret if all the scores for all the features are negative? For example, if there were 7 features and after running the function we got feature values as: Then what is the final interpretation? Is it something like all the features are not contributing to the prediction for this data point? The github code for the package can be found here . Please find a reproducible example to work with below 
 I am planning a study were I need to compare between questionnaire and accelerometer data, so basically I was thinking about pearson's and spearman's correlations (if you have another idea anyway I will be happy to try it, so feel free to say it) I think that a spearman correlation would be more suitable, but I was also thinking that I could use a linear regression to use the pearson's at the end I think I will use both but I would like to hear what people who understand statistic better than me think about it. 
 Unlike in the variable importance measures, feature contributions are computed separately for each instance/record and provide detailed information about relationships between variables and the predicted value: the extent and the kind of influence (positive/negative) of a given variable. For more reference please see Interpreting random forest models using a feature contribution method The paper can be also found in the describtion of the function from the linked package in the question post. 
 I have applied multiple linear regression over my data. And then I need to validate my model via the model validating techniques such as MMRE and PRED. For MMRE&lt;=0.25, we accept the model and for PRED(25) &gt;=0.75, we accept the model. I need to know the alternatives to MMRE and PRED. MdMRE, BRE, IBRE and Standard Deviation are its alternative, But I cannot find any such value for Standard deviation upon which I can say we will accept the model if the value of standard deviation is this and this. I have came to know that we can use Standard Deviation as an alternate to MMRE to validate the model. So What is the threshold value of SD on the basis of which we can say that the model predicted better than MMRE? As we say that if MMRE &lt;= 0.25, the model is good to be selected. So what is it for standard deviation? 
 I want to compute p-values after I used 'glmnet' to fit a lasso logistic model. Those p-values are adjusted for selection. The package tocalculate p-values for lasso in R is called 'selectiveInference', and it is very easy to handle. However, when I look at the manual of 'selectiveInference', there is no option to perform p-value calculation for logisitc (binomial) models. Here's the manual: https://cran.r-project.org/web/packages/selectiveInference/selectiveInference.pdf But, there are code examples on github that perform p-value calculations in the binomial case. Here are the examples (line 196 for binomial example): https://github.com/selective-inference/R-software/blob/master/selectiveInference/man/fixedLassoInf.Rd And here is my code: However, I always get the error: "Error in fixedLassoInf(x3, y, lasso.beta, bestlambda, family = "binomial") : unused argument (family = "binomial")" Does anybody know, if it is possible to use selectiveInference for a binomial model? If yes, what am I doing wrong here? This question is very important to me. So any advice would be superb. 
 I would like to find a similarity function $f$ between two values (each value is continuous and is bounded by $[0,1]$) that would have the following properties: $$ f(1, 1) = 0.5 $$ $$ f(0.5, 0.5) =0.25 $$ $$ f(1, 0) = 1 $$ $$ f(0, 1) = 1 $$ $$ f(0, 0) = 0$$ Is there such a function in math? If not how could I design it? 
 I have a dataset whose histogram showed above (the blue part), and I want to scale it for later machine learning process so I am trying to do a parameter estimation. Its histogram shows that looks like a gamma distribution, then I tried in matlab to get the fit curve(showed in orange). It's obviously that this curve does not fit the data very well so I decided to look for other distribution. I found a page on wikipedia called and there are just too many of them. One of my friends says that there is a general way to determine the distribution of certain data but she doesn't know how to do it. Is there any guidance to do that? 
 The function $$ f\colon [0,1]\times[0,1]\to[0,1], \quad(x,y)\mapsto \frac{1}{4}x+\frac{1}{4}y+\frac{3}{4}(x-y)^2 $$ does what you want. Plus, it's positive, symmetric and definite ($x\neq y$ implies that $f(x,y)&gt;0$). Neither it nor its root is linearly homogeneous like a norm-derived distance function, though ($f(\lambda x, \lambda y)\neq\lambda f(x,y)$) - but that does not seem to possible anyway given your requirements. I found it by estimating a linear model based on your input data, with covariates $x$, $y$ and $(x-y)^2$: 
 The gamma indeed already looks like a good bet, and your plot indeed shows that, in fact, it isn't. The way the red line lies to the right of the falling flank of the histogram indicates that you have a heavy tail, that is, more large values than would be expected from a gamma. So we should look for a distribution that is similar to the gamma (positive support, asymmetric etc.) but allows for a larger variance. The "related distributions" section of the Wikipedia article on the gamma distribution is a good place to start. It indicates the generalized gamma distribution as a, well, generalization of the "normal" gamma. It has three parameters, compared to the gamma's two, so it should be more flexible and be better able to fit your data. You appear to have enough data to reasonably estimate three parameters, too. 
 I conducted the Academic Motivation Scale. This instrument returned three types of motivations, namely extrinsic, intrinsic and amotivation. The latter motivations means the absence of motivation towards cetain activity. In other words, no motivation. The descriptive analysis shows that mean of extrinsic motivation (M = 5.15, SD = 1.13) is higher than the mean of intrinsic motivation (M = 4.72, SD = 1.31). In this regard, my research question is "What is the statistically predominant type of motivation among graduate students to pursue master's degree? Which test should I use to compare the means of these two motivation to examine possible difference? I quess that I could use paired sample t-test. However, from what I have read, this test compare means when the data collected in two times, before and after. In my case, I surveyed my sample at one time. 
 I am trying to get an estimate for the Drake Equation by simulation (context doesn't matter to this question) I have reached a point where I have a number of planets, $n_e$, and 3 probabilities; $f_l, f_i, f_c$. I want to take a sample from the distribution x~Bin ($n_e, f_l$). Then use this to get a sample from this distribution y~Bin ($x, f_i$). And finally get a sample from z~Bin ($y, f_c$). I know a shortcut is to simply take a sample from Bin~ ($n_e, f_l \cdot f_i \cdot f_c)$. But I was wondering why you can take this shortcut (assuming the 3 probabilities are independent)? Why can you chain independent probabilities like this, instead of doing each one consecutively, and using the result for the next one? Is this something that can easily be proven? 
 B = load('laser.dat'); n = length(B); X = B(1:n-50); %This is the Training Set lag = 50; Xu = windowize(X,1:lag+1); Xtra = Xu(1:end-lag,1:lag); %training set Ytra = Xu(1:end-lag,end); %training set Xs=X(end-lag+1:end,1); %starting point for iterative prediction [gam,sig2] = tunelssvm({Xtra,Ytra,'f',[],[],'RBF_kernel'},'simplex','crossvalidatelssvm',{10,'mae'}); [alpha,b] = trainlssvm({Xtra,Ytra,'f',gam,sig2,'RBF_kernel'}); prediction = predict({Xtra,Ytra,'f',gam,sig2,'RBF_kernel'},Xs,50); plot([prediction B(n-50+1:end)]); %plot the Prediction against the training set Try this code... 
 Below code is easier to read B = load('laser.dat'); n = length(B); X = B(1:n-50); lag = 50; Xu = windowize(X,1:lag+1); Xtra = Xu(1:end-lag,1:lag); %training set Ytra = Xu(1:end-lag,end); %training set Xs=X(end-lag+1:end,1); %starting point for iterative prediction [gam,sig2] = tunelssvm({Xtra,Ytra,'f',[], [],'RBF_kernel'},'simplex','crossvalidatelssvm',{10,'mae'}); [alpha,b] = trainlssvm({Xtra,Ytra,'f',gam,sig2,'RBF_kernel'}); prediction = predict({Xtra,Ytra,'f',gam,sig2,'RBF_kernel'},Xs,50); plot([prediction B(n-50+1:end)]); 
 You could also look at MUNGE. It generates synthetic datasets from a nonparametric estimate of the joint distribution. The idea is similar to SMOTE (perturb original data points using information about their nearest neighbors), but the implementation is different, as well as its original purpose. Whereas SMOTE was proposed for balancing imbalanced classes, MUNGE was proposed as part of a 'model compression' strategy. The goal is to replace a large, accurate model with a smaller, efficient model that's trained to mimic its behavior. There are many details you can ignore if you're just interested in the sampling procedure. The paper compares MUNGE to some simpler schemes for generating synthetic data. Basic idea: Generate a synthetic point as a copy of original data point $e$ Let $e'$ be be the nearest neighbor For each attribute $a$: If $a$ is discrete: With probability $p$, replace the synthetic point's attribute $a$ with $e'_a$. If $a$ is continuous: With probability $p$, replace the synthetic point's attribute $a$ with a value drawn from a normal distribution with mean $e'_a$ and standard deviation $\left | e_a - e'_a \right | / s$ $p$ and $s$ are parameters The paper: Bucila et al. (2006) . Model compression. Regarding the stats/plots you showed, it would be good to check some measure of the joint distribution too, since it's possible to destroy the joint distribution while preserving the marginals. 
 Q.In a survey conducted by a mail order company a random sample of 200 customers yielded 172 who indicated that they were highly satisfied with the delivery time of their orders. Calculate an approximate 95% confidence interval for the proportion of the company’s customers who are highly satisfied with delivery times. So I did an approximation using the normal (CLT) assuming each observation to be Bernoulli, and i guess assuming the mean and variance of the sum was unknown. So I used x_bar +- 1.96 * sqrt(sample_var / n) . The answers however say to use Binomial(200, p), estimating p and finding a confidence interval for that AND using Var(p) = np(1-p) so I guess they are assuming the variance is "known". I.e. they dont use sample variance, rather calculate sigma^2 from the binomial distribution after estimating "p=x_hat". From what I understand my method doesn't assume we know the variance, but does assume a distribution (Bernoulli). Assuming Binomial seems a bigger stretch though. Am I wrong? 
 It's hard to give a specific answer without the details requested earlier, but I think I can point you in the right general direction. First, let's consider a sample of n = 15 from an exponential distribution with a rate of 0.00029. When we run the ks.test, we fail to reject the null hypothesis, as expected. Now let's consider a case where n = 1,000, and the rate is still 0.00029. In this particular instance, we get a p-value of 0.9784. Again, we fail to reject the null hypothesis. Now let's look at something we're more likely to see in practice. When we take a sample, we usually have to estimate the parameters of distributions. So if your inter-arrival times come from a sample and you've estimated that the rate is 0.00029, that is only an estimate and doesn't tell us what the true population rate is. Why is this important? At a small sample size, you probably won't detect much of a difference between your estimated distribution and your population distribution. Let's assume that the population rate is actually 0.00030, but you've gotten a very, very close estimate of 0.00029. A difference of one hundred thousandth doesn't seem like much, does it? In a sample size of 15, we still fail to reject the null hypothesis (p = 0.8255). Now let's take a large sample of n = 1,000. In this example, even with such a small difference between the population rate and the estimate rate, we get a p-value of 0.07506, which is very close to that common 0.05 threshold of significance. In yet another sample of 1000, we can get a p-value of 0.008196, which rejects the null hypothesis at most significance levels. The moral of the story is that very small differences from the population parameter can be detected as "significantly different" given a large enough sample size. So failing to reject the null hypothesis in a large sample doesn't necessarily mean that your sample parameter or distribution is poorly fit. It only means that the KS test thinks they are significantly different. And as some of us are fond of saying, statistical significance is not the same thing as practical significance. 
 Question: Can you give any reasons/examples when it is more appropriate NOT to standardise continuous metric independent variables when performing multivariate analysis? Background: I am an undergrad business student with passion for data analytics and have taken a couple of statistics modules. I am also a self-learner in R with the help of Data Science specialisation on Coursera . This question has popped in my mind while revising for one of my statistics exams. From what I understand, it is ALWAYS better to use standardised data in multivariate analysis, for example: in linear regression, it helps to detect outliers, in logistic regression and discriminant analysis, it can help us to identify variables that best discriminate between groups, in factor analysis, we need it anyways to calculate factor scores, in cluster analysis, it is essential as different scales can dominate clustering (based on the distance method). Yet I often see my lecturers / lecturers in the Data Science specialisation using non-standardised data to perform all of the activities I mentioned. Can you please show me the limits of my understanding here and perhaps give examples when it is better not to standardise data? Please bear in mind that I do understand using non-standardised data for interpreting the model and drawing conclusions, but I don't get why people don't standardise data straight away when exploring it / fitting the first model. 
 Is there an ideal "inertia" for K-mean convergence. For example I'm trying to cluster to 64 clusters using sci-kit. the output is How do I Know this is the correct convergence? I guess the bigger question how do I know it converged to a global and not a local minimum? 
 Traditionally, there are three families of feature selection methods (filter, wrapper and embedded). For your problem, if you already know what classifier are you going to use, you can just wrap the feature selection within. Or in other words - use the training accuracy of your classifier directly, instead of a filter measure (MI, Gini, correlation...). A simple way of doing this would be: Split your training data into a train set and a validation set While there are untested feature subsets to test: Choose the next subset of features to test - next_ft Create a subset of train, using next_ft - subset_tr Build a model using subset_tr . Evaluate it using the validation set. If the performance of the model is the best one found so far, save next_ft as your current best feature subset - best_ft Use best_ft for your final model For generating the subsets of features to test, you can just test all possible subsets (if the number of features is not too large) or use some kind of heuristic (local search/hill climbing, Sequential Forward/Backward Selection (SFS or SBS), Genetic Algorithms... there are many possibilities) 
 If $X$ is a discrete and $Y$ is a continuous random variable then what can we say about the distribution of $X+Y$? Is it continuous or is it mixed? What about the product $XY$? 
 I want to run R Code on Windows IoT Core 10, is there any possibility for this? •I have tried to upload the R installation directory from my local PC to the Windows IoT core 10 device (Raspberry Pi 3) •Then from Power Shell I am executing following command Rscript.exe testscript.R but I get following error Program 'Rscript.exe' failed to run: The operation completed successfully. •CategoryInfo : ResourceUnavailable: (:) [], ApplicationFailedException •FullyQualifiedErrorId : NativeCommandFailed Is it possible at all to run R-Code on Windows IoT 10 core? Any help is appreciated please. I tried it from native command prompt on Windows IoT Core 10, and it gives error "The system cannot execute the specified program." So it looks like that it is not supported? Any suggestion/possibilities? 
 I can think of a few reasons. laziness : in many cases (such as linear regression or generalized linear models), standardizing variables changes only the interpretation of the models (specifically, the meaning of the parameter values), not the goodness-of-fit or inference (Z-scores/p-values/etc.). Standardizing also helps with numerical stability, but this is not generally an issue for simple (e.g. linear) models. So, depending on what you were doing, it might not be worth bothering. interpretability : when you standardize variables, you wash out their units, so you lose some ease of interpretation - "carbon flux increases by 2.3 G/m^2/year for every degree C of warming" may be more useful than "carbon flux increases by 4.7 ... for every SD" comparison across data sets/studies : if the model for each data set scales variables on the standard deviations observed within that particular data set, it makes it harder to compare results across data sets. Both of the latter issues can be partially resolved (at the loss of some comparability across parameters within models) by standardizing using round numbers to get parameters on approximately the same scale. 
 Let $X$ be a discrete random variable with probability mass function $p_X : \mathcal{X} \to [0,1]$, where $\mathcal{X}$ is a discrete set (possibly countably infinite). Random variable $X$ can be thought of as a continuous random variable with the following probability density function $$f_X (x) = \sum_{x_k \in \mathcal{X}} p_X (x_k) \, \delta (x - x_k)$$ where $\delta$ is the Dirac delta function. If $Y$ is a continuous random variable, then $Z := X+Y$ is a hybrid random variable. As we know the probability density functions of $X$ and $Y$, we can compute the probability density function of $Z$. Assuming that $X$ and $Y$ are independent, the probability density function of $Z$ is given by the convolution of the probability density functions $f_X$ and $f_Y$ $$f_Z (z) = \sum_{x_k \in \mathcal{X}} p_X (x_k) \, f_Y (z - x_k)$$ 
 Suppose $X$ assumes values $k \in K$ with discrete distribution $(p_k)_{k \in K}$, where $K$ is a countable set, and $Y$ assumes values in $\mathbb R$ with density $f_Y$ and CDF $F_Y$. Let $Z = X + Y$. We have $$ \mathbb P( Z \leq z) = \mathbb P(X + Y \leq z) = \sum_{k \in K} \mathbb P(Y \leq z - X \mid X = k) \mathbb P(X = k) = \sum_{k \in K} F_Y(z-k) p_k,$$ which can be differentiated to obtain a density function for $Z$ given by $$ f_Z(z) = \sum_{k \in K} f_Y(z-k) p_k.$$ Now let $R = X Y$ and assume $p_0 = 0$. Then $$ \mathbb P(R \leq r) = \mathbb P(X Y \leq r) = \sum_{k \in K} \mathbb P(Y \leq r/X) \mathbb P(X= k) = \sum_{k \in K} F_Y(r/k) p_k,$$ which again can be differentiated to obtain a density function. However if $p_0 &gt; 0$, then $\mathbb P(X Y = 0) \geq \mathbb P(X = 0) = p_0 &gt; 0$, which shows that in this case $XY$ has an atom at 0. 
 I have a doubt regarding standard deviation. I think the general formula for standard deviation should be like ( instead of general standard deviation formula ): ( │ X1 – Xm │ + │ X2- Xm │ + │ X3 –Xm │.........................+ │ Xn-Xm │ ) / n Or ( │ X1 – Xm │ + │ X2- Xm │ + │ X3 –Xm │.........................+ │ Xn-Xm │ ) / n-1 I.e average of the absolute values of differences between data set values and mean. Above will give a better approximation of the deviation for mean. X1, X2, X3 etc. are the data values where as Xm is the mean value of this data set. 
 I would like to ask a question about Gower dissimilarity, I was wondering how Gower measure handle missing values in numeric columns, especially that Gower standardized each column based on the range of the same attribute ? I have read both details of functions daisy and gower.dist in R and their original source (chapter 1 of Kaufman and Rousseeuw (1990)) but I got confuse. https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/daisy.html I tried also to look at similar discussions in this website Gower distance and MDS: How to determine which variables count? but I did not find an answer. also imputing the data with a dummy/mean values are not an option for me, I need my data as it is. my data is students' exams marks. 
 I ran xgboost with below parameter setting: But when i predicted the output it is giving double the rows as in test data. So I assume, first set of rows are for class '0' and other set of rows for class '1'. But the probabilities all look same: For '0': For '1': How do I go about interpreting the results? Or am I doing something wrong? Also train-merror keeps on changing as I change the nrounds. For nrounds=25 if, the minimum is at 15th round and I reran it with 15 rounds, it shifts again. How can I tune this parameter? Ok, this seemed to be the error: eval_metric = "merror", it should have been logloss for binary. 
 t-test should do your job. As a driver for further education, external motivation has a mean $\bar x_{extrinsic} = 5.15$ and internal motivation $\bar x_{intrinsic} = 4.72$. Findings from your sample indicates that students pursue master's degree motivated more extrinsically than intrinsically. Given the sample findings, you can test the hypothesis ($H_0: \mu_{extrinsic} \leq \mu_{intrinsic}$ ; $H_a: \mu_{extrinsic} \gt \mu_{intrisic} $) if the findings are significant for the entire population (all students pursuing master's are motivated more by extrinsic factors than intrinsic ones). Note that one-tailed t-test needs to be used because you are interested in whether one mean is larger than the other, not just in whether they are unequal. Consider the following t-test assumptions and investigate if your sample holds. Continuous variable, i.e. , extrinsic and intrinsic scores are measured as quantitative variable Each observation is independent of other observations Variable has a normal distribution (plot histogram of extrinsic scores and intrinsic scores to see if the distribution is normal) 
 Let us think of a simple case of sequence prediction. Based on 20 observed items, b was observed 16 times, c 3 times, and d only 1 time. The sequence is as follows: We try to predict the next item. We do not know anything about the independence of the items. Based on observations, P(b) = 16/20 , P(c) = 3/20 , and P(d) = 1/20 . Also, P(c|d) = 1.0 . Where the distribution P(X) is based on 20 observations, the distribution P(X|d) is based on only 1 observation. The amount of evidence strongly differs. How should we weight them in prediction? For example, let us observe one more item, d : How should we now predict the next item? By relying only on P(c|d) , we would predict c with probability 1.0. However, b is more probable apriori: P(b) = 16/21 = 0.76 , P(c) = 3/21 = 0.14 , and P(d) = 2/21 = 0.10 . This is related to this other question and seems to be somehow related to multinomial logistic regression . However, I cannot get my head around it, especially in the context of sequence learning. How should the weighting be done? Is logistic regression the correct approach? What mathematical concepts relate to it? Would there be any references that discuss the matter? 
 I'm having some issues in using new function "geom_forecast()". As an example, if I try to fit an auto.arima model to mdeaths time series using: It works fine and I get: While trying to employ a nnetar model using: I receive the following error: "Error in eval(expr, envir, enclos) : object 'ymin' not found In addition: Warning message: In data.frame(x = as.numeric(time(model$mean)), y = data[, 1], level = rep(-Inf, : row names were found from a short variable and have been discarded" What's wrong? Thanks in advance. 
 I have trained a Random Forest model in R with the package but the results are not very promising. I have decided to try with SVM models but I have a great dilemma: Would it be acceptable to use the "ranking" of important variables given by the Random Forest model ( function) to train SVM models with different number of predictors (based on this ranking)? I supose that this Variable Importance is specific to the Random Forest model but I would like to know if it would make sense to apply this ranking to other models like SVM. 
 Think of three row-vectors of $n_e$ cells, which can contain a 1 or a 0. I'll refer to these as our source rows. In the first row cells have a $1$ with probability $f_l$; in the second row cells have a $1$ with probability $f_i$; in the third row cells have a $1$ with probability $f_c$, and otherwise any cell will be $0$. Now we add a fourth and fifth row, which are computed results under two schemes. Under scheme 1, we cross out (ignore) the $i$th column of the result if the first row has a 0 in position $i$. Then in turn, if the second row has a 0, we cross out (ignore) those columns as well. We then copy the columns of the third row down to the result row, but only for the columns that are not crossed out. We the count how many $1$'s are in the scheme-1 result row (i.e. that are neither crossed out nor 0). This is the first scenario you described. Under scheme 2, the $i$-th result cell will be 1 if all three source cells above it are 1 (this happens with probability $f_lf_if_c$). We then count the $1$'s in the scheme-2 result row. This is the shortcut you described. Now the 1's in the scheme 1 result row are in exactly the same places as the 1's in the scheme 2 result row; the number of 1's in each case is therefore always the same. 
 For the probabilities that "look the same", either there is not enough information in your data to better distinguish the two classes, and thus the prediction is mostly based on the number of instances rather than on the features informations, or you do not give enough power to XGBoost to fit the dataset. From your question, I think the latter is more likely. In order to pick more meaningful parameter values, you need to read a bit about what XGBoost is doing and what it means to tweak each parameter. From your code, and without more information, some of your parameter choices seem odd. I would advise you to stick to the defaults , analyse your results (bias/variance decomposition) and work from there. Some comments on your parameter choices Eval metric The evaluation metric you choose, , is the multiclass classification error rate. Since you have only two classes, it seems weird to use this one. Moreover, the error rate is not a stable error measure (it can jump from a very small change), so this may explain why your is all over the place. Pick the traditionnal , unless you have good reasons to do otherwise. Tree Depth (, , ) This is gradient boosting, not random forests. You should not use few, deep uncorrelated trees but lots of very small trees. On the max depth, the parameter doc recommends a depth of 6, contrasting with your 15. Note that the number of nodes is $2^{\text{depth}}$, so the difference in complexity is huge. I would go so far as starting with a depth of 1-3, and change it only if you have good reasons to do so. The same goes for subsampling - the goal of subsampling is to reduce variance of the model, leave it to the default (no subsampling) unless you have good reasons to do otherwise. Learning rate (, ) The learning rate () defines how "quickly" you learn. A higher learning rate mean learning faster, more rough distinction between your two classes whereas a smaller means learning more slowly and carefully. Your value of is really small, and with only 25 trees, you are not able to learn something useful. For the number of rounds, you should aim for your XGBoost to stabilize at some point in the process (not using the number of rounds as a regularization parameter), so push it until the last 10% of rounds seems useless. 
 I'm trying to recalculate the REML log-likelihood given by the function from a linear mixed model with one random effect. I use the function in the package and the sleepstudy dataset from the package. My calculation : Does anyone know what am I doing wrong? EDIT Using the appropriate function from package to obtain the density for the multivariate normal distribution, I get : It is not always the right solution. 
 I am trying to use the PLS library in order to run Partial Least Squares Regression. I imported my data from MATLAB with 'R.matlab' library. One of my matrices I managed to insert to a dataframe quite easily since it was a 1D vector. It showed as my_1d_matrix ... num [1:205, 1] 124 138 38 76 155 ... My other matrix however is 205x4096 . And it shows like this in my workspace: my_2d_matrix ... Large list (205 elements, 6.5 Mb) How can I insert this as my second variable in the same dataframe? From the PLS introductory pdf , I see that the 'gasoline' dataset they use, consists of 'octane' (a 1d vector) and 'NIR' (a 2D large matrix), so I want to do the same with my data. Therefore, I thought that I should have the same format for my 2D matrix as this: which is: So the end result should be a dataframe similar to 'gasoline', where the first variable is my_1d_matrix and the second one is my_2d_matrix . 
 On what condition is the significant chi-square acceptable? I have checked all other indices meet the requirements, but the chi-square is significant. 
 Is it possible to perform embedding of a dataset that contains only categorical values e.g. n f1 f2 f3 1 2 1 3 2 2 2 3 3 1 3 2 4 1 4 1 where n is an object and f1, f2, f3 are features. 
 Let $L=1$ if a planet develops life and zero otherwise, $I=1$ when the planet develops intelligent life and zero otherwise and $C=1$ when is releases signals and zero otherwise. Assume the the outcomes for $L,I,C$ are independent. (note that in case of independence it could be that for some outcomes $C=1$ while $L=0$ but then no signal will be sent because life didn't even develop). Then, by definition $f_l=P(L=1)$, the probability that $L$ is one, and similar $f_i=P(I=1)$ and $f_c=P(C=1)$. The probability that a planet sends a signal is then $P((L=1) \ \&amp; \ (I=1) \ \&amp; \ (C=1))$ (note that if $(L=0) \ \&amp; \ (I=0) \ \&amp; \ (C=1)$ then no signal can be sent because no life has developped). $P((L=1) \ \ \&amp; \ (I=1) \ \&amp; \ (C=1))$ is, because of the independence assumption $P(L=1) P(I=1) P(C=1)=f_l f_i f_c$. So the probability that among the $n_e$ planets there are $x$ that send signals is given by the binomial density $Bin(n_e;f_l f_i f_c)$ i.e. $P(X=x)$ where $ X \sim Bin(n_e;f_l f_i f_c)$. 
 Classic auto-regressive models can handle cycles! Going way back, Yule (1927) and Walker (1931) modeled the periodicity of sunspots using an equation of the form: $$y_{t+1} = a + b_1 y_t + b_2 y_{t-1} + \epsilon_{t+1}$$ Sunspot activity tends to operate on 11 year cycles, and though it's not immediately obvious, the inclusion of two auto-regressive terms can create cyclic behavior! Auto-regressive models are now ubiquitous in modern time-series analysis. The U.S. Census Bureau uses an ARIMA model to calculate seasonal adjustment. More generally, you can fit an ARIMA model which involves: $p$ order auto-regressive terms (as above) $q$ order moving-average terms $d$ differences (to get the data stationary) If you dive into the math , there's a relation between ARIMA models and representations in the frequency domain with a Fourier transform. You can represent a stationary time-series process using an auto-regressive model, moving average model, or the spectral density. Practical way forward: You first need to obtain a stationary time series . For example with gross domestic product or aggregate consumption, people typically take the logarithm and compute the first difference. (Basic idea is that distribution over percent changes in aggregate consumption is invariant across time.) To obtain a stationary time series $\Delta c_t$ from aggregate consumption $C_t$. $$\Delta c_t = \log C_t - \log C_{t-1}$$ Once you have a stationary time series, it's easy to fit an auto-regressive AR(n) model. You can simply do least squares. For an AR(2) model you can run the regression. $$ y_{t} = a + b_1 y_{t-!} + b_2 y_{t-2} + \epsilon_t $$ Of course you can get more fancy, but often simple stuff can work surprisingly well. There are well developed packages for time series analysis in R, EViews, Stata, etc... 
 Either paired t test or Wilcoxon signed rank test (when assumption about normal distributon is not met) should be used for related samples. Your sample is related, however you are right that most statistical books talk about related samples in meaning that results are from the same population in different moments. But paired t-test is relevant also to your case. try to find and study: Cha-Jan Chang J., Torkzadeh G. Perceived Required Skills and Abilities in Information Systems Project Management, "International Journal of Information Technology Project Management (IJITPM)", 4 (1), 2013, s.: 1-12. BTW: Before using t-test I suggest to check for normality with Kolmogorov-Smirnov test (n&gt;100) or Shapiro-Wilk test (n&lt;100) 
 I have the following situation: Consider a dataset that is comprised of a number of factor levels I want to obtain the number of true independent pairwise comparisons (no pseudo-replication) of a distance matrix between those factor levels. Ignoring reciprocal direction (so $\ A-B == B-A $ ), the number of possible pairwise independent comparisons should be exactly $\ 2 * n -1 $ out of all possible combinations $\ n^2 $ The problem : In addition to the pairwise comparison between each level, they are each individually element of another group that limits the total number of comparisons possible. So for instance: A level in column can only have pairwise differences with values that have a different level in column So the pairwise difference $\ 1-2 $ is allowed, but the comparison between $\ 1-4 $ is not (as both have the same level in column ) How do I obtain the total number of possible unique pairwise comparisons limited by membership to a group? Is there a clever way (in ) how to specifically subsample the allowed comparisons? Right now I am taking the super-diagonal (diag + 1) of a distance matrix with multiple permutations to approximate truly independent comparisons. For different group ownership I would need to implement another permutation step that takes a random element out of all values belonging to a group. This double-permutation seems somewhat inefficient to me... 
 I have a short question about simulation. In R, there are a lot of build in simulation tools such as to simulate from the Normal distribution. Why is it so that numbers in the range, say $[-1,1]$ are most likely to be generated and their frequency will be the highest? It makes sense that these numbers are more frequently under the $N(0,1)$ distribution. However, I don't understand how a complete pseudo random simulation algorithm takes this in account. From what I know, the algorithm starts with a draw $u$ of a uniform distribution on the unit interval $[0,1]$ and plugs this value into the inverse cdf $\Phi^{-1}$ to obtain a simulation from $N(0,1)$. If I do: then the histogram is quite 'flat', so why does one end up with a high frequency of simulated values in $[-1,1]$ and less in say $[2,3]$ in case of the standard normal distribution? There should also be a lot of 'extreme' values? Thanks! 
 Will the signs of coef (Estimate) of lm and glm always be the same? ^ According to below toy example, it seems yes. Can you provide a case where they might be different? (If it matters in my real data the outcome is binary, hence used ) ^ Very much sounds like a dupe, found this similar post: Sign of coefficients in linear regression vs. the sign of correlation . Let me know if this is a dupe. 
 Let's say I want to measure the width of a road using a measuring tape. I've drawn a crap diagram here: Rather than measure the distance directly, I decide to be obtuse and measure the distances a and b in turn. I measure a 3 times and b 3 times, getting the results below: measurement no. a b ----------------- ----- ----- -- 1 1.5 3.5 2 1.7 3.3 3 1.6 3.4 Can I not pair my results to say that I have 9 measurements of the road width, x? If not, what is the problem with doing so? I have some supplementary questions about the implications of the statistical analysis of the measurement which I would like to ask, but this is the crux of my question. 
 I did find that $se(\hat y_h) $ is defined as followed: $$ \textrm{se}(\hat{y}_{h})=\sqrt{\textrm{MSE}(\textbf{X}_{h}^{\textrm{T}}(\textbf{X}^{\textrm{T}}\textbf{X})^{-1}\textbf{X}_{h})} $$ Which shows that the two formulas mentioned in my question are equivalent. I also found the following for approximating 95% prediction intervals: $$ \hat y \pm 2\times RMSE$$ Referenced Here Using only the Root-Mean-Square Error allows for a fixed prediction interval width that's conservative, however, you are ignoring the estimation error. Thanks for the help. 
 I'm performing some quantile regressions of a response variable (a ranking difference) over 14 predictors that describe each element of our ranking. Initially I succeeded in performing such regressions using the from in Python and then, for curiosity, I started performing the same regressions in R (using the library). I've obtained the same models regarding the coefficients returned, but the p-values estimated are much different. Just to compare, in R I've tested all options in to compute confidence intervals/pvalues (rank, iid, nid, ker and boot). I've noticed that the nid and boot results were more similar to the ones obtained from Python. However, the R coefficients are quite higher yet (for example, I get a p-value about 0.0001 from Python while using the nid or boot methods in R, I get p-values about 0.03, 0.06, and so on). Considering what I've read until now, I've got that the boot method is one of the mostly recommended, but it does also has some options (xy, pwy, or mcmb). In python I'm using: In R I'm using: Here are my questions: Does anyone have an initial suggestion of using Python or R for quantile regressions? If so, why? Can anyone suggest some explanations of such differences between Python and R in estimating p-values? Finally, does anyone know of a tutorial that can help to choose the right configuration of quantile regressions in R (when to use each of such methods)? 
 I have a question about the acceptance probability of Richardson &amp; Green's RJMCMC split/combine move from their paper "On Bayesian Analysis of Mixtures with an Unknown Number of Components" . In their paper they give the acceptance probability of the split move as $\alpha = \min{1,A}$ where $A$ is given in equation 11. I completely understand where all those parts are coming from. Then they say "For the corresponding combine move, the acceptance probability is $\min{1,A^{-1}}$, using the same expression for $A$ but with some obvious differences in the substitutions". This "obvious difference" is not so obvious to me. For example, for the split move we have to simulate $u_1,u_2,u_3$ from Beta distribution. These then show up in the acceptance probability. For the combine move we do not have to simulate those variables, so do they appear in the acceptance probability? 
 Let's say we have a gradient-based optimization problem. Every step we could compute gradients with respect to some parameters. The dataset however might be evolving over time. So, one procedure of optimization would be, For dataset in time t, we optimize the function and keep all gradients. For dataset in time (t+1), we start with all gradients in the previous time-stamp but discard latest R gradients, and then use the new data to optimize the function. I'm wondering, is there any existing theoretical reasoning for "dropping out" gradients like this way? Thanks. 
 So I'm trying to do propagation of uncertainty for the first time (read: I'm a noob at this), and it's proving to be a challenge. I'm trying to estimate the uncertainty in the friction factor of the flow of molten sodium through a pipe. (Note: My reputation, or rather lack thereof, prevents me from adding the link I originally had here explaining the friction factor. If you're curious about this, google "Darcy friction factor" and look at the Wikipedia article. It's basically a way to figure out how big of a pressure difference exists in the pipe.) In terms of what I can actually measure, it comes out to $f=\frac{16\pi D\mu}{\rho \dot{V}} $ where $f$ is the friction factor, $D$ is the diameter of the pipe, $\mu$ is the viscosity of the molten sodium, $\rho$ is the density of the molten sodium, and $\dot{V}$ is the volumetric flow rate (volume of sodium that flows through the pipe per unit time). Wikipedia gives me analytic expressions for the variance of $f$ in terms of the others; so far, so good. Here's my first problem, and arguably the most difficult one: I'm not measuring $\mu$ and $\rho$ directly, I'm calculating those from correlations given in a government report (see citation below) based on the temperature $T$, which is what I can actually measure. Wikipedia can tell me the variance of $\mu $ and $\rho$ in terms of the variance of $T$ if the correlations I'm using were 100% accurate, but they're not. The correlations for $\mu$ and $\rho $ have their own uncertainty, which is given in the linked report as a percentage. For instance, here's the correlation given for the density of sodium as a function of temperature (Note that $T_c$ is equal to the critical temperature of sodium, 2503 K): My thermocouple gives a known uncertainty of about 1 K. Then the correlation adds a further uncertainty of 0.4% (since I'm always in the region between 700 and 1400 K). This extra 0.4% is above and beyond the uncertainty I get if I plug my 1 K from the thermocouple into the Wikipedia analytic formulas. How do I include that in my calculations? Second, $\mu$ and $\rho$ have a covariance. Since both decrease with increasing temperature, can I assume that the coefficient of covariance is 1 and be done with it or do I have to do something more complicated than that? Report citation: U.S. Department of Energy, Argonne National Laboratory, Reactor Engineering Division. (1995). Thermodynamic and Transport Properties of Sodium Liquid and Vapor (Report no. ANL/RE-95/2). Retrieved from 
 Since $PAC(K) = Corr(Y_t, Y_{t-K}|Y_{t-K-1}, ..., Y_{t-1})$, $PAC(1)$ is equal to $\rho(1)$, i.e. the autocorrelation between $Y_t$ and $Y_{t-1}$ (there are no observations between $Y_t$ and $Y_{t-1}$, since they are two consecutive observations). It is easy to see that $PAC(1)=0$. That's because if you compute the autocovariance function $Cov(Y_t, Y_{t-1})$, the two observations are not correlated if you have defined $y_t$ as $y_t = \phi y_{t-2} + \epsilon_t$; there is no correlation between $y_{t-2}$, $\epsilon_t$ and $y_{t-1}$. Thus, also the autocorrelation function $\rho(1) = \frac{\gamma(1)}{\gamma(0)}$ is equal to zero and the $PAC$ is zero too for the reason above. 
 A better link to the blog on "Y-aware PCA" is here . The authors of that blog have an R package that implements this and other approaches to conditioning variables before analysis. As noted in some comments, Y-aware PCA is related to partial least squares (PLS). It weights predictor variables according to their single-variable relations to the outcome variable, which is effectively the first step in PLS. It stops at that step, however, and does not continue through the entire orthogonalization process. One of the vignettes shows those authors' approach to the binary-outcome issue in their package: categorical/logical y’s are treated as 0/1 indicators That might not seem terribly satisfying. As this review on PLS puts it: ... applying a regression method designed for continuous responses to categorical responses or performing dimension reduction with survival data without taking censoring into account is unappealing, although it is reported to give good results in many cases. The review goes on to cite work in which Cox or logistic regression coefficients were used instead of linear regression coefficients for PLS. That might be a reasonable extension of "Y-aware PCA" to the binary outcome situation. 
 I'm Building a logistic regression model and one of my independent variables is very skewed at zero. How do you suggest that i deal with this situation? 
 I am analysing very basic underwater visual census community data. I ran a range of diversity indices over the data and I would like to report Simpson's Dominance D2 and Simpson's Evenness value, which is derived from D2 . I used the following code: However, I need to determine the mean across 4 samples, i.e. from 4 D2 values. During one of the samples, no species were recorded. Therefore, the D2 value R provides for this sample is . As this is not a nominal value, I cannot derive the mean of the 4 samples. Omitting the data is not an option. Does anyone have suggestions or solutions as to how I can determine the mean? After reading information online regarding the reciprocal indices, the maximum number = the total number of species in that sample. So, as no species were recorded, theoretically the maximum value is 0. Is this suitable? I am slightly apprehensive of converting to 0 as also, the lowest possible nominal value for D2 is '1'. 
 The term you're looking for is "trend and seasonality decomposition of time series". Google this. There are many approaches. If you're really have only 100 points then Fourier will not work very well. Yule-Walker based approaches may work better. There are also filter based approaches. For instance, Google band pass filters such as bpassm from Atlanta Fed. The idea's that you filter out different frequency components from the series, so that low frequency would be trend, medium frequency the signal, and the high frequency - seasonality etc. There's a full set of code in this matlab example . It takes you step by step through the process of deseasoning, it works quite well for economic data in my experience 
 I've taken an ML course previously, but now that I am working with ML related projects at my job, I am struggling quite a bit to actually apply it. I'm sure the stuff I'm doing has been researched/dealt with before, but I can't find specific topics. All the machine learning examples I find online are very simple (e.g. how to use a KMeans model in Python and look at the predictions). I am looking for good resources on how to actually apply these, and maybe code examples of large scale machine learning implementations and model trainings. I want to learn about how to effectively process and create new data that can make the ML algorithms much more effective. 
 We have data for products daily revenue of the company and we know the date when the company started selling a product for the first time. We need to measure the impact of a product on the company global revenue. I am thinking of comparing the revenue trend (time-series) before starting selling this product and after. Any idea which statistical measure/function I may use? How to approach this from a time-series analysis perspective? 
 I am a marine turtle researcher attempting to understand the effect of a harmful algal bloom on our turtle capture rates and the body condition (BC=mass/length^3) of captured turtles. Field techniques overview: we set a large net (455m long) in the water, in approximately the same spot every time. The amount of time the net remains in the water is recorded, along with the number of captures of two species (loggerhead and green turtle). We conduct these sampling sessions two days per month, year-round. Harmful algal bloom overview: the HAB consisted of multiple, concurrent events whose effects lasted from April 2011-August 2013. I am using capture session data from January 2005-December 2015. In order to understand both whether the HAB affected our response variables during the effects of the HAB and whether there are lingering effects after, I have separated the capture session data into three categories: Before (1/2005-3/2011), During (4/2011-8/2013), and After (9/2013-12/2015). Season (Spring, Summer, Fall, Winter, as defined by solstices and equinoxes) has a strong effect on the number of captures of green turtles. Should I be accounting for this seasonal effect as a random effect or as a fixed effect? Thanks very much in advance for any help you can provide! 
 I have a bunch of independent variables which are skewed and have negative and zero values. I am seeing a lot of suggestions of using cube root as a transformation. I want to ask what is the harm in using this instead: Sign(x)*log(1+Abs(x)) 
 I am using the GIST descriptor database which contains the following files and the dimension are which contains the ground truth neighbors from the database I am trying to apply k nearest neighbor search by converting first the real valued features into binary. Then, using the hammingDist(B_base,Query) function I get a distance matrix Dh of size I am unable to apply k-nearest neighbor search, for k nearest neighbors. Using the ground truth, what is the correct way to compare if the nearest neighbor indices are indeed the near neighbors? This is my approach and shall be of immense help if the correct way is provided. The problem is on how I can find neighbors from the ground truth and compare it with the neighbors obtained after sorting the distances? Thank you 
 I do not have knowledge in ML. After a little web searching, I found a reddit thread that lists the following books - all of which are legally downloadable for free. You can research the titles of your interest for details. Also comment if you find any of the books helpful (and why). Machine Learning Elements of Statistical Learning Hastie, Tibshirani, Friedman All of Statistics Larry Wasserman Machine Learning and Bayesian Reasoning David Barber Gaussian Processes for Machine Learning Rasmussen and Williams Information Theory, Inference, and Learning Algorithms David MacKay Introduction to Machine Learning Smola and Vishwanathan A Probabilistic Theory of Pattern Recognition Devroye, Gyorfi, Lugosi Introduction to Information Retrieval Manning, Rhagavan, Shutze Forecasting: principles and practice Hyndman, Athanasopoulos (Online Book) Probability / Stats Introduction to statistical thought Lavine Basic Probability Theory Robert Ash Introduction to probability Grinstead and Snell Principle of Uncertainty Kadane Linear Algebra / Optimization Linear Algebra, Theory, and Applications Kuttler Linear Algebra Done Wrong Treil Applied Numerical Computing Vandenberghe Applied Numerical Linear Algebra James Demmel Convex Optimization Boyd and Vandenberghe Genetic Algorithm A Field Guide to Genetic Programming Poli, Langdon, McPhee Evolved To Win Sipper Essentials of Metaheuristics Luke 
 For tiny data sets, hierarchical clustering is the method of choice. The dendrogram visualization allows you to visually verify how well the data clusters, if there are outliers, how clusters nest , and how many clusters exist. 
 How to get the mean sum of products using R to populate an ANCOVA table as given in the following link? Analysis of Covariance Table for RBD Ryy, Tyy and Eyy can be obtained as follows using function. and similarly Rxx, Txx and Exx from How to obtain the mean sum of products Rxy, Txy and Exy? I need the error and treatment variance covariance matrix. Currently I am calculating this in a convoluted way using MSP of treatment and group totals. Is there an easier way to do this, especially when more number of variables are there (X1, X2, X3, X4) 
 Inertia is sensitive to evrerything. Scale, number of clusters, ... It does not allow for objective comparisons. The only value that allows absolute conclusions is 0: then every object is equal to a centroid. Finding the global optimum is too hard. You cannot do this on larger data and larger k. You have to rely on luck. Try running k-means multiple times, and see how different the values are. Beware to check all parameters. Many implementations have a threshold, and stop before converging, if the improvements get too small, or after a certain number of iterations. Then you don't even have a local optimum yet. 
 I have historical average temperature data for each day for about 30 years. I also have historical data for each hour population of certain kind of microbe. The microbe growth rate is about same as human population growth in the region. i.e. if I calculate the year over year growth of microbe population, it is same as human population growth in the area. But microbe population also is very positively correlated with temperature. Thus, I can use some form of fourier to fit the temperature model and hence get prediction for forecast period. Assuming I have a good forecast model for temperature, what is the best way to go about creating a microbe population model which understands the trend from microbe population growth itself but also accounts for seasonality variation from temperature. 
 Imagine data like this: 100 input nodes 5 output nodes Cost function: a scalar. So yeah, this is an optimization problem, not a "model". What I want to find is: given set of inputs what are the output values that minimizes my (empirical) cost function. Now, there are other ways to do this: I can try to estimate the utility function of each output and then use a regular old optimization framework to find the values of the output nodes (free variables) that minimize cost. But, I'm curious if I can set this up in such a way as to take advantage of neural networks and their ability to model non-linear processes because mine is quite non-linear and hard to estimate. That is, if i'm just trying to predict the cost, a neural network does a better job than a regression, but I would need to use a regression to have a convex optimization problem. 
 Here is the logic of the probabilities. The $n \choose k$ representation is hidden because I believe that is what you are going for. Try to work it out 1) What the probability that exactly two of the group of friends is chosen? 2 of the friends chose means $$P(Friend~draw1) = \frac{7}{35}$$ and then $$P(Friend~draw2) = \frac{6}{34}$$ and finally $$P(Not~ Friend~draw3) = \frac{28}{33}$$ This give a probability of $$P(2~friends~on~3~draws) = \frac{7}{35}\times\frac{6}{34}\times\frac{28}{33}$$ However you can draw {friend,friend,Not friend} or {friend,Not friend,friend} or {Not friend,friend,friend} there are $3 \choose 2$ ways of picking 2 friends out of 3. In general we can write the probability of picking from a group as $\frac{f}{f+n}$ and $\frac{n}{f+n}$ where $f$ and $n$ are the number of remaining people in each group respectively. $$~~P(friend,friend,Not friend) = \frac{7}{35}\times\frac{6}{34}\times\frac{28}{33}$$ $$~~P(friend,Not friend,friend) = \frac{7}{35}\times\frac{28}{34}\times\frac{6}{33}$$ $$+~P(Not friend,friend,friend) = \frac{28}{35}\times\frac{7}{34}\times\frac{6}{33}$$ Since multiplication is both associative and commutative these 3 probabilities are equivalent. Therefore we can write $$~~P(2 ~friends ~ on ~3 ~draws) = \frac{7}{35}\times\frac{6}{34}\times\frac{28}{33}\times 3$$ And this can be expressed in chooses as $$\frac{{7 \choose 2}{28\choose1}}{35 \choose 3}$$ 2) Probability that exactly none of the seven friends are chosen? For the first draw you have $$P(Not~ Friend~draw1) = \frac{28}{35}$$ on the second draw you now have 27 non-friends and 34 remaining students $$P(Not~ Friend~draw2) = \frac{27}{34}$$ and on the third draw you have again one less friend and one less student $$P(Not~ Friend~draw3) = \frac{26}{33}$$ So the probability of drawing 3 non-friend is $$P(3~ non~ friends~ out ~of~ 3) = \frac{28}{35}\times\frac{27}{34}\times\frac{26}{33}$$ This is the same as doing $$\frac{28 \choose 3}{35 \choose 3}$$ 
 In general if you have a small number of levels (in this case four) and you want inference to those levels then it is fixed. If you have a large number and you want inference to a more general population it is random. Another angle is to ask 'if someone else repeated it would they want to use the same levels?'. If yes then it is fixed. It is no doubt possible to find examples which do not fit my simple guide but it should get you started. 
 I have one data set with several variables and some of them have wide range. I want to predict sales from historical data. Sales from 0 to 6000000, epoch time like 1419656400 in seconds but I also have temperatures from 0-100 degrees. I think if I want to compare different statistical effects of different features, normalization of data might have a fair comparison. What else advantage or disadvantage of data normalization? Should I use same normalization method for all the variables? Thank you. 
 I have a time based data-set (similar to stock data) that I'm playing around with, trying to figure see if one of the variables I'm using in a regression should be shifted in time. To reiterate (if that wasn't clear) if I'm regressing the prices of paper, pens and graphite, I'm trying to see if using pricing data from different dates would be better. Rather than regressing each day's prices with each other, maybe I should use pen prices delayed by one week. In Excel I've been using the Analysis ToolPak to preform regressions, and display the data in a new worksheet. How can I enable the regression data to update dynamically as I change the input data? I also have Stata, would that be better for this? 
 I'm doing some time series modeling using R and the package, and found a minor difference I couldn't figure out. I'll reproduce my steps below. First, I generate some data. While I have "real" data, I'll just use simulated data so that anyone can reproduce them (it makes no difference). The generated data is divided into training and test sets. Then, I fit a model to my training data: Next, I calculate one-step ahead forecasts using the test set: For my purposes, a forecast horizon of 1 is fine. So, I should evaluate model accuracy comparing the one-step ahead forecasts -- given by -- to the test set (). I thought the first forecast value obtained using the original model () should be equal to the first point forecast using the model, since (I assume) both forecasts are calculated from the training data. However, they're different: Could anyone explain this difference, please? Am I assuming something wrong here? For what it's worth, this particular system has R 3.2.5 with version 5.4, but an installation with the latest exhibits the same behavior. EDIT 1: I had erroneously fit the model to the entire dataset, not just the training set. I corrected it above. EDIT 2: Stephan's answer below prompted me to dig a little deeper. gives forecasts past the end of the test set: So, it doesn't seem to be what I want (one-step ahead forecasts using observed data). The AR(1) model obtained using is $\hat{y}_t=0.8336y_{t-1} + 0.0462 + e_t$. I calculated by hand the first few forecasts using this model: With the exception of the first forecast, the numbers agree (assuming the differences are due to rounding). On the other hand, the first forecast calculated by hand (1.3565) is not too different from the first forecast given by , which is 1.3180. So, it seems that is what I'm after, I just don't understand why it gives a different value for the first forecast. EDIT 3: Rob's answer below mostly solves the issue. I'm still puzzled by the fact that the forecasts given by differ from those calculated by hand, and by a seemingly fixed amount: Can anybody shed some light on this? 
 Encode the problem: you have an urn with 35 white balls. You remove 7 balls, paint them blue, and put them back in the urn. You mix all the balls in the urn. If you draw 3 balls at random from the urn without replacement, what is the probability of getting exactly 2 blue balls? ($\approx 9\%$) The hypergeometric distribution is your friend here. The second question is similar: what is the probability of getting 3 white balls? ($\approx 50\%$) 
 There's a paper, Document Embedding With Paragraph Vectors , which does PV-DBOW doc-vector training simultaneous with skip-gram word-vector training and gets interesting results where word-vectors and doc-vectors can be meaningfully compared or even added/subtracted. Their mode is analogous to gensim's Doc2Vec mode. (Note that without , DBOW training does not need or train per-word vectors... so any you word-vectors see in the model are just the random initializations.) Other PV-DM modes () also create word-vectors that may be comparable to doc-vectors. (Note, though, that the "concatenative input" mode () never mixes candidate doc-vectors and candidate word-vectors into the same input slots. Thus they don't 'pull' against each other in the same weights/dimensions during training, and are unlikely to then be comparable as in the other modes. This mode also results in the biggest, slowest-to-train models and may only show benefits – if ever – with gigantic training sets.) However, there's not yet a lot of published experience about how to interpret these word-vectors and doc-vectors in the "same space", or how to make them more useful for certain purposes. 
 If we think back to linear models for a moment, we have Ordinary Least Squares (OLS) versus Generalized Linear Models (GLM). Without going too in-depth, it can be said that GLMs "improve" upon OLS by relaxing some of the assumptions, making it more robust to different types of data. The underlying training algorithm is also somewhat different; OLS minimizes the root mean squared error (RMSE) while GLMs minimize deviance . (I realize that RMSE is a special case of deviance). This allows us to build linear models based on, say, the gamma distribution, inverse gaussian, etc. My question is: does the same logic hold true for gradient boosted trees? Since we're working with tree based algorithms now, I'd think that it's not subject to the same assumptions/distributional restrictions as linear models. In the XGBoost package, for example, the default objective function for regression is RMSE. You can define a custom objective if you wish, but does it matter? Does it make sense to do so? In other words, can we possibly improve our predictive power by setting XGBoost to minimize deviance (say, of a gamma distribution) versus RMSE? 
 centroid is average of data points in a cluster, centroid point need not present in the data set whereas medoid is the data point which is closer to centroid,medoid has to be present in the original data 
 Can someone explain what identification means in the context of an OLS model? I have a fair grasp of the derivation using either the method of moments or by minimizing the squares, but am failing to grasp which part of this process corresponds to identification. Also, how does identification differ from estimation of the parameters? 
 One of the books that I would recommend is Introduction to Statistical Learning and it is free to download. This book is easy to follow with exercises in R. Another good one is Applied Predictive Modeling 
 Why rescaling/normalizing would not matter: At a mathematics level, rescaling your data will not affect a regression in the sense that the estimated coefficients would also be rescaled. In a sample of humans, if I regressed height in feet on: Arm length in feet Arm length in inches Either (1) or (2) would be the same in the sense that the estimated coefficient in case (1) would be 12 times the estimated coefficient in case (2). More broadly, you can also apply an affine transformation to a variable if a constant is included in a regression. Imagine two regressions: $$ y_i = a_1 + b_1 f_i + \epsilon_i \quad \quad f = \text{temp in Fahrenheit} $$ $$ y_i = a_2 + b_2 c_i + \epsilon_i \quad \quad c = \text{temp in Celsius} $$ We would have the coefficients linked by the conversion factor ($f = \frac{9}{5}c + 32$) between Celsius and Fahrenheit: $b_2 = \frac{9}{5} b_1 $ and $a_2 = a_1 + 32 b_1$. At a mathematical level, normalization doesn't really matter. But there are reasons to rescale/normalize! Some reasons to rescale, standardize, normalize, etc... Easier interpretation! Often times it can be easier to interpret whether a variable is big or small relative to its standard deviation. Standardizing a variable (i.e. subtract mean and divide by standard deviation) may have more interpretable coefficients. For example, it may be easier to interpret a one standard deviation increase in blood pressure rather than an increase of 10mmHg. Numerical properties Including very large and very small numbers in a regression can lead to computational problems. In technical terms, you may have a design matrix with a very high condition number leading to imprecise calculations when multiplying matrices and solving linear systems. In plain English, sh*# can hit the fan. Eg. If you're doing some corporate finance regression on big companies, maybe you want to measure revenue in units of millions of US dollars. 
 In the 2PLM, the discrimination parameter is on the logit scale and thus theoretically could take on any value from - to + infinity. I would not necessarily agree that discrimination parameters have to be above .5 to be acceptable; recall also that they can certainly be negative as well. Size of discrimination parameters will also depend on sample size, response patterns in the indicators, and the corresponding bivariate contingency tables. 
 it means that for your model two parameter estimates are somehow correlated with each other or one estimate is a linear combination of several other. This might be best understood in the context of contrast coding to test hypothesis in a meaningful manner. Suppose you had two groups that you wanted to compare, say a1 and a2, one comparison would be to consider the differences of a1-a2..this model is identifiable as you have two groups and you will fit two parameters. Another way to look at this question would be to say what the increase/decrease from a global mean . that is a1=a+b1 and a2= a+b2 .. now you have 3 parameters in this case a, b1 and b2 and the parameters are non-identifiable, as one parameter is redundant. However you can always fit the model using a linear combination that makes the model identifiable. This may not be the most clear answer and I will be glad if someone can either edit this or post an answer that better. There is also a much better answer on this stack overflow post What is model identifiability? 
 As the comment points out, $r_{ab}$ is the correlation between $a$ and $b$. This equation: $$\text{Beta:} \quad \beta_{x_1} = \frac{r_{yx_1} - r_{yx_2}r_{x_1x_2} }{1-r_{x_1x_2}^2}$$ can be achieved by solving the following equation: $$ \beta_{x_1} + r_{x_1x_2}\beta_{x_2} = r_{x_1y} \\ r_{x_1x_2}\beta_{x_1} + \beta_{x_2} = r_{x_2y} $$ where $x_1$ and $x_2$ are two predictors and $y$ is the dependent variable. Solving this above equation set by basic linear algebra will lead to the first equation. Details can be found in Chapter 5 Multiple correlation and multiple regression of An introduction to psychometric theory with applications in R by William Revelle. 
 I have $N$ objects. From each object, I sample $M$ values $(x,y)$ like so: I can obtain the sample mean and the sample standard deviation from each object. I hypothesize that $x$ and $y$ are related. I want to estimate $\alpha$ and $\beta$ for $$y=\alpha+x\beta.$$ However, since $x$ and $y$ have both been measured with uncertainty, I want to use an error-in-variables model. According to this answer , I can use principal components analysis on the covariance matrix of my data to estimate $\beta$. How do I form the covariance matrix of my data? 
 If we know the raw test score is 75, mean is 50 and standard deviation is 10, how do I answer the following question? Step 2: Determine the characteristics of the comparison distribution. The comparison population with M=50 and SD=10 reflects a ___________ distribution. Can someone give the answer and explain how to get it? 
 I am going to take an approach similar to others, but I will focus on deriving the general case and then moving to the particular. Consider instead that you have $N$ people of which $n$ are friends and you pick an arbitrary number $i$ people. You might then ask what is the probability that $j$ of those $i$ people are from the group of friends? The number of possible states can be found by looking at $N$ from which you choose $i$. This should be straightforward, but in case you need the answer: $\binom{N}{i}$ We can then calculate the number of states where this is fulfilled. First out of the $n$ friends we pick $j$. Out of the non-friends we pick $i-j$. How many states is this? $\binom{n}{j} \binom{N-n}{i-j}$ Overall then what is the probability? $\frac{\binom{n}{j} \binom{N-n}{i-j}}{\binom{N}{i}}$ We can check that our expressions work by summing over $j$ to ensure that the probability sums to 1. $\sum_j \binom{n}{j} \binom{N-n}{i-j} = \binom{N}{i}$ I will leave it to you to plug in the numbers. 
 I am working on a classification program using SVM RBF kernel. To find the best parameters C and gamma, I used grid search, and got the image below. What confuses me is that when gamma varies from 0.3 to 3, the accuracy changes so rapidly. I wonder what happens in this region. I think the good models should be found on the diagonal, of a higher C with a lower gamma , or a lower C with a higher gamma. Could anyone help me to explain the performance variation when gamma is between 0.3 and 3? 
 Sorry for not providing a reproducible example. Next time I will definetly give more data insight to work with. However, I contacted the creator of selectedInference Mr. Tibshirani and he wrote that it is not possible so far to use the package with other models than linear. But they're working on providing selectedInference for other regression models, and it will be available very soon. 
 Hoping for some help related to a survival analysis using R and the package. I've been relying heavily on a series of blog posts done by Dayne Batten, particularly this portion: [ I've collected and merged the data as instructed using the function. My model relies on a cumulative time-dependent covariate, which strays away from the example provided. So my first question is does a cumulative covariate affect the validity of the Cox Regression? Here is my code at the moment: My second question pertains to the lack of an ID being assigned within this model. For each customer ID within this data I have up to a few hundred lines of events with my covariate. I don't see how this regression could possibly be accounting for that. 
 In colloquial English, identification means that with enough data, you will estimate what you are trying to estimate, that the true value of parameters can be uniquely inferred from data. 
 I am using the GLM Summary in R to determine the significance of the variables in the logistic regression model. I am trying to figure out if there is a way to flip the null and alternative hypotheses such that the null is that a certain variable is not insignificant in the logistic regression model and the alternative is that a certain variable is insignificant in the logistic regression model. This way, if I reject the null, I can accept the alternative with some level of significance. Is there a way to do this? Or is there some other test or model I can use? 
 My sample size is small (n= 28). I have 2 cohorts who have carried out the same interventions at different times. I have collected pre and post intervention scales for both groups. What I have not done is pair the data, as it was anonymous. This is part of a bigger qualitative study. Any suggestions which test I should use to see if there are any differences in the scale scores pre and post? 
 Say I have a relationship between two variables, that I have successfully fit using JAGS via the package for R. Below is the code to generate the data, and the fitted JAGS model. data: JAGS model: I can then predict as a function of , based on known values of , and parameter estimates from my JAGS model, like this: However, this is only the estimate of the mean. Using R, how can I go about estimating +/- 1 standard deviation on this mean prediction estimate, accounting for uncertainty in all the parameter values used to generate it? If this was a fit done in lm/glm I would use the function to generate +/- 1 standard error of the fit. 
 As many people, I've been following Andrew NG's course and I'd like now to work on a multilabel (short) text classifier with Keras/TensorFlow/Scikit learn. Please bear with me , I'm new to this and I know this is not exactly the kind of questions that we're supposed to ask here but where can I find a small full example (data + implementation) of a multilabel text classifier that I could run out of the box? I've been looking all over the internets and couldn't find something like this. I need a first example to modify, change features etc to fully understand how it should work. I'm sure this could be useful to many beginners ... Thanks for your time and patience ! 
 No, this is not possible. To show if a parameter $\theta$ (in your case a population effect on the log-odds) is exactly 0 would need an infinite sample size. You can perform a so-called equivalence test though. It works as follows: Before the analysis, you specify a range $R$ for the $\theta$ which you would judge as being "similar to 0", e.g. $R = [-0.1, 0.1]$ (depending on the meaning and scaling of the variable in question). Then you let the software compute a 90% confidence interval for $\theta$. If it is fully contained in $R$, then you can claim "similarity to 0" at a level of 95%. (It is somewhat unintuitive why a 90% c.i. is sufficient, but that's life.) Depending on your research question, this can be quite useful. Comment: Your sentence the null is that a certain variable is not insignificant is not very good. The word "significance" (in the statistical sense) can not be part of the formal definition of the hypothesis. It is an attribute of the data, not of the parameters. 
 I have a particular question of which I am unsure of the answer: "Using illustrative examples, explain the role of skewness of a statistical distribution in a two-tailed hypothesis test." My initial thought is that a skewed distribution is by definition not normally distributed so would mean that the test result is in accurate? Couldn't find anything in lecture material/notes, so thought I would ask the question directly. Any help would be appreciated. 
 It seems that three common assumptions for the using classical linear regression with OLS is that: Error terms follow a normal distribution Homoskedasticity Error terms are uncorrelated Can somebody provide me with some real life examples where error terms are correlated? 
 Sorry for the somewhat confusing title. I was wondering about the use of confidence intervals in the context of chemical and biochemical experiments. The experiments must be repeated and the data have to be independent, I know. But - say you are doing an experiment where you mix two chemicals, let it react under some conditions, and measure for instance yield or purity of the product. If I now repeat the experiment, I will usually take the starting materials from the very same batch. Or maybe they are purified to contain 100 % pure starting material. In this way, I can only see the calculated CI reflect uncertainties in concentrations, temperature, time etc. Will this be acceptable? If you had to take a blood sample of a mouse, of course you have to use several mice to get a CI. Here, all starting chemicals are identical. Another case is if I take two different and pure chemicals and test e.g. toxicity on some cells. Will repetition of the experiment mean that I have to order several different batches of cells? Will each cell sample be like if I took a mean value of thousands of mice in each and every experiment? BR Steffen 
 I have a multiple regression with two predictors and one criteria. The ANOVA says this model is not significant p=.069. But in the coefficient-table I can see, that one predictor is significant p=.024, what does that mean? 
 This can be done by estimating the parameter, the measurement of the precision of , when calling while sampling the JAGS model output. Modify the coda line to include : Then estimate the upper and lower bounds of y by addding or substracting , which is the standard deviation of the estimate. You can then plot a shaded confidence interval that represents +/- 1 standard devation with the following code: 
 For artificial neural networks (the kind employed in machine learning) there is no "dimensionality". As @user20160 notes, convolution nets are often presented in 2D to help us understand the operations of the network, but there is no position in space for any of the units, just connections to different parts of an image. In the website you link to, the neural network has a connectivity rule that is defined by unit-to-unit hop distances, meaning that there is an implicit 3D spatial location for each unit. To answer your question: I don't think there are any packages in R for this type of architecture (I haven't done an exhaustive search though). But, the NEURON or Brian simulation environments could potentially let you do it with simple integrate-and-fire units. As well, implementing it in R or python wouldn't be that hard - just define a point in space for each neuron and set a connectivity rule based on distance. I would note: whether such a design is useful is an open question. We see some patterns of connectivity the depend on distance in the real brain (see e.g. this paper ), so there may be a good reason to do it. As far as I know, though, no one has ever actually demonstrated a good reason from a machine learning perspective. 
 I have a dataset which includes sequence of DNA nucleotides (A,C,G,T) and each sequence has a gene index that is binary. i.e I'm trying to classify unknown sequence by using a hidden markov model in R. However, I have trouble with using HMM package. Actually, I'm planning to fit a hmm model for binary class 1 and 0. - I'm not sure what will be the states and symbols. - How should I give the transition and emission prob. matrix? By using the training data or the model finds it itself? - Probably the next step should be apply viterbi algorithm? If you help by giving an example R code I will be very happy! Thanks in advance. 
 I have a quantity of tabular data, let us say $E=200$ experiments, and $V=20$ variables (all positives). I am trying to find EDA-like dependencies or "correlations" between some of the variables, at least pairwise, without solid initial evidence. The number of potential crossplots is $V(V-1)/2$, which may become huge. So I want to visually explore this quantity of plots in a sound order. From the observation, I guess I have three common behaviors: Variables $v_1$ and $v_2$ are positively related: $v_1$ (globally) grows with $v_2$ along the experiments, Variables $v_1$ and $v_2$ are negatively related or antagonist: $v_1$ (globally) decreases with $v_2$, $v_1$ and $v_2$ seem unrelated, the point cloud is spread, without meaningful monotony. What are methods to arrange the 2D plots in an order, or best in clusters, according to a best match with one the three above loose classes? I initially thought about fixing two models (e.g. $v_1 \sim v_2$, $v_1 \sim 1/v_2$), and perform some hypothesis testing based on divergence to model fit, to create a third class and get clustering. As the field of model selection is huge, wise alternatives are welcome. I have started exploring CATS: Clustering After Transformation and Smoothing which aims at: CATS – Clustering After Transformation and Smoothing – is a technique for nonparametrically estimating and clustering a large number of curves. Our motivating example is a genetic microarray experiment but the method is very general. The method includes: transformation and smoothing multiple curves, multiple nonparametric testing for trends, clustering curves with similar shape, and nonparametrically inferring the misclustering rate 
 There are several tools for LDA. But I don't know which one is better and when? I wonder if anyone could guide me in choosing one toolbox. My priorities are ease-of-use and supporting various variations and extensions of LDA. Some but not all of the candidates are: MALLET (Java) gensim (Python) topicmodels (R) Stanford Topic Modeling Toolbox Matlab Topic Modeling Toolbox scikit-learn 
 I've been playing around with non-negative matrix factorization (NMF), just doing some example computation, and arrived at the following question: Suppose I pick two non-negative matrices $\textbf{W}$ and $\textbf{H}$ and set $\textbf{M} = \textbf{WH}$. When I do NMF on $\textbf{W}$ and $\textbf{H}$ separately, I'll get two new non-negative matrices $\textbf{W'}$ and $\textbf{H'}$ such that $\textbf{M} \approx \textbf{W'H'}$. I know the solution is not unique, but is there any circumstance in which I would expect $\textbf{H}$ and $\textbf{H'}$ or $\textbf{W}$ and $\textbf{W'}$ to be similar? Thanks! 
 One reason to avoid such a transformation is that it will make the interpretation of the regression coefficient very difficult. Moreover, there is no requirement for independent variables to be normally distributed, and as a rule you should avoid doing so unless there are substantive reasons for it, such as a known nonlinear relationship, to deal with heteroscedasticity, or to help interpretation 
 If we have X~U(a,0) where a&lt;0 what is maximum likelihood estimation of a ? I tried to find on internet but I could not find any resource about (a,0) interval, there is resources only for (0,a). Is MLE same for both intervals ? 
 The best practice is to not transform independent variables unless there is a substantive reason for doing so, such as aiding interpretation, dealing with known non-linearity or heteroscedasticity. There is no distributional requirement or condition for independent variables, so usually there is no need to do so. 
 I am considering a binomial experiment where each trial can only have two possible outputs (0,1). I vary the value of a predictor (experiment parameter) and repeat the experiment several times for each predictor value. Averaging the responses over trials for each predictor value to obtain the probability of outcome=1 as a function of the predictors, yields s-shaped curves like the blue and red curves below (depending on the experiment fixed conditions): I would like to test if the two curves are significantly different and quantify the bias between them. I tried to solve this issue in my way and it works, I would be very grateful if anybody could confirm this is the good way to do it or there is a better one. Furthermore I would really appreciate any advice on how to calculate the optimal sample size for this kind of analysis. First I will try to explain in a general way what I did and then I will show my MATLAB implementation. General idea I assume the relationship between a vector of predictors x = [x1,x2,..xi,xn] and the vector of success probabilities P=[P(Y=1|xi), 1&lt;=i&lt;=n ] could be modelled by logit link function as: P=1/ 1+exp[(b1+ (b2*x)]; (1) where b1 and b2 determine the logistic intercept and slope. To test if the difference in intercept or slope btw the two datasets is significant I will add two other parameters to the equation {b3,b4} that will depend on a constant 'I' that will be I=0 for the fit of the first dataset and I=1 to fit the second one P=1/ 1+exp[(b1+ (b2*x) + (b3*I) + (b4*x*I)]; (2) Therefore I will fit the first dataset obtaining a maximum likelihood estimation (MLE) of b1 and b2 and their corresponding p-values using the model in equation (1) and assuming inter-trials binomial variability. Then I will fit the second dataset with the model (2) by fixing b1 and b2 as constants (obtained from the first estimation) to get an estimate of b3 and b4 and their p-values. If b3 and b4 are significantally different from zero (pvalue&lt;0.05), then the two datasets yielding to the blue and red curves are significantilly different. If the p-value of b3 is &lt;0.05 and the p-value of b4 is &gt;0.05 The difference is the result of an horizontal offset. Therefore I can use eq.2 to fit again the second dataset this time removing b4 (eq3): P=1/ 1+exp[(b1_estimate+ (b2_estimate*x) + (b3*I)]; (3) Now b3 relate to the horizontal offset. To quantify this offset in predictor (x) units I have to divide it by the slope parameter offset = b3/b2 ; (4) Matlab implementation 
 I aim to forecast the SP500excessreturns using the rolling window option in Stata with the moving window of 120 observations (there are 500 observations in total). The code looks in the following way: However, the results do not seem to be correct as the path of the predicted variable appears to be almost flat (with real data). What could be the reason for this issue? 
 I've read a lot about PCA, including various tutorials and questions (such as this one , this one , and this one ). The geometric problem that PCA is trying to optimize is clear to me: PCA tries to find the first principal component by minimizing the reconstruction (projection) error, which simultaneously maximizes the variance of the projected data. When I first read that, I immediately thought of something like linear regression; maybe you can solve it using gradient descent if needed. However, then my mind was blown when I read that the optimization problem is solved by using linear algebra and finding eigenvectors and eigenvalues. I simply do not understand how this use of linear algebra comes into play. So my question is: How can PCA turn from a geometric optimization problem to a linear algebra problem? Can someone provide an intuitive explanation? I am a software engineer and not a mathematician, by the way. 
 If your data is event times (lick times), I would not look for 60 Hz electrical noise with an fft. (In fact, I don't know how you'd calculate an fft for event times like you have. There are probably ways to do it, but I don't know them.) Here's what I would do. First, are you sure your data is what you think it is? Specifically, what the heck is going on with your lick time histogram? Are there really 50 licks occurring between 0 and 10 ms? That is either one very fast rat, or your units are wrong in the x-axis, or you're combining lick times across many trials. If you're combining lick times across trials, are you calculating ILIs on this combined data? If so you may be, for example, measuring the time difference between licks on different trials, which is not what you want to measure. Assuming your data is good , and you want to assess whether there's 60 Hz electrical noise in your lick times, I'd focus on the ILIs themselves. 60 Hz noise has a period of 1 / 60 Hz = 16.6 ms. That means electrical noise will produce events spaced 16.6 ms apart, meaning a histogram of ILIs with strong electrical noise should have a peak at 16.6 ms. To illustrate, I simulated some data below. I simulated your data assuming it contains some events due to electrical noise and some events due to real licks. There are 1,000 "licks" occurring randomly between 0 and 10 s. There are also ~500 events due to noise. To generate these, I made events spaced 16.6 ms apart, and I then removed 10% of them. Here's a histogram of the event times: Next I computed the histogram of ILIs. I used a bin width of 1 / (60 Hz) / 10 - that is, 1/10th the expected period of the electrical noise. You can see that there's a clear peak in the 10th bin, i.e at 16.6 ms. That's the electrical noise. Voila. Of course if electrical noise is only causing a few spurious events it will be harder to detect. Also if the noise isn't periodic you cannot use this method to detect it. In that case the only thing I can think of is remove ILIs below a defined cutoff . This is what I've done previously for similar reaction time data. 
 Suppose that you regress­ wage on years of education, but lack an ability variable to include in the regression. People with higher ability will earn more than others with the same years of education, so they will tend to have high positive error terms; but because they have higher ability they may find it easier (or more fun) to spend additional time in school. This creates correlation between the error term and the explanatory variable years of education, so the schooling coefficient will pick up both the fact that schooling makes you more productive (which is what you want), but also some of the effect of ability (which you don't want). 
 Suppose three equally capable candidates are interviewed for a job. One candidate is female, the other two male. The interviewer is biased. What is the probability that the female candidate is successful? The challenge is to define 'biased' in such a way as to make this question answerable. My first thought was to define it as follows: the interviewer has bias $p$ if, given two equally capable candidates, one of which is female and the other male, they choose the female candidate with probability $p$. However it is not clear how to translate this into the situation of three interviewees. A second approach might be to say that, if $X, Y_1, Y_2$ are the perceived abilities of the candidates (male 1, male 2, female) according to the interviewer, then we might say $X \sim N(\mu_f, \sigma_f^2)$ $Y_1, Y_2 \sim N(\mu_m, \sigma_m^2)$ Independently. Then the probability of the female candidate being selected is $P(X&gt;Y_1, X&gt;Y_2)$ However this gets nasty quickly, especially when extending to cases when more than one candidate is to be hired (since we need more than the top order statistic). Happy to introduce further assumptions if it will make the problem more tractable. Regards. (P.S. I posted this on Cross-Validated rather than Mathematics SE, as this seems like more of a question of statistical modelling than of probability theory). 
 I'm a cell biologist. I have a seemingly easy question for which I've received opposing answers. I'm trying to compare the number of peroxisomes (a cellular organelle) in control cells and those expressing a protein X. For this experiment I've cultured cells from the same batch in two separate dishes and used a transfection reagent to induce the expression of protein X in one of them. After 24 hours these cells were fixed and prepared for fluorescence microscopy. From each condition I then took 7-9 images of cells, and calculated the number of peroxisomes for each individual cells using imaging software. For each condition I've repeated the experiment 2 more times. My question is: is N=3 or N=21-27 (3 x 7-9)? Is each individual cell a replicate within the experiment or an independent data point? Of note, despite the seemingly homogeneous nature of cells in culture, the number of organelles between two cells in the same conditions can vary greatly. Additionally, cell transfection results in different levels of protein expression, creating variability between cells. From looking at statistical information I would point to N=3, but yet the majority of the articles that I've checked that use this sort of analysis go for N=21-27, or don't mention it. I would like to compare this data using a mean +/- SEM and calculate a two-tailed unpaired t-test. Thank you! 
 Background: I am trying to apply ridge regression in on-line mode. In batch learning we use cross-validation to calculate regularisation parameter. Is there a way to calculate regularisation parameter for ridge regression itteratively? 
 always produces forecasts beyond the end of the data. So produces forecasts for observations 401, 402, ... and produces forecasts for observations 501, 502, ... produces one-step in-sample (i.e., training data) "forecasts". That is, it gives a forecast of observation t using observations up to time t-1 for each t in the data. So gives one-step forecasts of observations 1, 2, ... It is possible to produce a "forecast" for observation 1 as a forecast is simply the expected value of that observation given the model and any preceding history. gives one-step forecasts of observations 401, 402, .... So it uses the model estimated on observations 1...400, but it uses the data from time 401...500. Note that will not be the same as due to differences in what they are conditioning on. conditions on the training data (observations 1...400) while conditions only on the test data and it does not "know" what the training data were. So is the estimate of observation 401 given the model but no history, while is the estimation of observation 401 given the model and the data up to time 400. Update Note that the model is actually \begin{align} y_t &amp;= \mu + n_t \\ n_t &amp;= \phi n_{t-1} + e_t \end{align} where $\mu$ is the estimated "intercept" and $\phi$ is the ar coefficient. So if you write it in the more usual way, $$ y_t = (1-\phi)\mu + \phi y_{t-1} + e_t $$ Thus forecasts are given by 
 ( This should be a comment, but I lack the reputation! I've tried to distil the essence of the question for everyone's benefit. ) So, Let $N_c$ be the number of peroxisomes in the control population. Let $N_x$ be the number of peroxisomes in the X-protein population. Assume $N_c \sim Distribution(\theta_c, \sigma_c^2)$ $N_x \sim Distribution(\theta_x, \sigma_x^2)$ We are testing $H_0:\theta_x = \theta_c$ $H_1:\theta_x \neq \theta_c$ With everything unknown. To perform this test we have taken three samples of size $n (\in {7, 8, 9})$ from each population. However, for various reasons, the homogeneity of the samples is questionable. The question is whether to subsume the three samples from each population into one sample of size $3n$, or whether to make further distributional considerations and attempt to model the error. Your first comment seems to be equivalent to the statement that $\sigma^2$ is large (which will be taken into account in the calculation of the MLE for $\sigma_c^2, \sigma_x^2$). Your second comment points to a source of measurement error, and the question is whether it is significant enough to model. (I would suggest that you have to choose this yourself, depending on how significant you consider the measurement error to be. At any rate, (assuming by $N$ you mean the sample size) I don't see how $N$ could be 3 - I believe it should either be $3n$ if you think the measurement error is insignificant, or the question becomes more complex if you incorporate the measurement error.) 
 I have seen that there are parametric and non-parametric statistical tests. What are the differences between them? 
 Why applying k-fold cross validation helps obtaining stable clustering results in unsupervised learning? How is this done? Thanks 
 In the package there are two infix operators and that can be applied to random variables to generate new random variables. Example code: However, these two operations do not produce the same thing. Let $X$ be a random variable; in our example $X \sim \operatorname{Norm}(0, 1)$. If I am interpreting the package maintainer correctly, the first is the image $\operatorname{Im}(X)$ of the map $X \to X^2$ and the second is the image of the map $X \times Y \to XY$. I have two questions: What is the difference between the two definitions above? In particular, is the first equivalent to the transformation $f(x) = x^2$ on the pdf of $X$? Is it usual to use this notation? It's quite confusing that $X^2 \neq X*X$. 
 I have 1000 gamma distributions, each with a different mean but constant ratio :$$\frac{mean}{s.d.}$$ also known as coefficient of variation. I extract one sample from each. Then I calculate : $$\Sigma_i\frac{|sample_i - mean\_of\_that\_distribution_i| }{ mean\_of\_that\_distribution_i} $$ I believe I cannot call this deviation. Is there a term for this? Any suggestions on how to analyse the samples? 
 I have a simple study in which I predicted the likelihood of an outcome as a function of a two-level grouping variable. Now I'd like to display these results in a graph. Initially I was thinking of using a bar plot of the predicted probabilities with confidence intervals, but because bar plots don't show the distribution of the data points, I'm leaning towards other options, like a violin plot. However, I'm not sure if this is possible or even desirable. 
 I was trying to make sure that BN worked the way I thought so I wanted to check how MatConvNet computed the forward pass of the batch normalization transform. I wrote a super duper simple script and checked BN manually but I can't get match MatConvNet's answer. I simply compute 1D conv net and then batch normalized right after the convolutional layer: to check if it worked the way I thought it should I did: which returns the "wrong" answer (note that also returns the wrong answer for m=1,k=1): instead of what MatConvNet thinks is the right answer: I have read the documentation from MatConvNet of BN but I still can't get them to agree. Anyone has some ideas? 
 You are correct: The first is simply a univariate transformation of the random variable $X$. The second is the product of two independent random variables. In fact, I would go so far as to say that it is incorrect to use the notation $X*X$ for this, since it suggests equivalence with $X^2$. A better notation would be to use $X_i*X_j$. They are completely different objects. 
 Parametric most often means that a test assumes the tested variables are Normally distributed. Such tests often use regular nominal values, and look at differences in Averages. Non-parametric tests do not assume that the tested variables are Normally distributed. Such tests at times use value ranks instead of nominal values, and also may look at difference in Medians instead of Averages. The mentioned metrics (Ranks and Medians) are more resilient to the shape of the data being further away from being Normally distributed than nominal values and Averages. Many variables are somewhat undetermined, meaning by that when you test them for Normality, depending on the test, you may not be able to reject that they are Normally distributed or you may be able to. There can be a lot of ambivalence regarding whether you should use a parametric or non-parametric test. In such cases, I would recommend you use both parametric and non-parametric tests. Quite often, both tests will give you directionally the same answer. Granted their respective p values will most always be a bit different (that is the case even within tests of same types (i.e. both being parametric)). But, pretty often they are in a fairly similar ballpark. If that is the case, regardless of what test type you use, your conclusion or answer is the same. If in typically the minority of the case when the two different test types give you pretty divergent results, you have to look very closely at your data, and your objective. Given that go with the test that gives you the more cautious, prudent answer given the specific situation or objective. Others have pointed out that the differentiation between parametric and non-parametric tests is and can be extended beyond the Normal distribution. This is a valid point, but in my view way beyond the basic question being asked. And, to its fullest extent the question would often not have a clear cut finish line. Some tests deemed non-parametric because they do not rely on the Normal distribution still rely on other parameters (i.e. the distribution being symmetric). Thus, some parties may view such a test as still being parametric. Meanwhile, the test would be classified in the literature as being non-parametric. To its fullest extend, few if any tests would be entirely non-parametric (most often they do rely on some mathematical structure assumption). 
 Are conjugate priors required when performing Gibbs sampling? 
 Parametric tests assume that your data come from some sort of parametric model. For example, it can be as simple as assuming your data are iid draws from a normal distribution, to a complicated time series model where the variables come from some parametric distribution and possess a parametric functional relationship. For example, we can say $Z_t=\phi_1\varepsilon_{t}+\phi_2\varepsilon_{t-1}\phi_3\varepsilon_{t-2}$ where $\varepsilon_t \sim F_{\theta}$ Here, we have two types of parameters, $\phi$ which define a functional relationship, and $\theta$, which specifies the particular member of a parametric family (e.g., $F_{\mu,\sigma}=N(\mu,\sigma)\to F_{0,2}:=N(0,2)$). So, if you have a bunch of $Z_i$, then you'd use the above model to estimate these parameters (e.g., using maximum likelihood estimation). Nonparametric models are somewhat of a misnomer, since they still impose structure on your problem. However, they usually only partially specify the model, often very partial. A good example is the signed rank test , which assumes only that the distribution is symmetric, and you are trying to estimate its median. 
 You don't require conjugate priors for Gibbs sampling. What you need to be able to do is produce samples from the full conditional distributions. Conjugate priors generally make that easier, but easier is not the same as necessary. In particular situations there may be any of a variety of ways of producing samples from the full conditionals without a conjugate prior. For example, one approach that is sometimes usable is via accept-reject; if you can bound the ratio of an approximation you can sample from to prior x likelihood, that may be doable (e.g. if you can bound the ratio of the density of some mixture of conjugate priors to the actual prior). Of course, possible doesn't necessarily imply its always the best available solution to the issue. 
 I am testing out various imputation methods on my data and would like to use imputePCA. It imputes the missing values with no error messages, but when I check the completeObs matrix some of the imputed values are negative. The original matrix has no negative values and the minimum value this type of data can have is 0. One of the variables that gets negative values after imputation has 30% missing values, the whole data set has ~15% missing values. Why do I get negative values and how can I prevent this? My R code is: I first estimate the number of components and then use that value in the imputePCA function. There seems to be no argument to set a minimum value for imputed data for this function. 
 I don't know if this question is too trivial for you guys, it is not for me though... Is there a closed-form equation for $b = \int_{0.5}^{1} Beta(\alpha,\beta)$? $$ Beta = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} $$ $$ B(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} $$ Google search doesn't help much when querying for a mathematical equation, I really tried... 
 I recommend two articles: Achen C. H. (2001) Why lagged dependent variables can suppress the explanatory power of other independent variables ( link ) Keele, L. and Kelly N. J. (2005) Dynamic models for dynamic theories: the ins and outs of lagged dependent variables ( link ). The upshot is that including a lagged dependent variable can have a large influence on the coefficients of the remaining variables. Sometimes this is appropriate (for the Dynamic models of Keele and Kelly) and sometimes not. As others have said, it's important to think about the process being modelled. 
 the question is A liquid drug is marketed in phials containing a nominal 1.5ml but the amounts can vary slightly. The volume in each phial may be modeled by a normal distribution with the mean 1.55ml and standard deviation $\sigma$ ml. The phials are sold in packs of 5 randomly chosen phials . It is required that in less than 0.5% of the packs will the total volume of the drug be less than 7.5ml. Find the greatest possible value of $\sigma$. I need to find the greatest value of standard deviation ($\sigma$) I worked out the following $\mu= 1.55*5 = 7.75$ We are asked to find value of $\sigma$ such that probability of(total volume of $5$ packs $&lt; 7.5)&lt;0.5\%$ $P(X&lt;7.5)&lt;0.005$ after standardizing $P(X\le\frac{7.5-7.75}{\sigma/5})&lt;0.005$ and I found $\sigma=0.2170$ however answer provided is $0.0434$. Please assist. 
 On this psychometrics website I read that [A]t a deep level variance is a more fundamental concept than the standard deviation. The site doesn't really explain further why variance is meant to be more fundamental than standard deviation, but it reminded me that I've read some similar things on this site. For instance, in this comment @kjetil-b-halvorsen writes that "standard deviation is good for interpretation, reporting. For developing the theory the variance is better". I sense that these claims are linked, but I don't really understand them. I understand that the square root of the sample variance isn't an unbiased estimator of the population standard deviation, but surely there must be more to it than that. Maybe the term "fundamental" is too vague for this site. In that case, perhaps we can operationalize my question as asking whether variance is more important than standard deviation from the viewpoint of developing statistical theory. Why/why not? 
 I have following sets of data: Maximum No of Trades/Day. Maximum Loss allowed. Profit Target/Day. Win rate. For "Instrument A" more two set of data Avg profit/ Trade &amp; avg loss/ Trade. For "Instrument B" more two set of data Avg profit/ Trade &amp; avg loss/ Trade. For "Instrument C" more two set of data Avg profit/ Trade &amp; avg loss/ Trade. Now I want to know what should be the distribution among 3 instruments for max number of trades to achieve profit target by keeping the maximum loss intact. What statistical analysis should I use to solve this problem. Thank you in Advance. Regards 
 Suppose a coin is tossed (with probability of head being $p$) 10 times,if the coin is tossed again 10 times independently,what is the probability that the two sequence will completely match? I am getting no clue about this problem.I have tried but don't know how to start. 
 Variance is defined by the first and second moments of a distribution. In contrast, the standard deviation is more like a "norm" than a moment. Moments are fundamental properties of a distribution, whereas norms are just ways to make a distinction. 
 The variance is more fundamental than the standard deviation because the standard deviation is defined as 'the square root of the variance', e.g. its definition depends completely on the variance. Variance, on the other hand is defined - completely independently - as the 'the expectation of the squared difference between a sample and the mean'. 
 Robert's and Bey's answers do give part of the story (i.e. moments tend to be regarded as basic properties of distributions, and conventionally standard deviation is defined in terms of the second central moment rather than the other way around), but the extent to which those things are really fundamental depends partly on what we mean by the term. There would be no insurmountable problem, for example, if our conventions went the other way -- there's nothing stopping us conventionally defining some other sequence of quantities in place of the usual moments, say $E[(X-\mu)^p]^{1/p}$ for $p=1,2,3,...$ (note that $\mu$ fits into both the moment sequence and this one as the first term) and then defining moments -- and all manner of calculations in relation to moments -- in terms of them. Note that these quantities are all measured in the original units, which is one advantage over moments (which are in $p$-th powers of the original units, and so harder to interpret). This would make the population standard deviation the defined quantity and variance defined in terms of it. However, it would make quantities like the moment generating function (or some equivalent relating to the new quantities defined above) rather less "natural", which would make things a little more awkward (but some conventions are a bit like that). There's some convenient properties of the MGF that would not be as convenient cast the other way. More basic, to my mind (but related to it), is that there are a number of basic properties of variance that are more convenient when written as properties of variance than when written as properties of standard deviation (e.g. the variance of sums of independent random variables is the sum of the variances). This additivity is a property that's not shared by other measures of dispersion and it has a number of important consequences. [There are similar relationships between the other cumulants, so this is a sense in which we might want to define things in relation to moments more generally.] All of these reasons are arguably either convention or convenience but to some extent it's a matter of viewpoint (e.g. from some points of view moments are pretty important quantities, from others they're not all that important). It may be that the "at a deep level" bit is intended to imply nothing more than kjetil's "when developing the theory". I would agree with kjetil's point that you raised in your question; to some extent this answer is merely a hand-wavy discussion of it 
 I wish to check the assumption that physical and mental health for elders are affected by two main latent factors the physical burden (PHB) and the emotional burden (EMB). The measurement model is as follows: AGE: Individual's age CAR: Cardiac disorder HYP: Hypertension DIA: Diabetes OTH: Other physical illness OAS: Shame (Inner) ESS: Shame (Outer) CAQ: Cardiac anxiety PHI: Physical health index (SF-36) MHI: Mental health index (SF-36) The results of the code is: For the above results it seems that the model is not the appropriate way to interpret the data. Is this correct or I could make any modification in order to improve the outcome? Thank you in advance for any suggestions. 
 Your count variable maybe Poisson distributed, so you could change your linear model for a poisson GLM, or use the linear model with a square root transformed response variable $\sqrt{\text{trafficcount}}$. The square root transform is the variance stabilizing transform for the Poisson family. 
 I am electronics and communication engineer. I want to make career in big data and data analytics domain. I have very good mathematical skills and also scored 168 in quantitative section of GRE. I have got admission with funding from BGSU. The curriculum of this program is Does the study really worth to me? Please suggest 
 five coins are tossed 320times the number of heads observed is given..examine whether the coin is unbiased? no of heads 0 1 2 3 4 5 total 15 45 85 95 60 20 320 
 That trick of getting a parameter for each level of the factor by removing the intercept only works when there is only one factor, as you have seen. You can understand why by counting degrees of freedom: Let factor $a$ have $a$ levels, factor $b$ with $b$ levels. Then factor $a$ have $a-1$degrees of freedom, which means that the indicator matrix with $a$ columns representing with, with a $1$ in each row for the level present at that row, has rank $a-1$. Likewise factor $b$ has $b-1$ degrees of freedom. The intercept has one degree of freedom. So the model formula $ ~ a + b$ (which really is $ ~ a+ b + 1$) has $1 + a-1 + b-1 = a+b-1$ degrees of freedom. Removing the intercept (model formula $ ~ a + b - 1$) represents the same model, only the parametrization changed. So it must also have $ a + b - 1 $ degrees of freedom. That $-1$ shows that that there cannot be $a+b$ parameters, so one of the factors still must get one parameter less than number of levels. That explains what you have seen. But still you can get a coefficient for the missing level of $b$, which should be zero, simply. (depending on the contrasts you are using). To make this a bit more explicit let us see at an example. I will use R for the matrix algebra. To make design matrices (in R parlance "model matrices") from factors, we need to define contrast functions. I use the default: First we make two factors for a simple, fully crossed design: Then design matrices for each of them: Each of them, separately, is of full rank: But when combined there is a rank deficit, so they must have one dimension "in common": To identify the common dimension we use the function from package , calculating the null space: Yes, the common dimension is the constant vector. 
 How do I decide if sample attributes(pass/fail) testing results on 2 populations are significantly different. Caps blowing off bottles of different colour glass filled and capped in same process. 
 You need to consider whether the assumption that given a coin gave a "Head" in the earlier toss, is the occurrence of a "Tail" dependent on the previous event or not? The sequence should only matter if the path taken to the last coin toss impacts the last coin toss. This would be true if we were sampling without replacement, but outcomes of coin tosses should be independent of each other. Taking an example of 3 coin tosses, lets say the first instance was {H,T,H}, you can build a tree of possible outcomes: The node marked with asterisk (*) denotes achieving the exact same sequence as the first time i.e. {H,T,H} . Here, the total probability of the sequence which you have calculated as is the same irrespective of the sequence; e.g. it could have been {H,H,T} or {T,H,H} with the same probability, but only one of these three sequences is correct. So the probability of obtaining this sequence should be p.(1-p).p/3 You can expand this for the 10-toss sequence in a similar manner. For the 10-toss example, you need to find out how many different ways you can arrange these 10 events in 10 time slots, after adjusting for the fact that there are only two distinct events Head/Tail being arranged in 10 unique time slots. 
 I obtained the following output after running the Hausman test: 1) CASE 1 Hausman Test chisq = 13.943, df = 4, p-value = 0.007478 alternative hypothesis: one model is inconsistent 2) CASE 2 Hausman Test chisq = 0.49157, df = 4, p-value = 0.9743 alternative hypothesis: one model is inconsistent According to the p-values and for significance &lt;0.05, should I go for the fixed effects in CASE 1 and for the random effects in CASE 2? Thanks 
 For any hypothesis test, the decision rule is: If p-value &lt; level of significance (alpha); then null hypothesis is rejected. If p-value &gt; level of significance (alpha); then we fail to reject the null hypothesis. Level of significance (alpha) is chosen by the researcher. How to chose alpha (also known as probability of rejecting the null when it is true/type_I error) is altogether a different issue. It depends on "how sure you want to be before rejecting a null" Most common value of alpha is 0.05 Now, for BP test, the null assumes homoskedasticity . So if p_val &lt; 0.05 (or your chosen alpha value); you reject the null and infer the presence of heteroskedasticity and if p_val &gt; 0.05 (or your chosen alpha value); you fail to reject the null and conclude there may not be heteroskedasticity. Note: A weakness of the BP test is that it assumes the heteroskedasticity is a linear function of the independent variables . Failing to find evidence of heteroskedasticity with the BP doesn’t rule out a nonlinear relationship between the independent variable(s) and the error variance. White test provides a flexible functional form that’s useful for identifying nearly any pattern of heteroskedasticity. It allows the independent variable to have a nonlinear and interactive effect on the error variance. So most commonly used test for homoskedasticity is White test. 
 How is the bond called which value is calculated with following formula: $P =\sum_{t=1}^{n} \frac{\frac{a(N-\frac{N}{n}(t-1))}{K}+\frac{N}{n}}{(1+r)^t}$ 
 In my problem I have a longer period of historical data of a time series. I need to predict for some specific points in time in the future. For these points in time five previous values are also available. So far my approach was to use a sliding window of size five, use lag features and apply machine learning methods. However I have a feeling that when doing like this I am not exploiting the historical data to the full extent. (The methods see only one sliding window at a time.) I am now looking for some method (or ideas to design my own) which takes as an input historical data and measurements just before the time point I need to predict for. Thank you! 
 I'm trying to estimate a Poisson regression model. My dataset spans 78 neighborhoods in some city, across 11 years, and has been drawn from multiple sources: the dependent variable (a crime count for every neighborhood-year combination) has actual observations for every year, but all the independent variables have observations for three years only (year 2, year 6, and year 10). This has led me to copy-paste their values in order to "fill in" what's missing. I get believable results when performing the regression without the use of neighborhood fixed effects, but when using them, the estimates get extremely counter intuitive, and their significance decreases drastically. The fixed effects estimates themselves are all highly significant and seem badly estimated, both sign wise and in size. I think this might stem from the lack of variance in the dependent variables, something along the lines of: "the dependent variable varies across the years, the explanatory variables do not, hence these changes must come from unobserved characteristics." Am I right? 
 Your "highly significant" estimates likely aren't. After all, any approach to inferring unknown predictors (you are copy-pasting, which could be fill-forward or fill-backward) is fraught with uncertainty - but your Poisson regression does not know about this. I'd recommend you try to incorporate this uncertainty in your parameter estimates. At the very least, you should try different ways of inferring your unknown predictors, like fill-forward, fill-backward, linear or spline interpolation. Even better: bootstrap the entire procedure. Bootstrap your original data, then perform whatever inferring algorithm you want to use on the bootstrapped data , then estimate your Poisson regression and save the parameter estimates. I suspect that they will come out a lot more variable than your original approach would suggest. Finally, use the averages of these bootstrapped parameter estimates - and check how many of these are on one side of zero to get a bootstrap measure of significance. 
 In CHAID control parameters we have to specify the alpha value for Merging Threshold and Splitting Threshold. Typically this alpha (p-value) is set at 0.05. How do we select the alpha2 and alpha4 parameters? What if we choose one of them as 0.05 and another as 0.01? 
 In addition to the answers given here, one could point out that variance is more 'fundamental' than standard deviation in some sense, if we consider estimation from a (e.g. normal) population. For a sample of size $n$ drawn from a population $X$ with $\mathrm{Var}[X] = \sigma^2$, it is known that the sample variance $S^2$ is an unbiased estimator of $\sigma^2$, but $S$ is in general not an unbiased estimator of $\sigma$: $$ \mathrm{E}[S^2] = \sigma^2\,,\ \mathrm{E}[S]\neq \sigma\,, $$ see here , which follows from Jensen's inequality. 
 I've a bunch of genes (~8000). Each of these genes has an associated p-value representing its "importance" in a specific biological pathway. Within these genes I'm interested in a subset of genes (~200). How can I test wheter these 200 genes are in the tail of the p-value distribution. In other terms, how can I test wheter these 200 genes are "more" significant than the rest of the genes. For now I did an hypergeometric test using a thresold on the p-value. Let's say : and then but I have to define a thresold (0.05) .... Edit : Can I do a Mann-Whitney U test on the p-values ? (comparing A vs B ; by removing the subset B from A of course) Thanks 
 I have an exploratory study where I investigate the relationship between several predictor variables (&gt;10) and a few outcome variables (4). The large number of variables is not random but conceptually justified. Is the following procedure (logic) correct: (1) First, run a bivariate correlation analysis to see which variables are significantly correlated with each other (+ taking into account confidence intervals). This is done so as to limit the number of possible variables/predictors subjected to further analyses. (2) After that, choose the predictors that are significantly correlated with the potential outcome variables and then run several multiple regression analyses with different outcomes. That way I can tease out how much unique variance individual predictors and the combination of these can explain in the outcome variable. Is that correct way to proceed? Also, should I control for multiple comparisons in the first correlation part of the analyses given that it is an exploratory study and the main conclusions are drawn from regression analyses in which there are much less (1-3) predictors? 
 I was following this tutorial for PLS regression in MATLAB. They show how to choose the number of components for the model, but the y fit that they calculate, refers to the training set of the model if I am not mistaken. Even if I use a plsregress(...'CV',5) argument for 5-fold cross validation in the plsregress function, I still think that I get the fit on the training set. If I were to split my data into training and test sets, create a plsregress model based on the training set, how would I be able to find the fit on the test set? 
 Different possibilities come to mind. You could run a two-sample Kolmogorov-Smirnov test to check whether both p value distributions come from the same population. This would also come out significant of the smaller set is systematically larger than the rest, though. You could do a simple one-sided two-sample unpaired t test (without assuming equal variance) to check whether one sample's mean is lower than the other. If you want to test against specific violations of uniformity, you could use Neyman's smooth test - the different components "look at" different sections of the unit interval. Ledwina (1994, JASA ) offers a way to select the number of components based on BIC - it's implemented in the package for R . Plotting the empirical cumulative distribution function of both samples against each other would certainly be informative (this is how the Kolmogorov-Smirnov test works) - it wouldn't be a test , though. I am a bit unsure about how valid it is to test the distributions of p values against each other the way you propose, so I personally would trust such an ECDF plot more than some test, anyway. 
 I have the following data, which I'm analysing in matlab: I would like to find out if the slope of these two lines are different, using some sort of statistical test. I can calculate the slope of each with: whch shows that, lm: and lm2: This means that the slopes are different, but is this difference statisitcaly signficant? Is there a test that I can do to show that the slopes are different or the same, in a statistical sense (considering the error in the regression model)? 
 I have a suppressioneffect, because a correlation between x and y is actually non-significant, but if I put in another variable (in a multiple regression), then the correlation between x and y becomes significant. Now is my question if this is also a partial mediation? Because in a multiple regression x and the third variable are significant and the ANOVA ist significant too. Or is this only a suppressioneffect? 
 The diagram is not a measurement model, since it appears that you are regressing the latent variables on two observed variables, and . This kind of recursive model looks like it might not be identified. However I don't see these regressions in the code, so I wonder if you have posted the correct diagram for the code ? The way I read your code, PHI and MHI are additional indicators for both latent variables. I would have thought that there should be residual covariances between the indicators for each latent variable. For example, there should surely be covariance between age, cardiac disorder, hypertension and diabetes not explained by your model, and possibly between these and cardiac anxiety - as well as others (theory and clinical expertise should drive these modifications, although you could use modification indices to help). This should improve the model fit. Why are you fitting intercepts ? This isn't necessary in a measurement model, though it shouldn't affect model fit. Note that you can use in the fitting function to do this more conveniently. Have you checked for normality of the observed variables (since you are using the MLR estimator) ? Also, you could try standardising your data since the variance of is very much larger than the variance of the others (though I doubt this will help model fit) Edit: Further to your comment, I would suggest adding , , as well as and with those 3 variables. Also, note that regressing an observed variable on a latent variable is equivalent to adding that variable as an indicator of the latent variable. To see this you can run the following models with the built-in dataset HolzingerSwineford1939 Note how the model fit statistics, and all the parameter estimates and standard errors are identical. The only difference is that in the regressions are now factor loadings. 
 Your dependent variable is not directly a count variable, since you do not know the exact count (and maybe the persons asked do not know exactly the count themselves, so there is some uncertainty in the measurement of the process). But, underlying the response variable there is a latent count variable, and you have observed an interval censored version of that. So, you could try to use Poisson regression, with a non-standard likelihood function based on interval censoring. I will write out the model below: The latent data is $Y_1, \dotsc, Y_n$ which is assumed poisson distributed with mean $\DeclareMathOperator{\E}{\mathbb{E}} \E \left\{ Y_i | x_i \right\} = e^{\eta(x_i)}$, where $x_1, \dotsc, x_n$ are the covariables and $\eta(x)$ the linear predictor (containing parameters to be estimated). Write the coresponding Poisson cumulative distribution function (cdf) as $F(y | x)$. But we do not observe $Y$, we only observe that $Y$ falls in one of a set of speciefied intervals, denote the upper interval limits as $c_j$. The intervals are then $[0,c_1], (c_1,c_2], (c_2, c_3], \dotsc$. Observation $i$ falls in interval $(c_{k_i -1},c_{k_i}]$ (where it is understood that in case $k_i=1$ the lover parenthesis changes from a $($ to a $[$). So $$ P(Y_i \in (c_{k_i -1},c_{k_i}] ) = F(c_{k_i}) - F(c_{k_i -1}) $$ so the likelihood function becomes $$ L = \prod_{i=1}^n \left\{ F(c_{k_i}) - F(c_{k_i -1}) \right\} $$ which you can optimize directly. You mentioned stata: I do not know how to do this in stata. 
 How to analytically express cov(X,Y), when: and Here C, A and B are independent variables with normal distributions. More specifically I would like to express cov(X,Y) using the expected means of each of the input variables C, A and B, as well as their standard deviations or variances. Thank you so much in advance! 
 I have come across this question and don't know what statistical method should be used. A business has 1000 customers who regularly spend between £100 per annum to £2000 per annum. The business creates a e-commerce "webshop" and 20 customers start to use this form of buying the product. We can assume that we have all the customer data. How can we determine if there is any revenue uplift as a result of ordering on-line, and how would you dimension what that increase is? 
 All documents (e.g., (1,2)) that I have read on approximate randomization test indicate that one needs to choose the number of shuffles prior to running the approximate randomization test. Example from (1), where the number of shuffles is fixed to 9,999: I wonder whether it would be OK to specify a convergence criterion on the significance level, instead of pre-defining a number of shuffles. (and if so, what would be a reasonable convergence criterion and how comes few people if any uses one.) E.g., one such convergence criterion could be stop the approximate randomization test if the significance level hasn't varied more than $\pm0.01$ over the last 50 shuffles. The main rationale would be that the convergence speed of the significance level depends on the dataset and the analyzed system. Below are some examples of number of shuffles (x-axis) vs. significance level (y-axis), as we can see the significance level sometimes converges before 9,999: Conversely, one could (?) imagine cases where the significance level hasn't converged yet after 9,999 shuffles. Chinchor, Nancy. "The statistical significance of the MUC-4 results." In Proceedings of the 4th conference on Message understanding, pp. 30-50. Association for Computational Linguistics, 1992. https://scholar.google.com/scholar?cluster=9100345284812509896&amp;hl=en&amp;as_sdt=0,5 Morgan, William. "Statistical hypothesis tests for NLP." https://scholar.google.com/scholar?cluster=14737537630072698879&amp;hl=en&amp;as_sdt=0,5 
 what is the appropriate approach to estimate the probability of a single sample belonging to a population? Specifically, I am calculating a particular value from the comparison of two time series. Then, I perform many rounds of shifting the time series against each other by random amounts of time and re-calculating those values. Finally, I would like to determine if the obtained single value from the original time series is different from the family of values obtained from the shifted time series. In other words, I would like to test if shifting the time series does or does not have an effect on my value. Obviously, I only have ONE un-shifted value which I want to compare to MANY values from time series shifted by various amounts. I looked up one sample t-tests but the logic is typically reversed from what I read online: typically, the mean of a 'single sample' is compared to a 'known' value. would this still be the right test? 
 I have $n$ instances in my data and I will do 5-fold cross validation on it (like in the picture): But when I read about "repeated cross-validation" I think that it will give me exactly the same answers because it's the same data, the same folds and everything. Am I right? 
 I work in retail and we want to complie a regression analysis to see how different factors impact our business and retail in general. So far, I have started to educate myself on R (by suggestion from a fellow member here). We currently analyze promotions, cycles and marketing Campaigns within our own organization and their outcomes. But I want to create a regression to see how well things correlate and what is the coeffecient of determination. Basically, I want to find out how well I can trust certain conclusions. Macro factors so far: -Weather -Paydays -Strikes I'm considering factors such as inflation and real estate (rents). We have business all over the World so there would eventually have to be an analysis per country. 
 I am currently having a read through the Statistical Drake Equation ; a method of taking the Drake Equation , letting each number be a uniform random variable, and then applying the Central Limit Theorem to get the combined distribution of these variables. But, as there are only 7 variables in this equation, why is it valid to say it tends to a Normal Distribution? Surely this isn't true as the number of variables definitely isn't large. 
 By using the package of R, these tests can be conducted: Perform Anderson-Darling normality test Perform Cramér-von Mises test for normality Perform Pearson chi-square test for normality Perform Shapiro-Francia test for normality Many other tests can be done by using the package. See description at https://cran.r-project.org/web/packages/normtest/normtest.pdf 
 I have a vector of values with zeros and some rare positive value (corresponding to the peaks in the hist) I'd like to calculate the most adequate interval between those peaks. The most adequate interval is the one that passes as near as possible from the highest peaks and do not passe as much as possible from 0 values. Visually it seams that the adequate interval is 12 I guess it's an optimisation problem but I don't know how to modelize it. Any help ? 
 I'm dealing with the following problem: I have a multiclass variable, y, with let's say 7 classes. The 7 classes are not evenly distributed, some are way more likely to occur than others. Let's say class A occurs in 40% of the cases, and the other classes all occur in 10% of the cases. I'm trying to predict these classes with the variables in the matrix X. Given that it's rather hard to predict y with the given variables in X (accuracy of about 50%), some out-of-sample distributions tend to be quite close to the prior distribution (40%, 10%, etc.). Hence class A has a larger distribution by default. When I move to predictions based on the largest probability, class A is chosen more often than expected (+/- 70%) and the other classes sometimes hardly occur, because by default the probabilities are smaller. What can I do to improve my model? I was thinking about: Weight the probabilities, based on the frequency of the classes Choose my predictions in a different manner Train my model on a different set, such that all classes occur with the same frequency. Example: if for observation k the probability for class B is 39%, and for class A 40%, I would prefer to assign this observation to class B, even though the probability for class A is somewhat larger. Any advice, inspiration or "best practice" is more than welcome! I'm using models like xgBoost/random forest, with multiclass probabilities. Output looks like: 
 But when I read about "repeated cross-validation"Ii think that it will give me exactly THE SAME answers because it's the same data, the same folds and everything. Am I right? No. Between each repetition you virtually shuffle your data, so you get different data in different folds each repetition. 
 I'm working in the area of analysis of logical computer systems (e.g https://goo.gl/images/KyLCCo ). Specifically in the field of anomaly detection of these systems. I was thinking about the field of spatial analysis where analysis is conducted on a topology over a physical space. I was wondering are the some comparable analysis techniques to analyse topologies there the area is not physical but rather logical. If you look at the link to a sample diagram you can see that systems are at different levels (strata). Additionally the physical distance between the systems is not important but rather the categories are more important (e.g. Database Vs Middleware Vs HTTP/Presentation layer) From the searching on Google-Scholar there isn't too much information to guide analysis techniques what is apparent is that using counts of categorical data may be appropriate. I'm checking to see what suggestions folks may have for analysis of this type of data topology. Regards Jonathan 
 I am studying the Public expenditure regarding Health and Pensions in the European Countries, during 17 years. Which means i have t=17 and n=28. Will this small sample lead to non-valid inference? Thank you, Maria 
 Data description: My dataset (~2 millon rows) includes cattle price records from weekly cattle markets during the period Jan 2008-Dec 2014. For each animal, the following information before the sell was recorded: Sex Breed Weight (Kg) Price (cents/kg) Age (weeks) County Mart code Date. An example data frame is: The IDs of those animals are unique, i.e we do not monitor the same animals over time. The dataset includes animals from 10 breeds, sold at ~ 70 marts around the country. Assuming that we have 1,...,K weeks in total and $\mathcal{M}$ marts, the hierarchical graph below shows how I understand the structure of the dataset. Research question: My aim is to build a predictive model on selling price given such records, that accounts for the temporal variation in the price within each year. For a farmer with an animal with specific characteristics (breed, age, sex, weight) what selling price can they achive during the year? What I have done so far: A similar question has been asked here , but it refers to units observed over time. Hence, a mixed effects model might be more appropriate in that case. I don't think this applies to my problem. Generalised least squares (GLS) model with ARIMA errors. This was my very first idea for modeling this data, but the fact that I have multiple observations per week troubles me. I've been thinking to average the points to get a smooth time series to fit. But, by doing this I would be discarding information. Plus, it doesn't make sense to average information like sex etc. Hierarchical time series using the hts package. By slightly modifying the graph above, the top level would refer to the price at time $t$. But I am uncertain on how to include information on the predictors in the time series model. How to proceed? As time series is not my field of expertise, I feel a bit lost. What would you suggest as a sensible statistical approach for dealing with this problem? 
 I'm trying to evaluate some classification algorithms' results in my imbalanced dataset. With imabalnced, I mean that there are much more negative labels than positive ones. Accuracy and precision are always good, but recall and Area Under the Precision-Recall curve (PR_AUC) are not so good. I'm seeking the classifier that maximizes the PR_AUC. 1.- Do you think this is a good criteria to select a classification algorithm in this case? 2.- Are recall and PR_AUC proportional? I mean, if a classifier gives better recall results than another one, but worse PR_AUC results... Am I doing something wrong? Or it has a logical explanation? Which one is the best criteria for imbalanced datasets? Thank you for your help! 
 As you have a discrete x-axis there is a finite number of candidates for intervals. You could calculate the means of every 9th element, every 10th element, every 12th element and so on, each with the different possible starting points and then decide for the interval and the starting point with the highest mean of values. If the vector of values is long and there are too many values, than Fast Fourier Transformation (FFT) might be a better answer, but for the given example, brute force should do. Edit: I have tried it with the given sample data and found the following table: The top values are achieved with intervals of 1, 2, 4, 6 and 12. The interval of twelve with starting point at 3 hit the two highest values and misses the 3rd, 4th and 5th highest value. That is why the smaller Intervalls perform so well: 12 Looks like the answer but is not neccessarily the best answer. If you want to include "almost hit" values, you will have to define, what an "almost hit" is and how it should be weighed. I will add my R code. It is probably easy to adapt to usefull definitions of "almost hit". 
 First check that the method is providing accurate absolute probabilities, using out-of-sample calculations or bootstrap bias-corrected estimates. This involves estimating a smooth calibration curve using e.g. the loess nonparametric smoother. Hovering about the line of identity indicates good calibration, and there are formal tests you can make (e.g., Spiegelhalter's as in the R package function). Once you have accurate probabilities, the majority of problems would see them as the final estimates with no need to translate them to categories (e.g., modal or most likely category as you have done). If you really wanted a "class prediction" you could draw from the multinomial distribution using the estimated probabilities of class memberships. Or you could devise a rule, depending on your utility/loss function where the modal category is used if its probability exceeds $\theta$ and a formal "no decision" is made otherwise. To always choose the modal category even if its probability is 0.18 may be misleading. You didn't state your sample size. For multinomial/polytomous problems such as yours very large samples are required. 
 I've come across the problem of missing data when doing GLMs. I'm using GLMs to make predictions in R. My dependent variable is continuous and my independent variables are factors. The question arises, what to do with NA values in the factor variables. What I've been doing in the past was making a separate factor level for NA and then combining it with another factor with a similar GLM coefficient. But as I've read, that can lead to biased results. Also what worries me is, what if the data with the NA values in some variable should all in reality be in the lowest or highest level? Then I discard additional information, and make predictions too high or too low, right? A possibility a colleague suggested was discarding all the data with NA values. But that way I might lose way too much data. What is the recommended way to deal with missing data in this case? I've read about imputation, but it seems to me as if that's going to lead to me making some variables significant when they are not (I'm already manually grouping similar ones together). 
 I would appreciate some feedback on my leave one out CV procedure, because I am not sure it works correctly. 1.Load the files I am using 26 binary classified articles. Files are shuffled when loaded. This is an array of the shuffled articles: 2. Vectorize data 3. Classifier selection and building a pipeline I wonder if this pipeline of mine tf-idf vectorizes the input data the way it is vectorized under the "Vectorize data" part of my question. Should I use X_data_tfidf data here? 4. Cross-validation I built a cross validation function in order to test k-fold CV and leave-one-out CV, but I am not sure if it is implemented correctly. 5. Results This is the result. Mean scores is 0.58, but when I compare the binary output it is identical as the values of my shuffled input articles. Unless it is further shuffled somehow. However, there is in my CV function. Classification score: Loaded array: It looks like something is wrong, even though mean score is 0.58, but arrays are the same. It looks like there is a flaw in my procedure and I cannot figure out where. Help would be very much appreciated. :S Thanks 
 Suppose each coin toss is independent and that the first sequence of (ten) flips is $X_1, X_2, \dots, X_{10}$, where $\forall i \in \{1,\dots,10\}: X_i \in \{H,T\} $ meaning each flip is either $H $ heads or $T $ tails. Let the observed value of the $i^{th}$ flip be denoted $F_i $ so that $\mathbb{P}(X_{i+10} = F_i) $ is the probability that the $i + 10$ flip is equal to the observed value of flip $i $. What we want to solve for is then the probability of repeating the same sequence in the next ten flips (flips 11 to 20) $$\mathbb{P}(X_{11}=F_1,\dots,X_{20}=F_{10} \mid X_1=F_1,\dots,X_{10}=F_{10})$$ Since each flip is independent, (or as others like to say, the coin is "memoryless") it doesn't matter that we condition on the past. Rather, we just need to calculate $$\mathbb{P}(X_{11}=F_1,\dots,X_{20}=F_{10} )$$ Again, by independence, each flip in this sequence does not depend on the flips before or after ("memoryless"), reducing our calculation to the product $$\prod_{i=1}^{10} \mathbb{P}(X_{i+10} = F_i) = \prod_{i=1}^{10} p\cdot I(F_i = H) + (1-p) \cdot I(F_i = T)$$ where $I (\cdot) $ is the standard Boolean indicator function. You'll find that this expression evaluates to $p^n (1-p)^{10-n} $ where $n $ is the number of heads in the first ten flips. 
 Using R, I got a bunch of confusion matrices from some model fitting. I'm trying to choose the best model by looking at their confusion matrix. Not an easy task. My current method of comparison is to choose the best model as the one that looks the most like a diagonal matrix. My interpretation is that the diagonal of this matrix would contain the higher values relative to the other matrices. My method for comparison in R is to take out each diagonal, append it to a new dataframe, one vector column for every confusion matrix, so it would be simpler to decide with just a look. Am I correct in comparing the diagonals in this manner? If no, what would be a better method of comparison? If yes, is there a better algorithm than the one I am using? 
 when you using time series data to run a regression and you've tested for unit root and you found out that your variables are stationary at different levels, how do you control for that? or more simply, some variables are stationary while others are non-stationary, how can one control for that? 
 I'm using an LSTM to explore some properties of long range dependencies in some data. One of the things that I was hoping to do was to examine the effects of limiting the "memory" of the network to different lengths, and seeing how well it could subsequently approximate the data for each. How would this be implemented? Would it just be a case of limiting the length of the input sequence, or would it involve manually changing the values of the forget gate? 
 Much depends on the reasons why data are missing. There are 3 commonly cited missingness mechanisms, missing completely at random (MCAR), missing at random (MAR) and missing not at random (MCAR). MCAR means that missing values occur randomly in that variable without any dependence on any other variable, observed or not. MAR means that missing values occur randomly in that variable but the probability of being missing depends on values of one or more other observed variables (which might include your outcome variable). If missingness depends on unobserved variables then the data are missing not at random (MNAR) Deleting observations with missing data (known as complete-case analysis or listwise deletion), is a bad idea at the very least because it discards information which results in larger standard errors, wider confidence intervals and loss of power. Under MCAR the estimates will be unbiased, but under MAR they may be biased: Complete Case analysis confines attention to cases where all variables are present. Advantages of this approach are .... . Disadvantages stem from the potential loss of information in discarding incomplete cases. This loss of information has two aspects: loss of precision, and bias when the missing-data mechanism is not MCAR , and the complete cases are not a random sample of all the cases". From: Statistical Analysis with Missing Data, Second Edition, Roderick J.A. Little &amp; Donald B Rubin, John Wiley and Sons, 2002. p41: Creating a factor/indicator/dummy variable for missingness is also a biased method, for example see: White IR, Carlin JB. Bias and efficiency of multiple imputation compared with complete-case analysis for missing covariate values. Stat Med 2010;29:2920-31. Jones MP. Indicator and stratification methods for missing explanatory variables in multiple linear regression. J Am Stat Assoc 1996;91:222-30. If the data are plausibly MAR or MCAR then multiple imputation will yield unbiased estimates when applied correctly and standard errors will be smaller than with complete-case analysis. If missingness depends on unobserved variables then the data are missing not at random (MNAR) and this is much more difficult to handle. Multiple imputation works by filling in missing values with plausible values from a model. This is done multiple times and each time the imputed values will differ to allow for uncertainty. The analysis model is run on each imputed dataset and the results are pooled. In essence, the method works because, on the one hand, while it is possible to estimate the most likely values for the missing data, the most likely values are in fact unlikely to be the correct values: there is inherent uncertainty. The variability in the values which are imputed between each completed dataset provides the uncertainty which is needed to reflect the uncertainty created by the missing values. is an excellent package for R which implements multiple imputation. https://www.jstatsoft.org/index.php/jss/article/view/v045i03/v45i03.pdf 
 I'm currently working on my thesis and would like some advice for the statistical analysis section. My hypotheses are: H1: Firms that belong to a product industry are more likely to establish a wholly owned subsidiary when investing abroad than firms that belong to a service industry. H2: Firms that belong to a primary industry are more likely to establish a wholly owned subsidiary when investing abroad than firms that belong to a secondary industry. H3: A firm's degree of internationalisation positively moderates the relationship hypothesized in H1. H4: A firm’s degree of internationalisation positively moderates the relationship hypothesized in H2. Thus: the dependent variable is "total stake in foreign affiliates" (dichotomous: 1=100% stake; 0= &lt;100%) The independent variable is industry class (dichotomous as well: primary=1; secondary=0 for H2 for example). The moderator variable in H3 and H4 is measured through a sales ratio. Control variables include firm size,age and performance. From my supervisor I know I'll have to apply a hierarchical regression model (First regress independent var*control variables, then also include the dependent variables, then also include the moderator variable etc.) Now, I'm kind of in doubt about which type of SPSS regression to use. I was thinking about binary logistic regression as the IV is of dichotomous nature, but would this also work with both IV and DV being dichotomous? I'm also not sure how to incorporate an interaction term. This is all pretty new to me,so would love to hear your opinion and suggestions! 
 I have two questions concerning planned contrasts: I would like to know how factor-based contrasts (obtained through an interaction term) compare to model-paramter-based contrasts (obtained by specifying model parameters). I would like to know this for a simple case of comparing one condition with another, but ultimately I am interested in comparing one condition vs all other conditions. Below are my attempts at understanding factor-based contrasts and model-parameter based contrasts: From Eric Fuchs' blogpost I think I figured out how to obtain factor-based contrasts. Let us assume for now that I am interested in the comparison of vs. . To this purpose, I create a contrast matrix where I assign equal weights with opposing signs to my two levels of interest, and 0 to the other two levels: Output: For comparison, my attempt at model-parameter-based contrasts: with $E[mpg]=b_0 + b_1 \text{am} + b_2 \text{vs} + b3 (\text{am} \times \text{vs}).$ Based on Matt Blackwell's answer I think that a comparison of vs. means that $H_0: b_1 = 0$ (i.e., the regression weight associated with ). Therefore: Output: The values differ, so one of the two solutions is not correct (and my suspicion is that I did not correctly translate Matt's answer to this case). My question now is: which one of the two approaches is correct and how can the other be rewritten correctly? Ultimately, I am interested in the comparison of 1 of the 4 levels of the interaction term vs. the other 3. Let's say I want to compare vs. the other 3 conditions. The factor-based version is simply an extension of what I have written above (if what I wrote above was correct): Output: Unfortunately I would not know how to start with the parameters for the model-parameter-based version, which is what I am ultimately interested in. 
 I thought I would provide the following paper which I find is reporting GLMM results quite well in a marine biology context: Laurich et al. (2015) Temporal variation in chemoautotrophic symbiont abundance in the thyasirid bivalve Thyasira cf. gouldi . 
 The lasso can be computed with the LARS or Coordinate Descent algorithm. What is their computational complexity and when one is quicker than the other? 
 It is usual to use all sorts of feature derivation you think are probably useful for your problem, then apply feature selection, feature reduction, and similar concepts to make best use of those features in your models. Not using a helpful feature derivation in the first place causes this information to be missing for your model, which will likely cause worse-than-possible results. So your feature selection and reduction approaches should account for not-helpful and redundant information (this is frequently coupled with training/evaluating models to avoid overfitting/selection bias). Concerning robustness of the resulting model: this should rely on using a thorough evaluation approach and choosing a reasonable simple model from all evaluated models - instead of limiting feature derivation in the first place. Using repeated CV (training and evaluating different model types and parametrizations, then selecting best model based on resample performances) together with data partitioning appropriate for your setup (completely held-back test partition for estimating performance of final model, poss. done population independent like leave-subject-out-CV) will be OK for most setups. 
 Say that I have a signal $f(t)$ composed of an offset, another signal and noise: $$ f(t) = offset + g(t) + \epsilon(t) $$ The function $g(t)$ is strictly possitive but the epsilon can make the observation just below and above the offset. Given $f(t)$ what is the best statistically sound way of getting the offset? As an example how would you answer $offset=5$ when analyzing the following signal 
 I've found a good error function to minimize. Result python code: 
 I never sought for help on a forum since I feel that many times the answer for a problem can be found if one reads books and searches for long enough in publications. But I would be very grateful if some of you could provide some help with the following problem: The issue at hand is the following: An organization wishes to analyze repair costs for different machines. There is literally no data available about the parts the organization will have to repair (age, hours of use, …) when the analysis is done so a regression seems impossible to me since the characteristics of the part to repair are not know. However, the history of reparation costs for such parts is known. So the average costs to repair this kind of part are known as well as the individual expenses for each repair. For example: Name of part: The number of past repairs for a part varies from zero to more than 100.000; in the table I randomly put 4. Furthermore it is known which part of the total costs was spent for labor, for material etc. How could one calculate the probability that the repair costs are higher than a certain value, e. g. for our example: “What is the probability that the repair costs more than 1000”? Would the best approach be to determine a cumulative distribution function based on the empirical data and thus predict the probability? This is not getting me the results I need since the ecdf is giving limited results when only few values from the past are available. In the example above the ecdf would give the same probability for costs lower than 704 as for 906 which does not make sense in my context. Besides trying to fit a distribution to the value from the past, would be there another appropriate solution for the issue I described? What would be a good number of data points to switch from a parametric to a non-parametric analysis of the cdf? What is the ideal technique to fit a distribution to cost data? Any help is appreciated, Rashid 
 when the data is from different types (numerical and categorical) of course euclidean distance alone or hamming distance alone can't help. so i have 2 approaches: standardize all the data with min_max scaling, now all the numeric data are between [0,1] now we can use euclidean distance alone calculate the euclidean distance for numeric data and calculate hamming distance for categorical data, and then combine both distances(with weights) my question is: 1-are my 2 approaches correct?if yes, then which is better?how can i combine the distances(choosing the weight for each feature)? is there an implementation of the second approach in sklearn in python? 
 Most often, the Cobb-Douglas production function is used to describe the relationship between production, capital, and labor. But sometimes we use this function to describe other sorts of relationships, for example, Milesi-Ferretti, et al. in their "Electoral Systems and Public Spending" use the parameters in a Cobb-Douglas production function to describe citizen preferences over government transfer spending vs. public goods spending. My question is about how to specify/estimate a similar model where outcome (utility) is binary. For example, a Cobb-Douglas specification for two goods $g_1$ and $g_2$: $$ y = g_1^\alpha g_2^{1-\alpha} + v + u $$ Where $u$ is a noise component, and $v$ is a non-negative inefficiency component. Ultimately, I want to use $v$ in a second stage model, similar to what one might do in stochastic frontier analysis. But setting aside the question of residuals, the case I'm having trouble considering is where $y$ is binary. Or in other words, I'm thinking of utility ($y$) here as the probability of success Pr($Y = 1$). Were $y$ to be continuous , I would take logs and estimate: $$ log(y) - log(g_2) = \alpha[log(g_1) - log(g_2)] + v + u $$ But obviously, I can't do this with a binary outcome, because $log(0)$ is undefined. Is there another common approach for specifying models similar to this, or is there something theoretically unreasonable about this approach? The two 'solutions' I've considered are: Still use a logit model, but instead of using the Cobb-Douglas production function, fit a model for $g_1$ and $g_2$ with some reasonable combination of polynomials. But because the model I want to estimate comes out of a theory where I assume either Cobb-Douglas or trans-log preferences, I'd really rather not depart from this in my empirical specification. Fit this with OLS or similar. While this may yield predicted $y$ values less than 0 or greater than 1, if I don't expect too many values too close to zero or too close to 1, then this might be "good enough," especially if I'm more interested in $v$ than $\alpha$. Example In case this helps, let me try to provide an example scenario. There is a civil war in some country with 200 cities. Each city has a dictator-mayor with equal wealth, and each dictator-mayor can produce some bundle of $roads$ or $hospitals$. More $roads$ and more $hospitals$ both contribute to a greater probability of holding this city from rebels (i.e., success), because troops need both. The dictator-mayor has preferences $\alpha^*$ over a bundle of $roads$ and $hospitals$. The median citizen in the town also has preferences $\alpha$ over both goods, though citizens still benefit from any quantity of either good (i.e., any roads are better than no roads). The dictator-mayor's probability of success does not depend on citizen preferences. But some benevolent-dicator-mayors care about citizens preferences nonetheless and try to accommodate these. I want to test the hypothesis that in some cases citizen preferences are accommodated, even if this reduces the likelihood of the dictator-mayor holding the city. I observe (a) whether the city is held, i.e., success, (b) the quantity of $roads$ and $hospitals$ in each city, and (c) the median citizen's preference $\alpha$ in each city. I do not observe the dictator-mayor's base preferences $\alpha^*$, and I do not know the unit or relative price for $roads$ or $hospitals$. So I have this model that I cannot estimate: $$ Pr(Y = 1) = roads^{\alpha^*} hospitals^{1-\alpha^*} + v + u $$ Then the residuals consist of noise $u$, and a systematic deviance $v$, where $v$ may then be modeled as a function of other characteristics. So for example, if $v$ is predicted by citizen preferences $\alpha$ in a second model, then this is evidence that some dictator-mayors choose baskets of goods that are less than optimal for maximizing $y$ but nonetheless accommodate citizen preferences. 
 I have got data containing the asylum applications received by all european countries from 2008 - 2015 and I need to build a few subtotals of the variable "AGG.APP" (asylum applications) from its values of the past 5 years depending on the respective country. So what I need to get at the end is a variable containing the aggregated number of asylum applications for the years 2013 (aggregated asylum applications in the years 2008-2012), 2014 (aggregated asylum applications in the years 2009-2013) and 2015 (aggregated asylum applications in the years 2010-2014) by country. Has anybody got a clue how I might do this??? 
 I'm looking at the Wooldridge textbook 2006 international version, where on p.96 in equ. 3.45 it says: $E(\tilde{\beta_1}) = E(\hat{\beta_1})+E(\hat{\beta_2}) \tilde{\delta_1}$. This is compatible with the previous answer by Alecos Papadopoulos, but it is not (in general) the equation that you stated in your question, without expectation operators. That is, this relation only holds in expectation, not in any given sample. You would have to conduct a Monte Carlo simulation exercise to calculate the expectations involved (up to arbitrary precision by increasing the number of draws) and then the stated equality should show up pretty much exactly. 
 I have a question regarding multilevel modeling. I am building a 3-level model: 1st level yearly observations (variables which vary between years), 2nd level variables on company level, which do not vary between years, 3rd level industries, 4th level country based variables. The problem is that I have only one country, and here is my question: Is it ok if I use country variables on the 3rd level besides industry variables? or is it better to just skip country variables? If I use country variables at 4th level, there is a problem as I have only one country (one group), which would lead to inaccurate estimates of regression coefficients. Thank you for your answers. 
 I have a 2*5*6 design with all factors being within subjects. The factors are: Condition, Laterality, and Gradient. I want to know whether there are differences between the averages in the two conditions and whether those difference are somehow connected to Laterality and/or Gradient. To do so, I wanted to use a linear mixed model but I am very new to this. I am unsure how to define the random factor subject. I wanted to use random slopes because I assume that the condition could affect the different participants differently. If I do so, do I still need to integrate random slopes for Laterality and/or Gradient as well? Do I need to add interactions to the random intercepts? My current R-code might explain better what I've been doing so far: Cheers, Max 
 This question has two reasonable interpretations. Neither requires any calculation at all, but only careful reasoning about independence and disjoint outcomes. Conditional on the first ten flips, what is the chance the next ten flips match them in sequence? If we were to re-order the sequences of both flips in the same way, the question would be the same and therefore would have to have the same answer. Consequently the answer does not depend on the specific sequence of results in the first ten flips, but only on how many heads appeared (say $x$ of them). We now only have to compute the chance for one specific such sequence, such as first flipping $x$ heads in a row and then flipping $10-x$ tails in a row. The independence assumption immediately gives the answer, which I leave to the reader. What is the unconditional chance? That is, what is the chance before any coin is flipped that the second sequence of ten results will be identical to the first sequence? The independence of the flips allows you to consider them in any order. Consider the first and eleventh flips together, which constitute two independent flips. The chance that they match is the chance that both are heads or both are tails, which (since these two events are disjoint) must be the sum of the two probabilities $$\Pr(\text{match})=\Pr(HH) + \Pr(TT)=p^2 + (1-p)^2.$$ Similarly pair flip $i$ with flip $10+i$ for all $i=1,2,\ldots, 10$. The two sequences match if and only if all $10$ pairs are matches. Since the pairs are disjoint, their outcomes remain independent, producing an immediate answer, which again I leave to the reader. An interesting way to check the answers is to note that the second answer must equal the first answer, summed over all possible outcomes of the first ten flips multiplied by their chances. Equating the two, generalizing from $10$ to $n$, and applying the Binomial distribution for the value of $x$ in the first $n$ flips gives the curious but easily proven identity $$\sum_{x=0}^n\left[p^x(1-p)^{n-x}\right]\;\binom{n}{x}p^x(1-p)^{n-x} = \left(p^2 + (1-p)^2\right)^n.$$ A more pedestrian, but very effective, check is to compare the answers with a simulation. Here is code to estimate the unconditional chance for specified $n$ (such as $10$) and chance of heads $p$. In each iteration, it counts the number of differences between the first $n$ and second $n$ sets of flips. The estimated probability is the proportion of simulations where those counts were zero. The code outputs the estimate and its standard error. If the estimate lies within a couple standard errors of your calculated result, you're probably correct. When I ran it for $p=1/3$ and $n=10$ the output was It took one second to run a million iterations. 
 I know there are lots of libraries for machine learning and deep learning like caffe, Theano, TensorFlow, keras, ... But for me it seems like I have to know the architecture of the neural net, that I want to use. Is there a (visual) tool that allows to experiment with different network designs and apply them on own data? I'm thinking about something like the TensorFlow Playground , but with n-dimensional data and different layer types. Thanks in advance! 
 Consider a decision-making computational model with 6 free parameters. The model mimics human behavior: for a given decision-making task, it generates decision times and accuracy. For each parameter, we sampled 100 values from uniform distributions (bounded by extreme values of previous fits of the model to human data). We then simulated 100 datasets using these parameter values in each of 4 conditions: N = 50 simulated trials vs 100 vs 200 vs 500. Finally, we fit these simulated data to analyze the quality of parameter recovery. I am wondering about the kind of statistical analyses I could perform to (i) assess model slopiness and (ii) establish practical guidelines for clinicians (that generally fit the model to data from patients and interpret parameters). -For (i), I already computed correlations between original versus recovered parameters. Any other idea? -For (ii) I was thinking about the relation between reliability, expressed as a correlation, and the standard error of measurement. It’s this: SEM = SD*sqrt(1 - r^2), where r is the correlation between the simulated and fitted parameter values (i.e., the reliability of estimation) and SD is the standard deviation of the parameter values in the population. Estimates of a given parameter for group A and group B will be significantly different if the difference between them exceeds twice the standard error of measurement, i.e., significant if &gt; 2*SD*sqrt(1-r^2). Any other idea? Many thanks in advance for your precious help. 
 To the first part of your question there is no general answer I think, because it's a question about the "correct" null hypothesis of a test. Maybe you would be interested only in a short-run effect, so just one lag might do. Or you suspect a yearly/seasonal correlation, then you might need the 250th lag, too. So it depends. As a rule of thumb for this sample size and for such financial data I would personally view anything above lag 20 as awkward, but it's a matter of taste. With respect to the consequences of misuse, if you include too many lags the test loses power. So it might not reject the null of no correlation if only the first lag is relevant but you include 250 lags. On the other hand, if you only test the first lag(s) but the action is in the 20th, then the chosen hypothesis itself might not be adequate or reasonable (also inducing non-rejection). 
 I want to know if the presence of one variable significantly determines the dependent variable (which is binary ie. can only be 0 or 1). Is there any statistical analysis, other than computing a correlation, that I can perform to determine any statistical significance? While I cannot disclose details of my research, I will give an example: If I go and knock on someone's door to campaign, the person can either reject or accept my campaign. So I would like to know how significant the presence of me knocking on someone's door to campaign is in influencing their decision to accept as opposed to maybe they endorse the campaign on their own. Hopefully that makes sense! EDIT: I will have 3 or 4 predictor variables, all of which are categorical with 3+ levels. 
 I am using keras and I am able to get activations of specific neuron using the following code get_3rd_layer_output = K.function([model.layers[0].input], [model.layers[3].output]) layer_output = get_3rd_layer_output([X])[0] Is there a way to get a bounding box over the part of image where activation is the highest? As you can see, if I input the first image in album my 3rd layer neuron activates mostly for a tyre, so how can I get a bounding box over the tyre? 
 I calculated the Cosine distances for binary data and got the relations between different variables. I need to cluster them. I tried passing the cosine matrix directly to the (clustering) function but got an error. So, I tried passing the cosine matrix as a parameter to the function ( computes Euclidean distances between all rows in a matrix.) and then plotted the result with . I am getting good output but want to check whether I am doing anything wrong. Does it makes sense to pass cosine distances to the function? 
 How about just running some MCMC? 
 Just one caveat to keep an eye on - if the fold sizes are not equal then the average of fold results is not the same as the average over whole dataset. You should weight fold results by fold size to get the correct average. Suppose you have a dataset $X=\{1,2,3,4,5\}$ and use 2-fold cross-validation. The first fold is $\{1,2\}$ and the second fold is $\{3,4,5\}$. Let's say your predictions for the first fold are $\{2,2\}$ and for the second fold $\{4,4,4\}$. Then MSE for the first fold is $(1 + 0)/2 = 1/2$ and for the second fold $(1 + 0 + 1)/3 = 2/3$. Naive cross-validation result would be $(1/2 + 2/3)/2 = 7/12$. But if you calculate the same thing over entire dataset you get $(1 + 0 + 1 + 0 + 1)/5 = 3/5$. The correct way to calculate cross-validation results would have been $1/2*2/5 + 2/3*3/5 = 3/5$, 
 Would very appreciate any comments on the following problem: Imagine one has N sets of elements (points on the plane), coordinates (x,y) of 30% of the points in each set are known. So if we have 30 points we know coordinates only of 9 of them in this set. Points are missing randomly in all N sets. The issue is to estimate NND for this data. Is it somehow possible to make a conclusion about the "real" average NND based on the NND that one can compute from known 30% of data? Or there is another way to do it? Thank you in advance! 
 As @Heisenberg pointed out in the comments, before doing any model training/evaluation/selection, you should create a hold-out test set, which you in the end use once to test your single, final, ready-trained model. You could do this e.g. as follows: Create a training and hold-out test partition. Using the training partition, do repeated cross validation for training and evaluating different model types, model parametrizations, and combinations of features $^1$: This gives you multiple performance measures for each model type, model parametrization, and combination of features, from which you can derive how well exactly this configuration is suited for your task, and what its performance spread might be. Chose a model type, parametrization, and combination of features from those results, then train this model using the complete training partition. Test the obtained model on the test partition once to assure its performance is correct. $^1$ The deletion of features you explained seems to be close to backwards feature selection, e.g. recursive feature elimination : this starts with all features and removes features until a peak is reached in the average repeated cross validation performance. This means it re-evaluates the model each time. The big difference to what you mentioned is that this does not involve the test data at all: if you overfit, you do it just on the training data and will at last notice on the test data. As you are using : the package has all this ready-made in their implementation, so its easy to use, just takes a while to evaluate all the options. PS: in your example, it could theoretically be that you initially overfitted using all features, then by removing some of them decreased variance at the cost of (some little) bias. But much more likely you just overfitted your test data as well, as you used it like training data in an optimization process. UPDATE If you did not use any test data in training and only used test data to ensure that your final model actually has a reasonable performance, using such features, which are labeled "insignificant" by your feature importance metrics, is legit. This could e.g. be the case with some "weak" features, which only in their combination will solve a problem well - for example, a binary classification over two features $A$ and $B$, that are organized as as $AND$ or $OR$: Each of those features might end up being labeled insignificant, but their combination enables the problem to be solved - so using them in your model is reasonable. 
 This is a very basic R question, but I can't seem to find the right packages to do what I want. I have an array 'X', with n values. I want to simulate an array, 'Y', that follows a known relation Y = alpha + beta*X. Furthermore, I want to add intrinsic scatter to the Y array. Alpha, beta, and the intrinsic scatter should be input values by the user. Can someone help me with how I would go about doing this? Thanks! 
 Let X be a n*p matrix whose p columns are sampled from a centred normal multivariate distribution with true covariance matrix $\Sigma$ and let $\Sigma_{n}$ be its unbiased sample covariance estimator for n observations, how can I find ${\mathbb{E}(||\Sigma_{n}-\Sigma||}_{2})$ ? 
 I want to add a constraint to my optimization. Setting: I optimize parameters using given likelihood function . Inputs and are given. As you see in the code below, the likelihood function uses which is product of and matrix results from the parameters . Problem: Using this code, I get random 's that are optimising the likelihood function through . What i need, however, are such 's that will create a matrix equal to a given matrix . After the optimization i want to have optimal such that . I can also show my restrictions in following way: i know, for example, that which is in my case. In this way, I can create 25 restrictions for each number of 5x5 matrix . My question is, how can I add these additional constraints into my optimization problem? The function used for the likelihood optimization: P.s.: the given can be also defined as Thanks a lot! 
 Without knowing much about your underlying process, I'd suggest fitting a distribution with a limited domain, e.g. Beta distribution . You can get different shapes that match your description of data: Then you can compare the parameters of the distribution. Otherwise, it's pointless to compare the data that "looks like normal distribution" around 50% with data that is concentrated (limited from above) at 1. 
 Even though the is calculated individually by each model (model type), the underlying information should to some extent generalize across models. For example, a random variable should be labeled as unimportant by all model types, while e.g. a single, very distinctive feature should be recognized as such by all model types. One can come up with situations where this is not possible: e.g. when trying to model an XOR-like-problem using a linear model (underfitting), the variable importance will be less useful. But in case the model is powerful enough I'd guess that the derived variable importance captures some general, model independent about variable importance too. And this should be true especially with trees (therefore to some extent with tree based models like forests too), where more important variable tend to be closer to the root node. Having said this: I'd try to use the RF just from curiosity, but I'd also try not using it and see for the differences, or derive new variable importance from your other model types and try to compare them (e.g. ranking). 
 I did a simulation and the outcome's size was almost 30k. The other sample I want to compare them has only 50. Can I still do the test or should I radnomly pick 50 from the first and do it? 
 I'm using the lassop {MMS} in R to run elastic net on fixed effects in a mixed model. My model includes several higher-order interactions, and I'm interested in keeping the main effects in the model to conserve hierarchy. The lassop function includes the penalty.factor argument from glmnet. My question, is it valid to disable penalties on main effects (set penalty factor to 0) to maintain hierarchy of interaction terms? 
 It's not very clear to me why they would use a normal distribution to generate direct and indirect weights? Yes, they are using the command to draw a value from a normal PDF, but more importantly they are drawing this from the PDF for the fitted values they obtained in the first two steps of their estimation procedure. Those two steps being: Estimate a suitable model for the exposure conditional on confounders by using the original data set. Estimate a suitable model for the mediator conditional on exposure and baseline variables by using the original data set. Hence, the specification of the mean and standard deviation from their predicted values. Given that the dependent variable ($logphys$) was already close to normal, and because with more observations (they have over 3k) and more variables in a model, the distribution of fitted values will quickly tend toward normal, this seems to be a reasonable assumption. And, they do caveat this somewhat in their conclusions, advising the reader to conduct a thorough sensitivity analysis. Before drawing from your own fitted values with your own fitted data, it probably wouldn't be a bad idea to test your normality assumptions. The authors do note that, This can either be done by resampling from the observed exposures or by drawing from a normal distribution with the mean and standard deviation matching the observed exposure. In the example code you give, they take the second approach, but as they note, if you don't want to draw from the PDF, for whatever reason, you can also resample your data to draw these values. Why is the final weight generated as the fraction of weights of direct and indirect effects? I'm going to assume you follow their explanation of the use of counterfactuals in a Marginal Structural Model (MSM), so I would then direct you to the second column of p. 4, where they define their stabilized weights as: $$ w_i^c = \frac{P(M = M_i \vert A = A_i^*,C = C_i)}{P(M = M_i \vert A = A_i,C = C_i)} $$ In this, $A_i^*$ refers to the counter-factual and thus to the indirect effect, whereas $A_i$ refers to the direct effect. Recall that $A$ is the causal predictor of interest. $M$ is the mediator, and $C$ are confounders. In the above, we therefore have 'indirect' in the numerator and 'direct' in the denominator, thus . For future readers / other answers I think I've summarized the key points of the article as they relate to this question, but here's a link to the original. No paywall; it's open-access. 
 As you have the same information available in your past data as for your future data (both in the form of time series), you could use a number of sequence data/time series based model types to predict your target variable. The core difference is that those models "remember" information between samples, so don't only derive the output on from the $N$ input features of the current input, but also from input they've seen before. Such models range from e.g. Hidden Markov Models to Recurrent Neural Networks - but there are many more suitable model types, with different advantages/disadvantages depending on the details of your problem. PS: have a look at e.g. this answer or this slideset for some more details and further references. 
 Does anyone know whether latent profiles can be used as a mediator variable? 
 Say I am forecasting sales for a company that has four regions using ARIMA models. Each region behaves a little differently so four different ARIMA models are used. In order to forecast overall sales for the company would it be better to build a new model or could I just add all of the forecasts for the four regions together? My instinct says that adding the forecasts would be more accurate as some of the features of the data could be lost when aggregated at a company level but I wasn't sure how statistically sound that methodology was. Also, would it be acceptable to add the 80%/95% prediction intervals as well to come up with overall prediction intervals? 
 Seems like a trivial question but one for which I can't seem to find an answer. Are PLS-DA ( partial least squares discriminant analysis ) and PLS-LDA ( partial least squares followed by linear discriminant analysis ) the same? If not, how does PLS-DA differ from standard PLSR ( partial least squares regression )? As far as I understand it PLS-DA is just substituting the classes/factors for numbers. 
 I've got m samples coming from n different groups. What tests do I have to do before applying ANOVA to ensure that the preconditions are satisfied? Thanks in advance. 
 In this post , the best answer gives excellent mathmetical explanation among pearson correlation, co-variance and cosine similarity. Where I quote here ($\mathbf A $ is the data matrix). If you center columns (variables) of $\bf A$, then $\bf A'A$ is the scatter (or co-scatter, if to be rigorous) matrix and $\mathbf {A'A}/(n-1)$ is the covariance matrix. If you z- standardize columns of $\bf A$ (subtract the column mean and divide by the standard deviation), then $\mathbf {A'A}/(n-1)$ is the Pearson correlation matrix: correlation is covariance for standardized variables. The correlation is also called coefficient of linearity. If you unit- scale columns of $\bf A$ (bring their SS, sum-of-squares, to 1), then $\bf A'A$ is the cosine similarity matrix. Cosine is also called coefficient of proportionality. In addition to math explanation, is there any intuitive plot such as pearson correlation in Wikipedia (shown below) to show the relationship between these three "similarity measures" , i.e., what kind of shape each similarity metric is able to detect ? 
 Probability with Martingales: About $(c)$ My understanding is that $(c)$ is equivalent to: $$\sup E[M_n^2] &lt; \infty \iff E[A_{\infty}] &lt; \infty$$ Since $E[M_n^2] = E[A_n]$, we have $$\sup E[A_n] &lt; \infty \iff E[A_{\infty}] &lt; \infty$$ Why is that so? What I tried: $$E[A_{\infty}] = \lim E[A_n]\ \text{by MCT}$$ $$= \sup E[A_n] \ \because \ P(A_n \le A_{n+1}) = 1 \ \forall \ n \ \because \ P(A_n \le A_{n+1} \ \forall \ n) = 1$$ Is that right? 
 I'm running CFA {lavaan}; function cfa() in R. This article https://psychology.okstate.edu/faculty/jgrice/personalitylab/Grice_FactorScores_PM_2001.pdf details a number of different methods for extracting factor scores from factor analysis, based on a linear combination of values from each variable. I'm wondering if anyone knows which of these methods is used when running the predict() function on a model defined by the cfa() function in {lavaan} in R? For exploratory factor analysis, using the fa() function in the {psych} package, you can choose which factor extraction method to use. I can't see an option to choose which method to use for cfa() in {lavaan}, nor can I find an explanation of which method is used. Thanks very much for your help. 
 Edit: I'm currently doing some research into Principal Component Analysis (PCA) and I'm looking to implement some different forms of PCA algorithms in Matlab. I'm looking to use it for fault detection, and possibly fault isolation (if possible). I've implemented a Moving window PCA algorithm in Matlab which allowed time varying aspects of my data set to be taken into consideration, however the PCA model was calculated using SVD and wasn't overly quick for the data sets I use. I know there are other ways of calculating PCA, Eigen Value Decomposition and NIPALS are 2 such ways. I'm looking to know what benefits other forms of PCA have that make them stand out from each others? Such as the moving window taking into consideration the time varying aspect. 
 We know that it is possible that a model predicts the outcome reasonably well (and hence high R-square) but is actually misspecified (and hence low goodness of fit), as this example shows. The outcome of that example is a continuous variable. I am sure there are thousands of similar examples for binary outcomes, but it is just hard for me to imagine one. Can anyone find such an example for me that shows a model predicts the binary outcomes well but is actually misspecified, causing low goodness of fit? I'd appreciate it. 
 I am currently in the following situation: I have a sequence of processes which converges weakly to a limit process in D[0,1]. I want to show that the supremum of the processes converges as well to the supremum of the limit process. For this I need the continuous mapping theorem. In on page 3 it is also said, that it works, however the author doesn't explain it thoroughly (In D[0,1] these functions are also appropriate...) What is the proper argumentation for this phenomenon and what do have the continuous paths of the limit random variable have to do with this? How do I show the continuity of sup: D[0,1] -&gt; R ? Thank you a lot for your help in advance 
 objects are an S4 class, so to view the predict method, you would type . Basically, the workhorse function is which you should consult the help on. Basically, the default method returns the predicted latent variable value which is estimated in the M step of the EM maximum likelihood estimation of the latent model. 
 I am looking at a data set that contains multiple predictors and a continuous response. Using dismo along with gbm I built (a terrible one?) model. Using the package sROC, I got an AUC or 0.48 - so my model is actually quite worthless. Can anyone suggest any general ways to improve a model? I can post an example but ultimately, I am just wondering how experienced modelers tweak there models, etc. 
 I also posted this question on the R-SIG-Finance mailing list (thanks to @aginensky's suggestion), and the author of replied it. The initial value of $\hat{\sigma}^2_1$ by default is mean of squared residuals, and in this case it's . 
 You need to search for logistic regression or probit regression. They are almost identical but each is favoured in different areas of science, for instance in my experience in health logistic is almost universal while economists tend to use probit. Both of them are available in standard software. 

MatConvNet is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications.

Batch Normalization is a technique to improve learning in neural networks by normalizing the distribution of each input feature in each layer across each minibatch to N(0, 1).
 You are asking about hierarchical forecasting . The two approaches you describe are known as "top-down" and "bottom-up" forecasting. I personally have usually found combining both approaches to yield better forecasts - on all levels of the hierarchy. This section in this extremely good online forecasting textbook describes the different ways to do hierarchical forecasting, with an emphasis on this "optimal combination" approach. The package for R implements it. 
 How do I generate values either -1 or 1 in R ? produces random 10 samples of either 1 or 0. If I modify this code to I see three different values -1 or 0 or 1. I am just interested in -1 or 1. Any advice or suggestions are much appreciated. 
 Instead of sampling from the range -1:1, sample from the list -1, 1. 
 I'm using MlLib library de Spark and trying to perform Gradient-Boosted Trees algorithm on my data, that has mostly categorical features (and just two numerical features). in the example given in Spark documentation they are using LibSVMFile, that has only numerical values. But my data is json file, containing the categorical features, that I can easily parse into the Spark Dataframe, but no in LibSVMFile. In the description of algorithm though it's clearly said that GBTs handle categorical features along with As I understand this introduction, I don't need to make preprocessing of categorical features and their conversion into dummy variables, as well as I don't need to scale my numerical features and create the additional ones to model interactions and non-linearity ( PolynomialExpansion ) . But in the same time the input for algorithm is RDD[LabeledPoint], that is a Vector[Double] as far as I know. So I don't quite understand how will algorithm handle categorical data if it only accepts the Double as input? I guess, I can misunderstand the introduction or even the mechanism of algorithm, so maybe I need perform come hashing on categorical features, but I don't find any documentation about preprocessing the data for Gradient Boosted Trees, so your help and advice will be appreciated. 
 Could anyone tell me why I am getting different results from R weighted least square and manual solution by matrix operation? I am trying to manually solve $\mathbf W \mathbf A\mathbf x=\mathbf W \mathbf b$, where $\mathbf W$ is the diagonal matrix on weights, and comparing to R function with parameter. 
 It sounds like you're doing the following: Given $n$ data points $\{X_1, ..., X_n\}$, calculate a pairwise cosine distance matrix $C$, where $C_{ij}$ is the cosine distance between points $i$ and $j$. Treat each row of $C$ as a vector. Find a Euclidean distance matrix $E$, where $E_{ij}$ is the euclidean distance between rows $i$ and $j$ of $C$ Perform clustering, using $E$ This seems like an unusual thing to do, but I don't see any reason you can't do it. One way to interpret your procedure is that you're doing clustering using 'higher order' distances (not sure if that's the correct terminology). The final distance between points $X_i$ and $X_j$ measures the discrepancy between the cosine distance from each of these points to all other points. According to this measure, $X_i$ and $X_j$ will be near when they have similar cosine distances to all other points. Another way to interpret your procedure is that you're doing clustering in a feature space. You could think of step 1 above as mapping the original points into a feature space with $n$ dimensions (as many dimensions as data points). The projection of each point along dimension $j$ of the feature space is the cosine distance between that point and $X_j$. In steps 2 and 3, you do clustering using pairwise Euclidean distances in this feature space. But, what was the error you got when trying to cluster using cosine distances? 
 I have a question about comparing two subgroups in a conditional logit model. We obtained exact the same preference data from psychians and patients and want to test whether there exist a significant difference. We entered 21 attribute-levels in the conditional logit model (one is the reference level). Is it possible to compare each individual beta for patients and physicians? Using a test in stata or any other options? Thank you! 
 I would recommend the following very good article on forecasting aggregated time series, it should answer your questions and provide useful references on the topic: Lütkepohl (2010): Forecasting Aggregated Time Series Variables - A Survey. OECD Journal: Journal of Business Cycle Measurement and Analysis , 3(2):1-26 . 
 In your question, I believe features refers to the size of the space. For example, measurements for population variation were taken on two features: hair color and shoe size. Principle component analysis is often can often be associated with dimension reduction, but it need not be. In it's most general formulation, it is simply a rotation of the space to the most coordinate system where the variances along the axes are uncorrelated. In such a coordinate system, the components are linear combinations of the features, and in some cases, they may be the features themselves. The questions is asking you why using all of the features and using all of the principle components provides the same predictive power. I'll let you work that part out yourself. 
 One can find a general solution, without assuming Normality. In particular, if $A$, $B$ and $C$ are independent, and noting that the Covariance operator is the {1,1} central moment, then $\text{Cov}( \frac{C A}{A+B},\frac{C B}{A+B})$ is: where I am using a developmental version of the function in mathStatica (alas, not in any public release yet). The reason the solution does not converge (assuming Normal parents) is because $A+B$ is Normal, and thus $E[\frac{1}{A+B}]$ is the expectation of the inverse of a Normal variable, which is known not to converge (see, for instance, Mean and variance of inverse of a normal RV ) 
 Centering shouldn't matter since the algorithm only operates on distances between points, however rescaling is necessary if you want the different dimensions to be treated with equal importance, since the 2-norm will be more heavily influenced by dimensions with large variance. 
 Without any context, if you have two noncollinear variables, you have two features and two PCs. The PC reflects the structure of the data. Multiple collinear variables lead to fewer PCs than variables, the PCs simply provide an orthonormal basis for a matrix of features. What is perhaps interesting is sparse representation. LASSO is a supervised learning approach where a small number of features can be selected for predictive value. Alternately, choosing a small number of PCs and their loadings provides sparse representation of a matrix of features in an unsupervised approach. In that sense, the implications would be vastly different. 
 I am fitting a logistic regression model to assess the probability of graduates moving to a certain area after graduation. Since my data is spatial (I know from which area each individual in the dataset originates), I map the predicted probabilities of the logistic regression to get a sense of any geographical variation. However, I would like to be able to also get the probabilities for certain groups so that I can map them separately. Is it possible to obtain the probabilities only for individuals with certain characteristics, for example individuals who are male or who have studied a certain program (these variables are included as covariates in the model)? I believe my question is similar to this question , although I fail to understand what "simply average your estimated probabilities" means. 
 I'm curious, for those of you who have extensive experience collaborating with other researchers, what are some of the most common misconceptions about linear regression that you encounter? I think can be a useful exercise to think about common misconceptions ahead of time in order to Anticipate people's mistakes and be able to successful articulate why some misconception is incorrect Realize if I am harboring some misconceptions myself! A couple of basic ones I can think of: Independent/Dependent variables must be normally distributed Variables must be standardized for accurate interpretation Any others? All responses are welcome. 
 I am trying to solve this problem, and get an answer I am not sure about. "On his way to work, Mr. Smith passes 3 traffic lights. The probability of green light is 0.6 in each traffic light, independently with the other two. X is defined as the number of traffic lights that Mr Smith came to in green light, until he stopped on red light for the first time (not including the first red light), or until he arrived to his destination (there was no red light). Y is defined as the number of red lights, from the first red light and until green light (not including a green light) or until arrival to his destination. Calculate P(X=1, Y=2)." How do you think this should be done ? 
 We can ignore the matrix formulation, and just consider two vectors $x$ and $y$ (since the matrix formulation is just the vector operation repeated over different pairs of vectors). One intuitive/geometric distinction between covariance/correlation/cosine similarity is their invariance to different transformations of the input. That is, if we transform $x$ and $y$, under what types of transformations will the scores keep the same value? Covariance subtracts the means before taking the dot product. Therefore, it's invariant to shifts. Pearson correlation subtracts the means and divides by the standard deviations before taking the dot product. Therefore, it's invariant to shifts and scaling. Cosine similarity divides by the norms before taking the dot product. Therefore it's invariant to scaling, but not shifts. Geometrically, it can be thought of as measuring the size of the angle between the two vectors (as its name suggests, it's the cosine of the angle). All of these quantities depend on the dot product, so they can only detect linear structure. To address a question from the comments, mutual information is fully general, and can detect structure for any distribution. But, it's harder to estimate from finite data than other quantities, and more care must be taken. Also, it measures dependence, but doesn't indicate the direction of a relationship (e.g. variables that are correlated or anticorrelated can have the same same mutual information). Mutual information is a valid measure of dependence when no 'direction of relationship' even exists (non-monotonic relationships). If the goal is to detect relationships that are nonlinear but monotonic, then Spearman rank correlation and Kendall's tau are good options. 
 Suppose I deliver a special note to 2% of a population at random. What is the probability that such note has been delivered : parent and child cousin and cousin Assume a birth rate with a mean of of 1.2 and a life expectancy of 78 years and an age of becoming a parent with a mean of 35 years (distributions for these would improve the model). Improved models and corresponding formulas welcome. NOTE: Imagine the "note" I mentioned was a disease. I am trying to get a hunch about whether the disease has a genetic factor or not based on the data, which I plan to compare with your answers. NOTE: I hope this is the right site for my question, if not please cross-post to mathematics SE. 
 I aim to infer on the prevalence of terms across and within corpora of different languages (where document length varies within and across corpora). Given Zipf’s and Heap’s laws a simple tf/n seems already problematic within same-language corpora if document lengths vary greatly. But if, in addition, the total amount and distribution of unique terms differs across languages, comparisons on the basis of tf/n cannot be considered valid, can they? Does soembody know about a suitable metric/strategy in such a setting? Also simple pointers to discussions/applications of term frequency normalization in a multi-language setting would be greatly appreciated! 
 False premise: A $\hat{\beta} \approx 0$ means that there is no strong relationship between DV and IV. Non-linear functional relationships abound, and yet data produced by many such relationships would often produce nearly zero slopes if one assumes the relationship must be linear, or even approximately linear. Relatedly, in another false premise researchers often assume —possibly because many introductory regression textbooks teach— that one "tests for non-linearity" by building a series of regressions of the DV onto polynomial expansions of the IV (e.g. $Y \sim \beta_{0} + \beta_{X}X + \varepsilon$, followed by $Y \sim \beta_{0} + \beta_{X}X + \beta_{X^{2}}X^{2} + \varepsilon$, followed by $Y \sim \beta_{0} + \beta_{X}X + \beta_{X^{2}}X^{2} + \beta_{X^{3}}X^{3} + \varepsilon$, etc.). Just as straight line cannot well represent a non-linear functional relationship between DV and IV, a parabola cannot well represent literally an infinite number of nonlinear relationships (e.g. sinusoids, cycloids, step functions, saturation effects, s-curves, etc. ad infinitum ). One may instead take a regression approach that does not assume any particular functional form (e.g. running line smoothers, GAMs, etc.). A third false premise is that increasing the number of estimated parameters necessarily results in a loss of statistical power. This may be false when the true relationship is non-linear and requires multiple parameters to estimate (e.g. a "broken stick" function requires not only the intercept and slope terms of a straight line, but requires point at which slope changes and a how much slope changes by estimates also): the residuals of a misspecified model (e.g. a straight line) may grow quite large (relative to a properly specified functional relation) resulting in a lower rejection probability and wider confidence intervals and prediction intervals (in addition to estimates being biased). 
 I meet a really weird problem. I have one independent variable and one dependent variable, in the first model did not adjusting by covariate but result is significant. In the second add first covariate variable, the association did not show significance. However, in the third model, another covariate add in the model, the main independent variable turns to become significant associate with dependent variable. What kinds of result could I conclude from this result? 
 There are some standard misunderstandings that apply in this context as well as other statistical contexts: e.g., the meaning of $p$-values, incorrectly inferring causality, etc. A couple of misunderstandings that I think are specific to multiple regression are: Thinking that the variable with the larger estimated coefficient and/or lower $p$-value is 'more important'. Thinking that adding more variables to the model gets you 'closer to the truth'. For example, the slope from a simple regression of $Y$ on $X$ may not be the true direct relationship between $X$ and $Y$, but if I add variables $Z_1, \ldots, Z_5$, that coefficient will be a better representation of the true relationship, and if I add $Z_6, \ldots, Z_{20}$, it will be even better than that. 
 If I understand the problem correctly P(X=1,Y=2) means that he hit one green light, then two red lights. Ignoring yellow lights, if P(green) = 0.6, then P(red) = 0.4 Since all are independent of one another, P(X=1,Y=2) = P(green, red, red) = P(green) * P(red) * P(red) = 0.6 * 0.4 * 0.4 = 0.0960 or 9.6% 
 I'm thinking about the same problem and I haven't come upon a satisfactory solution yet, particularly when you model phi as a random effect and/or add covariates. Because, ideally (I think...), those effects and covariates would be modeled on the scaled phi, right? Anyway, you can do something like this: where timeint is a vector of the time intervals, for example, in your case But again, the linear predictor does not include phi.s here. 
 As you can see from the mathematical expressions for your calculations, you are obtaining $$((WA)^\prime (WA))^{-1} \; ((WA)^\prime (Wb)) = (A^\prime W^2 A)^{-1} (A^\prime W^2 b).$$ Evidently your weights are $W^2$, not $W$. Thus you should be comparing your answer to the output of The agreement is perfect (to within floating point error--internally, uses a numerically more stable algorithm.) 
 I have data of the number of days it takes an object to change from state A to state B, and I am interested in knowing what is the probability that this change of states happens before n days. Can I use the empirical cumulative distribution of the data I have to know this? In particular, I am using the ecdf() function from R to obtain the cumulative empirical distribution of my dataset (n=35). If I want to get the probability of the change of states of objects not in the dataset happening between days 30 and 60 I use the command diff(ecdf(ages)(c(30,60))), where ages is the array with my data. Am I doing this correctly? I don't know if it is statistically correct to extrapolate the empirical cumulative function to answer questions about data that is not in my dataset. That is, to make inferences about this unknown data, but I cannot think about a better alternative for now. Also, I have explored the possibility of using the kernel density of my data. From my understanding, the kernel density is a non parametric process that "infers" the probability distribution of a dataset through interpolation. Could this somehow be helpful to solve my question. 
 I probably wouldn't call these misconceptions, but maybe common points of confusion/hang-ups and, in some cases, issues that researchers may not be aware of. Multicollinearity (including the case of more variables than data points) Heteroskedasticity Whether values of the independent variables are subject to noise How scaling (or not scaling) affects interpretation of the coefficients How to treat data from multiple subjects How to deal with serial correlations (e.g. time series) On the misconception side of things: What linearity means (e.g. $y = ax^2 + bx + c$ is nonlinear w.r.t. $x$, but linear w.r.t. the weights). That 'regression' means ordinary least squares or linear regression That low/high weights necessarily imply weak/strong relationships with the dependent variable That dependence between the dependent and independent variables can necessarily be reduced to pairwise dependencies. That high goodness-of fit on the training set implies a good model (i.e. neglecting overfitting) 
 I ran a tobit regression and I'm having a difficulty in understanding what the below values mean: Log likelihood = -172.8655 LR chi2 = 10.04 Prob &gt; chi2 = 0.2624 Can someone kindly help me understand their significance? Is there a threshold for such values? Thank you 
 I'm afraid the results are just all quite inconclusive with this dataset. In any case, with a sample size of about T=80 you're not very far into the asymptotics. That would mean that the p-values are artificially small, and that "in reality" they might not be below the 10% threshold. On the other hand, the point estimates for your autoregressive coefficient 'a' in the various test equations are quite low, sometimes basically zero (a-1 = -1). That indeed would point to stationary series, where the estimation uncertainty is just so large that almost no null hypothesis could ever be rejected. Of course one could also look at the plots of the time series and use other test approaches, but personally I think it would feel strange to impose the assumption of a unit root if my point estimate of the root is close to zero. But that may be a matter of taste, too. 
 First, when the series are trending upwards, they are not stationary. What you mean then is "trend stationary" and I(0), which is not the same as stationary. Having said that, if your variables are all I(0) in levels (with a linear trend term), unit roots and cointegration don't make sense. With respect to the Engle-Granger part: You would have to be more specific in which of the two steps you want to insert the terms. Or to put it differently: In the second step where you test the residuals no deterministic terms should be included anymore, only in the first step. But gretl does the right thing there anyway. 
 Generally for linear models, changing the basis doesn't matter: For linear models, representing your features in your original two-dimensional basis or a new two-dimensional basis from PCA won't change the predictive power. For the purposes of this question, there's nothing special about the PCA basis. Let $\mathbf{x} = \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right]$ denote an observation from a two-dimensional feature space. We can construct a new basis for this two-dimensional space using any set of two linearly independent vectors. These vectors could be from Principal Component Analysis (PCA), but they could also come from elsewhere (as long as they're linearly independent). Let $U$ be a matrix whose columns are vectors of the new basis. Coordinates of our observation $\mathbf{x}$ in terms of the new basis is given by multiplying by the inverse of the matrix $U$. $$\mathbf{z} = U^{-1} \mathbf{x}$$ In many machine learning contexts, you use linear transformations of your observations to do stuff. You use some $A\mathbf{x}$ where matrix $A$ represents some linear map. Clearly: $$ A \mathbf{x} = AUU^{-1}\mathbf{x} = AU \mathbf{z}$$ That is, it's entirely equivalent to use: linear transformation $A$ and vector $\mathbf{x}$ written in the original basis linear transformation $AU$ and vector $\mathbf{z}$ written in the new basis You can see though that this wouldn't hold for non-linear transformations. Simple example: OLS regression Model is: $$y_i = \mathbf{x}_i \cdot \boldsymbol{\beta} + \epsilon_i$$ OLS estimate: $$ \mathbf{b}_x = (X'X)^{-1}(X'\mathbf{y})$$ With data in new basis: $Z = X{U'}^{-1}$ $$ \begin{align*} \mathbf{b}_z &amp;= (Z'Z)^{-1}(Z'\mathbf{y}) \\ &amp;= \left( U^{-1}X'X {U'}^{-1} \right) ^{-1} {U^{-1}X' \mathbf{y} }\\ &amp;= U' \left( X'X \right) ^{-1} U U^{-1}X' \mathbf{y} \\ &amp;= U' \left( X'X \right) ^{-1} X' \mathbf{y} \\ &amp;= U' \mathbf{b}_x \\ \end{align*} $$ Keep going and the residuals will be the same etc... You have the same predictive power and the estimated coefficients are related by the change in basis. 
 In gretl's GUI (graphical interface), you get a new window with your estimated model. There you will find a menu "Tests" (or the translated version of that), and in that menu two entries "non-linearity (squares)" and "non-linearity (logs)" which will perform some tests that should be relevant, if I understand you correctly. In the CLI (command-line or script interface) of gretl you would do "modtest --squares" or "modtest --logs" to do the same thing, after having estimated a model with ols. Apart from the gretl specifics, these generic tests run auxiliary regressions with additional terms, namely squared and log'ed terms as the names suggest, and test for exclusion of these additional terms. 
 A school claims that their placement rate for graduates is 85%. a group of graduates of this school think this claim is inflated so they take a survey of graduates to determine if the school is inaccurate in their report. From this survey 721 of 874 surveyed indicate that they are employed. Is this a one tailed or two tailed test Report the p-value for this test complete this test at the alpha=0.05 level of significance and carefully state the result 
 This is a question that came up on Udacity's "Intro to Statistics" course, and I'm having trouble interpreting the problem. This is a classical Bayes problem where we're looking at probabilities of people who have cancer, as well as probabilities of whether a test will detect whether someone has cancer. $\ p(cancer) = 0.1 \\ p(!cancer) = 0.9 \\ p(positive|cancer) = 0.9 \\ p(negative|cancer) = 0.1 \\ p(positive|!cancer) = 0.2 \\ p(negative|!cancer) = 0.8 $ One of the questions asked, "What is the probability that the test will give a positive result, irrespective of whether the person has cancer or not?" I thought this would be: $\frac{p(positive|cancer) + p(positive|!cancer)}{2} = 0.55$ However, this is wrong. I suppose I don't really understand what the question is asking when they say "when will the test give a positive result". How do I get p(positive) from this? 
 I have two long matrix with Observations and Predictions, with 76 columns each. I need to reduce the residuals by means of autocorrelation of error correction. I also would like to select the best method which adjust to my data. Here below find one example of a graph between observations and predictions. Thanks in advance! 
 I want to find the MLE estimators of $\lambda&gt;0$ and $\mu &gt; 0$ in a distribution with score vector $$\left[ \frac{-n}{2\lambda} + \sum_{i=1}^n \frac{ (x_i - \mu)^2}{2\mu^2 x_i} \ \ \ \ \ \ \lambda \sum_{i=1}^n \frac{ \mu - x_i}{\mu^3} \right]$$ and information matrix$$ \begin{bmatrix} n/(2\lambda^2) &amp; \sum_{i=1}^n (\mu - x_i)/\mu^3 \\ \sum_{i=1}^n ( \mu - x_i)/\mu^3 &amp; \frac{\lambda}{\mu^4} \sum_{i=1}^n (3x_i - 2\mu) \end{bmatrix}.$$ Here, $x_i &gt; 0 \ \forall i$. I can find the stationary points quite easily, but I am unsure how I am supposed to show that the information matrix is positive definite? 
 I am comparing maths and chemistry scores of students in several schools. I am not comparing the schools with each other. I am just trying to say that students do better in maths than in chemistry in all of the schools studied. So I am comparing the maths and chemistry scores with each other within a school not between schools. The data is normally distributed for both maths and chemistry in certain schools and not others. Should I do a non-parametric for all? or just do parametric for those with normal distribution and for non-parametric for those without normal distribution? what is the wisest decision to make? Is it usual and not strange to report let's say Mann-Whitney U test for Schools 1 to 3 and one-Way anova for schools 4 and 5? Is this kind of reporting normal given that I want to put all the results in one bar chart as below? By the way, the results will be published as an article in a scientific journal. 
 0 down vote favorite I have a large multivariate abundance data and I am interested in comparing multiple models that fit different combinations of three categorical predictor variables to my species matrix response variable. I have been using anova() to compare my different models, but I am having difficulty interpreting the output. Below, I have given my code as well as the corresponding R output. How do I interpret these results? Is m2 the best-fitting model because it has the lowest deviance, even though it has a higher p-value than m1? Is this because the p-value is suggesting that there is a significant level of deviance, so the optimal model will have a higher p-value? Any suggestions on how to interpret these results would be much appreciated- I haven't been able to find a clear answer in my Google searches. Thanks! 
 I got two data sets (different omics data) from several observations across time (Two data sets will look like N*P1*T, N*P2*T, N=number of observations, P1/2=number of variables of first/second omics data set, T=number of time points we collected samples). I would like to find the robust associations between variables in different omics data. Right now, I can calculate the correlation between each pair of variables (among P1, P2) in each observation across time. Thus, I will get a P1*P2 correlation matrix for each observation. But how can I get one correlation matrix for all observations? Or, some other professional methods for this kind of questions? Thank you. 
 I've been reading Maraun et al , "Nonstationary Gaussian processes in wavelet domain: Synthesis, estimation, and significant testing" (2007) which defines a class of non-stationary GPs that can be specified by multipliers in wavelet domain. A realization of one such GP is: $$ s(t) = M_h m(b,a) W_g \eta(t) $$ Where $\eta(t)$ is white noise, $W_g$ is the continuous wavelet transform with respect to wavelet $g$, $m(b,a)$ is the multiplier (kinda like a Fourier coefficient) with scale $a$ and time $b$, and $M_h$ is the inverse wavelet transform with reconstruction wavelet $h$. One key result of the paper is that if the multipliers $m(b,a)$ only change slowly, then the realization themselves are only "weakly" dependent on the actual choices of $g$ and $h$. Thus $m(b,a)$ specifies the process. They go on to create some significant tests to help infer the wavelet multipliers based on realizations. My question is, how can this wavelet domain GP be related to a real-space GP ? Specifically, Is a scaled wavelet transform as above equivalent to some real-space Gaussian process? In that case, can we compute the covariance $K$ from $m(b,a)$? And in either domain, how do we calculate the likelihood? Edit: For comparison, the kernel of a stationary Gaussian processes is the Fourier dual of its spectral density (Bochner's theorem, see Rasmussen chapter 4) - which gives an easy way to switch between a real space GP and a frequency space one. Here I'm asking if there's such a relationship in the wavelet domain. 
 Probably it is duplicate (but I could not fine it). In experiment I measure the parameter "P" of the set of objects. This is extremely difficult and pricy. From the other hand I can easily measure the set of parameters "p1,p2,...,p10" of all objects. I know that they somehow correlates with parameter "P", but I do not know how. Is it possible reveal this hidden correlation? Should I use some modification of PrincipalComponents or cluster analysis? 
 I have a list of 1.2 million ILSVRC2012 training images with corresponding class codes, 000 to 999. Where can I find the semantic labels for these codes? 
 There is no doubt that these plots indicate heteroscedasticity. If an exact test needed my recent study will give the respond "A new test to detect monotonic and non-monotonic types of heteroscedasticity, journal of applied statistics, 2016" 
 Most software suites will use Euler's number as the default log base, AKA: natural log. You can use a higher base number to rein in excessively right-skewed data. How you do it syntax-wise depends on the software you are using. If you need to get back out of you transformed values once estimations have been done, it might be a little easier to use this method because all you have to do is perform a exponential operator on your variable with whatever your log base was. 
 If your model is correct, observing a likelihood function falling this far from the prior is extremely unlikely. Imagine the situation before seeing the data $X_1$. Based on on correct inferences from past data $X_0$ you believe that $\mu \sim N(1.6, 0.4^2)$. Suppose that the future set of data is a single normal variate $X_1 \sim N(\mu, 0.4^2)$. The likelihood will then have its maximum at $X_1$. From this it follows that the location of the likelihood functions maximum, once $X_1$ is observed, will fall at a normally distributed position with expectation equal to your prior mean 1.6 and standard deviation equal to $\sqrt{0.4^2 + 0.4^2}=0.56$. So the probability that it falls at 6.1 (as in your example) or even further away from your prior mean is $2\phi(-(6.1-1.6)/0.56)=9.3\cdot 10^{-16}$. So this is not going to happen in practice provided that your model including past inferences about $\mu$ are correct. For simplicity, suppose that the "past data" was also a single variate $X_0 \sim N(\mu,0.4^2)$ and that you started off with a flat prior before observing $X_0$. Put differently, in terms of $X_0$ and $X_1$ the above situation or something more extreme arise if $|X_1-X_0|&gt;6.1-1.6$ which is extremely unlikely given that both variates according to the model should have come from the same normal distribution with a standard deviation of 0.4. So if you come across something like this in practice, it would strongly suggest that either the inference made from $X_0$ or $X_1$ (or both) are wrong and in some way have ignored important sources of uncertainty. If you believed this to be the case but you happened to be unable to find any error, I would say the best you could do would be to adjust the standard deviations of both likelihood and prior post hoc to make them more compatible. This would indeed result in an adjusted wider posterior more compatible with your intuition. 
 I've spent a long time generating and analyzing a dataset and now it comes down to the statistical analysis... and I'm not sure how best to proceed. In my dataset, I have rats sorted into one of 4 treatment groups. Within each treatment group, data was recorded from each rat at 10 consecutive timepoints. At each timepoint, 23 datapoints (the dependent variable) were collected from each rat, corresponding to 23 discrete depths within the rat cerebral cortex. So basically, I have three factors to assess in terms of effect on the DV: treatment group, timepoint, and cortical depth. My intention is to run an ANOVA to determine the significance of effect of each of these factors and/or the interaction effects between factors. I'm not sure what exact paradigm I should use. I was advised that because I am using timepoint data, that I should use a repeated measures ANOVA. But since treatment group is clearly a fixed variable, does that mean I would need to use a mixed model approach (as I would have both fixed and random variables)? Furthermore, I am uncertain as to whether to treat depth is a fixed variable or a random variable (like time). Because I am sampling 23 depths from each animal, it seems to me that this is analogous to sampling 10 timepoints from each animal, making depth a random variable. But I'm not at all certain here. To make matters more complicated, my data was not really collected at discrete timepoints per se. I have averaged the data across designated time intervals to generate discrete values, but in actuality, the data was collected continuously. Taking this into account, perhaps ANCOVA would be appropriate? Or possibly just scrapping the ANOVA idea and going for regression? To cover all bases, I am currently using Matlab for the data analysis. Edit: See comment below for more specific experimental details. 
 I'm trying to understand the intuition behind the estimates I get from a single two-level meta-analysis with a moderator compared to separate two-level meta-analyses for each moderator value. My dataset This is what my dataset, , looks like: That is, I have data from two different studies, both containing five different, independent effect sizes, where each effect size can be categorized as either or . Here, the values are set so that the mean of all of them is 0, the mean of is 0, and the mean of is 0. Analyses On this dataset, I'm applying two different type of analysis methods. A single two-level meta-analysis with a moderator First, I run a two-level meta-analysis (using the function from the package) with as the moderator: This gives me the following output: That is, is estimated as 0.0333 and is estimated as -0.0333. Separate two-level meta-analyses for each moderator value Next, I run separate two-level meta-analyses for each moderator value. That is, I create two data sets that I then use for the basis of two sepatare two-level meta-analyses without moderators This gives me the following output for and the following output for That is, both and are estimated to 0 (which also is the grand mean for and respectively. My question Now, I'm trying to understand the intuition behind why this is the case, that is, that and get different estimations depending on if they are computed from separate analyses or a single analysis with moderators. 
 Given population, which is normal distributed with $\sigma = 4$ , the Z-test must be done with $H_0$: $\mu = 10$ and $H_A$: $\mu$ $\ne$ $10$ with significance level $\alpha = 0.05$ Is sample size $n= 45$ enough, in order to receive the probability of type error 2 = 0.2, when $\mu = 12$ This is my solution: first of all i found the value of $\bar X $, for which we will reject null hypotethis: $$\frac{\bar X - \mu_0}{\sigma * 1/\sqrt n} \le -1.96$$ $\frac{\bar X - 10}{4 * 1/\sqrt 45} \le -1.96$ and from this i got the inequality $ \bar X \le 8.824$. The same approach i have used for upper value and i got: $ \bar X \ge 11.276$ From this i derived the power of test :$$P(X \le 8.824 |\mu = 12) = P(Z \le \frac{8.824 - 12}{4 * 1/\sqrt 45}) = P(Z \le -5.2) = 0.000000..1$$ $$P(X \ge 11.176 |\mu = 12) = P(Z \ge \frac{11.276 - 12}{4 * 1/\sqrt 45}) = P(Z \ge -1.373) = 0.14$$ Power = 0.14 + 0.00000000..1 = 0.14 P(type error 2) = 1-0.14 = 0.86 From this i concluded, that sample size n = 45 is enought to receive probability of type error 2 = 0.2 So i have such questions: 1)Are my approach and calculations correct? 2)Do we have more efficient way of solving this problem? 3)If n would not be given, how to determine the sample size, if probability pf type 2 error is given( with alternatibe mean too) Many thanks 
 A defined set of products is distributed across different shops randomly across different regions in the world. I would like to know about the statistical techniques that i can use to do the segmentation of date. So that i can answer questions like 1) List of products that are concentrated or available only in particular region of the world 2) Most of shops concentrated on which part of world? 3) List of shops that are offering particular product on a single region? so data has three variables, one being the region, second being the shop and third being product. Many Thanks, 
 I am currently developing an anomaly detection algorithm to test for anomalies in system usage. This is obviously very seasonal in nature (for example, during the Super Bowl, for example, there will be much higher usage, and this is expected and not an anomaly). I have already written the clustering part of the algorithm that can detect portions of high usage that aren't normal. However, I need to incorporate the timing and seasonal aspect as mentioned. What I am thinking of doing is doing some kind of periodic regression of some sort over historical data and developing a weighting number for each day or time period that gives an estimate for the amount of expected usage on that day. So my basic questions are first, is this a reasonable way of doing it (i.e. are there better or simpler methods)? Second, are there existing functions or algorithms to help do this? I have a metric for each day that gives a number for how much activity there is for some day, so this can easily be used for the algorithm. 
 For a university project, we want to use reinforcement learning from raw camera input to teach a robot to hit a ball. It works when we detect the ball and feed that to the algorithm, so raw data is the next step. As this data is too high dimensional, we want to apply dimensionality reduction. In a first evaluation, we used PCA and other non-neural methods to reduce from 28x32 to around 20 dimensions while still be able to learn from it with a bit loss in precision. Currently, we want to see whether autoencoders also work for that. The code base of the team was matlab, therefore we used in the first part matlab autoencoders . We used a dataset of 100 pictures and reduced to 200 dimensions. Options are mostly default, from what I remember it where up to 200 episodes. The result of encoding and decoding for Matlab can be seen in the following picture: Right now, the lab is transitioning to Python instead of Matlab. Therefore, we used Keras to build a custom autoencoder, closely following this blog entry . But when we run our autoencoder on the same dataset to same dimensions, the following happens: The ball disappears. We have created a bigger dataset with up to 2500 of these pictures, but the ball always disappears: Also, we used the sparse autoencoder and deep autoencoder, played with the activations and loss function, but always the same result. Has anybody tips on how to improve the learning so we keep the ball? Why does it work on the small dataset in Matlab, but not in Keras on the same? 
 I have some data sets defining a random variable (already checked). and I tried to fit those data to different distributions. I have found a bunch of distributions that fit and have not been rejected by the Kolomogorov Smirnov test . However, a collegue told me that in the choice of the distribution, one criteria could be taken into consideration is the number of the parameters in the distribution. Is this really something to take into consideration ? If I have two distributions, one with 2 parameters and the other with 4 parameters that are fitting my data. Is it wise to choose the one with 2 parameters ? Is there an article discussing this ? Thanks ! 
 My question concerns the relationship between the efficient coding hypothesis which is outlined on the Wikipedia page on efficient coding and neural network learning algorithms. Are there any neural network models explicitly inspired by the efficient coding hypothesis? Or would it be fairer to say that all neural network learning algorithms are at least implicitly based on efficient coding? 
 Probability with Martingales: Do we need $$|A_{S(k) \wedge n}| \le k$$ ? I think $$A_{S(k) \wedge n} \le k$$ is enough Prove $$A_{S(k) \wedge n} \le k$$ What I tried: Case 1: $$S(k) &lt; n$$ $$A_{S(k) \wedge n} = A_{S(k)} \le k$$ because $A_{S(k) + 1}&gt; k$ and $A_p \le k$, $p &lt; S(k) + 1$ Case 2: $$S(k) \ge n$$ $$A_{S(k) \wedge n} = A_{n} \le k$$ Is that right? Do we have $$|A_{S(k) \wedge n}| \le k$$ because $$0 \le A_{S(k) \wedge n}$$ ? And we have that because $$0 = A_0 \le A_n \le A_{n+1}$$ ? 
 I'd say the first one you list is probably the most common -- and perhaps the most widely taught that way -- of the things that are plainly seen to be wrong, but here are some others that are less clear in some situations (whether they really apply) but may impact even more analyses, and perhaps more seriously. These are often simply never mentioned when the subject of regression is introduced. Treating as random samples from the population of interest sets of observations that cannot possibly be close to representative (let alone randomly sampled). [Some studies could instead be seen as something nearer to convenience samples] With observational data, simply ignoring the consequences of leaving out important drivers of the process that would certainly bias the estimates of the coefficients of the included variables (in many cases, even to likely changing their sign), with no attempt to consider ways of dealing with them (whether out of ignorance of the problem or simply being unaware that anything can be done). [Some research areas have this problem more than others, whether because of the kinds of data that are collected or because people in some application areas are more likely to have been taught about the issue.] Spurious regression (mostly with data collected over time). [Even when people are aware it happens, there's another common misconception that simply differencing to supposed stationary is sufficient to completely avoid the problem.] There are many others one could mention of course (treating as independent data that will almost certainly be serially correlated or even integrated may be about as common, for example). You may notice that observational studies of data collected over time may be hit by all of these at once... yet that kind of study is very common in many areas of research where regression is a standard tool. How they can get to publication without a single reviewer or editor knowing about at least one of them and at least requiring some level of disclaimer in the conclusions continues to worry me. Statistics is fraught with problems of irreproducible results when dealing with fairly carefully controlled experiments (when combined with perhaps not so carefully controlled analyses), so as soon as one steps outside those bounds, how much worse must the reproducibility situation be? 
 Are there ways in which $\text{Cov}(X,Y) = 1$ can hold and yet "something surprising" can still also be true? For example: it is possible for the mean depth of a pool to be only $10 \text{ ft}$, yet small portions of the pool may nevertheless extend downward to any arbitrary length. In this sense, the mean statistic can be surprising. Are there similar or analogous ways that perfect correlation can be surprising? 
 I have just gained some theory about how the SVM works and was curious how efficient it is in practice. After generating a dataset with two classes, I tried to check how good the SVM algorithm is by splitting the data into a training set and a testing dataset and then classifying the new observations to the suitable regions. Having fitted the SVM with some parameters using the Linear Kernel, I wanted to visualize the boundaries AND the contour plot of decision values. The kernlab package suggests that the plot(svp, data) does a good job for the latter problem. I implemented some code to be capable of visualising the boundaries. Unfortunately, the points that the plot(svp, x) command draws turn out to be rather different than the observations are! It's because the plots are in different units, however the boundaries are exactly the same in the built-in command as in my command, but it is unclear how the points are derived. I've spent hours on this, but I have not been able to come up with any idea why this is happening.. My code is as follows: Thanks in advance. 
 I'm very new to statistics, and I have a problem that may or may not exactly be considered a time series analysis problem. I have a large set of vehicle location measurements (x0, y0)...(xt, yt) taken over time measurements 0...t, and I would like to be able to predict the location at time t+1. The issue is, the differences between consecutive measurements aren't exactly equal; each is ~15 seconds, but can range from 14-16 seconds. Could this still be considered a time series analysis problem, or will certain methods fail when the delta of time isn't constant? 
 I am building new features for a binary classifier. The new features fall into two categories: categorical and ordinal. An example of the first feature would be the colours and one of the second would be integer counts . For the ordinal variables I can roughly get an idea at how good each feature is by computing the area under the roc ( AUROC ) curve. If the AUROC is close to 1, it means that there is a good threshold value for the new feature such that it can discern well between true and false positives. I would like to have a similar measure for the categorical features. For example, I know in each category what the rate of 1's is. However, it is hard to compare this rate across many category levels. Would be keen to hear your suggestions on what to do. One thought I had was to fit a logistic regression with the categorical variable as the only predictor, and then calculate the AUROC for the predicted probabilities under this regression. 
 Probability with Martingales This was answered here using bounds and here using PGF. I would like to try inclusion-exclusion. Let $H_1, H_2, ...$ be independent events with $P(H_k) = p$ where $H_k$ means the kth toss is heads. It seems that $$A_0 = H_1$$ $$A_1 = H_2 \cup H_3$$ $$A_2 = (H_4 \cap H_5) \cup (H_5 \cap H_6) \cup (H_6 \cap H_7)$$ $$A_3 = (H_8 \cap H_9 \cap H_{10}) \cup ... \cup (H_{13} \cap H_{14} \cap H_{15})$$ $$\vdots$$ $$A_k = (H_{2^k} \cap ... \cap H_{2^k + k-1}) \cup ... \cup (H_{2^k - k} \cap ... \cap H_{2^k - 1})$$ So by inclusion-exclusion, we have $$P(A_0) = p$$ $$P(A_1) = p + p - p^2$$ $$P(A_2) = p^2 + p^2 + p^2 - p - p = 3p^2 -2p$$ $$P(A_3) = 6p^3 - 5p^2 + 4p$$ $$P(A_4) = 13p^4 - 12p^3 + 11p^2 - 10p$$ $$\vdots$$ It seems like for even $k \ge 2$, we have $$P(A_k) = [2^k - (k-1)]p^k - [2^k - k]p^{k-1} - ... + [2^k - 2k + 3]p^2 - [2^k - 2k + 2]p $$ and for $\color{red}{\text{odd}}$ $k \ge 2$, we have $$P(A_k) = [2^k - (k-1)]p^k - [2^k - k]p^{k-1} - ... \color{red}{-} [2^k - 2k + 3]p^2 \color{red}{+} [2^k - 2k + 2]p $$ Hence, $$P(A_k) = \sum_{i=1}^{k} p^i [2^k - 2k + i + 1] (-1)^{k-i}$$ $$= \sum_{i=1}^{k} p^i [2^k - 2k + 1] (-1)^{k-i} + \sum_{i=1}^{k} p^i [i] (-1)^{k-i}$$ Observe that $$\sum_{i=1}^{k} (-p)^i = \frac{1-(-p)^k}{1-(-p)}$$ and $$\sum_{i=1}^{k} i(-p)^i = p\frac{d}{dp}\frac{1-(-p)^k}{1-(-p)}$$ Hence, we have $P(A_k)$ $$= \frac{1-(-p)^k}{1-(-p)}(-1)^k(-p) [2^k -2k + 1] + (-1)^k(p) \left[\frac{(-1)^kp^k[1+k+pk]-1}{(1-(-p))^2}\right]$$ Is that right? 
 The document of scikit-learn said it use coordinate descent for training multi-task lasso. I have tried to derive the update rule but its too hard for me. Can you show me what is update rule for multi-task lasso used in scikit-learn and how to derive it. Thanks! 
 I have 8 variables that I want to combine into one performance score. I am doing PCA and taking the first three components (~62% of variance), then adding the three scores for each record from these components to get one final score. My question: four variables load high on the first component, three load high on the second component, and one variable loads high on the third component. Does that mean that the last variable that is loading high on the third component will have more impact on the final score than the other variables? I am asking since I want equal representation for all variables in my final score. Thank you for your help in advance. 
 You misunderstood some values. Multivariate test table's Dev is decrement from upper model (When a model has a interaction term, become a litte more complex). So I think each p-value indicates the difference between the model and upper one is statistically significant or not. Here is my example code. 
 I am experimenting with stepwise regression for the sake of diversity in my approach to the problem. So, I have 2 questions: What are the advantages of stepwise regression? What are its specific strengths? What do you think about hybrid approach, where you use stepwise regression to select features, and then apply regular regression taking all the selected features together? 
 If you're referring to the actual correlation coefficient (i.e. calculated from population moments): In a practical sense, no, but it depends what you mean by surprising! Let's assume: $x$ and $y$ are random variables For simplicity, that $E[x] = 0$ and $E[y] = 0$ $E[x^2]$ and $E[y^2]$ both exist and are greater than zero, and that $E[xy]$ exists. Hence correlation coefficient exists. You might think that if $Corr(x,y)=1$ then $x$ and $y$ must be scalar multiples of each other. That's correct for a finite probability space, but not quite right in the more general case. Instead, you can show $\left(E[xy]\right)^2 = E[x^2]E[y^2]$, i.e. perfect correlation, if and only if there exists a scalar $\lambda$ such that: $$E[(x - \lambda y)^2] = 0$$ $x$ can be different from $\lambda y$ for some outcomes in the sample space, but the probability measure of those outcomes has to sum to zero. To show this, observe the expectation operator defines an inner product, and the problem reduces to finding a condition for equality in the Cauchy-Schwarz inequality. See the comments of Dilip Sarwate here . If the correlation coefficient is calculated from sample statistics: If the correlation coefficient is an estimate, the estimate may not be correct. You may especially worry that rare events. Example: In the data, a company's interest payments are perfectly correlated with the interest rates listed on the company's bonds. In the data, the correlation coefficient is one! But later, the company defaults! In actuality, the two weren't perfectly correlated, rather, the sample size was too small to observe the rare event of default. 
 I have encountered two problems that are almost identical. Consider this: $12$ basketballs numbered 1 through 12 are thrown at random into 20 boxes . I want to know the total number of possible outcomes (assume order matters). The answer is $20^{12}$. In the other problem (which I do not need to mention), follows the same pattern and the answer is also $n^k$, where $n$ is the number of slots and $k$ is the number of objects I have. I came up with the $n^k$ conclusion by myself. I am just wondering, why is this the number of possible outcomes? Here are my efforts to understand it: I Googled the problem, but I got no results. I wanted to confirm that this $n^k$ conclusion is true. I did a simpler example of 2 objects and 4 slots and the result was matching the $n^k$ form. I tried to use the multiplication principle (the counting one). We can say this: first stage throw all balls each basketball gets into a different box. It is . Well, there is no a second stage. If I want to "add" more ways, I need to use the counting principle again in another context. Let us say make two balls in each box... etc. It is just a crazy overhead counting. I tried to make an analogy with the problem of rolling two six-sided fair dice where the total outcomes of the pair they make is $6^6$. But it did not click with this basketball problem. Notes: You can throw more than one basketball (at random) into one box. This problem was encountered in a "probability stuff" context. I was able to go through the numerator but not this one. 
 What did you google? The key word you need is combinatorix. Specifically, combinatorix counting. This site has a decent description of why. https://en.wikibooks.org/wiki/Probability/Combinatorics I hope it helps. 
 In Hayashi's Econometrics, it is stated that one of the assumption of classical OLS is: $$\mathbb{E}(\epsilon_i\lvert\mathbf{x_1}, \mathbf{x_2}, \ldots, \mathbf{x_n}) = 0 \text{, for } i=1, \ldots, n. \tag{1}$$ And I know that the implications are that $\mathbb{E}(\epsilon_i) = 0$ for all $i = 1, \ldots,n$, and that the error term is uncorrelated with the regressors. But, what does the equation (1) in itself actually mean? A pedagogical example would be helpful. 
 I've got a whole lot of police data and was wondering what sort of approaches I could use to show strong correlation, and if possible, causal effects, between police killings/arrest &amp; call-ins for petty-crimes and class/race/gender/location (which is itself connected to class/race). Really sorry for the vague question. Any help would be much appreciated! 
 First, you throw the first ball (the ball numbered 1). There are $n$ possible outcomes for this Second, you throw the numbered 2 ball, there are also $n$ possible outcomes and so on ... Finally, you throw the last ball (the k-th ball), there are also $n$ possible outcomes You use multiplication principle then you get the anwser is $n \times n \times ... \times n = n^k$ possible outcomes Is that anwser your question? 
 It's very common to assume that only $y$ data are subject to measurement error (or at least, that this is the only error that we shall consider). But this ignores the possibility - and consequences - of error in the $x$ measurements. This might be particularly acute in observational studies where the $x$ variables are not under experimental control. Regression dilution or regression attenuation is the phenomenon recognised by Spearman (1904) whereby the estimated regression slope in simple linear regression is biased towards zero by the presence of measurement error in the independent variable. Suppose the true slope is positive &mdash; the effect of jittering the points' $x$ co-ordinates (perhaps most easily visualised as "smudging" the points horizontally) is to render the regression line less steep. Intuitively, points with a large $x$ are now more likely to be so because of positive measurement error, while the $y$ value is more likely to reflect the true (error-free) value of $x$, and hence be lower than the true line would be for the observed $x$. In more complex models, measurement error in $x$ variables can produce more complicated effects on the parameter estimates. There are errors in variables models that take such error into account. Spearman suggested a correction factor for disattenuating bivariate correlation coefficients and other correction factors have been developed for more sophisticated situations. However, such corrections can be difficult &mdash; particularly in the multivariate case and in the presence of confounders &mdash; and it may be controversial whether the correction is a genuine improvement, see e.g. Smith and Phillips (1996). So I suppose this is two misconceptions for the price of one &mdash; on the one hand it is a mistake to think that the way we write $y = X\beta + \varepsilon$ means "all the error is in the $y$" and ignore the very physically real possibility of measurement errors in the independent variables. On the other hand, it may be inadvisable to blindly apply "corrections" for measurement error in all such situations as a knee-jerk response (though it may well be a good idea to take steps to reduce the measurement error in the first place). (I should probably also link to some other common error-in-variables models, in increasingly general order: orthogonal regression , Deming regression , and total least squares .) References Smith, G. D., &amp; Phillips, A. N. (1996). " Inflation in epidemiology: 'the proof and measurement of association between two things' revisited ". British Medical Journal , 312 (7047), 1659–1661. Spearman, C. (1904). "The proof and measurement of association between two things." American Journal of Psychology 15 : 72–101. 
 Spoiler alert: My stats education stopped before ANOVA so everything that I know, I learned myself. Design of data: I have 2 groups and a response variable is measured across multiple blocks in the same participants. There is also another within subject factor, the type of task that they did. Both groups underwent the same tasks in the same amounts and for the same number of blocks. I thought this was a mixed ANOVA. I used ezANOVA and aov functions but then realized that one of my participants had to be excluded. Now they both mysteriously stopped working. How do I address this? Do I need to have equal groups for this to work or am I just missing some kind of correction that needs to be applied? I would appreciate tips or a direction to pursue... 
 I have a categorical variable with 3 levels: Attention Deficit, Other Types of Deficit, No Deficit. My hypothesis is only interested in comparing Attention Deficit and Other Types Deficit using regression. Is it right to exclude the subjects with no deficit from my sample when I run the regression? Meaning I will have only one dummy coded variable in my regression? 
 I have just taked a google search for what is Stepwise regression. I'm not sure if I fully understand it, but here is my first thought It's greedy so it can not produce the good solution as Lasso does. I prefer Lasso It's simple, easy to use, easy to code After you use Stepwise regression, you already end up with a trained model that use selected features, so you do not need to use an other regression step as you mentioned as hybrid approach 
 Stepwise selection is not generally a good idea. To understand why, it may help you to read my answer here: Algorithms for automatic model selection . As far as advantages go, in the days when searching through all possible combinations of features was too computationally intensive for computers to handle, stepwise selection saved time and was tractable. However, note that the problems discussed in my linked answer above apply just as much to 'best subset' regression, so stepwise does not produce a good solution, just a bad solution faster. Your idea of a hybrid approach would be fine, so long as the second model (with the selected features) was fitted on a new dataset . 
 I'm a month-old user of RStudio. Currently, I'm working with hierarchical time series analysis with the hts package. After reading my .csv data below, I've tried creating the matrix out of it with these codes: Then a matrix was successfully created, however the values were rnorm generated: How do I replace them with the original grouped series values? Or how do I start hts better, with my data import? Thanks and more power! :) 
 In English, it means that conditional on observing the data, the expectation of the error term is zero. How might this be violated? Example: omitted variable correlated with $x$ Imagine the true model is: $$ y_i = \alpha + \beta x_i + \gamma z_i + u_i$$ But instead imagine we're running the regression: $$ y_i = \alpha + \beta x_i + \underbrace{\epsilon_i}_{\gamma z_i + u_i}$$ Then: $$\begin{align*} E[\epsilon_i \mid x_i ] &amp;= E[\gamma z_i + u_i \mid x_i] \\ &amp;=\gamma E[ z_i\mid x_i] \quad \text{ assuming $u_i$ is white noise} \end{align*}$$ If $E[z_i \mid x_i] \neq 0$ and $\gamma \neq 0$, then $E[\epsilon_i \mid x_i] \neq 0$ and strict exogeneity is violated. For example, imagine $y$ is wages, $x$ is an indicator for a college degree, and $z$ is some measure of ability. If wages are a function of both education and ability (eg. the true data generating process is the first equation), and more able individuals are more likely to obtain a college degree (hence $E[z_i \mid x_i] \neq 0]$), then if one were to run a regression like the second equation, the strict exogeneity assumption would be violated. Education is linked to ability, and ability affects wages, hence our expectation of the error in equation (2) given education isn't zero. What would happen if we did run the regression? In this simple linear example, the estimated coefficient $b$ would pick up the effect of $x$ on $y$ plus the association of $x$ and $z$ times the effect of $z$ on $y$. It would pick up both channels! 
 The primary advantage of stepwise regression is that it's computationally efficient. However, its performance is generally worse than alternative methods. The problem is that it's too greedy. By making a hard selection on the the next regressor and 'freezing' the weight, it makes choices that are locally optimal at each step, but suboptimal in general. And, it can't go back to revise its past choices. As far as I'm aware, stepwise regression has generally fallen out of favor compared to $l_1$ regularized regression (LASSO), which tends to produce better solutions. Tibshirani (1996) . Regression Shrinkage and Selection via the Lasso LASSO penalizes the $l_1$ norm of the weights, which induces sparsity in the solution (many weights are forced to zero). This performs variable selection (the 'relevant' variables are allowed to have nonzero weights). The degree of sparsity is controlled by the penality term, and some procedure must be used to select it (cross-validation is a common choice). LASSO is more computationally intensive than stepwise regression, but a number of efficient algorithms exist. Some examples are least angle regression ( LARS ), and an approach based on coordinate descent . A similar approach to what you suggested in (2) is called orthogonal matching pursuit. It's a generalization of matching pursuit, which is the name for stepwise regression in the signal processing literature. Pati et al. (1993) . Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition On each iteration, the next best regressor is added to the active set. Then, the weights for all regressors in the active set are recomputed. Because of the reweighting step, this approach is less greedy (and has better performance) than the regular matching pursuit/stepwise regression. But, it still employs a greedy search heuristic. All of these approaches (stepwise regression, LASSO, and orthogonal matching pursuit) can be thought of as approximations of the following problem: $$\underset{w}{\min} \| y - X w \|_2^2 \quad \text{s.t. } \|w\|_0 \le c$$ In a regression context, columns of $X$ correspond to the independent variables and $y$ to the dependent variable. In signal processing, columns of $X$ correspond to basis functions and $y$ is a signal to approximate. The goal is to find a sparse set of weights $w$ that give the best (least squares) approximation of $y$. The $l_0$ norm simply counts the number of non-zero entries in $w$. Unfortunately, this problem is NP-hard, so approximation algorithms must be used in practice. Stepwise regression and orthogonal matching pursuit attempt to solve the problem using a greedy search strategy. LASSO reformulates the problem using a relaxation of the $l_0$ norm to the $l_1$ norm. Here, the optimization problem becomes convex (and thus tractable). And, although the problem is no longer identical, the solution is similar. If I recall correctly, both LASSO and orthogonal matching pursuit have been proved to recover the exact solution under certain conditions. 
 I have this corpus of documents, properly pre-processed. Working with R package tm, i do: I then build an adjacency matrix and do some network plots in igraph. How can I group the columns on identified entities? Let say, I want the first five columns of the DocumentTermMatrix to be the five cities of interest to me. Other cities follow from 6:n. Basically how do I re-order adtm other than by frequency? in above snippet, findFreqTerms(xx,xx=10) may not even show up Cordoba because it appears in less than ten documents. But what i want to study is column Cardoba and its adjacency + network graphs to other cities in Spain. Any help will be welcome. 
 I am trying to run a RandomForest on a dataset with 390343 rows in R with the randomForest package.I am getting this error : Error in randomForest.default(m, y, ...) : long vectors (argument 24) are not supported in .Fortran As my number of rows is very small comapred to 2^31, I am unable to figure out the cause of the error. The summary of the data being used is : I am running this on R version 3.0.2 (2013-09-25) on a 64 bit Windows server 2012 with 128 GB RAM and a Intel Xeon CPU(E5-2690 v2) The command used to run the RF is 
 I have a time series data, where each of the data point belongs to one of the known clusters. What I am interested is to perform a HMM so that we can obtain hidden states that further abstracts out the clusters. Now for the HMM each observation i.e. data point is given as a feature vector where the features belongs to the properties of the cluster, for eg: the mean inter arrival time of the cluster, inter quartile range (shifting from third to 4th quartile) and lifetime of a clsuter (span). Now what other features can be used for the task. I am new to time series analysis. My each data point is an image and is a social media post. 
 Yes, it probably does mean that the last variable will have a higher impact (I write "probably", as it depends on the magnitude of the the loadings and the standard deviation of the original variables). The way to achieve your requirement of "equal representation for all variables in my final score" is most readily met by standardizing each of the variables (e.g., dividing each by its standard deviation) and then summing them. But, by doing PCA you are implicitly saying that you do not want to have each variable have the same impact, and are instead trying to ensure that each of three dimensions in the underlying dimensional space are equally represented. To add some more complication: a three component solution only explaining 62% of the variance of 8 variables is not a strong fit, so some of your loadings are probably pretty poor, making the whole analysis a but suspect. 
 In a comment on this question Nick Cox mentioned that Jeffreys suggested that errors in data approximate t distributions with 7 degrees of freedom. Does anyone have a source/citation for this? 
 Currently I'm reading Eloquent Javascript book and at chapter 4 . This topic entitled ' The lycanthrope’s log ' is very confusing to me. How important is correlation and coefficient in real world programming? I'm trying to understand the formula used to find correlation. I've far forgotten maths since I left my high school. Could, someone who've read this book, explain to me in detail , please? Below is the picture I'm wondering about.. 
 In RNN package in R ( https://cran.r-project.org/web/packages/rnn/index.html ), there is an example to use RNN to train summation of two numbers. I tried to look up in Google, but did not find a good resource to explain how can we use RNN to train this kind of task. Could you give me some references for this task. The input of the task is: X1 X2 Y (with Y = X1 + X2) you gave X1, X2 and Y to RNN, and it can learn how to predict Y. 
 I have a set of execution times for some processes. But there is uncertainty in these values. I call this $$ET + \lambda ET $$, where $\lambda ET$ is the delay. I sample $\lambda$ from a gamma distribution, which is always positive. For this gamma distribution, $\lambda$ is the mean and I choose appropriate scale (parameter). Is there any specific name for this uncertainty representation? Are there any mistakes in it? 
 A suitable repeated measures (with something like a random timepoint with depth a random effect nested within timepoint) would account for multiple measurements at the same time and at different times being from the same animal. So, yes, depth would needs to also have a random effect associated with it, because the assessments in the same animal at different times and depths are presumably all to some extent correlated. If you do not have any missing data at any of these combinations, then looking at a set of data with one record from each animal (but never more than one record from the same animal) with ANCOVA should give more or less the same result as an equivalent repeated measures model that has all key interactions (such as pre-treatment value by timepoint by depth, treatment by timepoint by depth etc.), main effects (treatment, timepoint, depth and pre-treatment value), an unstructured covariance matrix (or unstructured combined with unstructured, if you treat the timepoint and depth as nested) and an appropriate calculation of the denominator degrees of freedom (e.g. Kenward-Rogers). If you have some missing assessments, then using a repeated measures model will do an implicit data imputation for you under a missing at random assumption. Once you are combining multiple depth measurements from the same animal at different times, or different depth at the same time, or different depths across times for the same animal, you would be violating the assumption of independence of residuals and any standard ANOVA/ANCOVA/linear regression would be invalid. The effect would be an inappropriate exaggeration of how much evidence you have, which is avoided by a repeated measures model. This all works nicely for discrete times and depths. Averaging over some time interval seems a plausible thing to do, if the time intervals are sensibly chosen - e.g. so that not much of a change beyond a bit of random fluctuation should occur during an interval (and if there is important changes e.g. during the course of a day, then you either split the day into sensibly small intervals or take the whole day so that you have a whole cycle). Of course there are also options (even more complicated) that would use the continuously collected data. 
 The formula is actually pretty straight forward. Consider the table from your question. The table states that there were a total 90 observations Out of all 90 observations: 76 times , no pizzas or squirrel 9 times , pizzas but no squirrel 4 times , squirrel but no pizzas 1 time , both squirrel and pizza So we want to find out how Pizzas &amp; Squirrels are related: Consider the equation along with your table now, (since squirrel is on the left) n00 denotes an observation where there was no squirrel or pizza (1) n01 denotes an observation where there was no squirrel but pizza (2) n10 denotes an observation where there was squirrel but no pizza (3) n11 denotes an observation where there was both squirrel but pizza (4) n0. refers to the sum of all measurements where we have no squirrel n.0 refers to the sum of all measurements where we have no pizza n1. refers to the sum of all measurements where we have squirrel n.1 refers to the sum of all measurements where we have pizza Now that we know what these values are, let's plug them in to our formula : $$ \mathit{correlation} = \rho = \frac{ 1 \times 76 - 4 \times 9}{\sqrt{5 \times 85 \times 10 \times 80}}$$ 
 You should figure if you model is doing poorly because it is learning too few or too much - if it is underfitting or overfitting. Once you know that, you can tweak the relevant parameters of your model to improve it. If it still does not give you anything useful, then either there is no information in the data, or your model is especially bad at finding the pattern. Only then should switch to another model, but only after you have found a reason why model 1 was bad and model 2 will be better. To learn how to diagnose this kind of problem, look for the following keywords and know their meaning: Bias/Variance tradeoff, Overfitting/Underfitting, Learning Curve. Here are some starting ressources: CV: Question about bias variance tradeoff CV: Intuitive explaination of the bias variance tradeoff Andrew NG MOOC: The problem of overfitting Andrew NG MOOC: The complete serie in the chapter "What to try next" 
 I'm wondering if when it comes to predictive analytics, whether a lower test MSE is really all that matters. Should I not even look at residuals - other model diagnostics when it comes to prediction if I'm getting a lower test MSE? Or does a lower test MSE model always satisfy the model assumptions? at least more than a higher test MSE model? 
 I have a time series of observations from a longitudinal study of individual objects. These observations are seen as discrete sequences of features, one sequence per object. The sequences have different lengths. At each discrete point of time an individual sequence may turn out to be defective. An error on an individual sequence does not mean that the sequence terminates. It might be that other, correct observations might follow. I am interested in the probability that a sequence is correct up to a certain length. For that reason I've modeled each sequence as a series of binary events where an error is a positive event. The probability $P(E, S, L, T) = p$ of an error $E$ in the sequence $S$ of length $L$ at time $1 \le T \le L$ is independent from the point in time when the error occurs the point in time when the sequence starts other errors at the same individual sequence the length of the individual sequence other sequences I want to estimate the probabilty that a sequence contains $n$ consecutive and correct observations starting from the first one. Can this be modeled by the probability $P(E|T \gt n)$? How is this probability estimated? In a first approach using binomial distribution I would think that it is simply $(1-p)^n$. However, I'm wondering how to consider the different lengths of the sequences? Do I have to account for the length only if the probability of an error ist not independent of the point in time when the error occurs? Note: In this special case it is possible to use survival analysis. However, I am interested in other ways to estimate the probability. 
 I have a random variable $z$ for which I've calculated the sample mean $x = \frac{1}{n} \sum z_i$ and the sample standard deviation $s$. How can I calculate the standard deviation of $\frac{1,000,000}{x}$ ? Edit: More concise and accurate question thanks to Matthew Gunn Original verbose question: I have a data set/sample of values and I calculate the average and standard deviation in the end. Now I have a calculated/derived value of this data set/sample that is calculated as $1000000/x$ (with $x$ being the average in the end). Now the question is, what is the standard deviation of my derived value? I thought I could take the percentage of the standard deviation (standard_dev/average) and just multiple my derived average by it but that doesn't seem right... Is there any relationship between them at all or do I need to compute a new standard deviation by first converting my distribution to my derived/calculated value? (calculate $1000000/x$ for each value and then compute the standard deviation there) Why do I need this? I want to graph the results with error bars, and I want to graph my derived value as well as the original value. Further details/domain: This is benchmarking. I measure run times in μs but want to also display/graph them in iterations per second ("how often could this be run within one second?" which is nice as higher -&gt; better). And of course, I want to show error bars. Here is an example of values I got (markdown tables don't work here, hope it's good enough): 
 Short answer: it will all depend on your loss-functions . For instance, I do forecasting for replenishment in retail stores, like supermarkets or drugstores. The only forecast that is important in this context is a high quantile, because we don't want to replenish based on average demand , but to attain a high service level . (This is not always easy to communicate to our clients.) Other use cases may indeed be most interested in getting the expectation right, so the MSE would indeed be useful (but see below). You may want to look at the tag wikis for some prediction error measures, like the mae , mape or mase , all of which are minimized in expectation for different functionals of the true future distribution. And of course you should still look at your errors to see whether there is any remaining structure you could exploit to improve your algorithms or models. Method A may have a lower MSE than method B on average - but it may completely (and unacceptably) break down in certain rare cases. In such a case, you want to investigate why it breaks down in such cases and hopefully improve it. (I have found that not only looking at average errors, but also at high quantiles of errors is often helpful.) Finally, of course, if you are not predicting numerically, but classifying, then MSE is pretty much meaningless, and you need to look into confusion matrices and base your [tags:loss-functions] on false positive and false negative rates and similar concepts. 
 Since the OP has figured out how to get the answer with a table here's how to do it without a table. You need a one-tailed test, so you simply need to find all the rank sets for sample 1 that have a lower rank sum. Note that the sum of the ranks in sample A is 18. Let's list all sets of sample-A ranks with a sum of 18 or less: That's a total of 7 sets of ranks out of 462 that are at least as extreme (in the direction of $H_1$) as the observed set of ranks. Consequently the p-value is 7/462 $\approx$ 0.01515 This can be done exactly, without tables, under exam conditions, in the space of a couple of minutes (with practice you can do it much faster); if the test statistic were higher it would take longer, but even with a 10% test, as soon as you write more than 46 sets of ranks for sample A you know you've exceeded the significance level and you can stop at that point. 
 Consider a time evolving phenomenon represented by a variable $y(t)$, whose dynamics is dependent on a parameter, say the temperature $\theta$. We have two series of measurements at different constant temperatures $\theta_1$ and $\theta_2$. The plot of look like this: Measurements at a given temperature cannot be considered independent because of say the imprecision of the control over experimental conditions. Measurements for different temperatures can reasonably be considered independent. We would like to be able to predict the time evolution of $y$ at different temperatures. We model the dynamics of $y$ as $y(t) = a(\theta) t$, to be used later on as $\dot{y} = a(\theta)$. Assuming an additive error term, the estimation of $a$ is a standard regression problem that can be solved by least square optimisation. The main issue is the limited amount of data, especially when considering the intra-series dependency. Thankfully, some physical consideration tell us that $a$ varies exponentially with the temperature: $a(\theta) = \alpha \: e^{\beta / \theta}$. Now we are interested into modeling the uncertainty on $\hat{\alpha}$ and $\hat{\beta}$ the estimators of $\alpha$ and $\beta$. What procedure would you recommend? One idea would be to use bootstrap after non-linear least square fitting. However, we are mainly interested into the extrapolation area, namely predicting the behaviour of $y$ for $\theta \notin [\theta_1, \theta_2]$ (actually $\theta \leq \theta_1 \leq \theta_2$, and we could probably provide a lower bound for $a$). 
 If you want to compare Attention Deficit with Other Deficit then it is convenient to exclude Other Deficit from your model. That way the parameter for Attention Deficit will be directly the comparison of Attention Deficit with Other Deficit. In the comments you mention that you will use this in logistic regression, so lets use an example with logistic regression. Say we want to explain union membership if women in the USA with marital status, race and education (, which is centered at 12 years of education($\approx$highschool)). In particular we are interested in the comparison never married versus married. The exponentiated constant is the odds of union membership for those women who are married, white, and finished highschool, that is, have the value 0 on all explanatory variables. So for married white highschool graduates we expect to find .24 union-members for every non-union-member. The other coefficients are odds ratios, which are ratios of odds. So if one is never married than the odds will increase by a (nonsignificant) factor $1.27$, or an increase of $(1.27 - 1)\times 100\%= 27\%$. So by excluding the married dummy, the coefficient for never married is directly a comparison between the married and never married. @Epaminondas mentioned that you do not have to exclude any dummy. This is true if you exclude the constant, but will dramatically change the interpretation of your coefficients. Now the exponentiated coeffcients of the marital status dummies are no longer odds ratios, but odds. So to continue the example: For married white highschool graduates the odds of being a union member is 0.30, while for never married white highschool graduates the odds of being a union member is 0.27. We can compare those odds by computing their ratio, and if we do so we get the odds ratio of 1.27 we got in the first example. So both are just different representations of the same model. Both can be useful, but if you want to focus on the comparison, then the first is a more direct way of doing so. 
 I have a list of products, including variables such as the product name (as it appears on the receipt) and the merchant where the product was bought. I have a good deal of them manually classified into a fixed group of categories (e.g. alcoholic drinks, vegetables, meat, etc.). The data are, as always, noisy. In this case, particularly noisy because it comes from scanned receipts and OCR on not so good scans is usually very noisy. I want to play around with algorithms to classify new data, using the two variables above. There are several major sources of variation here: The OCR, which means a product (e.g. chicken) can be found with many different but relatively similar spelling (e.g. chiken, hicken, chicen, ...). The same product can have different names, according to the merchant who sold the product. In this case, the names can be either similar or completely different across merchants, but rather similar within every merchant. The same product can have very different names within the same merchant (e.g. branded products whose name on the receipt is the brand name, vs. generic names; soft drink vs. coca cola). I've tried some (kind of naive) classifier, using for example the distance between strings (which tackles mostly the first main source of variation mentioned above), but I am not very happy with the results. So I wanted to reach out here to ask for ideas on how to tackle this problem. I guess many people have "solved" or at least worked much longer in this kind of problem than I did (a few hours) so I would really appreciate any guidance here. By the way, I use mostly R, so R-based solutions would be greatly appreciated. 
 If all you care about is predictive performance and your task is to select one model out of several alternatives, then it makes sense to pick the one with the lowest test error. But, modeling can be an iterative process, and you might learn something useful by looking at other aspects of a model's behavior. What you learn might help you formulate a new model with even better predictive performance. For example, you mentioned looking at the residuals. Imagine a regression task, where you want to predict $y$ given $x$. Say that $y$ is a quadratic function of $x$, but you don't know this ahead of time. As a first guess, you fit a linear model. You could fit a number of linear models and pick the one with lowest test error. But, predictive performance would still be sub-optimal because you have the wrong model class. The MSE alone can't tell you this because it doesn't distinguish between error due to 'noise' vs. error due to unmodeled 'structure'. If you looked at the residuals (e.g. as a function of predicted $y$ value), you could see remaining structure that the linear model hasn't accounted for. At the very least, this would tell you to try a more complicated model. Hopefully, it could give a hint about the nature of the model to try next. Of course, you'd have to be careful not to overfit by repeatedly peeking at the data. 
 I have the following model. where Mobile is a dummy variable that has the value of 1 if mobile, and 0 otherwise. Age is a customers age in months. I have been asked to add an interaction term to the model. however I feel this makes no sense as the result will be an additional Age variable, or 0. The Age variable will simply be included twice in the model and we do not gain any new information from this. Am I wrong in thinking this is incorrect? Or am I missing something here? 
 I'm not a statistics person so I'm not sure if this is possible or not. Wasn't too sure how to google this issue either. I have a number of genes from a sequencing experiment, three test samples, three controls, and I have fold change data for 46 genes. From the fold change I know that 44 of the genes are inhibited, 2 are activated. Is there a way to calculate whether the 44 out of 46 is significant statistically? Or is it just 44/46 is around 96% and leave it at that. 
 The starting point of your thinking is correct but the conclusion is wrong. The new variable is indeed either or 0 but this does change the model: It leads to different age slopes for the two mobile groups. Simply make the distinction of case ( = 0 vs. 1) for the entire equation. Without interaction for = 0: and = 1: Thus you get two different regression lines with the same slope w.r.t. , namely . But you have different intercepts: vs. . Thus, can be interpreted as the difference in intercepts. Now with interaction you still get for = 0: but for = 1: Thus, you have two regression lines with different intercepts and different slopes. is again the difference in intercepts. is the difference in slopes. 
 Let $s$ denote sales, $m$ be an indicator for mobile, and $a$ the age. Imagine you ran the regression: $$ s_i = b_0 + b_1 a_i + b_2 m_i + b_3 m_i a_i + \epsilon_i $$ What's the expected sales for a 30 year old that doesn't use mobile? $$ E[s \mid m = 0, a = 30] = b_0 + 30 b_1 $$ What's the expected sales for a 30 year old that does use mobile? $$ E[s \mid m = 1, a = 30] = b_0 + b_2 + 30 (b_1 + b_3) $$ By including a mobile indicator $m_i$, you're estimating a constant difference in sales between mobile and non-mobile users. By including the interactive term $m_ia_i$, you're estimating a different effect for age depending on whether they use mobile or not! 
 Assume generic RNN has update formula: $\mathbf{h}_{t+1} = f(\mathbf{x_t},\mathbf{h_t},\mathbf{\theta})$ [1] Where $\mathbf{x}$ is input vector, $\mathbf{h}$ is hidden state vector, and $\theta$ is network parameters. In RTRL,one need to maintain gradient (or jacobian matrix) of $\mathbf{h}$ w.r.t $\mathbf{\theta}$ at each step of iteration. $G_{t+1} = \frac{\partial f(x_t,h_t,\theta)}{\partial h}G_t+\frac{\partial f(x_t,h_t,\theta)}{\partial \theta}$ [2] Since $G$ often takes $O(n^3)$ memory ($n$ is number of hidden units), plus expensive GEMMs, RTRL in this form is rarely used in practical applications. Instead of using approximation methods like here , is it possible to design a $f$ such that equation [2] is fast to compute? Preferably no need to maintain $G$ at all. For example, if certain $f$ satisfies: $\frac{\partial f(x_t,h_t,\theta)}{\partial h}G_t=\frac{\partial f(x_t,h_t,\theta)}{\partial \theta}$ Then there would be no need to maintain $G$ at all. Intuitively, the memory of such RNN should contain complete information to compute full gradient for all previous timesteps. 
 It's not a statistics but a program topic.... Supposition in this example: There are four SITE and two OPERATION. (if not, you'll modify each number in below code.) 
 I am trying to detect evidence of warming in a monthly temperature time series over a 20-year period by testing for a trend. I have precisely followed the method of Crawley (2013) The R Book, 2nd Edition pgs 798-799. In his linear mixed effects model for monthly temperatures he treats the explanatory variables time and linear trend as fixed effects, and year as a categorical random effect allowing for different intercepts for the different years. He then uses ANOVA to compare the full model (with trend explanatory variable) with a reduced version (i.e. without the trend explanatory variable). A reviewer has questioned why year has been treated as a random effect and suggested that by doing so this would essentially remove a long-term trend. Can anyone clarify why it is correct to include year as a random effect and if by doing so this does or does not remove a trend? 
 When reading scientific articles I see different approaches for summarising continuous variables in describing the study population. For instance, if I want to describe the mean age of study population, I can display it in combination with the standard deviation, or with its 95% CI. Although these are obviously related, my question is which of those makes the most sense. The way I see it, the SD provides information regarding specific study population, whereas the 95% CI tells me how well my sample mean 'matches' the mean of the total population. Therefore, if I want to describe the study population the SD seems to be the best option to me. However, if the mean age is the outcome variable of my study, I can imagine using the 95% CI. Any thoughts on this? 
 I am working on a project where I am given a large table of numbers, in which we are hoping to see certain patterns. For example (using ): My goal is to be able to sort arbitrary sized matrices (not necessarily square) by switching rows and columns to minimize cell-wise differences. For example, all the values on line 1 should stay on line 1, but perhaps it makes more sense distance-wise for line 1 to appear on line 8, and vice versa regarding the columns. This problem reminds me of finding the inverse of a matrix using linear algebra. Note: I realize there is not a unique solution to this problem, so resampling is OK (ideally, rerunning the function will produce different clusterings). As a starting point, optimally larger values would float to the top while smaller values would tend toward the bottom. In the above example, perhaps the largest numbers are all in row 9 and so one move might be to bring it to the top. Similarly, perhaps the largest numbers are found in column c and that is moved to the left. Basically, I want to reorder the rows and columns of the table so that the magnitude of the difference between each entry (e.g. a1 vs a2/b1; c2 vs. the four surrounding entries) is minimized - but with the restriction that the operations happen row- and column-wise. I am primarily interested in a theoretical approach that will show me how to accomplish this, but will eventually be implementing the solution in , so assistance on that front would also be appreciated. To make this more concrete: this idea is to be applied to the results from large-scale simulation studies, where the entries might be Type I error or power rates, the rows might pertain to a design condition (like sample size) and the columns might pertain to different multiple comparison procedures or something like that. The goal is to rearrange the entries of this table so that we might be able to glance at the table and see where clusters of "good" procedures are and where the "bad" ones are. 
 Heckman selection model assumes that 1) error of both selection and main equation are correlated and distributed normally, 2) explanatory variables in selection equation are independent of the error term, 3) explanatory variables in main equation are independent of the error term. Doubt: how can explanatory variables of both equations be independent of error terms, and at the same time, the errors terms be correlated? 
 It depends on what information you want. The CI describes the population you sampled from, the standard deviation can describe both the sample and the population (if we take the sample standard deviation $s$ to be a good estimator $\hat\sigma$ of the true population value $\sigma$). Looking at your question, you seem to be bumping into one of those times when casual vocabulary and statistical vocabulary trample on each other's toes. What you call "specific study population" is your sample and the "total population" is simply population . With that terminological note, using confidence interval to describe your sample isn't appropriate because the sample is finite and "complete" and you use descriptive statistics (standard deviation,etc.) on completely sampled groups. Inferential statistics such as the CI should only be used make statements about the "incomplete", i.e. the population form which you drew your sample. In this sense, the CI doesn't describe your sample at all, but rather the population you drew your sample from. (Somewhat more precisely, but still simplifying a bit, the CI is an interval computed from the sample using a procedure that, in at least 95% of random samples from any population, will include the population's mean. [Thanks @whuber!] But make sure to look at the comments below for some "fine print" and discussion on the definition of the CI.) You correctly got at this intuition in your question, even if you stumbled a bit on the vocabulary. In terms of practical advice, it really depends on what you want to do. If you just want to claim that your sample was well-balanced / representative / whatever, use descriptive statistics on your sample. If you want to make inferences about certain parameters in the broader population, use inferential statistics. Bottom line: it depends on whether you want to describe your sample or use your sample to make statements about the general population. 
 I used the qvalue R package ( https://bioconductor.org/packages/release/bioc/html/qvalue.html ) to analyze a set of 550 genes each having an associate p-value of interest. The package computed for me a $π_0$ of 0.991. So if I understand well 99.1% of my 550 genes reject $H_0$ (in other therm I have a proportion of true null hypothesis = 1-0.991 = 0.009 ). Correct ? What is the best way to report such value in a scientific paper ? And if anybody know a paper that use such statistics ($π_0$) it would help me a lot. Thanks 
 I have 200 fd curves to be taken as response with 10 or so scalar coefficient variables. Attempted using fRegress {fda} and fosr {refund} but getting very different results. Ideally I would like to obtain coefficient function estimate plots and extract matrices for the coeff func estimates and associated standard errors. 
 I have the following problem: Consider two probability density functions on $[0,1]: f_0(x) = 1$, and $f_1(x) = 2x$. We want to test such hypotheses: $H_0: X \sim f_0(x)$ versus the alternative $X \sim f_1(x)$, with significance level $\alpha = 0.1$, how large can the power possibly be? My approach: Let $\{\Gamma_0, \Gamma_1\}$ be a partition of $[0,1]$ into sets and the decision rule be that hypothesis $H_i$ is true if the observation $X$ belongs to $\Gamma_i$. Then, the probability of type 1 error is $$\alpha = \int_{\Gamma_1} f_0(x)\,\mathrm dx = 0.1$$ while the power of the test is $$power = 1 - \beta = \int_{\Gamma_1} f_1(x)\,\mathrm dx.$$ Now,since $f_0(x)$ is the uniform density, we know that the total "length" of the set $\Gamma_1$ is $0.1$ Then looking at the graph of density $f_1(x)$ i have realized, tha in order to maximize the power of the test i should take the region from 0.9 to 1. So the maximal power of the test is : $$power = 1 - \beta = \int_{0.9}^{1} 2x\,\mathrm dx. = 0.19$$ Is my solution correct? And are there some other approaches for solving this exercise? I have thought about Likelihood-ratio test but i don't know how to apply it to my problem. 
 I have a set of 260 sets of measurements (for each set of measurements there is an amplitude measured as a function of 8 radii). Since I do not get measurement errors and I am interested in the properties of the distribution of measurements, I am using jacknifing to determine the errors for the distribution, and bootstrapping in order to estimate the 8x8 covariance matrix. The problem is that even at one million bootstrap samples the determinant is very close to 0 (10^-21). Would you suggest alternative ways of estimating the covariance matrix? I need it for finding the minimum reduced chi square for a set of models that I am fitting to the data. 
 (Hi and welcome to cross validated) You raise very good questions: the outcome of the experiments may depend on a large number of factors. Doing really independent replicates of the experiment is often not feasible from a practical point of view (ordering multiple times the same cell line, etc.). However, here are some things you can do: Report precisely the conditions of your experiment, and at which level your replicates are independent (and on which they are not), e.g. that your test cases were prepared from newly prepared stock solutions but from the same lot of the chemicals (= variance includes preparation error, but not differences between lots / manufacturers of educt). From a pragmatical point of view, you could focus the replication experiments on being independent on the important factors. Unfortunately, there is not too much knowledge which factors are typically important. I'd suggest to speak to experienced experimenters where they see the critical points. The problem is that while experimental results on these questions would be needed, it is really hard to get funding to do this rather tedious = expensive in terms of wage, possibly also expensive in terms of materials purchase work which is unfortunately often not regarded as being of much importance - particularly if you are successful in showing that e.g. lot/manufacturer do not matter. Publication bias will be very much against you if you show that a replication study (= already low on the novelty scale) did not find differences (= on the first glance, nothing interesting was found, because patterns/differences are interesting, lack of them is not). If I now repeat the experiment, I will usually take the starting materials from the very same batch. This is IMHO a case for: speak with experienced experimenters in the field whether they's expect problems. If they say that, you may be able to convince your supervisor that you should replicate on the lot/manufacturer level. Will this be acceptable? Usually it will. Particularly if you make clear that you are aware of the limitations that implies for general conclusions. Personally, I'm most concerned about people who have no sense for how "local" their results are. And the fact that your question shows you are aware and concerned about these makes me relax and trust your conclusions far more than I'd trust someone who claims to rescue the world on the basis of a single calibration with no whatsoever validation... Will repetition of the experiment mean that I have to order several different batches of cells? Yes. My experience (vibrational spectroscopy of biological samples/cells and what I have heard from colleagues who do microarray studies on such samples) unfortunately suggests that batch-to-batch variation for cells is indeed an important source of variance. Will each cell sample be like if I took a mean value of thousands of mice in each and every experiment? No, it will be as if you looked on several cells (of the same tissue) of the same mouse. 
 I have this distribution function of a random variable X: I wish to find E(X). I have used derivatives to get the density function, compared it to 1, and found that f(t) = (4/5) t+(3/5). I then used integral of t f(t) over the range of 0 to 1, and got 0.56667. According to the answer I got, it's incorrect (maybe the answer is incorrect, not me?). Can you please assist? Thank you ! 
 Your answer looks correct to me. The likelihood ratio test would have the rejection rule for the form reject if $\frac{L_0}{L_1} \leq c$ , where $L_0$ and $L_1$ are the likelihood under the null and alternative. So in your problem the rejection rule becomes: " reject if $\frac{1}{2x} \leq c$ ". i.e. reject if $x \geq k$ (for some $k$) Now the $k$ required to get $\alpha = 0.1$ is $k=0.9$. So in this case the LRT is the same as your test. 
 The tree size of my C5.0 output is around 90. When I plot it, I am not able to visualize it properly as they are sliding on each other. I got some code online where we can plot c5.0 tree from branches. shoptree_forplot &lt;- C50:::as.party.C5.0(shoptree) plot(shoptree_forplot[10]) This code plots tree which are downwards from 10th level . It would be great it anybody can help me to plot only first 10 levels. 
 Question in short: How can I make two histograms equal by explicitly downsampling the buckets? More detailed version: I have realizations of sequences of two discrete random variables both giving values in some finite space $X = \{1, ..., n\}$, i.e. I have two data vectors $(x_1, x_2, ..., x_A)$ and $(y_1, y_2, ..., y_B)$ with $x_i, y_j \in X$. I consider their distributions in terms of histograms, i.e. for each value $i \in X$ I count $a_i = |\{u \in \{1,...,A\} : x_u = i\}|$ and $b_i = |\{v \in \{1,...,B\} : y_v = i\}|$. Now I draw two histograms in percentage, i.e. the height of bar $i$ is $A_i = a_i/(a_1 + ... + a_n)$, respectively $B_i = b_i/(b_1 + ... + b_n)$. In general, these histograms look different. How can I make the $A_i$ roughly equal to the $B_i$ by explicitly taking out some (as less as possible) of the data points $x_i$, respectively $y_j$? More precisely, I want a solution that gives a tradeoff between 'how equal the histograms look alike' and how much data points I took out. It does not make sense for me to take out neither 90% of the $x_i$ nor 90% of the $y_j$. Results so far: For one single variable and a given fixed 'desired' histogram I succeeded more or less by doing the following (working R example below). Note: If one does the same thing for two variables (explicitly writing out what one wants as a system of equations), then the system becomes an optimization problem with quadratic constraints which is utterly difficult to solve in general. CAUTION: even if the desired probabilities (i.e. heights of the bars in the desired histogram) come from another data vector, the answer is not as simple as 'take out as much until you arrive at the size of the desired bucket' as the bucket size of the given distribution is lower than the desired one (in order to demonstrate that I take out some elements from the first bucket in the R code below). Given target percentages $p_i \in (0,1)$ with $p_1 + ... + p_n = 1$, find the minimal numbers $s_i \in \mathbb{N}_0$ such that $$\frac{a_i - s_i}{a_i - s_1 + ... + a_n - s_n} = p_i$$ rewriting $w_i = a_i - s_i$, this yields a linear system of equations $$ w_i = p_1w_1 + ... + p_nw_n$$ i.e. $$ p_1w_1 + p_{i-1}w_{i-1} +(p_i-1)w_i + p_{i+1}w_{i+1} + ... + p_nw_n = 0$$ which I solve [meaning that I obtain one single solution $w = (w_1, ..., w_n)$, recompute the $s_i = a_i - w_i$ and then just round. The tradeoff is done by searching for the minimal $s_i$, i.e. the system of equations is not uniquely solveable but is one dimensional, so we can adjust a factor $\lambda$ in front of the $w_i$, i.e. put the conditions $$ 0 \leq s_i = a_i - \lambda w_i \leq a_i$$ on $\lambda$ and then take the $\lambda$ that makes the $s_i$ as small as possible. Although this is not 100% correct, it yields good results: Before downsampling: After downsampling: R code: 
 To do a in Stata I was thinking of Verify if both regressions are "ok" = verify errors, variables' significance , etc. If everything is okay with both regressions, I will do the command (SUR). Am I wrong? Or should I do directly the regression, and verify through that regression the coefficients' significance? Resume of the question : do I have to do two 2-way-fixed effects model first (for 2 dependent variables), verify everything as if they were going to be always independent, and finally estimate the SUR? I also posted this question on Statalist. 
 I am facing the following problem: I have (large) sample of unevenly distributed points $(X_i,Y_i)$ in a 2D space. I would like to determine the local extremas of the density of the distribution. Does the function KernelDensity allow to estimate the density of the sample in a point outside the sample ? if yes, i cannot find the right syntax ? Here is an example: If I make this query i get a negative number, clearly not a density !! array([-2.88134574]) I don't know if its the right approach. I would like then use that function to use an optimizer to get local extremas. (which library/function would you recommend ?) 
 I am trying to implement a random variable in R, and I want to generate a sample from it. The random variable looks like this: we have $P(X_{n}(\omega)=\frac{n}{n+1})=0.5$ and $P(X_{n}(\omega)=(-1)^{n})=0.5$ for a sequence $X_{1}, X_{2},...$of random variables which are independent. Can I inplement it in r as a binominal? If so, how? Would $rbinom(n=n, size=1, prob= 0.5* (1/(1:n))+0.5*(-1)^{n})$ be a prober solution? 
 BACKGROUND: I run a 2X2 Between-Subjects ANOVA. I obtained: a low power (&lt; 0.30) for both Independent Variables a not statistically significant result for one of the Independent Variables. Still, the effect size of the two independent variables were: eta-squared 1,17% for the non statistically significant one and eta-squared 5,64% for the other one. According to Cohen, these could be considered respectively a small and medium effect sizes. Also, as he said "a medium effect is visible to the naked eye of a careful observer. A small effect is noticeably smaller than medium but not so small as to be trivial." QUESTION: Is there anything valuable that I could take out from this experiment? Or given these results (low power, no statistical significance and medium-low effect size) there's nothing valuable that can be taken? I'm especially interested with regards to the non statistically significant independent variable: it is "gender" and I was evaluating whether or not it had an impact on measuring investment alternatives. Could I conclude my analysis saying "the experiment shows no difference exists" or better say "the experiment shows that only a subtle difference exists, and a bigger sample size could detect a significant difference"? Thank you. 
 I have results where the sum of squared errors are very similar (SSE=Sum(y_Est-y)^2) but the corresponding R^2 are very different as are the F-test results. The coefficient of determination can be defined as R^2 = 1 - SSE/SST which can be interpreted as 1-(unexplained variation)/(total variation). Now I have a situation in which the SSE's are very similar, but the SST are very different. The net result is one regression has an R^2 of over 0.6 and the other has 0.05! It's having a similar effect on the F-test: SSM/SSE. For similar reasons the SSM for the less volatile regression is much lower and so it has a much lower F-score. It seems strange to me that although both regressions give a fit of similar quality the second one gets penalised simply because it is applied to less volatile data? Clearly the disparity in volatility between the two time series is skewing the results. In such a situation how should I compare the regression results between data that has different levels of volatility. Thanks 
 First, you should have done the power analysis before starting the whole process. That would have told you that you either needed more subjects than you actually go or to do a different project, at least if you were hoping to find statistically significant results. Next, the whole question of whether you can report on statistically nonsignificant results has been debated a lot. Different statisticians have different views. My own view is strongly towards the "yes you can report them" point of view. Others will say no. But this is probably not the place to get into a long discussion about it. 
 I've a bunch of 500 p-values and I want to estimate the pi0 (proportion of true H0). When using the qvalue R package, two method can be used (smoother or bootstrap). Both giving very different results : smoother : pi0 = 0.9913 bootstrap : pi0 = 0.234 Here are the histogram for smoother (top) and bootstrap (bottom). Do I miss something ? Which one to use ? Thanks 
 Imagine a data set with approximately 100 variables and 5000 cases. The outcome is a two-level factor. All variables are factors, most of them three levels (yes, no, or indifferent). After building a simple logistic regression model I'm in doubt about how to reduce the amount of variables used in the final model (and find a proper validation method). Before building the model I've used chi-squared to examine individual relations between some predictors and the outcome, but this becomes kind of stupid. Any advice about how to tackle this issue, preferably in an automated way? I've some understanding of basic probability but would use Lasso or Ridge Regression more as a black box now. As tools I use R and Python. Thanks in advance! 
 SSE and SST are not really very interpretable by themselves. You can increase or decrease them, for instance, by changing the scale. That's one reason for using F tests and R squared and so on. The volatility of a time series should absolutely affect the results. That's not "skewing" and it's not odd. 
 I need to generate a sample from three distributions (I assume them Gaussian), respectively sand, silt, and clay. Actually, these variables are found in three soil maps. For each of them we have the mean, the upper prediction limit (95 percentile) and the lower prediction limit (5percentile). The maps were produced following a digital soil mapping approach. The problem is that their distributions were modeled independently, and if I draw three values, it could be that their sum does not add up to the unit (or 100%), as corresponds to compositional data. Normally I use the cholesky or eigendecomposition in R (e.g., mvrnorm finction from MASS package), but an additional problem is that I do not have the covariance matrix, just the means and the SD of each variable. How can I generate a sample with the constraint that they add up to 100 %? Thanks. 
 Assume we have a dataset $X_{full}$ with labels $y_{full}$. We train a kernel ridge regression model on this data with the Gaussian kernel. This model is used to generate predictions on the whole dataset $X_{full}$, the predictions are given by $\hat{y}_{full}$. This model uses a regularization parameter $\lambda$. The kernel ridge regression model that I use is minimizes the following in the featurespace of the Gaussian kernel: $$\frac{1}{N} \sum_1^N (\hat{y}-y)^2 + \lambda ||w||^2_K$$ So note it minimizes the mean squared error(!). Because of this it is possible to show that our model will always have $||w||^2_K \leq \frac{1}{\lambda}$. Now we obtain a subset of the data, $X$ and a subset of the corresponding predicted labels $\hat{y}$. We know the labels $\hat{y}$ are given by $\hat{y} = w^T x$ in the featurespace of the kernel, where $w$ is the model trained above. Let's call this model the oracle model. On this subset of the data we are going to train a kernel ridge regression model. To train this model we use the set $X$, $\hat{y}$, thus we use the outputs of the oracle model as labels to train this model. We want this model to generalize as well as possible to the whole dataset $X_{full}$ in terms of the squared error with respect to the labels $\hat{y}_{full}$. Given a (small) training sample $X$,$\hat{y}$, is it then better to train the model with the same regularization parameter as the oracle model, or is it better to regularize less? For kernel ridge regression we require a small regularization term in order to fit the model, for example if we use the Gaussian kernel we can only train it if we have a small lambda, otherwise we might be unable to invert the kernel matrix. So we could use a different $\lambda$, for example make it small enough so we can still invert the kernel matrix (for example we can use the pseudo-inverse). Is this then better than training with the same regularization parameter as the oracle model? The question here of course becomes: how to compute the pseudo-inverse, or how do we set the regularization parameter smaller than $\lambda$ that does not introduce numerical difficulties... My intuition tells me that a small regularization parameter will be better, since we can then fit $w$ better: if we regularize we introduce bias in estimating $w$. However, on the other hand, if we make the regularization parameter smaller, we are essentially using a too complex model to fit the data, therefore we might expect it to generalize worse: the training procedure might estimate a $\hat{w}$ for example with much larger norm than $w$ since we constrain the norm of $\hat{w}$ less. 
 The way you code it just modifies the probabilities, not the outcomes. If I understand your aim correctly, you rather want something like this: This first generates 0s or 1s, and if element $n$ of the sequence is 0, it transforms to $n/(n+1)$ and to $(-1)^n$ else. 
 I am trying to apply the word2vec model implemented in the library gensim in python. I have a list of sentences (each sentences is a list of words). For instance let us have: I implement two identical models: I realize that the models sometimes are the same, and sometimes are different, depending on the value of n. For instance, if n=1000 I obtain while, for n=5000: How is it possible? Thank you very much! 
 I have two variables ; First variable is ordinal and it represent years series from 2006 to 2015 and another variable which is interview score and I want to study if there a relationship between the graduation year and the interview score. What is the appropriate test I shall use ? Thanks ! 
 Can someone suggest an algorithm for an outlier detection system? My requirements are: it supposed to be a one class classifier, where on training phase, it only fed 'normal' data however the normal class can have multiple sub-class. (we can see it as multi-class classification with an 'other' class, where this 'other' class is never learned) all features are categorical features have sequence relation, so I need it analysed in sequence. I need a lighter algorithm for real time application optionally - I need an on-line learning capability I have tried HMM. However it still very heavy to my liking. Furthermore it required me to create different model for each sub-normal-class. Next I tried one-class-svm (with one hot encoder). It was light enough. However results was incorrect. I'm guessing it was because this approach is not appropriate for sequence analysis? Now I am in the middle of Neural Network trial. I read that Autoencoder is an approach for one-class-classification. My results so far is not satisfying in term of learning time and detection results. Can someone suggest a better (machine learning algorithm) approach? 
 I am analyzing proportion data with GLMM in which the number of occurrence a behaviour of interest has been displayed and has not been displayed are concatenated and fitted as a single response variable. Fitting a conventional bimodial model with glmer and computing the dispersion statistic as the sum of square of the pearson's residuals/residual degree of freedom returns a value of 4.9, suggesting a high overdispersion. I then fitted a similar model in glmmADMB but specified a beta-binomial error structure instead, which I thought would be appropriate to account for the large variance in my data. The model runs smoothly and estimated parameters, standard errors and p-value look more realistic than the binomial model. However at my surprise, the dispersion statistic calculated as described above returned a value very similar to the one obtained when fitting the binomial model. I was wondering whether this is something I have to worry about and indicate that the model is inappropriate or that the overdispersion statistic is of no relevance anymore when using beta-binomial error structure 
 Edit: This answer is wrong. It does work out that way to 0.56667. I made an error with fractions. Check you integral. It is not the problem. Your approach is correct. Your probability density is correct, but your $$ \int_0^1 t f(t) dt \ne 0.56667 $$. Rework this integral. 
 The problem is with your example that it is possible to have zero $tp$ and zero $fp$, therefore the precision $prec = tp/(tp+fp)$ becomes undefined because we divide by zero. Because of this the PR curve only contains points for one $x$-value, and therefore the area under the PR curve becomes zero for your example. You can see this by plotting the PR curve: So plotting a PR curve doesn't really work well when all your scores are the same. To gain more insights between the difference of the PR curve and the ROC curve, compare these two prediction lists. We consider the case where we predict all zeros, and predict one 1, but it should be zero (score1). This one doesnt work very well, it predicts 0 everywhere, except for one object where it predicts 1 where it should be zero. We consider another case, where we predict one 1 correctly, and the rest we classify as 0. Here we thus predict 1 one correctly, and the rest we classify as 0. We compare the area under the PR curve and the area under the ROC. Observe that the AUC varies little between score1 and score2. However, the area under the PR curve is significantly different. It rewards score2 much more than score1. This indicates it is better suited to outlier detection: it rewards detecting the outlier much more than the AUC. In case of outlier detection you would prefer score2 much more, since it predicts the 1 that you want to detect correctly, while score1 predicts a 1 for a zero and never catches any outliers. In general, the AUC is more informative to give an idea how well your predictions work for varying priors. Thus the AUC characterizes how well the classifier works for varying number of ones and zeros. The PR curves indicates more well how it performs for the current class imbalance considered. Therefore the PR curve is more interesting for you: it takes into account there are little 1's in your dataset than 0's. Because you are only interested in this case when you are interested in outlier detection, the PR curve is more informative. While the AUC characterizes how your predictions would do if there are much more 1's as well. For more information see also: https://www.quora.com/What-is-Precision-Recall-PR-curve ROC vs precision-and-recall curves Finally, you might be interested in how to compute an ROC / PR curve, a detailed explanation is given here for ROC curves: 
 I have following dataset: Based on that, I've created the tree: However, when analyzing the attributes, I run into an issue: We have 2 records, with a1 = 4, a2 = 1, that lead to two different decisions. What should I do with this - is this correct way of structuring the tree, or simply speaking it impossible to build? 
 Here is where I would start. Bayesian inference for non-linear regression model Let $y_{i,t}$ be the observation at time $t$ with temperature $\theta_i$ where $i=1,2$ (since there are two temperatures) and $t=1,\ldots,T$ (for simplicity I'm assuming observations are taken at the same times, but this could easily be modified for situations where this is not the case.) Assume an additive normal error, i.e. $$ y_{i,t} \stackrel{ind}{\sim} N(\alpha e^{\beta/\theta_i}, \sigma^2). $$ A Bayesian approach to estimation requires a prior over the parameters, i.e. $p(\alpha,\beta,\sigma^2)$, and then the posterior is $$ p(\alpha,\beta,\sigma^2|y) \propto p(\alpha,\beta,\sigma^2) \prod_{i=1}^2 \prod_{t=1}^{T} N\left(y_{i,t}; \alpha e^{\beta/\theta_i} t, \sigma^2\right). $$ which will likely need to be estimated computationally, e.g. Markov chain Monte Carlo. This posterior can provide point estimates, e.g. $E[\alpha|y]$, and uncertainties, e.g. $V[\alpha|y]$. Forecasts can be obtained for a new temperature $\tilde{\theta}$ and time $\tilde{t}$. Assuming the response is independent of the previous data given the parameters, the forecast distribution is $$ p(\tilde{y}|y) = \int \int \int N\left(\tilde{y};\alpha e^{\beta/\tilde{\theta}} \tilde{t}, \sigma^2\right) p(\alpha,\beta,\sigma^2|y) d\alpha d\beta d\sigma^2 $$ which will also likely need to be estimated computationally. Although this forecast distribution will have more uncertainty when you are extrapolating, the extrapolations will still be heavily influenced by the model which may or may not be very good in the extrapolated regions and there will be no way for you to tell. Turning the model into a standard regression model If the observations are positive and it is reasonable to consider a multiplicate rather than additive error, you can turn this problem into a standard regression problem. Then, we could assume $$ y_{i,t} = \alpha e^{\beta/\theta_i} t e^{\epsilon_{i,t}}$$ where $e^{\epsilon_{i,t}}$ is the multiplicative error. If we take logarithms, then $$ \log y_{i,t} = \log(\alpha) + \beta \frac{1}{\theta_i} + \epsilon_{i,t}. $$ If we assume $\epsilon_{i,t} \stackrel{ind}{\sim} N(0,\sigma^2)$, this is a standard simple linear regression model where $\log(\alpha)$ is the intercept and $\beta$ is the slope for inverse temperature. This model can be trivially fit using any regression software. If you are using a Bayesian approach, then you can also trivially get uncertainty on $\alpha$ rather than $\log(\alpha)$ by taking samples of $\log(\alpha)$ and exponentiating. If you are using the standard prior ($p(\log(\alpha),\beta,\sigma^2) \propto 1/\sigma^2$), the forecast distribution for $\log(\tilde{y})$ is a Student $t$ distribution and can be found in most Bayesian textbooks. The statement about extrapolation above still applies. If it is reasonable, I would certainly opt for this second approach. 
 It certainly depends on the purpose of the tree. In many cases, the best result may be 0.5 as a result. It appears from the column name, that 'dec' is a decision and 0.5 may not be a suitable answer. Then you might consider giving this combination a random result until more of these constellations (4/1/?) appear in the training data set. The third way might be to check, whether a1=4 or a2=1 is in itself a strong indicator of dec=0 or dec=1, Independent of the other ai. However, there are many degrees of researcher's freedom in the last approach, so most of the time I would favour the first two. 
 I can't find any papers regarding Dende Kars model for fertility. Can someone please suggest me any paper or book, I should go through to clear this concept? 
 Schervish's (1995) Theory of Statistics defines exchangeability like this (p. 7): A finite set $X_1, …, X_n$ of random quantities is said to be exchangeable if every permutation of $(X_1, …, X_n)$ has the same joint distribution as every other permutation. An infinite collection is exchangeable if every finite subcollection is exchangeable. Likewise, Wikipedia currently says: Formally, an exchangeable sequence of random variables is a finite or infinite sequence $X_1, X_2, X_3, …$ of random variables such that for any finite permutation of the indices, the joint probability distribution of the permuted sequence is the same as the joint probability distribution of the original sequence. and also : In probability theory and statistics, a sequence or other collection of random variables is independent and identically distributed ( i.i.d. ) if each random variable has the same probability distribution as the others and all are mutually independent. But what exactly is meant in these cases to say that, e.g., a random variable $X$ has "the same distribution" as another random variable $Y$? Does it mean that the CDFs of $X$ and $Y$ are equal? Or does it mean the CDFs are equal almost everywhere? Or is it something else? 
 In my experience, students frequently adopt the view the that squared errors (or OLS regression) are an inherently appropriate, accurate, and overall good thing to use, or are even without alternative. I have frequently seen OLS advertised along with remarks that it "gives greater weight to more extreme/deviant observations", and most of the time it is at least implied that this is a desirable property. This notion may be modified later, when the treatment of outliers and robust approaches are introduced, but at that point the damage is done. Arguably, the widespread use of squared errors has historically more to do with their mathematical convenience than with some natural law of real-world error costs. Overall, greater emphasis could be placed on the understanding that the choice of error function is somewhat arbitrary. Ideally, any choice of penalty within an algorithm should be guided by the corresponding real-world cost function associated with potential error (i.e., using a decision-making framework). Why not establish this principle first, and then see how well we can do? 
 Let $X_1, \dots , X_n$ be observations from the AR(1) model. For large $n$, the maximum likelihood estimator $M(\phi_1)$ of $\phi_1$ is approximately normally distributed as $N(\phi_1, (1−\phi_2)/n)$ For the unit root case, this normal approximation is no longer applicable, even asymptotically, which precludes its use for testing the unit root hypothesis $H_0 : \phi_1 = 1$ vs. $H_1 : \phi_1 &lt; 1$. why? and why can't use t-student test? 
 A Kruskall-Wallis test would be suitable to test, whether ordinal scores are differing between different groups (years). Spearman correlation would be suitable to tell, whether older applicants tend to get higher oder lower scores. (Call that 'ANOVA' and 'Pearson' instead of 'Kruskall-Wallis' and 'Spearman' if you want to deal with problems like normality assumptions in your small sample - hint: you don't want to). 
 Can we predict / formulate regression equation for a dichotomous variables and scale predictors ? Example : If I want to predict if applicant will success or fail in interview based on scale variables like GPA , etc ? Can we do that statistically ? 
 I am interested in finding the sample size formulas for proportions using the Wilson Score, Clopper Pearson, and Jeffrey's methods to compare with the Wald method. Also if anyone has code to replicate these methods in R or Excel would help to be able to repeat the task for different tests. I understand how these methods work conceptually but was hoping to find a better method to calculate them because the project I am working on will need to eventually be run by someone without the same conceptual understanding. Some of these methods, i.e. Jeffrey's and Wilson, are computationally very difficult in determining sample size alone and this is where I am requesting help. 
 Im looking for help interpreting an CA Factor MAP my is : I used the following R code : I understand that there is a clear association between A and 1, and B and 2 in the other side, etc... What is the way to interpret these "cluster" of qualitative variables in each part of the two axis? Why isn't the axis centered? Finally, what does the value dim1=65.45% represent? I looked on the site, but I didn't find explanation for such question, or maybe I misunderstood. 
 I've trained two random forests for a regression, based on the same data and variables, only with a different number of tree in each one. If I'm not wrong, having more trees is always better in terms of how the model performs. (Can you confirm that ?) However, due to practical considerations, we can't afford to load a random forest of 10Gb, so we need to lower the number of trees in the forest. So how can I compare the respective performance of the forests ? Thanks 
 I was looking into the function in the package for R. In the parameters I found the following options for and : Could someone please help me understand the details of each and , and when one should use each option. The documentation of the package did not cover these in detail. 
 I have a matrix of pairwise Gower dissimilarities (i.e. a non-Euclidean metric) among 48 objects derived from observations of 54 variables. I want to use fuzzy clustering to obtain a matrix of group membership probabilities for these objects. I successfully applied function from package {cluster} with k (number of groups) = 4. However, for each sample, the resulting group membership probabilities for groups 1, 2 and 4 are identical. That is, they differ between samples , but within a sample they are identical, and differ from the membership probability for group 3. I tested the function with k of 5, 6 and 7 and obtained the same result. For each k, the group membership probabilities were identical (within an object), except for group 3. Does anyone know why this might be happening? I've read up on the fanny() algorithm a little but I'm out of my depth. Reproducible code (NB: actual data follow due to size): Used dump() to generate this expression to recreate my data (warning: 294 lines): 
 I'm trying to build a sentence classifier using Caffe, for example the following dataset: 1 -&gt; 'Fell energy at my home' 2 -&gt; 'I liked the city's new buses' 3 -&gt; 'It has a hole in the road in front of my house' 4 -&gt; 'The new public soccer field is perfect' I have to classify those sentences like: infrastructure , recreation , security and so on. How can I do this using Caffe? What logic should I use for this? Like labels and to transform my text to lmdb that can be read by Caffe ? 
 Each row has a distance to every other row, each column has a distance to each other column. You could use hierarchical clustering on each of them to sort similar ones next to each other. Please have a look at this heatmap: https://upload.wikimedia.org/wikipedia/commons/4/48/Heatmap.png As you can see, they used hierarchical clustering for sorting the rows and hierarchical clustering for sorting the columns and thus find squares of similarity within the matrix. Is that, what you are looking for? Here is more on hierarchically sorted heatmaps, including instructions for R: 
 You should always have loss calculated to compare how good the model is. In regression the loss can be least square or least absolute value. You can compare how good the model is in a "testing data / validation data". mtry should be the only parameter you are tuning on. More trees is not "better", with sufficient number of trees, the "estimated testing error / out of bag error" will "converge". I personally never use more than 1000 trees. And you can observe when does it converge by plotting random forest. 
 From your comment, I understand that country variables such as GDP, while they are constant for each company, vary from year to year. Since year is the lowest level (level 1) in your data hierarchy, you can include these as level 1 variables. There is no need to add an additional (4th) level of nesting, at least in software that I use (such as in R). 
 Just a general question that I couldn't find too much on. What would be some good approaches to one step ahead forecasting of financial time series with mixed frequencies? Often a lot of the available data influencing the price of say a stock or commodity is published at different frequencies, some daily, some weekly, some monthly etc. which in my head makes it tricky to use normal models for anything but the longest time interval. Edit: found this paper which goes some way to cover the topic 
 I'm a graduate student, so I'm looking for a little more expertise around including family income in a regression model because of a high percentage of missing data (50%). I'm hoping you all can correct my thinking about multiple imputation, practical limitations in the use of variables with a high percentage of missing data, and provide some thoughtful ways to proceed. Context: I was brought into a project in its later phases. The project is exploring the relation between teacher training and student achievement. The sample size of models without family income is 20 thousand, while the sample size with including income is around 10K. So, this is not an issue of loosing power because of including a variable with a lot of missingness; rather, of understanding how estimates are biased, and effectively communicating that result. Thoughts on missing data for income variable: Income is often one of those variables that contains a high percentage of missing data. I've taken courses on multiple imputation, but I feel like this particular situation falls outside of what I am used to. In the past I've worked on projects where we use multiple imputation in datasets that contain income because we had variables in the survey that helped us understand that the data missing for income was likely MAR. Maybe it is my still nascent understanding of MI, but I've always wondered how can MI deal with the lower bounds of income that were not reported by individuals? Obviously, we don't know what the lower bound of income would be because we'd have to follow-up with individuals that did not report income. On other projects I've worked with methodologists have successfully argued income was MAR; however, I've still always wondered: Aren't the results still biased upwards to those who report income, and are likely not to be reflective of lower income families? With this particular dataset the demographic data is sparse, income being one of the only student-level covariates, and in probing the data I can't find evidence of MAR. Theoretically, I feel like income should be in the model. There is a vast literature showing the relation between family income and student achievement. However, literature shows that people with low incomes are less likely to report it on surveys. So, while the only way to prove NMAR would be to go back to collect data from people, I feel there's a compelling enough rational to assume it is NMAR. MI would not apply, the same would go for FIML approaches to estimation of parameters. This would also make the listwise deletion approach invalid. I feel conflicted with how to proceed. Theoretically, it makes sense to keep income in the models and just accept the bias. Is it okay to do that, as long as that bias is communicated in the results? It would seem not including income, when we know from the literature that it matters, would also introduce some bias in the estimates. Is this a "pick your poison" situation? Your thoughts and guidance is appreciated. 
 Logistic Regression will be the wrong answer in case of complete separation. Using Google I found, that the error message given in the comments is what SPSS writes in case of complete Separation. That's not to bad, as it is particularly easy to give a threshold in cases of complete Separation. Any number in between the largest GAP that leads to a Zero and the smallest that leads to a One will do. You might use the mean between those two as a threshold. 
 I just want to know.. Where does the "12" in the Kruskal Wallis formula comes from? I hope someone can give me a very good explanation. 
 Suppose $X_1$ is a standard normal variable. Define, $$X_2=\begin{cases}-X_1, &amp; \text{if } |X_1|&lt;1,\\X_1, &amp; \text{otherwise}\end{cases}$$ Show that $X_2$ is also a standard normal random variable. My approach: Let,$$F(X_2)=P[X_2\leq x_2]\\=P[X_2\leq x_2\mid|X_1|\leq 1].P[|X_1|&lt; 1]+P[X_2\leq x_2\mid|X_1|\geq 1].P[|X_1|\geq 1]\\=P[-X_1\leq x_2].P[|X_1|&lt;1]+P[X_1&lt;x_2].P[|X_1|&gt;1]$$ But I cannot compute the probabilities.May be,my approach is not right. 
 if there is no correlation between IV and UV, there can't be a mediator effect by putting a third variable in the model, is that right? 
 I have a question about determining the "population" in regression analysis. I know that when you regress on a population, your coefficients are no longer estimates about the population with sampling variability, but the true coefficient for the population. I have a question about determining what that population actually is. Suppose you're in a business setting and you have a number of stores. You want to conduct a regression analysis to determine the relationship between two variables concerning this store, say sales and GDP. You collect data on all of your stores for use in your regression. SO, would this mean that you are conducting the analysis here on the population? IE, no confidence intervals on your coefficients would be useful since you know the true betas for all of your stores. Furthermore, given that you're regressing on the population, do assumptions like normality in this case matter? 
 "The sum of the first N integers is 1/2*N(N+ 1) and the sum of their squares is 1/6*N(N+ 1) (2N+ 1). It follows that the mean and variance of the first N integers are 1/2 (N+1) and 1/12*(N^2-1)." This is where the 12 appears first in William H. Kruskal and W. Allen Wallis: Use of Ranks in One-Criterion Variance Analysis, Journal of the American Statistical Association, Vol. 47, No. 260 (Dec., 1952), pp. 583-621 I suppose you read more on that in 
 Medical student doing his first research here. This is a randomized controlled trial. Basically the study compares the effectiveness of skin glue over skin sutures. Since the length of each scar will be different in different patients, and I have the closure time as the primary outcome, how do I go about this? 2) Alternatively, can I calculate a sample size expecting a 50% difference in the primary outcomes? If so, how? Any link to a calculator for the same? 
 If you randomise them then the effect of scar length will, in the long run, balance out. You should record it though and you can ultimately do a regression analysis with scar length as a covariate which might add extra precision and would help to correct for any residual imbalance between the groups. If your outcome is closure time you do not want to base your sample size on a difference between proportions as this will through away data. 
 I have a modeling problem and would be grateful if anyone has ideas on its solution. The dataset I am using consists of time-to-event observations for organizational units within government ministries. These units may experience one (and only one) of several events: division into one or more units, merger with one or more units into another unit, or complete organizational death. I consider each event as a kind of “death” in the sense that if a division occurs, then the units that emerge from the division are all new units at risk of experiencing one of the events, and likewise for the unit that emerges if a merger occurs. I am interested in modeling these times-to-event with a variety of organizational level characteristics. If these organizational units were something like independent agencies (e.g., not structurally related to one another), then I would be comfortable modeling their times-to-event using some kind of off-the-shelf event history model . The problem arises with the multilevel structure of these data. Directorates general (DGs) contain divisions, and both DGs and divisions are at risk of experiencing the same kinds of events described above. When a DG experiences an event, it likely increases the risk that its divisions also experience an event (e.g., if the DG experiences a merger with another DG, then some divisions may be unaffected but others may be merged or dissolved). Similarly, many changes at the division level may affect the survival of the parent DG. Thus I’m afraid that a normal event history analysis would ignore the dependence of the risk among lower level units nested within the same upper level unit, and the dependence of the risk at the upper level to events that occur at the lower level. I note that the data have a multilevel structure. I have seen references to multilevel event history analyses, but I have not come across any with the same kind of structure as the data that I have. So my question: how can I model these multilevel times-to-event in such a way that accounts for the non-independence among some clusters of observations? 
 I have collected data from a questionnaire with multiple questions, all answer possibilities being on a 5-point Likert scale. After initial analysis, my data does not have a normal distribution nor can I detect a linear relationship from the scatter plots. I thus concluded that I need to use non-parametric tests, but I read in another thread here that Hayes' Mediation with bootstrapping is also an option. However, the result of the Hayes macro indirect effects give me ULCI and LLCI which include 0, so the results are not significant, right? What else can I try, or would my conclusion after this be that there is no mediation relationship (but: the total and direct effects are significant, so I can still interpret these?) 
 I don't know whether standard errors or confidence intervals are more liable to misinterpretation &amp; suspect there's not much in it. If pairwise differences in parameter estimates are of particular interest you should report them together with their SEs/CIs, &amp; thus forestall readers' drawing wrong conclusions from overlapping, or non-overlapping, SEs/CIs of individual parameter estimates. Reporting CIs is usually preferable for estimates whose sampling distribution is highly skewed: reporting SEs is rather an invitation to imagine a corresponding (symmetric) normal confidence distribution around the point estimate; &amp; the intervals implied, as well as having incorrect coverage, will do a poor job of separating parameter values better supported by the observed data from those worse supported. (When the sampling distribution is not skewed, but otherwise not well approximated by the normal, e.g. a Student's t distribution with few degrees of freedom, incorrect coverage is usually the only concern.) 
 What statistics concepts can be applied easily for exploratory data analysis on datasets that have sales as the independent variable. I have data available regarding sales in certain cities, regions, countries, genres, etc. 
 My question concerns Gibbs sampling. Suppose that I have three unknown quantities, $\mu, \sigma^2$ and $c$. I have given prior information and I have given the likelihood which allows me to compute the posterior $g(\mu, \sigma^2, c \ | \ \mbox{data})$. Now, I want to write a Gibbs sampler to generate from the posterior. To that end, the exercise says to consider the conditional distributions $[c \ | \ \mu, \sigma^2]$ and $[\mu, \sigma^2 \ | \ c]$. Using the posterior I can easily find the corresponding distribution for $[c | \ \mu, \sigma^2]$. However, the conditional density $[\mu, \sigma^2 \ | \ c]$ does not have a known functional form. Therefore, my initial idea was to split it as follows: $$g[\mu, \sigma^2 \ | \ c] = g(\mu \ | \ \sigma^2,c) \underbrace{g(\sigma^2 \ | c)}_{(A)},$$ where $(A)$ can be found by integrating $g(\mu, \sigma^2 \ | c)$ w.r.t $\mu$. This gives me some nice distributions to work with, however, I do not have a dependence with $\mu$ anymore in $(A)$ since it has been integrated out. Therefore, I don't think this is the correct way. Instead, to write the Gibbs sampler I should consider the following conditional densities: $g(c \ | \ \mu, \sigma^2)$ $g(\mu \ | \ c, \sigma^2)$ $g(\sigma^2 \ | \mu, c)$. Question: is the first idea wrong? I guess we can use it but it is not a Gibbs sampler then? 
 Is there some way to do a proportional hazards model where the "treatment" occurs multiple times? Here's an example: Let's say you are running a Ford dealership where you sell only one car... The Ford Stat. Then let's say the price of your cars change daily. Your business model is to have customers call in and check for prices. On day 1, you have Customer A call in and ask for prices. Then you estimate the likelihood of a purchase. Then on day 3, Customer A calls again. The estimate will change somehow. Then say you have Customer B, who is calling for prices every day for a month. But you figure he's probably not a customer after all, but probably a competing dealership just checking your prices. So the likelihood of a purchase is probably pretty low. Is a proportional hazards model the right choice for modeling the purchase likelihood? If so, how does one incorporate the effect of multiple treatments? 
 Once you have loaded forecast package, you must use Arima() function for AIC, AICc and BIC. Notice upper "A" in Arima() function. If you use arima() function with lower "a", then R will use the function that comes with base R. 
 There are several ways to concentrate your efforts.. Operationally, you could look at the inflow of patrons at a given time. This could help in staffing theaters with appropriate number of resources at the right time. The same data divided by different cities can help with cost reduction. A low turnout for showings at night means the cities are sleeper cities and do not maintain an active night life. This will help adjust operational hours and reduce unnecessary staffing costs. You could also look at trends to find correlation in the times movies are most often played and the overall box office results. The aim is to find whether more showing at night times (assumed to be prime times) result in higher box office totals, or are there movies with the Money Ball effect where they are assumed to me non-prime-time worthy but net healthy box office totals. 
 I like the @hxd1011 answer, and this is only to expand on it slightly. Here is my code: You could, if you wanted, put other tree parameters in there instead of tree-count. And now to plot the results: This gives the following plot: About the better, and agreeing with @hxd1011, eventually more trees doesn't do much good. It doesn't improve error. It does take more memory and cpu. You can observe that the 125 tree forest is, most of the time, pretty close to the MSE of the 500 tree forest. When you start pruning, both by requiring at least so many samples to make a leaf, and by only allowing the tree to get so many levels deep, it substantially improves memory. A "not bad" start is 5 elements per leaf, and max depth of 8. This really is emprical and depends on the problem being solved. If we change the code as follows: Then the plot changes as follows: You can see that when we use around 7 leaves per node, the MSE is still reasonably consistent with the "1" or "4" elements. If we had used 19 leaves, that requirement substantially impacts the MSE, and the MSE could be nearly double that of a "7" elements per leaf on tree rule. The red lines are the mutant child of an eyeball norm and a scree plot. After 21 leaves per tree are required the RF becomes essentially no more useful than the midrange. You can also do things like round the input data, or truncate digits. This allows less discrimination on inputs and can drive generalization. Take baby steps when doing this because while throwing away a little data can be a good thing, throwing away a lot of data is usually a bad thing. Fun observation: Brieman's randomForest beat h2o here. It takes much longer to do this exercise with h2o from today than with Brieman's work from a decade ago. 
 The exercise asked you implement a Gibbs sampler using the full conditionals of $[c \mid \mu, \sigma^2]$ and $[\mu, \sigma^2 \mid c]$. This is a valid Gibbs sampler and has the special name of Blocked Gibbs Sampler , since $\mu$ and $\sigma^2$ are treated as one variable using their joint conditional distribution. To see this think of $\theta = (\mu, \sigma^2)$. Then the blocked Gibbs sampler would look a usual Gibbs sampler with the conditionals being $[c \mid \theta]$ and $[\theta \mid c]$. The tricky part is, how do you find $[\theta \mid c]$? Your idea of using $$g[\theta \mid c] = g[\mu, \sigma^2 \mid c] = g[\mu \mid \sigma^2, c]\, \underbrace{g[\sigma^2 \mid c]}_{(A)}$$ is absolutely the right idea and is correct. With these distributions, your blocked Gibbs sampler will update as follows. Let $c_0, \mu_0$ and $\sigma^2_0$ will be the starting values. $c_1 \sim g[c \mid \mu_0, \sigma^2_0]$ $\sigma^2_1 \sim g[\sigma^2 \mid c_1]$ $\mu_1 \sim g[\mu \mid \sigma^2_1, c_1]$ and so on. 
 A categorical distribution for $k$ categories is parameterized by a $k$-vector of probabilities $Q= \langle Q_1,Q_2,...,Q_k \rangle$ summing to 1, and for categorical random variable $C$, $p(C=c)=Q_c$. Let $C$ be a random variable distributed as $Categorical(Q)$, where $Q$ is a random variable (i.e. the categorical distribution has a prior distribution), and $Q$ is drawn from a distribution with support on the $k-1$ standard simplex (i.e. probability $k$-vectors summing to 1) and mean value $\mu_Q=\langle \mu_{Q,1},\mu_{Q,2},...,\mu_{Q,k}\rangle$. Is it always the case (for any such prior distribution) that the marginal distribution of $C$ (after marginalizing out $Q$) is a Categorical distribution with parameter $\mu_Q$, regardless of the shape of the prior distribution? My reasoning (using the notation that $\hat Q_c$ is the $c$-th element of $\hat Q$): $p(C=c)=\int p(C=c|Q=\hat Q)p(Q=\hat Q)d\hat Q$ $p(C=c)=\int \hat Q_c p(Q=\hat Q)d\hat Q$ $p(C=c)=\mu_{Q,c}$ Therefore $C$ is a categorical random variable with parameter $\mu_Q$. 
 I have a data set that contains the dates and times of purchases. Say I have one month of data for the purchases. I am looking for the interval of time, say 10 mins / 30 mins / hourly, for which 90% of such intervals will contain at least a specified number of sales. So if I had 100 purchases occur over a 1 month period, and I wanted to find the smallest time interval within which I could count the number of purchases for that interval and have 90% of the counts for the month be &gt;5 purchases, how might I go about determining this time interval? For example, if I were to divide the data into 15 minute chunks, I might come up with this for the first 90 minutes of the monthly data: This does not satisfy the criterion: The purchase counts are too small in too many intervals, presumably because the time interval is too small. So I could try 1 hour chunks next: Now I am satisfying the condition, but perhaps this is not the minimum time chunk: I can probably use a smaller interval than 60 minutes and still have 90% of the counts be above 5. Is there way I can do this mathematically or not with brute-force? 
 You have the correct idea so far, but are missing a key trick. Recall that if $X_1$ is a standard normal distribution, then $X_1$ is symmetric around 0. This means that $$P(X_1 &lt; x_2) = P(X_1 &gt; -x_2) = P(-X_1 &lt; x_2) = P(-X_1 \leq x_2). $$ Using this, the result will follow directly from your last step. 
 SVM is minimizing hinge loss with ridge regularization $$ \min_\mathbf w \sum_i(1-y_i \mathbf w\cdot \mathbf x_i)_+ +\lambda ||\mathbf w||^2 $$ So, the scaling will make differences when we have the regularization term. My hypothesis would be the original scale of your features impacts regularization on different features and make the performance better, but after the scaling, that disappears. For example, you have 2 features, the first feature is in scale of $10,000$ and second feature is in scale of $0.1$. if you do not perform scaling, the SVM will regularize much more on 2nd feature, and almost have no effects for the weight of on 1st feature. if you do perform scaling, the SVM will regularize both features equally You can validate my hypothesis to check the "feature importance" in your data. If you see, features in larger magnitude is much more important, at the same time you have "many useless features" in small scale. Then, my hypothesis might be right. 
 I have 4 dataset which are independent measurement of the same physical quantity. I have fitted each dataset with a certain model, as a result of the fit I estimated one parameter with its uncertaity (I did the fit using scipy curvefit and I took as uncertainty the squareroot of the covariance matrix, which is a scalar in this case since I am fitting one parameter only). Therefore, performing the fit on the 4 independent dataset I got the 4 estimation of the parameter $K$: $K_1\pm\delta_{K_1}$, $K_2\pm\delta_{K_2}$, $K_3\pm\delta_{K_3}$, $K_4\pm\delta_{K_4}$. Now, I would like to use the information coming from each dataset to give a unique value $K_{u}\pm \delta_{K_{u}}$. I have several ideas about how to estimate the best $K_{u}\pm \delta_{K_{u}}$: 1) perform the weighted average: $$K_u=\frac{\sum_{i=1}^{4}K_i\cdot w_i}{\sum_{i=1}^{4}w_i} $$ where the weights are given by: $$w_i = \frac{1}{\delta_{K_i}^2}$$ and $$\delta_{K_{u}}=\frac{1}{\sqrt{\sum_{i=1}^4 w_i } }$$ 2) Neglect the uncertainties of the $K_i$ and compute a final value $K_u\pm \delta_{K_{u}}$ as sample mean and sample standard deviation: $$K_u=\frac{\sum_{i=1}^{4}K_i}{4} $$ $$\delta_{K_{u}}=\sigma=\sqrt{\frac{1}{4}\sum_{i=1}^4(K_i-K_u)^2}$$ 3) considering all the results of the fit as gaussian random variables I could also give a final value having $$K_u=\frac{\sum_{i=1}^{4}K_i}{4} $$ $$\delta_{K_{u}}=\sqrt{\frac{1}{ 4}\sum_{i=1}^4 \delta_{K_i}^2 }$$ i.e. I am threating $\delta_{K_i}^2$ as the variance of my gaussian random variable, therefore I am computing the variance of the average as the average variance and then I take the squareroot of it. I am not too happy with method 1) since it consider more good results (having low uncertainty) than bad results. Method 2 is not considering the uncertainty given from each fit. Method 3 seems the best one but I have no rigorous argument to prove that it is the most appropriate to use. Has anyone a better idea about how to summarize my 4 fit results in a unique value? 
 I would divide it into 3 cases: $X_2 \le 1$ $-1 \lt X_2 \le 1$ $1 &lt; X_2$ The first is trivial, since $X_1$ is standard normal. For the second, $ \begin{align} F(x_2) &amp;= F(-1) + P(-1 \lt X_2 \le x_2) \\ &amp;= F(-1) + P(-x_2 \le X_1 \lt 1) \\ &amp;= F(-1) + P(-1 \lt X_1 \le x_2) ~by~symmetry \end{align} $ which is standard normal since $X_1$ is. The third case is similar. 
 We used the R library to make predictions for the next 24 hours. We have the following: Now we have 2 problems: We need 'time' (in hour e.g. 01,23,19 etc) instead of 'point'. We wish to plot the trendline against time showing the actual observed values against these predicted values. We have loaded actual observed values from a CSV file. We tried: Doesn't work, and using just shows some points in a straight line instead of curved trendline. 
 Keep in mind why people typically scale features prior to estimating an SVM. The notion is that the data are on different scales, and this happenstance of how things were measured might not be desirable -- for example, measuring some length quantity in meters versus kilometers. Obviously one will have a much larger range even though both represent the same physical quantity. However, there's no reason that the new scaling must be better. While it's true that the rescaled features will all vary in comparable units, it's also possible that the original scaling happened to encode the data such that some important features had more prominence in the model. Consider the example of two different versions of the Gauissian RBF kernel: $K_1(x,x^\prime)=\exp(-\gamma||x-x^\prime||^2_2).$ This is an isotropic kernel, meaning that the same scaling ($\gamma$) is applied in all directions. A more general kernel function might have the form $K_2(x,x^\prime)=\exp\big(-(x-x^\prime)\Gamma(x-x^\prime)\big);$ it is anisotropic as $\Gamma$ is a diagonal PSD matrix, with each element applying a different scaling to each direction. The advantage of this kernel function is that it will vary more strongly in some directions than others. Coming back to your question, it's possible to imagine that your data have, for whatever reason, some features that are more important than others, and that this coincides with the scale on which they are measured. Placing them on the new scale where they all appear on similar scales and are all treated as equally important means that unimportant or noise features cloud the signal. As an aside, don't use accuracy as a metric for comparing models .(1) (1) "The Case Against Accuracy Estimation for Comparing Induction Algorithms", Foster Provost, Tom Fawcett, Ron Kohavi 
 Consider a change of variables: $$\min_{w'} \lVert y - X' w \rVert^2 + \lVert w' \rVert_1$$ where $w' = \lambda \circ w$ and $X'_{ij} = X_{ij} / \lambda_j$. This problem is equivalent, and allows us to directly apply the original SAFE rule with $\lambda = 1$: $$\lvert {X'_i}^T y \rvert &lt; 1 - \lVert X' \rVert_2 \lVert y \rVert_2 \frac{\lambda_\max - 1}{\lambda_\max}$$ with $\lambda_\max = \lVert {X'}^T y \rVert_\infty$. Note that it is not equivalent to your proposed rule, but is about as easy to compute. It may be the case (by following through the original proof) that your proposed rule is also valid – it seems reasonable enough – but without doing that I'm not sure. One drawback of this, relative to your proposed rule, is that all the quantities must be recomputed if you change the relative contributions of $\lambda$. (A global scaling can be easily handled by adding a \lambda_0 back into the reformulated problem.) The advantage is that it certainly works, unlike your proposal which may or may not be true. :) 
 This answer should be thought of as an extension to the helpful answer from @Björn, with more detail than could fit into a comment. As you are measuring field potentials (EEG) at what must be closely spaced depths, the correlations among depths certainly must be taken into account. (Were these, say, action-potential firing rates from different cortical layers, that might not have been such an issue.) Note that the mean values over depth and time aren't at issue; as @Björn notes, the issue is to perform statistical tests that take the correlations into account. You need, however, to think carefully about what you are trying to learn. From your explanatory comment after @Björn's answer was posted: What we are trying to learn are the effects (i.e. changes in power at a specific frequency) of treatment over time and depth. It sounds like you want to track systematic changes over space and time, with a one-dimensional spatial variable. That would make your problem similar to analysis of spatial time-series data. I suppose in the classic agricultural applications of statistics that used to fill textbooks, this would correspond to measuring crop yields (power at your specified frequency) over 10 years (10 time periods) in multiple treatment plots (animals/treatments) on a field that has a gradient of fertility (cortical depth), sampling within each plot along the fertility (depth) gradient. This Cross Validated page and its links may provide some help if you wish to explore such an approach. Given the decades over which time courses of EEG and more recently fMRI data have been analyzed in brain regions, I suspect that there is precedent in the literature for analysis of your data. You should consider that precedent, as it may already provide an established solution or at least will need to be addressed in your manuscript if you decide on some other analysis approach. Consultation with a local statistical expert might also be wise. The terminology of "repeated measures" and "mixed model" is partly historical artifact. Back when much emphasis was on ANOVA, "repeated measures" was terminology for multiple measurements from the same individual. The "mixed model" terminology seems to come from the move toward generalized linear models, so that a linear model with both fixed and random predictor variables is "mixed." Don't get hung up on the terminology; do the analysis that best answers the question that you are asking. 
 The basic issue is that when $X_t$ is a non-stationary process with a unit root, you can't apply the usual central limit theorems that imply convergence to the normal distribution: their assumptions are violated. Instead, you can apply a functional central limit theorem, and it turns out the sample statistic doesn't even converge to a constant but rather a functional of Brownian motion: the test statistic converges to a random variable. A much more thorough discussion than I can give is available here: As a practical matter, the Dicky-Fuller distribution has a somewhat bell curvish shape but with significant left skew. 
 I am currently writing a proposal and am confused regarding which statistical method I should use. I aim to examine the motivation of runners in participating a charity marathon. It will use runners categorized into 8 motivations and 4 types of running groups. Hence, there is two categorization involved. I will know what the 8 motivations and 4 running groups are and I will know the classifications to each runner. I just want to try to relate the motivations and the groups. I am still new to statistics and am confuse if do I use two different statistics or can I use one? 
 I am not a statistician or mathematician and I need some help. I did three experiments as I labeled on the figure, 3,4,5, each experiment has x and y results. The markers are the real data and the lines are fitted lines I draw. Clearly, there is a trend to go up, but I want to know when the data is relatively stationary? or in another word, when the change of the data is relative small. Is there any relevant test I can use? I tried to differentiate the trend line and try to find when the change is small, but the result is not clear and and hard to tell. Thanks for your help!!!! 
 This sounds like a classic linear regression hypothesis test. To find out if the slope is not 0, you need to reject the null hypothesis that the slope is 0. Paraphrasing this website : Calculate the standard error (SE). Calculate the degrees of freedom (n-2) where n is the number of points in one of your experiments. Calculate the t statistic = b/SE where b is the slope. Use the t-distribution calculator in the link above to calculate the p value. If the p value is less than 0.05, then you really do have a trend. 
 In my data analysis I must always keep the errors (or uncertainties). In physics, a result without an error estimate is worthless. Take this trivial example: I measure $x$ and convert it to the quantity of interest, $y$ with the relation $y = f(x) = x^4$. In my first year I have used Gaussian error propagation, that means that given $\Delta x$ I can compute $$\Delta y = f'(x) \Delta x = 4 x^3 \Delta x \,.$$ This needs to have a few assumptions to work: The error is Gaussian The error is small The errors are independent if there are multiple dependencies (at least I think so) The errors are symmetric Then it is also tedious as one has to take the derivative of all the transformations used. For some transformations, like the median, this is not even possible. Later on I learned about the class of bootstrap methods. Those are nice because one can do arbitrary complex transformations, have asymmetric errors (in principle), and correlations between various quantities. Say we have $x = 1.0 \pm 0.1$. This implies that $X$ is a Gaussian distribution with $\mu = 1.0$ and $\sigma = 0.1$. I can resample this distribution to give me a lot of “observations”. Then I plug each of them into the transformation and look at the distribution $Y$. From the Gaussian error propagation I would get that $y = 1.0 ± 0.4$, that is $Y$ is a Gaussian with $\mu = 1.0$ and $\sigma = 0.4$. Looking at the actual distribution of $Y$, one sees that it is not that Gaussian any more: The descriptions of this distribution are: So far I have just used “Mean ± Std” which seems to be a bad idea with this picture now. But “Median ± Std” seems to be wrong because the standard deviation is the deviation from the mean, not the median. I once used the median and some percentiles to get an asymmetric error estimate. There I had the problem that the central/original value, $f(x)$, would lie to the far side of the distribution. Depending on the percentiles chosen, I would have a negative upper uncertainty, which does not seem to make sense. For the actual value it makes totally sense to use the central value given, that is $f(1.0) = 1.0$ in this case. Depending on the asymmetry of the $Y$ distribution, that value can lie far to either side. This gives me three options: Ignore all that, think that other people would have used Gaussian error propagation anyway and just take “mean ± std” or “median ± std”. Take “central value ± std” although this might not describe the distribution $Y$ very well. Just taking percentiles for lower, median and upper bound will always give self-consistent results. But then I have not used the central value but have only relied on the resampling. What is the correct way to do this? The code which generated the image: 
 I am conducting post hoc power analysis using G*Power. The test conducted was Anova, fixed effects, one-way, with one control and two test groups. Control had 25K records, each of the test groups had 2K. Now, G*Power outputs a message whenever the total sample size is not a multiple of the number of groups, saying that "the analysis was based on the average group size". Does this imply that the G*Power calculation is correct only for samples of equal size? 
 I created a simulation to compare a number of regression type models/estimators, lets call them M1, ...,Mn. for each iteration of the simulation run: I generate randomly data set X I generare randomly a set Y, which is conditional on X (i.e. the distribution of Y_i depends on X_i) I apply all n models for "predicting" Y from X I estimate the following: A) conditional mean of Y given X=X0 (for some values of X0). I know the true conditional mean, since I know how Y has been generated conditional on X B) mean square error for comparing the values of Y with the predicted values Y.hat from the model Then I summarize all these over all simulation runs, and get the MSE for estimating E[Y|X=X0], as well as the distribution of the MSE for prediction (as described in B) above. I find that based on the MSE for estimating the conditional mean E[Y|X=X0], model M1 (lets say) is the best, for most of the values of X0. But then, regarding the MSE of prediction for the actual values of Y (not the conditional means) model M1 is in fact the worst of all!!! I find this strange. Does it make sense in the first place to estimate these different MSE? Should I say that M1 is optimal choice for estimating conditional means but the reverse for prediction? Any help wound be appreciated. Thanks. 
 Question: What model (Likelihood/prior family) is appropriate to use when attempting to do inference on a vector of boolean outcomes given continuous factors? Elaboration: I am only aware of machine learning approaches to this at present (for instance, an Artificial Neural Network may have multiple output layer nodes). I am interested in learning Classical/Bayesian approaches to this problem. I would also greatly appreciate being directed to some resource demonstrating how to implement the model so I don't have to reinvent the wheel (MCMC or fisher scoring or whatever). Toy Example: In the FooBar Baseball League, there exist two Halls of Fame (A and B). I have historical information about players' batting averages, seasons played, and so on, which I will call my factors, and historical information on which players were accepted into the halls of fame (I don't know if this is a rare event in real life, but don't worry about that aspect). I would like to do inference on which players will be inducted in the future. It is known that the Hall of Fame leaders take different approaches in making their decisions (maybe A cares more about batting average than B). It is known that the Hall of Fame leaders play golf together on weekends, and often discuss which players they are thinking of inducting, meaning that they influence each other's decisions (that is, the events of being inducted into each hall are conditionally dependent given the factors). By request, here is what the data would look like: factors(X): Player || Batting Average || Seasons Played Jon A || 0.23 || 3 Ben C || 0.32 || 7 ... ... ... ... ... ... ... ... ... ... .. Response(Y): Player || Accepted (HoF A) || Accepted (HoF B) Jon A || NO || YES Ben C || YES || YES So the response is a matrix instead of a vector as is usual in logistic regression. Bonus: I am especially interested in being able to conduct sequential updating on this model (be it Bayesian or not). 
 I recently completed a research project that involved motor learning in control (non-disease) subjects. The outcome of the motor learning is binary: 1 - subject learned, 0 - subject didn't learn. 13 out of 30 (43%) of control subjects learned. I ran a best-subset logistic regression procedure and ended up with a good model that predicted the outcome correctly 88% of the time (on a test set). The model had two continuous variables that reflect motor performance, A and B. Now, I am expanding these findings to a disease group. 2 of 20 (10%) of the disease subjects learned. With a Fisher's test that is a significant difference from 13 out of 30. I want to know if it is the disease or the motor performance that causes the reduced learning in the disease group. The problem is that my disease group is older than my control group. (t(48)=-2.3, p=0.03). Of course, variable A is correlated with age, but variable B is not. Here's why I've gotten myself into a circular argument: If I compare the control and disease group on variables A and B with age as a covariate, there is no difference (A; F(1,47) = 1, p = 0.3) (B; F(1,47)= 4, p=0.05). The original model (Learning ~ A + B) works well if I just include the data from the disease group without adding diagnosis to the model. That makes me think that motor performance (not diagnosis) predicts learning. If I add age into the model (Learning ~ A + B + age), age is not a significant contributor (Wald = 0.6, p = 0.4), but A and B are significant contributors. That also makes me think motor performance predicts learning. If I add diagnosis to the model either with age or without age (Learning ~ A + B + age + Dx; Learning ~ A + B + age + Dx), Dx is never a significant contributor. That makes me think that performance predicts learning. So, if it's A and B that predict learning, and I have the same levels for A and B across my groups when controlling for age, why am I seeing such a difference in the binary outcome for my groups? Maybe I'm missing an interaction term, or this is simply because linear and logistic regression work differently. What's real? 
 I would try a bivariate probit , which allows errors to be correlated across equations (like the golf outings). 
 A Tolerance Interval (TI) is an interval that contains $p\%$ of a population with $(1-\alpha)$ confidence. A tolerance interval can be contrasted with a prediction interval that has a $p\%$ probability of containing a future value (realization) drawn from the population, and a confidence interval that contains the true value of a parameter (e.g., the mean) with $(1-\alpha)$ confidence. Note that a tolerance interval is specified by two values (the desired proportion, and the desired confidence), whereas prediction intervals and confidence intervals are specified by only one. 
A tolerance interval is an interval that contains $p\%$ of a population with $(1-\alpha)$ confidence. A TI is distinct from (but sometimes confused with) prediction intervals & confidence intervals.
 In short: yes, you could use a (simple) model(s) to compute the AUC (AUROC) for categorial features too. When you compute the AUC for an ordinal feature, you use the feature itself like you would use a classification model output and apply the threshold to it (of which one class lies below and the other lies above). Note that the complexity is determined by the - in this case non-existing - model: using a threshold on an ordinal feature boils down to using a linear separation that divides the feature into two parts. If you would use a more complex model instead (e.g. tree), you could easily obtain multiple parts too. For a categorial feature, doing so might make sense. This essentially is just answering the question "how likely is class if my feature is ?", which you could employ many model types for (small trees, etc). Note that you can of course overfit this too, so using models with little complexity might be reasonable (like the linear separation for the ordinal feature) . PS: you might need to encode your categorial variable in one-hot encoding for some models (that cannot make meaning of categories themselves), e.g. if you want to use it in logistic regression. This makes the problem $N$ dimensional instead, with $N$ being the amount of categories of your variable (though this is automatically done with most implementations). 
 When we compute the cross-entropy within the machine learning context, we use the following formula: $$ CE(t, p) = -\sum_{i=1}^{N} t_i \ \log(p_i) $$ Where $t$ is the target class probability, and $p$ is the predicted class probability. My question is, this metric obviously gives us different loss values if the $t$ and $p$ were reversed in the above formula. So, what is the rationale, and reason(s), for the the targets $t$ being outside the log, and the predictions $p$ being inside the log? Why this configuration, and not the opposite? 
 I have trouble understanding fixed effects models. I thought that they take out across groups variation by subtracting group-specific means from the values (which in my eyes would be group mean centering). Now I read that you can specifically group mean-center fixed effects models. Which, to me, looks like doing it twice :D Sorry, super confused! Can someone help me clarify this? Thanks! 
 See Jeffreys, [Sir] Harold. 1961, paperback 1983. Theory of probability Oxford: Oxford University Press. Ch.5.7 for discussion of various error distributions. The original paper is Jeffreys, Harold. 1938. The law of error and the combination of observations. Philosophical Transactions of the Royal Society, Series A 237: 231-271; DOI: 10.1098/rsta.1938.0008. 
 I want to verify whether Weibull is a good candidate for my distribution in survival analysis. So I plot log(t) vs log(-log(Kaplan-Meier). But instead of two lines, I get plot where the lines are somehow connected . Any ideas what am I doing wrong? 
 Is there a [hinge] loss function that is suitable for one-class classifiers? i.e., anomaly detection. Thanks. 
 I have an anonymized dataset with 1.2 million dog and cat surgical sterilization records from 81 clinics. I would like to measure inter-clinic variation in mortality rate as part of a preliminary study that would help justify a second study to survey the clinics in an attempt to find reasons for any detected special cause variation. At first pass funnel plots seem to be a nice way to visualize the data as the number of surgical cases per clinic varies greatly (between 16 and 80,000 cases). These data are not normally distributed (extreme right skew), and I am additionally concerned that the size of the dataset may lead to apparent over dispersion. I am using the spreadsheet from the Public Health Observatories to generate the funnel plot. The non-normalized mortality data looks much like I expect (nearly all clinics within bounds, a few above and a few below), but I wonder if that's two wrongs (non-normal data and uncompensated apparent overdispersion) making a right? When I normalize the data via square root transform (partial normalization) or logarithmic transformation (better normalization but must drop clinics with 0 deaths) the control limits are much tighter and the majority of clinics are outside of the control limits. I am concerned that I may need to change the method for calculating the limits. 
 So I have done my dickey fuller test on the variable exchange rate. The result table indicated that I do have unit. In addition, I have tried lagging the variable but still ended with a result that say there is unit root. My question, how do i correct for unit? 
 (1) indicates that one can compute a confidence level from the approximate randomization test (step 4): However, (1) simply refers to (2) to view the actual calculation or table lookup. Unfortunately, I don't have access to (2). How can I compute the confidence level in the approximate randomization test? Chinchor, Nancy. "The statistical significance of the MUC-4 results." In Proceedings of the 4th conference on Message understanding, pp. 30-50. Association for Computational Linguistics, 1992. https://scholar.google.com/scholar?cluster=9100345284812509896&amp;hl=en&amp;as_sdt=0,5 Noreen, E. W. (1989) Computer Intensive Methods for Testing Hypotheses: An Introduction. New York." John Wiley &amp; Sons. https://scholar.google.com/scholar?cluster=9751452310612539454&amp;hl=en&amp;as_sdt=0,5 
 I have been trying to find the asymptotic normality of the non-linear least squares estimator. If I start with $0=X_t(\beta)'(y_t-x_t(\beta))$. I know that I have to perform Taylor expansion around the true parameter $β_0$. I can do that for the $x(b)$ (the one in the brackets). Then I will substitute $y=Xβ_0 + u$ and then the goal is to find an expression such as $n^{1/2}(\beta-\beta_0)$. This expression will be equal to something that I can apply the CLT. What troubles me is the first $X(b)'$ (the one outside the brackets). Should I perform Taylor expansion for this term as well? Thanks in advance. Any help will be highly appreciated as I have been struggling with this problem for a while. 
 Is it correct to analyse data as described in the following? N Participants were asked to watch different videos showing the outline of a hand grasping an object. Four different types of hands were shown: a 'real' human hand (h_human), and three different 'artificial' robotic hands (h_robot1, h_robot2, h_robot3). Additionally, not only one but 6 different grasping motions were shown. All in all the participants watched 4*6=24 videos. After each video the were asked to decide whether they video showed a human hand (outcome 1) or a robotic hand (outcome 0). The following questions are interesting for me: Can the participants distinguish between the human hand and the robotic hands? Which one of the three robotic hands appears to be most human-like I was thinking about calculating the percentage/rate of human-likeness for each participant and hand over the grasping motions. For example, for N=4 the original outcome looks like this (with P_i being participant i and G_j being grasping motion i) This is reduced the an Nx4 Matrix if the human-like rate is calculated with Number_of_ones_per_hand/6: My questions are now: Is it appropriate to just calculate this rate and use the Friedman Test on this Nx4 matrix? Are the other methods better suited? (The number of participant will be around N=200) 
 A credible interval is an interval that includes the true value of a parameter of interest with $(1−\alpha)\%$ probability. Credible intervals treat the interval as fixed and the parameter as random. They are a Bayesian concept. The frequentist analog of a credible interval is a confidence interval . Although they are superficially similar, the are philosophically distinct. 
A credible interval is an interval in Bayesian statistics that includes the true value of a parameter with $(1−\alpha)\%$ probability. Credible intervals treat the interval as fixed and the parameter as random.
 Try differencing $x_{t}-x_{t-1}$ and test again. You might also use differences of the logs to stabilize the variance. Sometimes doing seasonal differencing (with or without logs), rather than consecutive observations, will work better. Sometimes you may need to difference more than once. Essentially, differencing can sometimes help stabilize the mean of a time series by eliminating trend and seasonality. 
 I'm trying to make sense of a SAS program that I have inherited. It uses a linear mixed model to examine the effects of applied fertilzer rate on the grain yield of corn. There are five different fertilzer rates and three replicates of each rate, and the experiment was repeated each year for three consecutive years. The existing SAS model is as follows: Can anyone tell me why treatment would be specified as a random effect in this model? 
 I'm doing an econometric analysis to try to explain the gender wage gap. Here's my regression: Bp-test, White test outputs and residuals VS fitted values plot (rvfplot command in stata) So for the bp-test i'm rejecting the null hypothesis of homoscedasticity. For the White test i'm NOT rejecting the null hypothesis of homoscedasticity. White test shouldn't be more general than bp test? so shouldn't it be more precise than bp test? Why they give opposite outputs? From the residuals plot i can see there's no much difference in variance exept for some outliers. Am i wrong about this interpretation? Thanks to anyone's help/criticism!! :) 
 I'll rewrite the cross entropy in terms of distributions $q$ and $z$, so we can save your notation $t$ and $p$ for explicitly talking about classifiers. The cross entropy is: $$H(q, z) = -\sum_{x} q(x) \log z(x)$$ The information theoretic meaning of the cross entropy is this: Say $q$ is a distribution that generates some data. We want to encode the data using a set of symbols (e.g. to transmit it over a channel, or store it). We'd prefer to use short codes, because they require fewer resources to transmit/store. There's a fundamental relationship between code length and entropy . It turns out that, for the optimal code, the average number of bits per symbol is given by the entropy of the distribution. No encoding can be shorter than this without destroying information. The intuition is that we should assign short codes to high probability events (because they'll occur more frequently) and longer codes to low probability events. Looking at the definition of entropy: $$H(q) = -\sum_x q(x) \log q(x)$$ This is the expected value of $-\log q(x)$, which is the optimal code length for event $x$ (given in bits if the log is base 2, or nats if it's the natural log). But, say we don't know $q$, and instead we have some 'proxy' distribution $z$. We can design an optimal code for $z$, even though the data are generated by $q$. In that case, how many bits/nats per symbol will we use on average to encode the data? The answer to this question is given by the cross entropy $H(q, z)$. Looking at the definition of the cross entropy, we can see that it's the expected value of $-\log z(x)$ with respect to distribution $q$. Here, $-\log z(x)$ is the optimal code length for event $x$, using the code based on $z$. The expectation is taken over $q$ because that's what generated the data. As our proxy distribution $z$ becomes closer to the data-generating distribution $q$, our code will be more efficient, and the cross entropy will be lower. It's minimum possible value is $H(q)$, when $z$ = $q$. Bringing things back to classification, we have $t$ as the true/observed distribution of class labels, and $p$ as the classifier's estimated distribution over class labels. Plugging these concepts into the description of cross entropy, it makes sense to use $H(t, p)$ because $t$ is the data-generating distribution and $p$ is the 'proxy distribution'. Here's another argument. When using hard class labels, people treat distribution $t$ as the empirical distribution, which assigns probability $1$ to the true/observed class label $i$, and $0$ to all others. In that case, the cross entropy reduces to: $-\log p(i)$. Summing over all data points, this is just the negative log likelihood. In that case, minimizing the cross entropy loss is equivalent to maximum likelihood estimation. 
 I'm new with U test and I have some doubts about the rejection of the null hypothesis with the U test with normal distribution approximation. In my example I used this data for a 1 tailed test: $$ H_0: median_1 = median_2 $$ $$ H_1: median_1 &lt; median_2 $$ $$ \alpha = 0.05 $$ $$ Z_\alpha = 1.645 $$ I obtained $$Z = 1,0313$$ Do I reject the null hypothesis if $$Z &lt; Z_\alpha$$ or if $$Z &gt; Z_\alpha$$? Thanks in advance for your answers. 
 I'm trying to find the P-value for each of my variables for each regression I run in a rolling regression. I've figured out that to get the betas and standard error for each coefficient I need to include _b and _se in my command. What do I include to get the P-value for each coefficient? 
 A point process is a stochastic process in which the data are sets of points ordered in a mathematical space. A common example is points which are ordered in time. If the arrival of each point is independent of those that preceded it, and all points have the same arrival rate, the interarrival times will be exponentially distributed. Such a process is called a Poisson process . The distribution of the total number of points occurring within a given temporal interval will be Poisson. 
A point process is a stochastic process in which the data are sets of points ordered in a mathematical space. A common example is the Poisson process, in which points are ordered in time with the interarrival times exponentially distributed.
 I am working on a model to predict the risk of some outcomes and could really use some advise: Let's say we have x number of patients, each patient have anywhere between 0 and y number of visits (such as ED visits or hospitalization); we want to develop a model to predict the risk of certain outcome (say hospitalization within the next 30 days) each time the patient comes for a visit (say each time a patient shows up at the ED). In this dataset, we will have visit level variables (such as main reason for the visit, timing of visit, etc.) and patient level variables (such as demographic variables). My two questions: 1. Would a hierarchical model (visit level and patient level, with random intercept) be a valid approach? In this approach, each patient will have its own intercept, essentially capturing the unknown and unobserved "latent" characters of the patient on that risk we are trying to predict. One concern is the ability (or lack of ability) of the model derived this way to accurately predict the risk for new patients not in the derivation dataset, as there is no prior information for the intercept for the new patients and the accuracy would be significantly worse than what you would get predicting existing patients; 2. The related question of the most appropriate way to derive and validate such a model. One potential approach would be to simply randomly split the dataset into two on the visit level, and use one to derive and the other to validate. The issues include overfitting as most patients will be included in both datasets. Another approach would be to split the patients into two groups, one to derive the model and the other to validate, so that the validation set will have entirely different patients compared to the derivation set. Is either of these two approach valid, if we are talking about a hierarchical model? Or is there a third, and better or more appropriate way to handle this? I realize that what I have said may be confusing or I may not have provided all relevant information; in that case, please let me know how I can clarify or which additional information would be helpful. Thanks a lot! Modeler 
 I have a data set of profiles that describes a set of people and a binary classification describing when some of these people have been grouped together in the past which I am assuming implies similarity. I would like to classify every person in this data set as either similar or dissimilar. One option is to take the features of profile A and profile B, calculate the distance between each of them individually, and then use these distances to predict similarity or dissimilarity. Another option is to use the features of profile A and profile B directly to predict similarity or dissimilarity. Once I've decided on feature preprocessing this problem becomes a "Positive Unlabeled" or PU problem that I may be able to deal with using domain knowledge (that is I know which profiles will never appear together and can thus label them negative). I'm looking for advice on how to approach defining the features in this problem. 
 U test can be explained with Area Under ROC see this post . It is a non-parametric test, not necessary to tell if mean or median is different but two distributions are different. Do not know how if you like to try following code to see how the numbers are calculated. 
 From measurements (only) the values $a_i =\int_{x_i}^{x_{i+1}} f(x) dx$ for some sequence of (equispaced) $x_i$ are known about a non-negative function $f$. What would be techniques to find a reasonably smooth approximation to $f$? 

The time between the arrival of one point & the next, as in a point process or queue. More abstractly, 'time' can be a distance between 2 points in a space.
 I found some difficulties in here. We know that if $X$ has Binomial distribution with $1$ trial and $p$ success, or what we called $X$~$Bin(1,p)$, we have $\mu=p$ and $\sigma^2=p(1-p)=pq$. From that, we have $\bar{X_n}$ stochastically convergent to $p$ with variance equals $\frac{pq}{n}$. Then, we know that $\frac{\bar{X_n}-p}{\sqrt{\frac{pq}{n}}}$ has limiting distribution to $N(0,1)$. But, what if the denumerator is $\sigma^2=\frac{pq}{n}$ instead of $\sigma=\sqrt{\frac{pq}{n}}$? Here, we can't use the central limit theorem to find the limiting distribution of $\frac{\bar{X_n}-p}{\frac{pq}{n}}$, I guess. I've tried to find with limiting Moment-Generating Function, and what I found is very complicated. It says that the limiting MGF is equal to $e^{-t(\frac{p}{pq/n})}((1-p)+pe^{\frac{t}{pq}})^n$. So, I can't try to find the distribution with such MGF. So, is there any simpler method to find the limiting distribution of $\frac{\bar{X_n}-p}{\frac{pq}{n}}$? Thank you. 
 In the past days I began getting familiar with R (I come from MATLAB and Python). I wanted to try out the caret package (pretty awesome) and I keep getting the following error message when I try to train with LOOCV Now, at the beginning I thought "ok, I'm doing something wrong here". But then I used the code from : which returns the same error. The full code can be found at the following address . Did I mess up my installation of R? Am I doing something wrong? 
 i read about it but i didn't get the idea, and actually i didn't find many pages that talk about uniform noise with boosting, is it rare to happen or what? another question: i read in some pages that boosting overfit with large number of weak learners because it will get the train error to 0 because it trusted the data too much and be confidence and as a consequence it captured the noise and in other pages they say that in ONLY boosting as you increase the number of learners and be more confident you will increase the margin due to margin theory. which is correct? 
 For $n$ events, here is a $O(n\log(n))$ algorithm that uses simple, common tools. For simplicity, let's normalize the times so that the start of sales is time $0$ and the end is time $1$. The normalized times themselves are $$0 \le t_1 \le t_2\le \cdots \le t_n \le 1.$$ Let $n(t)$ be the cumulative distribution through time $t$: $$n(t) = \#\{i\,|\, t_i \le t\}.$$ The number of events in the interval $(t-dt, t]$ of length $dt \ge 0$ therefore is $$\#(t-dt, dt]=n(t) - n(t-dt).$$ Such intervals are wholly included within the range of times $[0,1]$ when $dt \le t \le 1$. I will interpret the "proportion" of intervals with more than $k$ events as the proportion of this time: $$p(dt,k) = \frac{1}{1-dt}\int_{dt}^1 \mathcal{I}(n(t) - n(t-dt) \ge k) dt.$$ ($\mathcal{I}$ is the binary indicator function.) For a given count $k \gt 0$ and proportion $1-\alpha$ (such as $\alpha=0.1$ for $90\%$ of the time), the question asks to find the smallest possible $dt$ for which $p(dt, k) \ge 1-\alpha.$ In particular, $dt$ is a zero of the (discontinuous but monotonic) function $$f(dt) = p(dt, k) - (1-\alpha).$$ A good root finder will efficiently discover such as $dt$. (If you wish, you may conduct a binary search instead.) Here are illustrative plots for a synthetic dataset with $n=500,000$ in the course of one month, comparable to the situation posited in the question. Code reproduced below found a solution for $k=1000$ purchases and $\alpha=0.25$; it is $2.72$ hours. The top figure plots $n(t) - n(t-dt)$ for $dt = 2.72\text{ hours}$ = $2.72/(30\times 24)$ $\approx 0.00377\text{ months}.$ The blue horizontal line is located at a count (height) of $k=1000$. Only the segments at times where the preceding width-$dt$ interval contains $k$ or more purchases are shown. Collectively they comprise exactly $1-\alpha=75\%$ of the month (starting from time $dt$ and ending at time $1$). This graph perhaps is even more useful than the answer, because it reveals potentially important details, such as the trend over time. The bottom graph illustrates this by plotting the total proportion of time against the time interval $dt$ for various values of $k$. The lower red and yellow curves plot values of $k$ largerthan $1000$ while the higher blue and purle curves plot values of $k$ smaller than $1000$. The black curve is the plot for $k=1000$. The value $1-\alpha=0.75$ appears as a black horizontal line. The interval where the black curve crosses this line is the solution, $2.72$ hours . It should now be visually evident how this problem amounts to finding a crossing (or zero) of a function. It takes $O(n\log(n))$ time to sort the events, which is the core calculation in computing $n(t)$. A binary search or decent root finder will take $O(\log(n))$ iterations, each one taking $O(\log(n))$ time for an evaluation (in principle: the implementation is a bit worse, but is simple to implement). Here is the code to produce the synthetic dataset (which oscillates daily and has a slight upward trend) and compute this solution. implements the function $f$ (but divides its values by $n$). It simply samples the unit interval at a large number of equally spaced points. is the built-in root finder to find the interval. It took only $25$ iterations to find a solution precise to $10^{-8}$, requiring less than $0.1$ seconds to complete. Because these data are so numerous, the solution cannot be appreciably greater than the smallest possible value of $dt$ that works (as the previous plots confirm). 
 I have data with two biological factors. The first factor has 2 levels (say, Alive vs. Dead) and the second factor has 80 levels (protein of interest). I'm only interested in whether the mean value differs between alive vs dead for each protein. That is, I'm interested in comparing Alive vs Dead for 80 pairwise comparisons. Since I'm not interested in main effects - I don't care whether the mean differs between Alive and Dead for all proteins, and I don't care if the mean differs between proteins - is it acceptable/valid to run an ANOVA + post-hoc? Or should I just run 80 t-tests followed by multiple comparison correction? 
 That sounds like an acceptable approach as long as you correct for multiple comparisons when running the 80 t-tests. However, a lot of times these conventions differ by area of research, so it might be worthwhile checking with someone in your area on how things are normally done. 
 I think I found a simple solution myself: Let $F(x) := \int_{x_0}^x f(y) dy$, then $F(x_i) = \int_{x_0}^{x_i} f(x) dx = \sum_{j=0}^{i-1} a_j$. From these values I can interpolate $F$ (e.g. by polynomial or splines) and then $f(x) = \frac{d}{dx}F(x)$ will have the desired properties. 
 Let's say I want to predict the value of Y using the value on my predictor X. X is correlated with Y with some strength $r$ (let's say 0.5). In order to correct for regression towards the mean I should take X's deviation form the mean (of the X distribution) divided by the standard deviation of that distribution. $$y=\frac{\bar{x}-x}\sigma*r_{xy}$$If X is 1 SD above the mean then I should predict Y to be: $1*0.5 = 0.5$, right? Now let's say I have another predictor, Z. It's in exactly the same situation as X, the same deviation and the same correlation, it yields a predicted Y-value of 0.5. How do I combine these two predictors? Taking the mean of the predicted values seems wrong, I mean the second predictor adds more information. If I had a hundred predictors all correlated with Y moderately and all 1 sd above the mean then I would assume that Y is 1 sd above the mean. Also, do I need to know to what degree my predictors are correlated? If so is there a way to work around that? 
 I have identified 17 studies for use in a meta-analysis. In order to make sure that I analyze the data correctly, I had a couple of questions regarding how to approach two of them. First, one study uses two different Cognitive-Behavioural techniques (one more cognitive and one more behavioural) and uses the same outcome measure to test the difference between groups. No statistical difference was demonstrated between the two and pre-post scores were used to measure effects without the use of control groups. I am interested in CBT techniques overall, so would it be best to pool the scores together? Also, two studies were conducted whereby the first one used a delayed-treatment group as a control for an RCT, comparing an outcome measure after the first group received treatment. Four years later, the data from those two groups were combined and then divided between 'depressed' and 'non depressed' and included follow-up data for up to two years after treatment. This means that there is one study that uses an RCT model with fewer participants in the treatment group or a pre-post design with more participants. I am particularly interested in the impact of self-esteem interventions on depression, so my gut instinct is to use the 'depressed' group from the latter study. Would that be reasonable or would it be better to use an RCT instead? For that matter, would it be best to perform a t-test on the effect sizes between the two? Thank you for any suggestions. 
 I'm reading Data mining with decision trees by Rokach, and i've got to a chapter about ensemble methods (using multiple classifiers) and this is where I can't wrap my head around this concept of Plurality Voting (PV) . class(x) = argmax ($\Sigma$ g($y_k$(x), $c_i$) ) Where g($y_k$(x), $c_i$) = { 1 if y = c; 0 if y=/= c} Most of the other metrics in the chapter weight classifiers, yet this seems to just essentially pick the one which has the highest chance of returning the class you want. Which seems useless? If thats the case how would this be helpful? 
 TL;DR: 3 variables score low on cronbach’s alpha but are very effective at predicting a DV when grouped as a latent variable. I am analyzing screenplays and I used software that would classify screenplays on a variety of psychological (e.g, aggressive, reward, risk, negative) dimensions. So a score of 0.10 for reward means that 10% of the meaningful language dealt with rewards. My hypothesis was that people who are what is commonly called "Type-A" will be more analytic - which is a dimension, my DV. So TYPEA was a latent variable. It was comprised of (1) REWARD, (2) RISK, (3) POWER. And using TYPEA to predict ANALYTIC yielded an R2 of 0.79. However, when when I ran cronbach’s alpha on those 3 variables to justify my TYPEA scale/latent variable, the scale reliability coefficient was just 0.25 Can somebody explain how this can be the case? 
 I have conducted an experiment on a sample that consisted of both male and female students to see if a specific emotion influences moral judgment. My research goal didn't involve any hypothesis about the effect of gender, but focused entirely on the effects of emotion induction. While analyzing my data I found that female and male participants in the control group differ significantly in moral judgment, i.e. they differ without the independent variable being applied. My question is how do I treat these groups (males and females) in my further analysis? Do I have to treat them separately, meaning we are talking about two different populations with respect to the variable being researched? Or is it acceptable to continue on with the analysis on females and males together, as planned? I have to add that both subgroups (males and females) react in the same way (or maybe "direction") to the independent variable, which is also the effect that is predicted by my research hypothesis. While this effect is stat. significant when I have males and females combined, it loses it's "power" on the sub-sample of females alone (and becomes marginal at p = 0,075; it is still significant in the group of males, though). A further issue that I have to take into consideration is that while I have a satisfying amount of participants in both the control (N=31) and the experimental group (N=32), having to "dissect" those further would result in quite a small amount of male groups (7 for C group, and 5 for E), which messes up the significance for some other important variables (that I don't really need to get into right now). All in all, I would like to treat these two groups together, but am hesitant to do so since they do differ in one (somewhat) important aspect. In writing the research I will of course dedicate a section to explain this difference, but I would like to circumvent the nag of having to analyse everything separately for males and females, especially since gender differences weren't an issue I planned on exploring (and since this would take a lot of precious space). This is more of an (unpredicted) side issue that could be explored in further research. I would like to hear your thoughts about the problem, and any evidence for and against grouping these two together. Thank you! 
 "not being white" is the base or reference point. beta(not being white)=0 by definition, i.e. odds ratio(not being white)=1. If odds ratio(white)=0.6 then you are expected to be 100%-60%=40% less likely to be a smoker as compared to someone who is not white. Good Luck! 
 I could be wrong but this is what I believe is happening: Normally the reliability between two observed variables limits how strongly these variables can be observed to correlate with each other. Since a lot of models are designed to estimate "true effect sizes" (or in other words the relationships between latent constructs) they correct for this. It's called correction for attenuation . When it's a correlation between two measures you divide that correlation by the square root of the product of the reliabilities of these two measures. $$r_{x'y'} = \frac{r_{xy}}{\sqrt{r_{xx}r_{yy}}}$$ Where $r_{xy}$ is the observed correlation and $r_{xx}$ and $r_{yy}$ are the reliabilities. I suspect your model is doing something similar (though not necessarily exactly this); weak correlations between measures with low reliabilities are turning into strong assumed/extrapolated "true explained variances". 
 The trick to the problem is to think big with causal and conceptual models. A causal model is, of course, interested in the measurable (and unmeasurable) things in an analysis and their relationship with an outcome of interest. A conceptual model builds on this slightly to incorporate things that might predict other factors in the analysis like your exposure of interest. With missing data analysis and causality, these models are expanded to include the missingness variable, which in a sense "predicts" the missing measure in question (using principles from weighting and standardization). Multiple imputation does not correct bias . It improves efficiency. When people say something like, "50% of X is missing" it is both the best situation to apply MI when the assumptions are met, and the worst time to apply MI when they are not. You are saying that 50% missingness is a problem because you're losing those data, not because they're "unlike" anyone else. Complete case analysis and MI provide unbiased estimates when you have NMAR and MAR. Basically adjustment for the right thing creates strata that interpolate what would have been answered by those who didn't answer, provided the modeling assumptions are correct. 
 Receptive field refers to the pixels in the input image which contribute to a feature in any layer of a network. Layer 1: Each point in the feature map comes from 3x3 pixels from input image, so RF is 3 Layer 3: Each point in the feature map comes from 3x3 patch of feature map from layer 1 which in turn map to 3x3 pixels of input image, if you map it back you realize it maps to 5x5 patch of the input image. Similarly for layer 5 you get a 7x7 patch. 
 I've done nested cross-validation with 10 outer runs and 10 inner runs to evaluate different classifiers. In the end I average the accuracies of the 10 outer cross-validation runs to get a final accuracy for one classifier. Now, I would like to do some statistical test among the classifiers to see if one classifier performs better than another. How can this be done in Matlab (or in general) based on the cross-validation accuracies? 
 A note: reading about Jensen's inequality, $\mathop{\mathbb{E}}\left[1/x \right] \geqslant 1/\mathop{\mathbb{E}}\left[x \right]$ in your case, so I'm not sure it's a useful quantity. Check this thread . Anyways, you have sample estimates of the mean and standard error of a variable and want to calculate the standard deviation of a transform of that variable. $$\begin{align}\overline{y}=&amp;\frac{1000000}{\overline{x}}\\SD(\overline y)=&amp;\left|\left|SD(\overline x)\cdot\frac{\partial{\overline y}}{\partial{\overline x}}\right|\right|=\\=&amp;1000000\space\cdot\space \frac{SD(\overline x)}{\overline x ^{2}}\end{align}$$ 
 I am working on a project where the explanatory variables include soil attributes, land use and land cover properties, stream flow and climate (precipitation, temperature etc) measurements recorded at multiple locations across a study area. I am proposing to use a random forest regression model to predict the response (an eco-indicator) at these locations. Soil attributes and land-use/land-cover data are available as single measurements at each location. Stream flow and climate data are available as a time series. From these time series data I was hoping to extract long-term representative values such as mean daily stream flow, average annual precipitation total, average annual temperature and so on. However during exploratory data analysis I noticed that at several locations in the study area the annual time series of stream flow and climate variables are exhibiting non-stationarity. I am aware that it is possible stationarize these series using techniques such as detrending or differencing. However, I would like to know if random forests can handle non stationary inputs (without stationarizing)? Are there any best practices when dealing with such data sets? Thanks! 
 Hi I need an example for pure deterministic time series which is not just dependent on time but on previous values, i.e Yt = βYt-1 + f(t). It should not have stochastic trends in it. I need this for academic purpose. 
 I am trying to understand the relationship between three variables, namely dependent, independent and a third variable I have some theoretical evidence that the third variable has mediation or moderation effect on the dependent variable. How can I test this relation? Also, is it possible that a variable functions as both mediator and moderator variable at the same time? 
 I'm not a demographer, but here's my stab at what it would be: Let $d_{a,t}$ be the mortality rate for women age $a$ in year $t$. Let $x_{a,b}$ be the fraction of women age $a$ that were born in year $b$ and still alive. Let $f_{a,t}$ be the fertility rate for women age $a$ in year $t$. One hundred percent of women born are alive, hence: $x_{0,b} = 1$. Then we have: $$ x_{a+1,b} = (1 - d_{a,a+b}) x_{a,b}$$ Let $l_{a,b}$ be live births for women age $a$ and born in year $b$. You might want to think about this more, but somewhere to start might be: $$ l_{a,b} = f_{a,a+b} x_{a+1,b} \quad \text{&lt;--- assumes all women that die don't give birth... probably not right}$$ Then lifetime cohort fertility would be: $$c_b = \sum_a l_{a,b}$$ 
 I am planning to develop a prognostic model that would identify a particular group of head neck cancer patients who will do better if chemotherapy is added to standard radiation therapy. The data for the patients in question is derived from a randomized controlled trial. I have developed a prognostic model following the methods proposed by Dr. Harrell and Dr. Steyerberg and several posts in Cross Validated. The model was derived in patients who received radiotherapy alone and on application on patients receiving chemotherapy along with radiation it shows that the outcomes are different. I have used the median value of the prognostic index to divide my patient population into a group with good prognosis and a group with poor prognosis. I can see that the addition of chemotherapy makes a large difference in patients with poor prognosis but not in patients with good prognosis. What I want to know however is how to determine the optimal cutoff for the prognostic index at which this difference becomes large enough that the physician will prefer to add chemotherapy (primarily as giving it in all patients is associated with the issues of excess toxicity with little additional benefit). One way I have thought of is deriving the Number needed to treat for each cutpoint - for each value of cutpoint calculate the benefit that addition of chemotherapy provides in the poor prognostic group and then calculate the NNT. Then use a predefined NNT threshold to determine the cutoff for the prognostic index. This, in turn, can be used to derive the optimal cutoff point on the nomogram that I will develop using the prognostic model. My question is if this approach is a sound and if a better approach is available? 
 I am training a neural network (details not important) where the target data is a vector of angles (between 0 and 2*pi). I am looking for advice on how to encode this data. Here is what I am currently trying (with limited success): 1) 1-of-C encoding: I bin the set up possible angles into 1000 or so discrete angles and then indicate a particular angle by putting a 1 at the relevant index. The problem with this, is that the network simply learns to output all 0's (since this is almost exactly correct). 2) Simple scaling: I scaled the networks output range ([0,1]) to [0,2*pi]. The problem here is that angles naturally have a circular topology (i.e. 0.0001 and 2*pi are actually right next to each other). With this type of encoding, that information is lost. Any suggestions would be appreciated! 
 Your problem doesn't relate to statistics. works because has the method to draw . But your arguments aren't specific class and don't have any information about Drug . So, connects all points by one line. You need to treat time, surv, and Drug information. Here is my example: 
 If a sub-sample of the survey sample, selected based on certain demographic characteristics of the data (e.g. age, race etc.), is used, which means the sub-sample might not be representative of the population anymore, is it better to not use original sampling survey weights provided or is it still better to use the survey weights calibrated for the original survey sample data? P.S. If I choose to generate my own weights based on a sub-sample data, what is the best methodology for this, and could someone please point me to the references that might be helpful? If the option of generating own weights isn't available, what would be the second best option? 
 I have a sample of companies classified into 9 groups by industries. In each industry percentage of firms with particular characteristic is different. Is there any way to test whether subsample (companies in some industry) have different proportion of firms having this characteristic then overall sample (here all firms in all industries)? So if I know that 45% manufacturing companies pay dividends while 50% of all companies (including manufacturing companies) pay dividends, how can I say whether this difference is significant? Edit: This question is not a duplicate of "How can I test the difference between a population proportion and sample proportion?", since I do not deal with population and I want to test difference in proportions between sample (all firms) and subsample (firms from some industry). 
 I have a dataset with over 1 million observations, and a few dozens of predictors. My target variable is binary (gold customer vs. not gold customer) and I wish to build a classifier for prediction. Out of the 1 million observations, only around 30,000 are gold customers. I read the file into a couple of computer software packages that struggled with the size. I tried making a prediction, using decision trees (CART), but got a poor result with specificity nearly perfect and sensitivity of less than 30%. I calculated the effect size (cohen's D) for each variable, and 3 variables showed higher values of D. I got the median difference for them, and it was higher than other variables too. Then I took all the gold customers and a random sample of 30,000 non gold customers, and the decision tree gave much better results, with both sensitivity and specificity being over 80%. I tried it on a different random sample, and same again. The results I am mentioning are the validation results, not the training. Can you please help me understand what it means ? Is it possible that a very unbalanced data creates a problem that damages prediction? Thanks! 
 I keep looking over the lecture and they just seem to skip how they get from the critical value to the p-value, but we need to be able to do this for the exam! So I've calculated the test statistic $(t) = 0.95669$ $df = 9$ and the critical value of t at $0.05 = 2.26$ (using a 2-tailed 0.05 t distribution table). Using R, I can see that the p-value is $0.36361$. How do I calculate this by hand? 
 You will typically be given a table in which you can look those values up. An example of such a table can be found on the wikipedia page for the t-distribution . More importantly you will probably be given the table that is printed at the end of the textbook that you are using in your course. Look at that and familiarize yourself with its layout. 
 Some background: I am working on a certain insurance claims related data-set to classify newly acquired customers as either claim or non-claim. The basic problem with the training set is the extremely large imbalance in claim and non-claim profiles, with the claims amounting to just ~ 0.26% of the training set. Also, most claims are concentrated largely towards the final few years (data is sorted by date). On applying Logistic Regression or even Random Forests, to train on 70% of the data, the test results were well below satisfactory - by which I mean to imply that nearly every new policy went into the non-claim class, while what is actually needed is correctly classified claims . My question is: I've been looking at alternatives and I came across this blog post . A particular line caught my attention GBM is better than rf_t. In the paper, the best classifier for two-class data sets was avNNet_t, with 83.0% accuracy But no clarification was given as to why that was. Can someone help me open this "blackbox"? Which model really works (in the case described above) and more importantly, why? Re-posting this from datascience.stackexchange . Perhaps you guys can help me figure this out. 
 I have a doubt on the meaning of a conditioning within a definition. In a book I've found the following definition of upper tolerance limit: $P(P(X&lt;\bar X+kS|\bar X, S)&gt;p)=1-\alpha$ where $X$ is a random variable, $\bar X$ is the sample mean of values taken from the distribution of $X$ and $S$ is their standard deviation. $p$ and $\alpha$ are numbers between 0 and 1. The question is: Why do we need the condition on $\bar X$ and $S$, and what does this conditioning mean in this context? I.e. What's wrong with just writing the definition as $P(P(X&lt;\bar X+kS)&gt;p)=1-\alpha$ 
 I am trying to fit several distributions to monotonically decreasing data, and pick the one that fit the best based on several criteria, e.g. mle estimate. I am able to do this by fitting a curve to the data, but this will sometimes result in a non-monotonically decreasing distribution (except for the exponential distribution, which generally give a bad fit). Are there any way to do this? I.e. like in the image bellow? 
 Well , I have the following problem.. Let $X_1,\cdots ,X_{2n}$ be iid $N(0,1)$ random variables. Define $$U_n=\left({X_1\over X_2}+{X_3\over X_4}+\cdots +{X_{2n-1}\over X_{2n}}\right)$$ $$V_n=X_1^2+X_2^2+\cdots+X_n^2$$ $$Z_n={U_n\over V_n}$$ Find limiting distribution of $Z_n$ My work : I have found out that $$f_{_{U_n}}(u)={n\over \pi(n^2+u^2)}$$ and $$V_n\sim \chi^2(n)$$ Now I am stuck . I know that to find $Z_n$ I first need the joint $pdf$ of $U_n\ \&amp;\ V_n$ and then apply convolution formula. But I am not being able to find the joint $pdf$. I know that had $U_n\ \&amp;\ V_n$ been independent, it would have been just the product of the two densities. But how to do that when the variables are dependent? 
 I'm assessing the accuracy of the prediction of my model using the RMSE on a new data set. Now the RMSE in itself doesn't give any indication of whether it is a good model since there is no threshold that says it is 'good'. My question is, would it make sense to calculate the RMSE for a null model with just the mean as a predictor and compare this to the RMSE of my model? Or should I compare the RMSE of the model on the 'train' data to the RMSE of the 'test' data? The model that I'm currently using is the best with all my available predictors based on BIC scores, but I'm trying to figure out how well the model actually does. I've also calculated the adj. R-square, which says that 20.7% of the variance is explained by my model, but I doubt whether this is a good accuracy measurement. 
 I’m currently working on a data-set where we used a diary-design. As I’ve got multiple measure points for each individual, I decided to use mixed models to analyze the dataset. Our participants filled out a baseline-questionnaire with the continuous variables baseA and baseB and the diaries with the continuous variables DailyC and DailyDV. Unfortunately, I’m not very familiar with mixed models and got stuck in the middle of my analysis. My data-set has the following structure (long format): Person Day baseA baseB DailyC DailyDV 1 1 2 3 3 2 1 2 2 3 4 3 1 3 2 3 2 2 2 1 4 5 3 4 2 2 4 5 5 1 2 3 4 5 1 4 My goal is to predict the dependent variable DailyDV from the two baseline variables base A and base B and the daily variable DailyC. I analyzed the data using a two-level Mixed Model with persons at level 2 and days at level 1. I'm not sure how to structure my data. For now I used the following structure: Level 2: Person BaseA BaseB Level 1: Day DailyC DailyDV I used the following function to analyze the data: Model 1 = lmer(DailyDV~1+preA+preB+DailyC+(1+DailyC|userid), data=mydata) Is it correct that I can't nest my baseline predictors in the person? Does this mean that I can’t specify random slopes for the Baseline-predictors baseA and baseB because they were measured at level 2? I only found mixed models where the predictors were measured multiple times as well. Or should I analyze my data using the following function: Model 2 = lmer(DailyDV~1+preA+preB+DailyC+(1+preA+preB+DailyC|userid), data=mydata) Thanks! 
 I have a data with a binary target variable and some predictors. I tried running a random forest model and failed. First of all, I found it hard to enter all predictors to the model. I did: randomForest(Target ~ P1 + P2 + P3, data = dataset), however, I have more than 3 predictors, and I do not know how to declare them all together. Can I say to R to take P1 to P30 ? Secondly, the model did not run. After some research I realized, that random forest cannot handle missing values. I wanted to ask you, how can I set a random forest if I have missing values in my data ? Can you please guide me, or give me a package that will actually work? (I tried the randomforest package). Thank you ! 
 I believe what you're looking for is a chi-squared test . From wikipedia: "The chi-squared test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories." 
 Although the brute force method I mentioned in the comments may work, there is an easier way which does not rely on Basu's theorem, and it also avoids integration of the joint density of transformed random variables. I leave some details out because the question is of homework-style. Write $$ Z_n = \frac{\frac{1}{n}U_n}{\frac{1}{n}V_n}. $$ Find the distribution of the numerator random variable using a density transform and consider how it depends on $n$ Note that all moments of $X_i$ exist and are finite. In particular, the variance of $X_i^2$ is finite. Thus, you can use Chebyshev's inequality to prove that $V_n/n \to 1$ in probability Prove that, in general, if for some sequence of random variables $W_n$ it holds that $W_n \overset{p}{\to} 1$, then $Y/W_n \overset{d}{\to}Y$. Note that you do not need independence for this. 
 I measured behavioural multivariate time series ( 3 variables) from 7 dyads performing an experimental task. For each dyad I have a multivariate time series (that is, I have not data from each person composing the dyad, but only data about dyad). I need to verify if there is a difference among the dyads. With difference I mean a difference in their behaviour (descriebd by the 3 variables). I am thinking about a repeated measures MANOVA: I have 3 DVs and the data are time series (that is repeated measures). Is it correct? If yes, how can do this in R or Matlab? If no, any other idea? thank you 
 From definitions, I feel that a variable can not simultaneously function as mediator and moderator. Let's try to investigate both effects: Mediaiton Mediation is a hypothesized causal chain in which one variable affects a second variable that, in turn, affects a third variable. The intervening variable, $M$, is the mediator. It mediates the relationship between a predictor, $X$, and an outcome, $Y$. Graphically, mediation can be depicted in the following way: $$X \longrightarrow M \longrightarrow Y$$ Testing mediation Inspect if $Y$ is influenced by $X$ with $\hat y = \beta_0 + \beta_1x$ See if $M$ is influenced by $X$ with $\hat m = \beta_0 + \beta_1x$ See if $Y$ is influenced by $M$ with $\hat y = \beta_0 + \beta_1m$ If one or more of these relationships are nonsignificant, researchers usually conclude that mediation is not possible or likely. Assuming the above steps yield significant results, Conduct a multiple regression to see the influence of $X$ and $M$ on $Y$ with $\hat y = \beta_0 + \beta_1x + \beta_1m$ If $X$ is no longer significant when $M$ is controlled, the finding supports full mediation. If X is significant, i.e. , both $X$ and $M$ both significantly predict $Y$, the finding indicates partial mediation. Testing moderation Let's assume a student's GPA (outcome variable) is affected not only by study-time (independent variable), but also by gender (moderating variable). In order to test moderation effect of gender, add to regression equation the interaction term between study-time and gender. $$GPA = \beta_0 + \beta_1x_{studytime} + \beta_2x_{gender} + \beta_3x_{studytime}x_{gender}$$ If $\beta_3$ is significant, there exists moderation. 
 Time is usually treated as a continuous variable but in some cases it is discrete. An example would be with a drug study and measurements are taken at 1, 2 and 3 hours. Am I right to think an appropriate test for this data would be a repeated measures ANOVA? I guess my question is what causes time to be treated as discrete in this case, and continuous in others? 
 I am currently writing a proposal and am confused on which statistics method I should use. I aim to examined the motivation of runners in participating a charity marathon and categorizing it. It will categorize runners into 8 motivations and categorize into 4 types of running groups. Hence, there is two categorization involved. I know the 8 motivations and 4 running groups are and I know the classifications to each runner. As this is a proposal, I do not have the data yet. I am trying to figure out what statistics method would actually be able to link the two together. I am still new to statistics and am confuse if do I use two different statistics or can I use one? I appreciate any feedback! Thank you very much. 
 Although time is theoretically continuous, and many mathematical models (like geometric distribution) model continuous time, in an empirical setting, events or states are measured at selected points in time. Because of this measurement structure, we often have to use discrete time models. The repeated measures ANOVA is an example of a model that can be used in a discrete time setting. A related example is panel regression models such as the fixed effect model. 
 I'm new with the U test and I have some doubts about the rejection of the null hypothesis with the U test with normal distribution approximation. H0:median1=median2 H1:median1$&lt;$median2 The Mann-Whitney U test is not of itself a test of the hypothesis of equality of population medians -- at least no more than it is a test of equality of population means. If accompanied by additional assumptions (that would make what it does test equivalent to a test of equality of populations) then it can function as such a test. I obtained Z=1,0313 Do I reject the null hypothesis if $Z&lt;Z_α$ or if $Z&gt;Z_α$ Possibly neither of those -- it depends on exactly what the statistic you have is computing. Note, however, that it would be very unusual indeed to have a test where the rejection region included a Z-score of 0 and this is not one of those rare cases. If your $U$ is the number of times an observation from sample 1 exceeds an observation from sample 2 (what I'd expect to be the most likely definition of $U$) then you'd reject for unusually small values of the statistic, so most likely you'd actually reject for $Z\leq -Z_α$. However, you should double check how $U$ has actually been defined for your situation (and strictly speaking, you should also double check how $Z$ is defined in terms of that $U$ as well). That would then confirm which direction the comparison should go (i.e. whether the rejection region is $Z\leq -Z_α$ or $Z\geq Z_α$). 
 I have conducted an experiment where I am interested in how well different models for directional hearing works. For each model a sound is presented to a test subject, and the test subject expresses its perceived direction of the sound. The angle deviation from the presented sound is my data, and it can be both positive and negative depending on if it is to the right or to the left of the sound. All the test subjects are tested for all the models. I am therefore expecting a mean of 0 degrees for all the models tested, but with different variances. A small variance should indicate a good model. Is there a statistical test for my needs? My supervisor suggests to use ANOVA, where I think Repeated Measures ANOVA should be used. However, as my understanding of ANOVA is so far, ANOVA compares the means of the different groups, while I am interested in the difference in variance. Help is much appreciated! Please tell me if I have expressed myself unclear, and I will try to elaborate. 
 I want to calculate standard error for estimated GMM coefficient. I just have the coefficient value. Kindly help. thanks saakshi 
 You should refrain from setting a pre-defined cutoff. Think instead of your work as providing information that a physician and patient can use to choose whether to add chemotherapy, based on the risks and benefits that apply in individual cases. Individuals may differ in the tradeoffs they will accept between potential prolongation of life with chemotherapy and the associated risks. Your data and analysis, with its prognostic index that presumably combines several prognostic variables, provide a well-defined place to start in assessing those tradeoffs. For each value of your prognostic index you have some measure of the expected added benefit of chemotherapy and of your confidence in that expected benefit. That measure may be NNT if you really are talking about cures, or months of additional survival if these are patients with advanced disease. Concentrate on that continuous measure of the expected benefit of adding chemotherapy, and on the reliability of your estimates. Two patients with the same prognostic index may have different risk-benefit tradeoffs, or clinical characteristics not included in your model might indicate whether a particular patient could tolerate chemotherapy. Such issues must be considered in addition to the predictions of your survival model. You seem to be pursuing a high-quality approach to this difficult clinical problem. Do not undercut your work by jumping to a cutoff that will lose detailed information that is needed for intelligent decision-making in individual cases. 
 Is there a way to get the document vectors of unseen and seen documents from Doc2Vec in the gensim 0.11.1 version? For example, suppose I trained the model on 1000 thousand - Can I get the doc vector for those 1000 docs? Is there a way to get document vectors of unseen documents composed from the same vocabulary? Thanks! 
 You can do this with the chi-squared test of independence for a 2-way contingency table . You simply collect information on which group a person is from and which motivation they have for a group of runners: Then you form a contingency table with the counts for every combination: Then run the chi-squared test. If $p&lt;\alpha$ the results are statistically significant, which means that certain groups seem to be more likely to have certain motivations than others than you would expect from chance alone. Here are a couple of extra notes: With so many groups and motivations ($32$ total combinations), you will need a lot of data. A basic rule of thumb is that you want the expected count (not necessarily the observed count) to be 5 in each cell. That means you would need at least $160$ runners to meet that criterion. However, $N=160$ doesn't necessarily mean you would have good statistical power (that is, your results could easily be non-significant even though the groups differ in motivation). Depending on how strong an association you want to be able to detect, you will probably need a lot of data. If you won't be able to get a lot of data, one thing you could try is to see if you can group the motivations (and/or the groups) into a smaller number of more overarching categories before collecting data. For example, maybe motivations are of a similar type, and so are ; then you would have a $4\times2$ table instead of a $4\times8$ table. Under the hopeful assumption that your results are significant, you may have trouble figuring out what they mean. Again, this is because you have so many groups and motivations. To enhance interpretation, you could compute the row-wise percentages for each cell (each row adds up to 100%). You could also try plotting the table, some options for plotting this type of data can be found here and here . 
 On a side note, is there any connnection between the entropy that occurs information theory and the entropy calculations in chemistry (thermodynamics) ? Yes, there is! You can see the work of Jaynes and many others following his work (such as here and here , for instance). But the main idea is that statistical mechanics (and other fields in science, also) can be viewed as the inference we do about the world . As a further reading I'd recommend Ariel Caticha's book on this topic. 
 In conducting a meta-analysis, I have pre-post data from single case designs. Unfortunately, I do not have access to the correlations between the pre and post scores. Is it correct that if I calculate the effect sizes using the independent measures, then the effect sizes will remain the same, but the standard error will increase and the confidence intervals will widen? 
 This is just an obfuscating way to express a simple idea: an upper tolerance limit is just an upper confidence limit for a percentile. The random variable $X$ is supposed to have some definite (but unknown) distribution $F$. The statistic $\bar X + k S$ (derived from a random sample from $F$) is intended to estimate the $p^\text{th}$ percentile of $X$, $F^{-1}(p)$. That is, we hope that $$F(\bar X + k S) = p.$$ Of course that won't be exactly true, because $\bar X + k S$ is random. An upper tolerance limit is a procedure intended not to underestimate $F^{-1}(p)$. The value of $\alpha$ is the chance you can tolerate of the procedure being wrong. In other words, you want it to overestimate its target at least $1-\alpha$ of the time. In many cases you can choose $k$ to assure this chance is exactly $1-\alpha$. Thus, $${\Pr}_F(F(\bar X + k S) \ge p) = 1-\alpha\tag{1}$$ is the defining criterion for a "$1-\alpha$ confidence upper tolerance limit of coverage $p$." In English we could read it as There is a $1-\alpha$ chance that the true percentile corresponding to the sample statistic $\bar X + k S$ will exceed $p$. If you wanted to make expression $(1)$ look more complicated, you could unravel it using the definition of $F$; to wit, $$F(z) = {\Pr}_F(X \le z)\tag{2}$$ for any real number $z$. Fixing $z = \bar X + kS$ for the moment and plugging it into $(2)$ would give $$F(\bar X + k S) = {\Pr}_F(X \le \bar X + k S).$$ That's a mighty ambiguous expression, though, because $F$ determines the distribution of both $X$ (thought of as an abstract random variable in $(2)$) as well as the distribution of $\bar X + k S$ (because that is determined by a random sample from $F$). To make it clear we are talking in this context only of $X$ as the random variable, with $\bar X + k S$ being treated as a constant, we might write $$F(\bar X + k S) = {\Pr}_F(X \le \bar X + k S\,|\, \bar X, S).$$ Plugging this into $(1)$ gives an expression like that in the book. (It differs only in that I have been more careful in distinguishing $\ge$ and $\gt$, but that is of no matter.) 
 Best I've several weather-station (200), placed across the country (508 municipalities). Now, I would like to prescribe the weather info, e.g. temperature, to each of the municipalities of that country. Thus my general idea is : Take the closest x (e.g. x = 5) weather stations and attach the average temperature to my municipality (according to the distance of the weather stations). e.g. something like this (α * temp_A + (1-α) * temp_B) --- whereby α = {dist_B / (dist_A + dist_B)} but then for every municipality and with multiple, x , weather-stations-info My main problem is : The closest weather-station, should have the highest weight ... like in my example with 2 weather-stations - because, I assume that the temperature won't change that much according to the e.g. 5th closest weather station. Also, I would like to have it, as smooth as posible, as in my example... (but then for 3 or more weather stations) Thus, I would be very thank full, if you can help me Kind regards 
 In the sklearn implementation of random forest and extra trees classifier a class_weights parameter is available I am wondering how this parameter effects the obtained variables importance 
 I need to model the health outcomes from an immuno oncologic treatment to which patients are responding differently depending on their immune condition, which is unknown to the investigator. I was thinking on tackling the heterogeneity of those patients by means of a Hidden Markov Model and the use of some prognostic signals associated with the immune condition of the patient with a certain probability. Would it be better to use a Discrete Event Simulation model instead? The idea would be to associate some characteristics for individual patients derived from the population and based on those prognostic signals. Please, let me know your thoughts. Thank you in advance. 
 At a critical stage in the development of a new aeroplane, a decision must be taken to continue or to abandon the project. The financial viability of the project can be measured by a parameter $\theta\in(0,1)$, the project being profitable if $\theta &gt; 1/2$. Data $x$ provide information about $\theta$. If $\theta &lt; 1/2$, the cost to the taxpayer of continuing the project is $(1/2 - \theta)$ [in units of \$ billion], whereas it is zero if $\theta &gt; 1/2$ (since the project will be privatised if profitable). If $\theta &gt; 1/2$, the cost of abandoning project is $(\theta - 1/2)$ (due to contractual arrangements for purchasing the aeroplane from French), whereas it is zero if $\theta &lt; 1/2$. Derive the Bayes decision rule in terms of posterior mean of $\theta$ given $x$. The Minister of Aviation has prior density $6\theta(1-\theta)$ for $\theta$ and the Prime Minister has prior density $4\theta^3$. The prototype aeroplane is subjected to trials, each independently having probability of $\theta$ of success, and the data $x$ consists of the total number of trials required for the first successful result to be obtained. For what values of $x$ will there be serious ministerial disagreement? For me, this question is somewhat difficult to understand. However I know Bayes rule, conditional probability, and beta distribution. I am just revising these topics again, but I would appreciate if any of you want to answer correctly this question. 
 I am currently researching the relationship between social media (IV) and body dissatisfaction (DV) in adolescent women, mediated by processes of Social Comparison (M). In my thesis, this mediation is hypothetically moderated by Thin-Ideal Internalization (V). For those of you who use SPSS, the model of my thesis corresponds with Model 14 as posed by Hayes. With all my statistical (significant!) results ready in SPSS (after using Hayes Process Macro) and ready to be reported, I can't figure out how to make a proper graph presenting the moderated mediation. Any tips on how I should make this graph? And which variable goes where in it? Thanks so much in advance! 
 Suppose that we transform the original predictors X to Yˆ by taking the predicted values under linear regression. Show that LDA using Yˆ is identical to using LDA in the original space. 
 I'm calculating a PCA using the function in and I have a short question about the prediction capabilities within this object. I'm using the data set: The code is as follows: When one wants to run a prediction based on this PCA, one can easily run: The question is, lets say I have a similar data set as but MINUS one column. Why can't I run the prediction using the set of columns I have? Are there any recommendations on how I should address this problem? 
 I have a set of features extracted from the same samples and I'm learning a kernel ridge regression. Now, especially for feature fusion, reducing the number of features before combining them seems like a good idea. However, I have some concerns about this: PCA will not necessarily give me the most relevant features Since I'm learning the regression function from the kernel, its dimensionality will not change after dimension reduction with PCA. ( Details of the dataset: I have $\sim7000$ samples (which are videos) and the features are visual descriptors like LBP-TOP, LPQ-TOP etc., which usually have $&gt;5000$ dimensions.) When I try to do it, I see that each feature has its k-fold cross validation error increased a little. So is it actually a good idea to reduce the original features? If so, are there better options than PCA? I also suspect if the kernel is too big, especially for the reduced data. I think having more samples is usually a good thing, but for the case of kernel regression, can it actually be increasing the complexity, hence overlearning occurs? I'm optimizing the regularization coefficient, not sure if it completely prevents overfitting though. So does it make sense to somehow reduce the kernel itself? Thanks for any help, 
 I am currently trying to write the methodology section for my thesis. I am doing a fixed regression and am not sure whether the notation I use is correct or complete nonsense :D It would be really cool, if someone could have a look and let me know which parts of it are still not correct... Thanks!! 
 The formula for the standard error of the difference between two correlated means $m1$ and $m2$ is $$s_{md} = \sqrt{s^2_{m1} + s^2_{m2} - 2 r_{12} s_{m1} s_{m2}}$$ So if $r_{12}$ is positive the third term under the radical $2 r_{12} s_{m1} s_{m2}$ is also positive and so the standard error is reduced. By all means try a grid of values for the correlation but assuming it to be zero is conservative. 
 Suppose there is a irreducible, reversible Markov chain with known states $1,\ldots,N$ and unknown transition matrix $T_{ij}$ and unknown limiting distribution $\pi_i$. I am able to repeatedly initialize the system in an arbitrary state $s$ then observe a random transition $s\rightarrow t$ where $t$ is a sample from $P(S_{k+1} | S_k=s)$ in the Markov chain. I would like an algorithm to choose the initial states $s_k$ dependent on the previous results $(s_1 \rightarrow t_1), \ldots, (s_{k-1} \rightarrow t_{k-1})$, to optimally estimate $\log \pi_2 - \log \pi_1$. A naive method would be to choose $s_k = t_{k-1}$ to obtain a sample of length $M$ from the Markov chain. Then, the estimator $\log \frac{1+\sum_k (s_k = 2)}{1+M} - \log \frac{1+\sum_k (s_k = 1)}{1+M}$ would converge to $\log \pi_2 - \log \pi_1$ as $M \rightarrow \infty$. This method is inefficient when either $\pi_1$ or $\pi_2$ is very small, which is the case I am interested in. Is there a better, or even optimal, scheme to choose $s_k$ dependent on $(s_1 \rightarrow t_1), \ldots, (s_{k-1} \rightarrow t_{k-1})$ to estimate $\log \pi_2 - \log \pi_1$ for a fixed number of observed transitions $M$? 
 How to interpret these results into a suitable explanation for a given regression model? 
 If you combine it with common sense (e.g. examining a plot) then it does mean something, but correlations' sensitivity to outliers make can make it deceiving in isolation, e.g. throwing a huge outlier into otherwise uncorrelated data will generate near perfect correlation. In R: 
 Macro (jsut above) stated the correct answer. Just some precision because I had the same question The condition of normality of the residuals is useful when residuals are also homoskedastic. The result is then that OLS has the smallest variance between all of the estimator (linear OR non-linear). The extended OLS assumptions: E(u|Xi = x) = 0 (Xi,Yi), i=1,…,n, are i.i.d Large outliers are rare u is homoskedastic u is distributed N(0,σ^2) if 1-5 verified, then OLS has the smallest variance between all of the estimator (linear OR non-linear) . if only 1-4 verified, then by Gauss-Markov, OLS is the best linear (only !) estimator (BLUE). Source : Stock and Watson , Econometrics + my course (EPFL, Econometrics) 
 I've defined an model in R with 2 fixed effects, 2 random intercepts and a random slope: I want to look at a specific contrast in this model and I do this with lsmeans: The output is the following: I'm interested in contrasts 0,1-1,1 and contrast 0,2-1,2 So I would report the following: There is a significant difference in condition A1: condition B1: M=0.24, 95%confidence interval=[-0.05, 0.53]; condition B2: M=0.37, 95%CI=[0.06, 0.68], t(2031.01)=-1.97, p=.049. There is no significant difference in condition A2: p=.79. This seems incorrect though (if I look at the confidence intervals). What am I doing wrong / is there a better way to compute the contrasts? 
 I have categorical looking-time data (looks to visually presented items A, B, C and D over a number of seconds), containing 0s and 1s. I want to compare groups (adults, children; n=~30 for each group) and conditions (type of auditory input). Looking times to A and B are both of theoretical interest, and so I do not have a single 'target'. I am an SPSS user. I am not at all well-versed in stats but have tried to use information from this site to inform my approach (also some psycholinguistics papers, e.g. Barr, 2008 and Jaeger, 2008 in Journal of Memory and Language). I believe my best option is a logistic regression -- perhaps multinomial, if I can find a good way to fit it in SPSS. My 0s and 1s are likely generated via the same mechanism as the rest of the data, and so I would perhaps start by trying a fractional logistic regression, as advised elsewhere here. However, for various non-statistical reasons I may have to go the less-favoured ANOVA route with logit-transformed data. That means choosing a value to add to my 0s and subtract from my 1s. I recognize that choosing a minimally disruptive value is crucial, but other than 'symmetry' on a plot I don't know how to pick values to try, conceptualize the different ways in which 'disruption' might show up, or measure the disruption. What criteria should I use to compare the results from trying out different ways of mapping 0s and 1s to a particular value? The first comment on this question suggests 'At a minimum you would need to do some sensitivity analysis to see the effects of the constants you used' - how would I go about sensitivity analysis? Is there anything in the literature that specifically supports this approach : setting epsilon to half of the smallest non-zero value and replacing all 0 values with epsilon and all 1 values with 1-epsilon? Should I calculate the logit using the natural log or base 10? 
 We know that a linear regression Y on X doesn't imply causation X-&gt;Y. It just means that Y is dependant on X in this model. For example, in a general case, I cannot simply run a regression of Test score on class size and conclude that class size is a cause of test score. Now, assuming we have a randomized controlled experiment. This means: all subjects ideally follow the protocol there is a controlled group and a treatment group. subjects are randomly assigned the experiment is measuring if a treatment has an effect Then, we run a regression for example test score on class size. Can we say that class size is a cause of test score ? And what do you think of this sentence ? A causal effect is defined to be the effect measured in an ideal randomized controlled experiment. 
 By default, assesses the null hypothesis of no association between the response and all regressor variables using a quadratic correlation test statistic. Depending on whether the involved variables are numeric, categorical, or survival, different types of scores are used in these correlation tests. See also: What is the test statistics used for a conditional inference regression tree? Alternatively, a maximally-selected statistic can be used which makes a difference if one of the score functions used are multivariate (e.g., for multi-level categorical variables). And, by default, the resulting p-values are Bonferroni-corrected for multiple testing across the number of regressor variables. Alternatively, the Bonferroni correction can be omitted () or the test statistics themselves rather than their p-values can be used (). In most situations there should be no need to modify the defaults. For more details about the algorithm see: as well as the original manuscript: Torsten Hothorn, Kurt Hornik, Achim Zeileis (2006). Unbiased Recursive Partitioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics , 15 (3), 651–674. doi:10.1198/106186006X133933 For the underlying conditional inference techniques: as well as the original manuscript: Torsten Hothorn, Kurt Hornik, Mark A. Van de Wiel, Achim Zeileis (2006). A Lego System for Conditional Inference. The American Statistician , 60 (3), 257–263. doi:10.1198/000313006X118430 
 Most of the simple machine learning models have high bias, and low variance. Under what circumstance, if any, can a simple model have high variance too. Follow up question: Is high variance a analytical way to figure out if you overfitted your model? 
 I was implementing a super duper simple network and I wasn't able to have the forward pass match for both the wrapper and the hardcoded version of the network: Does someone know why this might be? It seems very mysterious to me. It seems it only happens when the Batch Normalization (BN) layer is included. Is there an additional randomization that I am not aware of that is used in the BN layer? check it out yourself: 
 The trick is to write down your likelihood exactly. There are many approaches to EM normal mixture estimation. Is the number in each group known? (hypergeometric likelihood) or will you potentially have everyone in one group (binomial likelihood)? Do the 2 normal mixture variates have common variance $\sigma^2$ or are there two separate variances? Using the law of total variance: $$\mbox{var}(\bar{x}_1) = \mbox{var}(E(\bar{x}_1|p)) + E(\mbox{var}(\bar{x}_1|p))$$ Where the $\bar{x}_1$ denotes the mean of the first normal mixture variate. Expressions for the two terms on the RHS depend on the likelihood! 
 I have a study looking at a particular blood measure after performing 3 activities (sitting (control), walking (intervention 1) and cycling (intervention 2). There will be a minimum wash out period between each trial and it will be randomised (15 participants). The primary aim is to look at whether the interventions have effect on this blood measure compared to control. Secondary aim will look at whether the interventions differ. I am struggling to come to the conclusion of which statistical test would be appropriate. I may also need to consider covariates such as gender and age. 
 As I'm reading from wikipedia, and this Cross Validated question: Gradient for hinge loss multiclass , the gradient value for a training feature set is somewhat straightforward. However if I'm interpreting this correctly, this only gives a gradient value for the weight of the 'true' class. How do I find the gradients for the other weights? That is, if my total weight vector is [W1, W2, ... Wy... Wk], where y is the class of the training sample then what are the gradient/loss values for every weight that isn't Wy? 
 As far as I know, PDFs always have positive co-domains, but here is an example of one that outputs negative numbers: Any idea what's going on? And what's the solution? Here is me trying to do the same but with my own data : 
 I fitted an ARMA(1,1)-GARCH(1,1) model to a time series with the following code: As a next step I would like to extract the model parameters (in order to use them for one-step ahead predictions). For the AR and MA parameter I used This worked fine for those two parameters but I got an error for the ARCH and GARCH parameters when I used Does anyone know what I have to use instead to store those parameters as a vector? Thank you very much. 
 Apologies if this has been asked before, I have done a lot of googling but finding it hard to wrap my head around this particular problem. Also apologies if I use the wrong terminology for things, I am self learning statistics and I am just starting. I am using R to find a logistical model for a data set consisting of 314 rows. I am using 3 variables to find a "risk" value. The code I wrote so far is as follows. The estimated coefficients given were I can calculate my risk value as follows In my data 57 out of 314 rows are equal to 1. I now want to calculate an optimum threshold value for "risk" which will reduce my 1 to 0 ratio so that for every 10 rows I should only have 1 row equal to 1. Is R capable of doing this sort of thing, if so, how? I looked into 2 functions tfromx and optim.thresh already, but I am not sure if these are what I need or how I can apply them. Please don't hesitate to ask more questions, I hope I made sense. 
 Here $x \sim N(0,1)$. I realize that the expectation won't be defined for $\alpha$ when the integral goes to infinity. I can't seem to figure out which specific values of $\alpha$ would cause this. My intuition is that it would have to be negative values of alpha, as I have already derived the values of \begin{align} E[x^{2n+1}] &amp;= 0 \\[5pt] E[x^{2n}] &amp;= \frac{2n!}{n!2^n} \end{align} 
 I have read that to perform a score fusion from two different classifiers on two different datasets then the score must be normalised. I understand I can use z-score normalisation / max-min normalisation for this. I am confused as to how to apply it? Do we apply it to the entire dataset (training + testing) Do we apply it to the training and testing partitions separately? Do we apply it to each input value or each score? One article I read online it appeared they used the average and standard deviation of the training data and applied a z-score normalisation to the next sample input. However this, to me, seems odd because calculating the mean/standard deviation relies on a premise we have the max/min values we will ever have and it's possible the input value will be higher/lower. Can someone please make sense of this? Thank you. 
 Alot of machine learning people when they talk about model selection: 1-they say use X algorithm if you expect that the data is linearly separable! how can i know that? there alot of features it's NOT data with 1 or 2 features it's alot 2-they say use X algorithm if the data has Y distribution! how can i know the distribution of my data ,again there alot of features 3-they say do feature engineering when you expect the features to be linearly dependent with y 4- Are features independent, how can i check for the independency ?which one ,there are linear dependency and non-linear independency ? 
 Suppose we have continuous stream of data which length we cannot predict and discretize. Is there a type of neural network that can hold this stream and makes output based on the information stored in this stream? 
 I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is: Investigate which of the variables are the most significant ones to determine my output. After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability... I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression? Thanks in advance! 
 Yes, I suggest running a random forest classification model via . Compare its results with a gradient boosting model and a neural network and pick which one is best. Random Forest is really good for data of this size. You can also explore dozens of alternatives in and try an extreme gradient boosting model but they will probably be significantly slower. When you run your prediction (using ) you'll be given both the discreet classification predictions as well as the probability of each row falling into each class of your mutliclass dependent variable. Any of the 180 following models in that are "classification" or "dual use" should work for you as viable alternatives, though again I favor random forest. More over, a better specified multinomial logit run with or may perform better than your prior results. With and most other packages in R (asides from ) just make sure you specify in your predict statement. As a rule of thumb, when you have this many rows of data gradient descent or advanced optimization solver models will usually perform better than models using normal equations to minimize their cost function. 's 180 models: https://topepo.github.io/caret/modelList.html Here's how to do eXtreme gradient boosting in R: 
 The results are negative because returns the log density. From the help message: 
 It comes down to how well your experiment isolates the hypothesized causal factor. As you mention, the general procedure is to randomly assign subjects to control or treatment groups. The purpose of doing this is to be able to make an all-else-equal comparison between groups. Ideally, the factor of interest (e.g. class size in your example) is the only thing that differs between the two groups. There are various circumstances where this can break down. For example, imagine you're testing a drug to see whether it's effective. You randomly assign patients to the control or treatment groups. The treatment group receives the drug. Say the treatment group has a better outcome than the control group. Can we say the drug caused the caused the improved outcome? Not necessarily. For example, say the control group received nothing. It could be that the act of receiving treatment caused the improved outcome, but not the drug itself. This is why the control group in such studies must receive a placebo (fake treatment). Furthermore, they must not know that they're in the control group, because simply knowing whether you're receiving a treatment or not can affect the outcome. This why blind studies (where subject don't know which group they're in) are important. It's also import to conduct such studies in a double-blind fashion, where the experimenters are also ignorant about whether they're administering a treatment or placebo. Otherwise, they might be biased to treat patients differently (for example, they might treat patients in the treatment group better than those in the control group, because they have a vested interest in the experiment having a positive result). Of course, it's not always possible to conduct placebo-controlled, double-blind experiments. In the classroom example, there's no way that anybody could be ignorant of the class size. Placebos and blinding are just a couple examples of a more general principle: that causal inference can be thrown off when other causal factors 'leak' into the experiment (meaning that the control and treatment groups have differences other than the factor of interest, and these differences affect the outcome). The way to combat this is to design good controls. 
 Because the distribution is symmetric about zero, with PDF $\exp(-x^2/2)/\sqrt{2\pi}$, the expectation is just twice the integral over the positive numbers $$\mathbb{E}(|x|^\alpha) = 2 \frac{1}{\sqrt{2\pi}}\int_0^\infty x^\alpha \exp(-x^2/2)\, \text{d}x.$$ The one-to-one transformation $y = x^2/2$ entails $\text{d}y=x\, \text{d}x$ whence $$\text{d}x=\frac{\text{d}y}{x} = \frac{\text{d}y}{\sqrt{2y}},$$ converting the integral into $$\frac{2}{\sqrt{2\pi}}\int_0^\infty (2y)^{\alpha/2}e^{-y} \frac{\text{d}y}{\sqrt{2 y}}=\frac{2^{\alpha/2}}{\sqrt{\pi}}\int_0^\infty y^{(1+\alpha)/2-1}e^{-y}\text{d}y=\frac{2^{\alpha/2}}{\sqrt{\pi}}\Gamma\left(\frac{1+\alpha}{2}\right)$$ provided $\mathfrak{R}(\alpha) \gt -1$. When the real part of $\alpha$ is $-1$ or less, the integrand between $0$ to $1$ is bounded below by $y^{-1}e^{-1/2}$. This integral diverges logarithmically at $0$, showing the expectation does not exist in such cases. It is noteworthy that negative real values of $\alpha$ between $-1$ and $0$ still lead to finite expectations. 
 i have created a SEM model and want to know while reporting which part should be treated as Structural model and which portion as and outer / measurement model . Whether i should take portion A or portion B thank you 
 Note: This is not a homework. Additionally, I have some level of good knowledge of statistics but getting the work done through R (or any other) has been a problem. But, I want to stay with R. I have the TPM (transcripts per million) values generated for my RNAseq data. My overall goal is to identify genes that show allele specific expression differences. In the table below the TPM values (gene.erc.M and gene.erc.S) are for two different haplotypes (flagged as M and S) and the total TPM for that locus (gene or id) is T, so reads for M+S = reads for T. See the data table and description below for more details. Here is my data structure: There are about 16000 gene_id (after filtering), thats why there is going to be a some good amount of over dispersion. Description: - is the gene name - represent the total number of transcripts from the - This values is different than RPKM, FPKM, so I don't have to worry about normalization any more. - represent the total number of transcripts from the corresponding but with haplotype M. - same as above but for haplotype S. This is what I want to do specifially: Find the appropriate distribution of my data (for M, S and T); poisson vs. negative binomial? Generally data from RNAseq have over-dispersion so negative binomial regression might be appropriate, but I want to check for dispersion and distribution for the data either way? If any transformation is needed? - I have tried some log transformation but I am not getting anything useful. My main goal is to see if there is significant expression differences between two haplotypes (M vs. S) for the same gene ID. - I tried looking for similar examples around but not finding anythig useful. DeSeq, edgeR mainly focus on variation between samples and condition. I tried to apply those but looks like I am having brain freeze in here. 
 I have to implement this formula: $K(x) = \int_{0}^{0.5}([q_{\theta}(x) - q_{0.5}(x)]d{\theta}$ where $q_{\theta}(x)$´s are the conditional quantiles in some $\theta$. using a range of $\theta = [0.99; 0.975; 0.95; 0.90; 0.85; 0.80; 0.75]$ I need some direction, as i dont have ability in using R. : My data: I thought in doing this, for $\theta = .90$: And then, i calculate: $K(x) = \int_{0}^{0.5}([q_{\theta}(x) - q_{0.5}(x)]d{\theta}$ So i suppose to have a vector of 100 observations, right? I dont know how to build this integral function. Any sugestion? Thanks. 
 Any neural network trained using some variant of online learning (e.g. stochastic gradient descent) will be able to do this. If the stream contains independent samples, then a feedforward network would work. If the stream contains sequence data with time dependencies that you want to model, then a recurrent network would be the tool of choice (trained using a method like backpropagation through time). 
 How can we do weighted ridge regression in R? In MASS package in R, I can do weighted linear regression by passing a weight parameter to . It can be seen that the model with weights is different from the one without weights. However, when I try to replicate the same with , model generated with and without weights are same. Edit 1: In linear model, I can calculate stderr of coefficients as follows: 
 I am using STATA to analyze count data (weekly disease counts), and I am trying to pick the best test between Poisson, negative binomial and zero-inflated Poisson / negative binomial but I am not sure how to do this. My outcome variable has a variance that is much larger than the mean and contains a large number of zeros. I've read a couple of articles mentioning that I should look at the predicted / actual residuals, but I don't know how to do it or what I should be looking for to pick the right test. 
 Given data points where each entry of a point represents a value of a feature, and we need to use this data to model a binary Logistic regression model. Is there a way to determine which data points are important for the model? As an inspiration, look at the following data: We have 100 point where each point is composed of two values, i.e. two features. 99 of the points were the given 0 as a label where 1 of the points was given 1 as a label. Clearly, the point which have the label of 1 is crucial for the model so we need to take this point seriously so that the model will guarantee better results. Please advise and Thanks in advance. 
 It's not absolutely clear to me what B involves. But the structural model is the relationships between the latent variables, and the measurement model is the relationships between the measuremed variables and the latent variables. So B is the closest to the structural model, but it l0oks like it includes csome measurement parts. However, unless the measurement arrows are in the wrong direction, that's a very strange model. (Also, does commitment have any indicators? 
 I've been studying a manuscript from the chemistry literature (not mine) that resorted to a trick to obtain convergence of a non-linear model fitted to experimental data. They wanted to estimate a total of 5 parameters (a1-a5), but the model wouldn't converge. So they estimated a1-a4 (with a5=0), then held a3 constant at the value just obtained while estimating a1-a2 and a4-a5. Then they re-estimated a1-a4 with a5 constant at the value just obtained. And once again fitted a1-a2 and a4-a5. They used this process using the same model for 9 different data sets and reported the averages and standard deviations for the 3 parameters of real interest (a3,a4, and a5), which should be the same regardless of dataset. The data analysis tool they are using doesn't permit all 9 datasets to be combined and analyzed together. In any case, the results are supported by independent evidence that the parameters are consistent with values of parameters for related reactions. As I understand it, there can be 2 causes for non-convergence: poor initial guesses and under-constrained model (data truly don’t allow all parameters to be estimated). The process they use should help get past bad initial guesses, but not an under-constrained model. If bad initial guesses were the cause, I’d think once the iterative process “converges” they should have been able to use those estimates as initial guesses and obtained convergence with all parameters estimated simultaneously. Would that be considered a legitimate approach to handling lack of convergence? I would think that without the last step, the iterative process described would not be recommended. In the case of an under-constrained model, convergence would never be obtained with a single curve, but there may have been enough information in all 9 curves together to make it work. I would have liked to have seen a simultaneous fit to all the curves, but in the absence of the proper tool, it seems like what they did was decent. So, what I'm asking is whether the conclusions that I reached about the aforementioned data analysis are correct. Responses will be appreciated. 
 You can start by fitting a zero-inflated poisson model and a zero inflated negative binomial model. The latter is particularly suited if your outcome data are overdispersed as well as zero inflated. In Stata you can use and , for example: You can compare models using AIC or BIC and by looking at their predictions. 
 To my (very basic) understanding, Naive Bayes estimates probabilities based on the class frequencies of each feature in the training data. But how does it calculate the frequency of continuous variables? And when doing prediction, how does it classify a new observation that may not have the same values of any observation in the training set? Does it use some sort of distance measure or find the 1NN? 
 Your suggestion about using a null model is similar to $R^2$. $R^2$ is defined as $1-MSE/V$, where $MSE$ is the model's mean squared error and $V$ is the variance of the observed output. You can think of the variance as the mean squared error of a null model that always gives the mean as its predicted output. Even here, the question is: how much better can you do? This is very hard to answer. The reason is that it's hard to know whether the error reflects variation in the output that's fundamentally unpredictable from the input (e.g. 'noise', but could be something else), or whether additional structure is present that the model has simply failed to capture. Sometimes looking at the residuals can give a hint. Under some circumstances, it's possible to estimate the 'noise' level. For example, if you have many repeated trials where inputs are identical, you can measure variability of the output for equal inputs. This gives a bound on the maximum possible performance. You would typically encounter this situation in the context of controlled experiments. Or, you may be able to do something similar if you have access to a known 'correct model' (e.g. in a theoretical setting, or if you're modeling a well understood physical system). Otherwise, it's hard to know whether there's a better model out there. Looking at the training vs. test error can give you some idea about the extent to which your model is overfitting (the expected training error would be lower than the expected test error). There can be variability here when using a small number of samples and/or few repetitions. A gap between training and test error isn't a problem per se, but a large gap might signal a problem. Even so...one model that overfits might still have better generalization performance than another model that doesn't. Instead of asking how good your model is, you can also ask how bad it is. You could use a significance testing approach to see whether your prediction is better than 'chance'. For example, you might compare the test error on real data to the test error on permuted data (where relationships between the input/output have been destroyed, and any apparent performance is due to sampling variability or overfitting). 
 For a given set of parameters I get a prediction from a model. Can I bootstrap my data for the given parameters and take the variance of my estimates for the given parameter values to approximate the variance associated with the original prediction. For example, Given the parameters $$[ x_1,x_2,…, x_n]$$ I get prediction $$F([x_1,x_2,…, x_n])=y$$ I bootstrap my data 10,000 times and get 10,000 predictions with parameters $$[x_1,x_2,…, x_n]$$ $$F_1([x_1,x_2,…, x_n]),…,F_{10000}([x_1,x_2,…, x_n])$$ Will the variance of the distribution of 10,000 bootstrap predictions approximate the variance of $$F([x_1,x_2,…, x_n])=y$$ 
 There are many ways to perform NBC classification. A common technique in NBC is to recode the feature (variable) values into quartiles, such that values less than the 25th percentile are assigned a 1, 25th to 50th a 2, 50th to 75th a 3 and greater than the 75th percentile a 4. Thus a single object will deposit one count in bin Q1, Q2, Q3, or Q4. Calculations are merely done on these categorical bins. Bin counts (probabilities) are then based on the number of samples whose variable values fall within a given bin. For example, if a set of objects have very high values for feature X1, then this will result in a lot of bin counts in the bin for Q4 of X1. On the other hand, if another set of objects has low values for feature X1, then those objects will deposit a lot of counts in the bin for Q1 of feature X1. It's actually not a really clever calculation, it's rather a way of discretizing continuous values to discrete, and exploitation thereafter. The Gini index and information gain can be easily calculated after discretization to determine which features are the most informative, i.e., max(Gini). Be advised, however, that there are many ways to perform NBC, and many are quite different from one another. So you just need to state which one you implemented in a talk or paper. 
 I am an undergraduate student studying mathematics and microbiology. I recently got a research project to study the evolution of viruses from the computational perspective, particularly from machine learning, where I would like to use it to predict the future viral strains. Unfortunately, I am very new to machine learning, so I would like to seek your advice on some books to start learning machine learning; it would be great if you can recommend one introductory book on ML and one comprehensive, detailed reference on ML. My plan is to learn ML with those books and conducting my research (I learn best when I have a specific project/problem to pursue). My preferred programming languages are R and C/C++. My background: real analysis, number theory, point-set topology, axiomatic set theory, and theoretical linear algebra. I am currently learning representation theory and algebraic topology. I have not taken a mathematical statistics course, but I am willing to learn the necessary concepts alongside with studying the ML. Unfortunately, I have not found any ML book that focuses on biological applications, so I welcome any recommendations you have on an elementary book and a comprehensive, detailed reference on ML. 
 My target is to cluster the spatial area based on the location(X,Y) and pollutant concentration(Z). So there would be three different attributes along the spatial area(n_sample = grid point) I have tried two method: &amp; . But the results are so different. Here is my figure. My research area is not in square shape but in the interior of administrative division. Due to the difference between two methods, I presented my doubt here: Which method fit my target(spatial clustering by three attributes which are the geographical location and the concentration there)best?(Any method beside these two are OK) Directing to the best method, what parameters setting is recommended? 
 Since you're in biology, the book Data Mining, Multimedia, Soft Computing and Bioinformatics (Wiley, New York(NY)) by Mitra and Acharya is wonderful. Their chapter on classification will empower you like no other book! -- since it starts with an explanation of information gain, entropy, Gini index, and then builds a classifier that discretizes feature values and shows all intermediate calculations. (there's actually no other book as informative). 
 It's really a difference between similarity (cosine, or correlation) and dissimilarity (Euclidean distance) coefficients. The spectral will assign cluster colors next to one another which are closer in terms of distance. The K-means looks like its assigning cluster colors based on correlation between regions over the three feature values. Spectral reveals similar levels of features values while the K-means reveals clusters with the same correlation (not level of feature values). 
 This should do the trick: 
 This question considers Poisson regression with a log-linear model for the mean. Observations on the response variable y and the explanatory variable x are given below: X: 10 3 17 10 20 24 27 29 30 33 Y: 20 18 30 26 36 40 35 42 40 40 Observations on the response variable y are assumed independent. For a given x, each y observation is assumed to come from a Poisson distribution with mean u=b1*exp(b2*x) Note the logarithm of this relationship is a linear function of x. (1) Compute the maximum likelihood estimates of ? ? 1 2 and . (You need to provide the details of this maximum likelihood computation, including the loglikelihood function, its derivatives and the algorithm you used for finding the maximum likelihood 
 I've seen that there have been several advancements made since the original k-medoids clustering algorithm by Kaufman &amp; Rousseeuw (1990) that is used by in the package. For example: Van der Laan, M., Pollard, K., &amp; Bryan, J. (2003). A new partitioning around medoids algorithm. Journal of Statistical Computation and Simulation, 73(8), 575-584. Park, H. S., &amp; Jun, C. H. (2009). A simple and fast algorithm for K-medoids clustering. Expert Systems with Applications, 36(2), 3336-3341. Have any of these newer methods been implemented in any current R packages? If not, is there a reason k-medoids methods have lagged behind, given the popularity of the k-means family of algorithms? 
 These are all incorrect. The null hypothesis is a paradox and thus something no mechanical contrivance or "computer" can understand ("the machine must talk to something" meaning literally the Number 1) Once your machine starts with 1 it is now ready for any inputs you wish to assign it as no machine can talk to itself (null or "zero")...though said machine may be confused by the fact that you do talk to yourself, speak extemporaneously, hypothesize, etc...and in so doing may register an "input error" actually The function of null is to use this paradox to turn off the machine "computationally" (hitting the reset button.) This is an "error" since the computer cannot in any way comprehend "null" thus causing the entire mechanical contrivance to turn itself off and then back on in order to run "self diagnostics" (the command codes for "all is nominal") If the code matches the parameters set forth in the encoding (the expected numbers=the actual numbers) then the machine will "run" in the sense that you can start having it count things or "be programmed." (Coding) The "computer" will start "counting time" which is odd since there is no "accounting" for time actually. (No machine "counts time" literally meaning 0,1,2,3,4... So what is the purpose of having a machine "count time"? Certainly not make music.... 
 Totally new to R language and have studied the basics of R for past 2 days. Got a recently to convert binary data into text. Binary data will be stored in a file, but how can I convert that binary into text. Any idea will be appreciated? 
 If you like Python or Java, you may consider the book "Numeric Computation and Statistical Data Analysis on the Java Platform" by Chekanov It describes the DMelt ( ) platform. There is dedicated section in this book on machine learning. 
 I know that when the model is a simultaneous equation, you can't always use OLS to estimate the parameters. You will get biased estimators and most important inconsistent. But if all the variables are endogenous, then instead of a simultaneous equation model you'll have a VAR model, and I read that in that case you can use OLS to estimate each equation alone.... I don't understand why you can do this without getting incompatible results. I thought the best solution would also be solving for all the equations at the same time. Why this is not the case? 
 My preference goes to Python, and perhaps, Java. First, they are real programming languages. Second, they are the most popular languages (TIOBE Index). You can also convert between these languages using several scripting languages. In the past I use DMelt platform to perform statistical calculations, and I was very impressed by the graphics in 2D and 3D, which can be easily achieved for professional papers. R package did not impress me with the graphics. 
 I am dealing with a situation where I need to use a joint matrix of approximately gaussian distributed data (limits - Inf to + Inf) and beta-distributed data (both produced using different experimental methods for the same samples) to generate clusters. So far I have tried clustering a Random Forest proximity matrix , and the thing that works best is an unconventional approach where I do not model the proximity matrix explicitly as a dissimilarity. I am looking for advice on whether transforming everything to approximately Gaussian using either Z-scores or a logit transform of the beta-distributed variables is a good way of creating a matrix suitable for direct clustering. 
 I have done a one-way MANOVA with 2 groups of 15 sample units each and 10 variables measured on each sampling unit, followed by separate ANOVAs and a CDA. I find that the Univariate F statistics for each variable have the same rank order as the structure coefficients on the first CV. Why is that? 
 I want to do a clustering task with my data to find some interesting patterns. I have two groups/vectors of features V1 and V2 which are fundamentally different and are supposed to measure different concepts. Another issue is that these vectors are high-dimensional. V1 has more than 300 variables and V2 around 50. Therefore, it is very likely that we have a lot of noise in each vector. Based on this, I want to accomplish the following task: I want to do a clustering in subspaces of V1 and V2 to get rid of unnecessary and noisy variables. In each subspace I want at least on dimension form each of the two vectors because this way I can make comparisons and find some potential correlations between the elements of the two vectors. Note that if I do a PCA or feature selection globally, I might miss some interesting local clusters or patterns. In other words, V1 and V2 would be two clustering bases. Any ideas how I can approach this problem? I do not insist on using clustering. Thanks 
 I've made a bar plot and I calculated confidence intervals by hand, and I want to be sure that it's interpretable. That is, do they have the same interpretation that they would if the point estimate was a mean or a logit? I understand that CIs for odds ratios and probabilities are inherently non-symmetric, but does this affect their interpretation? For example, if I had two means and 83% of the total length of their CIs were non-overlapping, then the means are significantly different at .05. Would the same be true of CIs for predicted probabilities? 
